[
    {
        "id": "068fc4e5-d40c-4788-a5a7-bad858cbd7c7",
        "table_caption": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.",
        "table_column_names": [
            "Feature",
            "LR P",
            "LR R",
            "LR F1",
            "SVM P",
            "SVM R",
            "SVM F1",
            "ANN P",
            "ANN R",
            "ANN F1"
        ],
        "table_content_values": [
            [
                "+BoW",
                "0.93",
                "0.91",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+BoC (Wiki-PubMed-PMC)",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.91",
                "0.91",
                "[BOLD] 0.91"
            ],
            [
                "+BoC (GloVe)",
                "0.93",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+ASM",
                "0.90",
                "0.85",
                "0.88",
                "0.90",
                "0.86",
                "0.88",
                "0.89",
                "0.89",
                "0.89"
            ],
            [
                "+Sentence Embeddings(SEs)",
                "0.89",
                "0.89",
                "0.89",
                "0.90",
                "0.86",
                "0.88",
                "0.88",
                "0.88",
                "0.88"
            ],
            [
                "+BoC(Wiki-PubMed-PMC)+SEs",
                "0.92",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ]
        ],
        "question": "Is it true that The models using BoC outperform models using BoW as well as ASM features?",
        "answer_label": "yes"
    },
    {
        "id": "4ab2d01c-b861-49be-9fd3-b87ffdc7ac20",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
        "table_column_names": [
            "[EMPTY]",
            "Difference Function",
            "Seanad Abolition",
            "Video Games",
            "Pornography"
        ],
        "table_content_values": [
            [
                "OD-parse",
                "Absolute",
                "0.01",
                "-0.01",
                "0.07"
            ],
            [
                "OD-parse",
                "JS div.",
                "0.01",
                "-0.01",
                "-0.01"
            ],
            [
                "OD-parse",
                "EMD",
                "0.07",
                "0.01",
                "-0.01"
            ],
            [
                "OD",
                "Absolute",
                "[BOLD] 0.54",
                "[BOLD] 0.56",
                "[BOLD] 0.41"
            ],
            [
                "OD",
                "JS div.",
                "0.07",
                "-0.01",
                "-0.02"
            ],
            [
                "OD",
                "EMD",
                "0.26",
                "-0.01",
                "0.01"
            ],
            [
                "OD (no polarity shifters)",
                "Absolute",
                "0.23",
                "0.08",
                "0.04"
            ],
            [
                "OD (no polarity shifters)",
                "JS div.",
                "0.09",
                "-0.01",
                "-0.02"
            ],
            [
                "OD (no polarity shifters)",
                "EMD",
                "0.10",
                "0.01",
                "-0.01"
            ]
        ],
        "question": "Is it true that [CONTINUE] OD significantly outperforms OD-parse: We observe that compared to OD-parse, OD is much more accurate?",
        "answer_label": "yes"
    },
    {
        "id": "cc5ee69b-76a7-4812-b9f9-9bfbdb654101",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 2: POS and SEM tagging accuracy with baselines and an upper bound. MFT: most frequent tag; UnsupEmb: classifier using unsupervised word embeddings; Word2Tag: upper bound encoder-decoder.",
        "table_column_names": [
            "[EMPTY]",
            "MFT",
            "UnsupEmb",
            "Word2Tag"
        ],
        "table_content_values": [
            [
                "POS",
                "91.95",
                "87.06",
                "95.55"
            ],
            [
                "SEM",
                "82.00",
                "81.11",
                "91.41"
            ]
        ],
        "question": "Is it true that The UnsupEmb baseline performs rather poorly on both POS and SEM tagging?",
        "answer_label": "yes"
    },
    {
        "id": "d3c5de39-44f7-4eff-93d4-8295045a1db1",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 1: Classifier performance",
        "table_column_names": [
            "Dataset",
            "Class",
            "Precision",
            "Recall",
            "F1"
        ],
        "table_content_values": [
            [
                "[ITALIC] W. & H.",
                "Racism",
                "0.73",
                "0.79",
                "0.76"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.69",
                "0.73",
                "0.71"
            ],
            [
                "[EMPTY]",
                "Neither",
                "0.88",
                "0.85",
                "0.86"
            ],
            [
                "[ITALIC] W.",
                "Racism",
                "0.56",
                "0.77",
                "0.65"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.62",
                "0.73",
                "0.67"
            ],
            [
                "[EMPTY]",
                "R. & S.",
                "0.56",
                "0.62",
                "0.59"
            ],
            [
                "[EMPTY]",
                "Neither",
                "0.95",
                "0.92",
                "0.94"
            ],
            [
                "[ITALIC] D. et al.",
                "Hate",
                "0.32",
                "0.53",
                "0.4"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.96",
                "0.88",
                "0.92"
            ],
            [
                "[EMPTY]",
                "Neither",
                "0.81",
                "0.95",
                "0.87"
            ],
            [
                "[ITALIC] G. et al.",
                "Harass.",
                "0.41",
                "0.19",
                "0.26"
            ],
            [
                "[EMPTY]",
                "Non.",
                "0.75",
                "0.9",
                "0.82"
            ],
            [
                "[ITALIC] F. et al.",
                "Hate",
                "0.33",
                "0.42",
                "0.37"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.87",
                "0.88",
                "0.88"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.5",
                "0.7",
                "0.58"
            ],
            [
                "[EMPTY]",
                "Neither",
                "0.88",
                "0.77",
                "0.82"
            ]
        ],
        "question": "Is it true that In particular, we see that hate speech and harassment are particularly difficult to detect?",
        "answer_label": "yes"
    },
    {
        "id": "47261858-023d-413f-aad0-4d850bd3ffb3",
        "table_caption": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations Table 3: Cue and token distribution in the conversational negation corpus.",
        "table_column_names": [
            "Total negation cues",
            "2921"
        ],
        "table_content_values": [
            [
                "True negation cues",
                "2674"
            ],
            [
                "False negation cues",
                "247"
            ],
            [
                "Average scope length",
                "2.9"
            ],
            [
                "Average sentence length",
                "13.6"
            ],
            [
                "Average tweet length",
                "22.3"
            ]
        ],
        "question": "Is it true that The average number of tokens per tweet is not 22.3, per sentence is not 13.6 and average scope length is not 2.9?",
        "answer_label": "no"
    },
    {
        "id": "b4c1d97b-4782-4575-a319-1b40d0ece452",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
        "table_column_names": [
            "[EMPTY]",
            "Acc",
            "Sim",
            "PP",
            "GM"
        ],
        "table_content_values": [
            [
                "M0: shen-1",
                "0.818",
                "0.719",
                "37.3",
                "10.0"
            ],
            [
                "M1: M0 [ITALIC] +para",
                "0.819",
                "0.734",
                "26.3",
                "14.2"
            ],
            [
                "M2: M0 [ITALIC] +cyc",
                "0.813",
                "0.770",
                "36.4",
                "18.8"
            ],
            [
                "M3: M0 [ITALIC] +cyc+lang",
                "0.807",
                "0.796",
                "28.4",
                "21.5"
            ],
            [
                "M4: M0 [ITALIC] +cyc+para",
                "0.798",
                "0.783",
                "39.7",
                "19.2"
            ],
            [
                "M5: M0 [ITALIC] +cyc+para+lang",
                "0.804",
                "0.785",
                "27.1",
                "20.3"
            ],
            [
                "M6: M0 [ITALIC] +cyc+2d",
                "0.805",
                "[BOLD] 0.817",
                "43.3",
                "21.6"
            ],
            [
                "M7: M6+ [ITALIC] para+lang",
                "0.818",
                "0.805",
                "[BOLD] 29.0",
                "[BOLD] 22.8"
            ]
        ],
        "question": "Is it true that [CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation?",
        "answer_label": "yes"
    },
    {
        "id": "bfbfa38c-d46e-475e-817c-e5161057ae5f",
        "table_caption": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance Table 1: Absolute Pearson correlations with segment-level human judgments in 7 language pairs on WMT17 dataset.",
        "table_column_names": [
            "Setting",
            "Metrics",
            "<bold>Direct Assessment</bold> cs-en",
            "<bold>Direct Assessment</bold> de-en",
            "<bold>Direct Assessment</bold> fi-en",
            "<bold>Direct Assessment</bold> lv-en",
            "<bold>Direct Assessment</bold> ru-en",
            "<bold>Direct Assessment</bold> tr-en",
            "<bold>Direct Assessment</bold> zh-en",
            "<bold>Direct Assessment</bold> Average"
        ],
        "table_content_values": [
            [
                "Baselines",
                "METEOR++",
                "0.552",
                "0.538",
                "0.720",
                "0.563",
                "0.627",
                "0.626",
                "0.646",
                "0.610"
            ],
            [
                "Baselines",
                "RUSE(*)",
                "0.624",
                "0.644",
                "0.750",
                "0.697",
                "0.673",
                "0.716",
                "0.691",
                "0.685"
            ],
            [
                "Baselines",
                "BERTScore-F1",
                "0.670",
                "0.686",
                "0.820",
                "0.710",
                "0.729",
                "0.714",
                "0.704",
                "0.719"
            ],
            [
                "Sent-Mover",
                "Smd + W2V",
                "0.438",
                "0.505",
                "0.540",
                "0.442",
                "0.514",
                "0.456",
                "0.494",
                "0.484"
            ],
            [
                "Sent-Mover",
                "Smd + ELMO + PMeans",
                "0.569",
                "0.558",
                "0.732",
                "0.525",
                "0.581",
                "0.620",
                "0.584",
                "0.595"
            ],
            [
                "Sent-Mover",
                "Smd + BERT + PMeans",
                "0.607",
                "0.623",
                "0.770",
                "0.639",
                "0.667",
                "0.641",
                "0.619",
                "0.652"
            ],
            [
                "Sent-Mover",
                "Smd + BERT + MNLI + PMeans",
                "0.616",
                "0.643",
                "0.785",
                "0.660",
                "0.664",
                "0.668",
                "0.633",
                "0.667"
            ],
            [
                "Word-Mover",
                "Wmd-1 + W2V",
                "0.392",
                "0.463",
                "0.558",
                "0.463",
                "0.456",
                "0.485",
                "0.481",
                "0.471"
            ],
            [
                "Word-Mover",
                "Wmd-1 + ELMO + PMeans",
                "0.579",
                "0.588",
                "0.753",
                "0.559",
                "0.617",
                "0.679",
                "0.645",
                "0.631"
            ],
            [
                "Word-Mover",
                "Wmd-1 + BERT + PMeans",
                "0.662",
                "0.687",
                "0.823",
                "0.714",
                "0.735",
                "0.734",
                "0.719",
                "0.725"
            ],
            [
                "Word-Mover",
                "Wmd-1 + BERT + MNLI + PMeans",
                "0.670",
                "0.708",
                "<bold>0.835</bold>",
                "<bold>0.746</bold>",
                "<bold>0.738</bold>",
                "0.762",
                "<bold>0.744</bold>",
                "<bold>0.743</bold>"
            ],
            [
                "Word-Mover",
                "Wmd-2 + BERT + MNLI + PMeans",
                "<bold>0.679</bold>",
                "<bold>0.710</bold>",
                "0.832",
                "0.745",
                "0.736",
                "<bold>0.763</bold>",
                "0.740",
                "<bold>0.743</bold>"
            ]
        ],
        "question": "Is it true that Table 1: In all language pairs, the best correlation is achieved by our word mover metrics that use a BERT pretrained on MNLI as the embedding generator and PMeans to aggregate the embeddings from different BERT layers, i.e., WMD-1/2+BERT+MNLI+PMeans?",
        "answer_label": "yes"
    },
    {
        "id": "7ce0aea2-aa42-4cc8-99eb-6bdfe53cf6da",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that We hypothesize that the gating mechanism can better capture longdistance dependencies between nodes far apart in the graph?",
        "answer_label": "yes"
    },
    {
        "id": "4f80f854-aa9a-4f6d-b67f-0edd0cb7c126",
        "table_caption": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] ROUGE-1  [BOLD] R (%)",
            "[BOLD] ROUGE-1  [BOLD] P (%)",
            "[BOLD] ROUGE-1  [BOLD] F (%)",
            "[BOLD] ROUGE-2  [BOLD] R (%)",
            "[BOLD] ROUGE-2  [BOLD] P (%)",
            "[BOLD] ROUGE-2  [BOLD] F (%)",
            "[BOLD] Sentence-Level  [BOLD] R (%)",
            "[BOLD] Sentence-Level  [BOLD] P (%)",
            "[BOLD] Sentence-Level  [BOLD] F (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] ILP",
                "24.5",
                "41.1",
                "29.3\u00b10.5",
                "7.9",
                "15.0",
                "9.9\u00b10.5",
                "13.6",
                "22.6",
                "15.6\u00b10.4"
            ],
            [
                "[BOLD] Sum-Basic",
                "28.4",
                "44.4",
                "33.1\u00b10.5",
                "8.5",
                "15.6",
                "10.4\u00b10.4",
                "14.7",
                "22.9",
                "16.7\u00b10.5"
            ],
            [
                "[BOLD] KL-Sum",
                "39.5",
                "34.6",
                "35.5\u00b10.5",
                "13.0",
                "12.7",
                "12.3\u00b10.5",
                "15.2",
                "21.1",
                "16.3\u00b10.5"
            ],
            [
                "[BOLD] LexRank",
                "42.1",
                "39.5",
                "38.7\u00b10.5",
                "14.7",
                "15.3",
                "14.2\u00b10.5",
                "14.3",
                "21.5",
                "16.0\u00b10.5"
            ],
            [
                "[BOLD] MEAD",
                "45.5",
                "36.5",
                "38.5\u00b1 0.5",
                "17.9",
                "14.9",
                "15.4\u00b10.5",
                "27.8",
                "29.2",
                "26.8\u00b10.5"
            ],
            [
                "[BOLD] SVM",
                "19.0",
                "48.8",
                "24.7\u00b10.8",
                "7.5",
                "21.1",
                "10.0\u00b10.5",
                "32.7",
                "34.3",
                "31.4\u00b10.4"
            ],
            [
                "[BOLD] LogReg",
                "26.9",
                "34.5",
                "28.7\u00b10.6",
                "6.4",
                "9.9",
                "7.3\u00b10.4",
                "12.2",
                "14.9",
                "12.7\u00b10.5"
            ],
            [
                "[BOLD] LogReg [ITALIC] r",
                "28.0",
                "34.8",
                "29.4\u00b10.6",
                "6.9",
                "10.4",
                "7.8\u00b10.4",
                "12.1",
                "14.5",
                "12.5\u00b10.5"
            ],
            [
                "[BOLD] HAN",
                "31.0",
                "42.8",
                "33.7\u00b10.7",
                "11.2",
                "17.8",
                "12.7\u00b10.5",
                "26.9",
                "34.1",
                "32.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT",
                "32.2",
                "42.4",
                "34.4\u00b10.7",
                "11.5",
                "17.5",
                "12.9\u00b10.5",
                "29.6",
                "35.8",
                "32.2\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU",
                "32.1",
                "42.1",
                "33.8\u00b10.7",
                "11.6",
                "17.6",
                "12.9\u00b10.5",
                "30.1",
                "35.6",
                "32.3\u00b10.5"
            ],
            [
                "[BOLD] HAN [ITALIC] r",
                "38.1",
                "40.5",
                "[BOLD] 37.8\u00b10.5",
                "14.0",
                "17.1",
                "[BOLD] 14.7\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.5",
                "16.8",
                "[BOLD] 14.4\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.6",
                "16.9",
                "[BOLD] 14.4\u00b10.5",
                "33.9",
                "33.8",
                "[BOLD] 33.8\u00b10.5"
            ]
        ],
        "question": "Is it true that [CONTINUE] Pretraining the HAN models, although intuitively promising, yields only comparable results with those without?",
        "answer_label": "yes"
    },
    {
        "id": "da1ceb61-ee17-4e2e-970e-222fe51acd08",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 7: Scores for initialization strategies on probing tasks.",
        "table_column_names": [
            "Initialization",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "N(0,0.1)",
                "29.7",
                "71.5",
                "82.0",
                "78.5",
                "60.1",
                "80.5",
                "76.3",
                "74.7",
                "[BOLD] 51.3",
                "52.5"
            ],
            [
                "Glorot",
                "31.3",
                "[BOLD] 72.3",
                "81.8",
                "78.7",
                "59.4",
                "81.3",
                "76.6",
                "[BOLD] 74.6",
                "50.4",
                "57.0"
            ],
            [
                "Our paper",
                "[BOLD] 35.1",
                "70.8",
                "[BOLD] 82.0",
                "[BOLD] 80.2",
                "[BOLD] 61.8",
                "[BOLD] 82.8",
                "[BOLD] 79.7",
                "74.2",
                "50.7",
                "[BOLD] 72.9"
            ]
        ],
        "question": "Is it true that While Glorot achieves slightly better results on BShift and TopConst, CMOW's ability to memorize word content is not improved by our initialization strategy?",
        "answer_label": "no"
    },
    {
        "id": "f447aac8-3df2-4446-82f9-89b20ad46901",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 5: An ablation study on the effects of pretraining for multitasking-based and standard GPT-2 finetuning. Results show that pretraining greatly accounts for almost half of performance on both finetuning techniques. \u201cAcc. Inc.\u201d refers to the boost in performance contributed by the pretraining step. \u201c% of Perf.\u201d refers to the percentage of the total performance that the pretraining step contributes.",
        "table_column_names": [
            "Finetuning",
            "Pretrained?",
            "Accuracy",
            "Val. Loss",
            "Acc. Inc.",
            "% of Perf."
        ],
        "table_content_values": [
            [
                "Multitasking",
                "No",
                "53.61%",
                "0.7217",
                "-",
                "-"
            ],
            [
                "[EMPTY]",
                "Yes",
                "96.28%",
                "0.2197",
                "+42.67%",
                "44.32%"
            ],
            [
                "Standard",
                "No",
                "51.02%",
                "0.7024",
                "-",
                "-"
            ],
            [
                "[EMPTY]",
                "Yes",
                "90.99%",
                "0.1826",
                "+39.97%",
                "43.93%"
            ]
        ],
        "question": "Is it true that In Table 5, it can be seen that generative pretraining via language modeling does not account for a considerable amount of performance, constituting 44.32% of the overall performance (a boost of 42.67% in accuracy) in the multitasking setup, and constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup?",
        "answer_label": "no"
    },
    {
        "id": "286a8de8-bba3-4a30-8e62-b75d6d91ed7d",
        "table_caption": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.",
        "table_column_names": [
            "[EMPTY]",
            "dev perp \u2193",
            "dev acc \u2191",
            "dev wer \u2193",
            "test perp \u2193",
            "test acc \u2191",
            "test wer \u2193"
        ],
        "table_content_values": [
            [
                "Spanish-only-LM",
                "329.68",
                "26.6",
                "30.47",
                "322.26",
                "25.1",
                "29.62"
            ],
            [
                "English-only-LM",
                "320.92",
                "29.3",
                "32.02",
                "314.04",
                "30.3",
                "32.51"
            ],
            [
                "All:CS-last-LM",
                "76.64",
                "47.8",
                "14.56",
                "76.97",
                "49.2",
                "14.13"
            ],
            [
                "All:Shuffled-LM",
                "68.00",
                "51.8",
                "13.64",
                "68.72",
                "51.4",
                "13.89"
            ],
            [
                "CS-only-LM",
                "43.20",
                "60.7",
                "12.60",
                "43.42",
                "57.9",
                "12.18"
            ],
            [
                "CS-only+vocab-LM",
                "45.61",
                "61.0",
                "12.56",
                "45.79",
                "58.8",
                "12.49"
            ],
            [
                "Fine-Tuned-LM",
                "39.76",
                "66.9",
                "10.71",
                "40.11",
                "65.4",
                "10.17"
            ],
            [
                "CS-only-disc",
                "\u2013",
                "72.0",
                "6.35",
                "\u2013",
                "70.5",
                "6.70"
            ],
            [
                "Fine-Tuned-disc",
                "\u2013",
                "[BOLD] 74.2",
                "[BOLD] 5.85",
                "\u2013",
                "[BOLD] 75.5",
                "[BOLD] 5.59"
            ]
        ],
        "question": "Is it true that Note that using discriminative training, even with no additional monolingual data, leads to better performance than that of the best language model: the CS-ONLY-DISCRIMINATIVE model achieves an accuracy of 70.5%, 5.1 points more than the accuracy of the FINE-TUNED-LM model?",
        "answer_label": "yes"
    },
    {
        "id": "b2165172-65ea-4be7-808b-e6bd633803e9",
        "table_caption": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set. For \u201c- Hierachical-Attn\u201d, we remove the residual connections between the attention modules in the CMR decoders and all the attention memory access are based on the output from the LSTM. For \u201c- MLP\u201d, we further replace the MLP with a single linear layer with the non-linear activation.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Joint Acc."
        ],
        "table_content_values": [
            [
                "COMER",
                "88.64%"
            ],
            [
                "- Hierachical-Attn",
                "86.69%"
            ],
            [
                "- MLP",
                "83.24%"
            ]
        ],
        "question": "Is it true that [CONTINUE] The effectiveness of our hierarchical attention design is proved by an accuracy drop of 1.95% after removing residual connections and the hierarchical stack of our attention modules?",
        "answer_label": "yes"
    },
    {
        "id": "4169c808-fdbd-4bc2-a58c-0ad6872535b1",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Feature",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Feature",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams"
            ],
            [
                "not",
                ".154",
                "[URL]",
                ".150"
            ],
            [
                "my",
                ".131",
                "!",
                ".082"
            ],
            [
                "working",
                ".124",
                "he",
                ".069"
            ],
            [
                "still",
                ".123",
                "thank",
                ".067"
            ],
            [
                "on",
                ".119",
                ",",
                ".064"
            ],
            [
                "can\u2019t",
                ".113",
                "love",
                ".064"
            ],
            [
                "service",
                ".112",
                "lol",
                ".061"
            ],
            [
                "customer",
                ".109",
                "you",
                ".060"
            ],
            [
                "why",
                ".108",
                "great",
                ".058"
            ],
            [
                "website",
                ".107",
                "win",
                ".058"
            ],
            [
                "no",
                ".104",
                "\u2019",
                ".058"
            ],
            [
                "?",
                ".098",
                "she",
                ".054"
            ],
            [
                "fix",
                ".093",
                ":",
                ".053"
            ],
            [
                "won\u2019t",
                ".092",
                "that",
                ".053"
            ],
            [
                "been",
                ".090",
                "more",
                ".052"
            ],
            [
                "issue",
                ".089",
                "it",
                ".052"
            ],
            [
                "days",
                ".088",
                "would",
                ".051"
            ],
            [
                "error",
                ".087",
                "him",
                ".047"
            ],
            [
                "is",
                ".084",
                "life",
                ".046"
            ],
            [
                "charged",
                ".083",
                "good",
                ".046"
            ],
            [
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)"
            ],
            [
                "VBN",
                ".141",
                "UH",
                ".104"
            ],
            [
                "$",
                ".118",
                "NNP",
                ".098"
            ],
            [
                "VBZ",
                ".114",
                "PRP",
                ".076"
            ],
            [
                "NN_VBZ",
                ".114",
                "HT",
                ".076"
            ],
            [
                "PRP$",
                ".107",
                "PRP_.",
                ".076"
            ],
            [
                "PRP$_NN",
                ".105",
                "PRP_RB",
                ".067"
            ],
            [
                "VBG",
                ".093",
                "NNP_NNP",
                ".062"
            ],
            [
                "CD",
                ".092",
                "VBP_PRP",
                ".054"
            ],
            [
                "WRB_VBZ",
                ".084",
                "JJ",
                ".053"
            ],
            [
                "VBZ_VBN",
                ".084",
                "DT_JJ",
                ".051"
            ]
        ],
        "question": "Is it true that Mentions of time are not specific of complaints (been, still, on, days, Temporal References cluster)?",
        "answer_label": "no"
    },
    {
        "id": "2ac86f7a-9e75-41a0-a2c9-f36542cb12cf",
        "table_caption": "What do Deep Networks Like to Read? Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.",
        "table_column_names": [
            "[EMPTY]",
            "<bold>RNN</bold>",
            "<bold>CNN</bold>",
            "<bold>DAN</bold>"
        ],
        "table_content_values": [
            [
                "Positive",
                "+9.7",
                "+4.3",
                "+<bold>23.6</bold>"
            ],
            [
                "Negative",
                "+6.9",
                "+5.5",
                "+<bold>16.1</bold>"
            ],
            [
                "Flipped to Positive",
                "+20.2",
                "+24.9",
                "+27.4"
            ],
            [
                "Flipped to Negative",
                "+31.5",
                "+28.6",
                "+19.3"
            ]
        ],
        "question": "Is it true that This is especially true in the case of DAN where we see a large increase as the decoder repeatedly predicts words having high sentiment value?",
        "answer_label": "yes"
    },
    {
        "id": "0ee9a5d8-8b90-424c-9e68-f02437594591",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Type",
            "[BOLD] English-German #P",
            "[BOLD] English-German B",
            "[BOLD] English-German C",
            "[BOLD] English-Czech #P",
            "[BOLD] English-Czech B",
            "[BOLD] English-Czech C"
        ],
        "table_content_values": [
            [
                "BoW+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "12.2",
                "-",
                "-",
                "7.5",
                "-"
            ],
            [
                "CNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "13.7",
                "-",
                "-",
                "8.7",
                "-"
            ],
            [
                "BiRNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "16.1",
                "-",
                "-",
                "9.6",
                "-"
            ],
            [
                "PB-SMT (Beck et al.,  2018 )",
                "Single",
                "-",
                "12.8",
                "43.2",
                "-",
                "8.6",
                "36.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Single",
                "41.4M",
                "15.5",
                "40.8",
                "39.1M",
                "8.9",
                "33.8"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Single",
                "41.2M",
                "16.7",
                "42.4",
                "38.8M",
                "9.8",
                "33.3"
            ],
            [
                "DCGCN (ours)",
                "Single",
                "[BOLD]  29.7M",
                "[BOLD] 19.0",
                "[BOLD] 44.1",
                "[BOLD]  28.3M",
                "[BOLD] 12.1",
                "[BOLD] 37.1"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Ensemble",
                "207M",
                "19.0",
                "44.1",
                "195M",
                "11.3",
                "36.4"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Ensemble",
                "206M",
                "19.6",
                "45.1",
                "194M",
                "11.7",
                "35.9"
            ],
            [
                "DCGCN (ours)",
                "Ensemble",
                "[BOLD]  149M",
                "[BOLD] 20.5",
                "[BOLD] 45.8",
                "[BOLD]  142M",
                "[BOLD] 13.1",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that Our models DCGCN(single) and DCGCN(ensemble)consist of full GCN layers, removing the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers?",
        "answer_label": "yes"
    },
    {
        "id": "14c7a0f9-247b-4a40-bd54-5ad510a0a091",
        "table_caption": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task Table 1: Image-caption ranking results for English (Multi30k)",
        "table_column_names": [
            "[EMPTY]",
            "Image to Text R@1",
            "Image to Text R@5",
            "Image to Text R@10",
            "Image to Text Mr",
            "Text to Image R@1",
            "Text to Image R@5",
            "Text to Image R@10",
            "Text to Image Mr",
            "Alignment"
        ],
        "table_content_values": [
            [
                "[BOLD] symmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Parallel\u00a0gella:17",
                "31.7",
                "62.4",
                "74.1",
                "3",
                "24.7",
                "53.9",
                "65.7",
                "5",
                "-"
            ],
            [
                "UVS\u00a0kiros:15",
                "23.0",
                "50.7",
                "62.9",
                "5",
                "16.8",
                "42.0",
                "56.5",
                "8",
                "-"
            ],
            [
                "EmbeddingNet\u00a0wang:18",
                "40.7",
                "69.7",
                "79.2",
                "-",
                "29.2",
                "59.6",
                "71.7",
                "-",
                "-"
            ],
            [
                "sm-LSTM\u00a0huang:17",
                "42.5",
                "71.9",
                "81.5",
                "2",
                "30.2",
                "60.4",
                "72.3",
                "3",
                "-"
            ],
            [
                "VSE++\u00a0faghri:18",
                "[BOLD] 43.7",
                "71.9",
                "82.1",
                "2",
                "32.3",
                "60.9",
                "72.1",
                "3",
                "-"
            ],
            [
                "Mono",
                "41.4",
                "74.2",
                "84.2",
                "2",
                "32.1",
                "63.0",
                "73.9",
                "3",
                "-"
            ],
            [
                "FME",
                "39.2",
                "71.1",
                "82.1",
                "2",
                "29.7",
                "62.5",
                "74.1",
                "3",
                "76.81%"
            ],
            [
                "AME",
                "43.5",
                "[BOLD] 77.2",
                "[BOLD] 85.3",
                "[BOLD] 2",
                "[BOLD] 34.0",
                "[BOLD] 64.2",
                "[BOLD] 75.4",
                "[BOLD] 3",
                "66.91%"
            ],
            [
                "[BOLD] asymmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Pivot\u00a0gella:17",
                "33.8",
                "62.8",
                "75.2",
                "3",
                "26.2",
                "56.4",
                "68.4",
                "4",
                "-"
            ],
            [
                "Parallel\u00a0gella:17",
                "31.5",
                "61.4",
                "74.7",
                "3",
                "27.1",
                "56.2",
                "66.9",
                "4",
                "-"
            ],
            [
                "Mono",
                "47.7",
                "77.1",
                "86.9",
                "2",
                "35.8",
                "66.6",
                "76.8",
                "3",
                "-"
            ],
            [
                "FME",
                "44.9",
                "76.9",
                "86.4",
                "2",
                "34.2",
                "66.1",
                "77.1",
                "3",
                "76.81%"
            ],
            [
                "AME",
                "[BOLD] 50.5",
                "[BOLD] 79.7",
                "[BOLD] 88.4",
                "[BOLD] 1",
                "[BOLD] 38.0",
                "[BOLD] 68.5",
                "[BOLD] 78.4",
                "[BOLD] 2",
                "73.10%"
            ]
        ],
        "question": "Is it true that AME performs better than FME model on both symmetric and asymmetric modes, which shows the advantage of finetuning word embeddings during training?",
        "answer_label": "yes"
    },
    {
        "id": "4a50e39b-a5db-4dce-b474-46fda3d1159b",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 2: Ratings of annotated NLDs by human judges.",
        "table_column_names": [
            "# steps",
            "Reachability",
            "Derivability Step 1",
            "Derivability Step 2",
            "Derivability Step 3"
        ],
        "table_content_values": [
            [
                "1",
                "3.0",
                "3.8",
                "-",
                "-"
            ],
            [
                "2",
                "2.8",
                "3.8",
                "3.7",
                "-"
            ],
            [
                "3",
                "2.3",
                "3.9",
                "3.8",
                "3.8"
            ]
        ],
        "question": "Is it true that On the contrary, we found the quality of 3-step NLDs is relatively higher than the others?",
        "answer_label": "no"
    },
    {
        "id": "4cef41a6-d5a3-4308-9bc0-95a8154255d8",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
        "table_column_names": [
            "<bold>Model</bold>",
            "R",
            "MUC P",
            "<italic>F</italic>1",
            "R",
            "B3 P",
            "<italic>F</italic>1",
            "R",
            "CEAF-<italic>e</italic> P",
            "<italic>F</italic>1",
            "CoNLL <italic>F</italic>1"
        ],
        "table_content_values": [
            [
                "<bold>Baselines</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Cluster+Lemma",
                "76.5",
                "79.9",
                "78.1",
                "71.7",
                "85",
                "77.8",
                "75.5",
                "71.7",
                "73.6",
                "76.5"
            ],
            [
                "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
                "71",
                "75",
                "73",
                "71",
                "78",
                "74",
                "-",
                "-",
                "64",
                "73"
            ],
            [
                "KCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>)",
                "67",
                "71",
                "69",
                "71",
                "67",
                "69",
                "71",
                "67",
                "69",
                "69"
            ],
            [
                "Cluster+KCP",
                "68.4",
                "79.3",
                "73.4",
                "67.2",
                "87.2",
                "75.9",
                "77.4",
                "66.4",
                "71.5",
                "73.6"
            ],
            [
                "<bold>Model Variants</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Disjoint",
                "75.5",
                "83.6",
                "79.4",
                "75.4",
                "86",
                "80.4",
                "80.3",
                "71.9",
                "75.9",
                "78.5"
            ],
            [
                "Joint",
                "77.6",
                "84.5",
                "80.9",
                "76.1",
                "85.1",
                "80.3",
                "81",
                "73.8",
                "77.3",
                "<bold>79.5</bold>"
            ]
        ],
        "question": "Is it true that Our joint model does not outperform all the base [CONTINUE] The results do not reconfirm that the lemma baseline, when combined with effective topic clustering, is a strong baseline for CD event coreference resolution on the ECB+ corpus (Upadhyay et al., 2016)?",
        "answer_label": "no"
    },
    {
        "id": "965efd28-0126-4d8a-92dc-216b431dafaf",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.",
        "table_column_names": [
            "[BOLD] Language pair",
            "[BOLD] Model type",
            "[BOLD] Oracle model",
            "[BOLD] Decoder configuration  [BOLD] Uniform",
            "[BOLD] Decoder configuration  [BOLD] BI + IS"
        ],
        "table_content_values": [
            [
                "es-en",
                "Unadapted",
                "36.4",
                "34.7",
                "36.6"
            ],
            [
                "es-en",
                "No-reg",
                "36.6",
                "34.8",
                "-"
            ],
            [
                "es-en",
                "EWC",
                "37.0",
                "36.3",
                "[BOLD] 37.2"
            ],
            [
                "en-de",
                "Unadapted",
                "36.4",
                "26.8",
                "38.8"
            ],
            [
                "en-de",
                "No-reg",
                "41.7",
                "31.8",
                "-"
            ],
            [
                "en-de",
                "EWC",
                "42.1",
                "38.6",
                "[BOLD] 42.0"
            ]
        ],
        "question": "Is it true that BI+IS with EWC-adapted models gives a 0.9 / 3.4 BLEU loss over the strong uniform EWC ensemble, and a 2.4 / 10.2 overall BLEU loss over the approach described in Freitag and Al-Onaizan (2016)?",
        "answer_label": "no"
    },
    {
        "id": "e6163646-e624-431a-a99d-c4f2450a0183",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
        "table_column_names": [
            "Metric",
            "[ITALIC] \u03c1",
            "[ITALIC] r",
            "G-Pre",
            "G-Rec"
        ],
        "table_content_values": [
            [
                "ROUGE-1",
                ".290",
                ".304",
                ".392",
                ".428"
            ],
            [
                "ROUGE-2",
                ".259",
                ".278",
                ".408",
                ".444"
            ],
            [
                "ROUGE-L",
                ".274",
                ".297",
                ".390",
                ".426"
            ],
            [
                "ROUGE-SU4",
                ".282",
                ".279",
                ".404",
                ".440"
            ],
            [
                "BLEU-1",
                ".256",
                ".281",
                ".409",
                ".448"
            ],
            [
                "BLEU-2",
                ".301",
                ".312",
                ".411",
                ".446"
            ],
            [
                "BLEU-3",
                ".317",
                ".312",
                ".409",
                ".444"
            ],
            [
                "BLEU-4",
                ".311",
                ".307",
                ".409",
                ".446"
            ],
            [
                "BLEU-5",
                ".308",
                ".303",
                ".420",
                ".459"
            ],
            [
                "METEOR",
                ".305",
                ".285",
                ".409",
                ".444"
            ],
            [
                "InferSent-Cosine",
                "[BOLD] .329",
                "[BOLD] .339",
                ".417",
                ".460"
            ],
            [
                "BERT-Cosine",
                ".312",
                ".335",
                "[BOLD] .440",
                "[BOLD] .484"
            ]
        ],
        "question": "Is it true that More importantly, their G-Pre and G-Rec scores are all above .50, which means that more than half of the good summaries identified by the metrics are actually good, and more than 50%?",
        "answer_label": "no"
    },
    {
        "id": "7e741dca-daea-49be-9ccc-ca533cf8b802",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. \u201cPref%\u201d: in how many percentage of documents a system receives the higher human rating.",
        "table_column_names": [
            "Reward",
            "R-1",
            "R-2",
            "R-L",
            "Human",
            "Pref%"
        ],
        "table_content_values": [
            [
                "R-L (original)",
                "40.9",
                "17.8",
                "38.5",
                "1.75",
                "15"
            ],
            [
                "Learned (ours)",
                "39.2",
                "17.4",
                "37.5",
                "[BOLD] 2.20",
                "[BOLD] 75"
            ]
        ],
        "question": "Is it true that Again, when ROUGE is used as rewards, the generated summaries have higher ROUGE scores?",
        "answer_label": "yes"
    },
    {
        "id": "73207db1-84fa-40bf-b763-0c0b9f859942",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that This suggests that graph encoders based on gating mechanisms are very effective in text generation models?",
        "answer_label": "yes"
    },
    {
        "id": "961db06c-7cce-438a-ad9b-89e45a05da2a",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "36.85",
                "5.3782",
                "35.14",
                "55.01",
                "1.6016",
                "00.34",
                "09.81",
                "00.15",
                "10.31"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen",
                "39.23",
                "6.0217",
                "36.97",
                "55.52",
                "1.7623",
                "00.40",
                "03.59",
                "00.07",
                "04.05"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen+",
                "40.25",
                "6.1448",
                "37.50",
                "56.19",
                "1.8181",
                "00.21",
                "01.99",
                "00.05",
                "02.24"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.88",
                "3.9310",
                "32.11",
                "39.90",
                "0.5036",
                "07.73",
                "17.76",
                "09.52",
                "35.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.19",
                "6.0543",
                "37.38",
                "55.88",
                "1.8104",
                "00.17",
                "01.31",
                "00.25",
                "01.72"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen",
                "40.73",
                "6.1711",
                "37.76",
                "56.09",
                "1.8518",
                "00.07",
                "00.72",
                "00.08",
                "00.87"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen+",
                "40.51",
                "6.1226",
                "37.61",
                "55.98",
                "1.8286",
                "00.02",
                "00.63",
                "00.06",
                "00.70"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.66",
                "3.9511",
                "32.93",
                "39.29",
                "0.3855",
                "07.89",
                "15.60",
                "08.44",
                "31.94"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.48",
                "6.0269",
                "37.26",
                "56.19",
                "1.7999",
                "00.43",
                "02.84",
                "00.26",
                "03.52"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen",
                "41.57",
                "6.2830",
                "37.99",
                "56.36",
                "1.8849",
                "00.37",
                "01.40",
                "00.09",
                "01.86"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen+",
                "41.56",
                "6.2700",
                "37.94",
                "56.38",
                "1.8827",
                "00.21",
                "01.04",
                "00.07",
                "01.31"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "35.99",
                "5.0734",
                "34.74",
                "54.79",
                "1.5259",
                "00.02",
                "11.58",
                "00.02",
                "11.62"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen",
                "40.07",
                "6.1243",
                "37.45",
                "55.81",
                "1.8026",
                "00.05",
                "03.23",
                "00.01",
                "03.29"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen+",
                "40.80",
                "6.2197",
                "37.86",
                "56.13",
                "1.8422",
                "00.01",
                "01.87",
                "00.01",
                "01.88"
            ]
        ],
        "question": "Is it true that the models more often fail to realise part of the MR, rather than hallucinating additional information?",
        "answer_label": "yes"
    },
    {
        "id": "b8186df4-979b-4b04-bb09-a484f0e6dfd6",
        "table_caption": "Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
        "table_column_names": [
            "Dataset",
            "Metric",
            "Illinois",
            "IlliCons",
            "rahman2012resolving",
            "KnowFeat",
            "KnowCons",
            "KnowComb"
        ],
        "table_content_values": [
            [
                "[ITALIC] Winograd",
                "Precision",
                "51.48",
                "53.26",
                "73.05",
                "71.81",
                "74.93",
                "[BOLD] 76.41"
            ],
            [
                "[ITALIC] WinoCoref",
                "AntePre",
                "68.37",
                "74.32",
                "\u2014\u2013",
                "88.48",
                "88.95",
                "[BOLD] 89.32"
            ]
        ],
        "question": "Is it true that These results show significant performance improvement by using Predicate Schemas knowledge on hard coreference problems?",
        "answer_label": "yes"
    },
    {
        "id": "0150f1a0-fe1d-4497-8489-a649003ab619",
        "table_caption": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set",
        "table_column_names": [
            "Method",
            "WER (%)",
            "DCE"
        ],
        "table_content_values": [
            [
                "No enhancement",
                "17.3",
                "0.828"
            ],
            [
                "Wiener filter",
                "19.5",
                "0.722"
            ],
            [
                "Minimizing DCE",
                "15.8",
                "[BOLD] 0.269"
            ],
            [
                "FSEGAN",
                "14.9",
                "0.291"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
                "15.6",
                "0.330"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
                "[BOLD] 14.4",
                "0.303"
            ],
            [
                "Clean speech",
                "5.7",
                "0.0"
            ]
        ],
        "question": "Is it true that The AAS method with weights wAC=1 and wAD=105 shows the lowest WER and DCE?",
        "answer_label": "no"
    },
    {
        "id": "4667459d-519c-4af0-9060-21a22cd745f1",
        "table_caption": "Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches Table 2: Comparison of exact-match accuracy achieved on selected benchmarks by a random or majority-choice baseline, various neural contextual embedding models, and humans. ELMo refers to the highest-performing listed approach using ELMo embeddings. Best system performance on each benchmark in bold. Information extracted from leaderboards (linked to in the first column) at time of writing (October 2019), and original papers for benchmarks introduced in Section\u00a02.",
        "table_column_names": [
            "[BOLD] Benchmark",
            "[BOLD]  Simple Baseline ",
            "[BOLD] ELMo",
            "[BOLD] GPT",
            "[BOLD] BERT",
            "[BOLD] MT-DNN",
            "[BOLD] XLNet",
            "[BOLD] RoBERTa",
            "[BOLD] ALBERT",
            "[BOLD] Human"
        ],
        "table_content_values": [
            [
                "[BOLD] CLOTH",
                "25.0",
                "70.7",
                "\u2013",
                "[BOLD] 86.0",
                "\u2013",
                "\u2013",
                "\u2013",
                "\u2013",
                "85.9"
            ],
            [
                "[BOLD] Cosmos QA",
                "\u2013",
                "\u2013",
                "54.5",
                "67.1",
                "\u2013",
                "\u2013",
                "\u2013",
                "\u2013",
                "94.0"
            ],
            [
                "[BOLD] DREAM",
                "33.4",
                "59.5",
                "55.5",
                "66.8",
                "\u2013",
                "[BOLD] 72.0",
                "\u2013",
                "\u2013",
                "95.5"
            ],
            [
                "[BOLD] GLUE",
                "\u2013",
                "70.0",
                "\u2013",
                "80.5",
                "87.6",
                "88.4",
                "88.5",
                "[BOLD] 89.4",
                "87.1"
            ],
            [
                "[BOLD] HellaSWAG",
                "25.0",
                "33.3",
                "41.7",
                "47.3",
                "\u2013",
                "\u2013",
                "[BOLD] 85.2",
                "[EMPTY]",
                "95.6"
            ],
            [
                "[BOLD] MC-TACO",
                "17.4",
                "26.4",
                "\u2013",
                "42.7",
                "\u2013",
                "\u2013",
                "[BOLD] 43.6",
                "\u2013",
                "75.8"
            ],
            [
                "[BOLD] RACE",
                "24.9",
                "\u2013",
                "59.0",
                "72.0",
                "\u2013",
                "81.8",
                "83.2",
                "[BOLD] 89.4",
                "94.5"
            ],
            [
                "[BOLD] SciTail",
                "60.3",
                "\u2013",
                "88.3",
                "\u2013",
                "94.1",
                "\u2013",
                "\u2013",
                "\u2013",
                "\u2013"
            ],
            [
                "[BOLD] SQuAD 1.1",
                "1.3",
                "81.0",
                "\u2013",
                "87.4",
                "\u2013",
                "[BOLD] 89.9",
                "\u2013",
                "\u2013",
                "82.3"
            ],
            [
                "[BOLD] SQuAD 2.0",
                "48.9",
                "63.4",
                "\u2013",
                "80.8",
                "\u2013",
                "86.3",
                "86.8",
                "[BOLD] 89.7",
                "86.9"
            ],
            [
                "[BOLD] SuperGLUE",
                "47.1",
                "\u2013",
                "\u2013",
                "69.0",
                "\u2013",
                "\u2013",
                "[BOLD] 84.6",
                "\u2013",
                "89.8"
            ],
            [
                "[BOLD] SWAG",
                "25.0",
                "59.1",
                "78.0",
                "86.3",
                "87.1",
                "\u2013",
                "[BOLD] 89.9",
                "\u2013",
                "88.0"
            ]
        ],
        "question": "Is it true that The most representative models are ELMO, GPT, BERT and its variants, and XLNET?",
        "answer_label": "yes"
    },
    {
        "id": "81c0bde7-fe67-476c-bf82-5fe7def3b1f3",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that In general, increasing the number of GCN layers from 2 to 9 boosts the model performance?",
        "answer_label": "yes"
    },
    {
        "id": "6d133fca-e9b6-4507-ba72-5a65564bf8da",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
        "table_column_names": [
            "<bold>Model</bold>",
            "R",
            "MUC P",
            "<italic>F</italic>1",
            "R",
            "B3 P",
            "<italic>F</italic>1",
            "R",
            "CEAF-<italic>e</italic> P",
            "<italic>F</italic>1",
            "CoNLL <italic>F</italic>1"
        ],
        "table_content_values": [
            [
                "<bold>Baselines</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Cluster+Lemma",
                "76.5",
                "79.9",
                "78.1",
                "71.7",
                "85",
                "77.8",
                "75.5",
                "71.7",
                "73.6",
                "76.5"
            ],
            [
                "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
                "71",
                "75",
                "73",
                "71",
                "78",
                "74",
                "-",
                "-",
                "64",
                "73"
            ],
            [
                "KCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>)",
                "67",
                "71",
                "69",
                "71",
                "67",
                "69",
                "71",
                "67",
                "69",
                "69"
            ],
            [
                "Cluster+KCP",
                "68.4",
                "79.3",
                "73.4",
                "67.2",
                "87.2",
                "75.9",
                "77.4",
                "66.4",
                "71.5",
                "73.6"
            ],
            [
                "<bold>Model Variants</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Disjoint",
                "75.5",
                "83.6",
                "79.4",
                "75.4",
                "86",
                "80.4",
                "80.3",
                "71.9",
                "75.9",
                "78.5"
            ],
            [
                "Joint",
                "77.6",
                "84.5",
                "80.9",
                "76.1",
                "85.1",
                "80.3",
                "81",
                "73.8",
                "77.3",
                "<bold>79.5</bold>"
            ]
        ],
        "question": "Is it true that The results of CLUSTER+KCP indicate that pre-clustering of documents to topics is not beneficial, performing substantially worse than our joint model?",
        "answer_label": "no"
    },
    {
        "id": "0fbadeff-af49-4236-b0b4-749c3e102f94",
        "table_caption": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. \u201cRaw\u201d indicates the usage of original prosodic features (Section 3.2), while \u201cinnovations\u201d indicate the usage of innovation features (Section 3.3).",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Model",
            "[BOLD] dev mean",
            "[BOLD] dev best",
            "[BOLD] test mean",
            "[BOLD] test best",
            "[ITALIC] \u03b1"
        ],
        "table_content_values": [
            [
                "single",
                "text",
                "86.54",
                "86.80",
                "86.47",
                "86.96",
                "\u2013"
            ],
            [
                "single",
                "raw",
                "35.00",
                "37.33",
                "35.78",
                "37.70",
                "\u2013"
            ],
            [
                "single",
                "innovations",
                "80.86",
                "81.51",
                "80.28",
                "82.15",
                "\u2013"
            ],
            [
                "early",
                "text + raw",
                "86.46",
                "86.65",
                "86.24",
                "86.53",
                "\u2013"
            ],
            [
                "early",
                "text + innovations",
                "86.53",
                "86.77",
                "86.54",
                "87.00",
                "\u2013"
            ],
            [
                "early",
                "text + raw + innovations",
                "86.35",
                "86.69",
                "86.55",
                "86.44",
                "\u2013"
            ],
            [
                "late",
                "text + raw",
                "86.71",
                "87.05",
                "86.35",
                "86.71",
                "0.2"
            ],
            [
                "late",
                "text + innovations",
                "[BOLD] 86.98",
                "[BOLD] 87.48",
                "[BOLD] 86.68",
                "[BOLD] 87.02",
                "0.5"
            ],
            [
                "late",
                "text + raw + innovations",
                "86.95",
                "87.30",
                "86.60",
                "86.87",
                "0.5"
            ]
        ],
        "question": "Is it true that [CONTINUE] We found that innovations are helpful in both early and late fusion frameworks, while late fusion performs better on average?",
        "answer_label": "yes"
    },
    {
        "id": "c45cc229-e5f8-4a18-b769-42397cd1f57d",
        "table_caption": "Zero-Shot Grounding of Objects from Natural Language Queries Table 3: Category-wise performance with the default split of Flickr30k Entities.",
        "table_column_names": [
            "Method",
            "Overall",
            "people",
            "clothing",
            "bodyparts",
            "animals",
            "vehicles",
            "instruments",
            "scene",
            "other"
        ],
        "table_content_values": [
            [
                "QRC - VGG(det)",
                "60.21",
                "75.08",
                "55.9",
                "20.27",
                "73.36",
                "68.95",
                "45.68",
                "65.27",
                "38.8"
            ],
            [
                "CITE - VGG(det)",
                "61.89",
                "[BOLD] 75.95",
                "58.50",
                "30.78",
                "[BOLD] 77.03",
                "[BOLD] 79.25",
                "48.15",
                "58.78",
                "43.24"
            ],
            [
                "ZSGNet - VGG (cls)",
                "60.12",
                "72.52",
                "60.57",
                "38.51",
                "63.61",
                "64.47",
                "49.59",
                "64.66",
                "41.09"
            ],
            [
                "ZSGNet - Res50 (cls)",
                "[BOLD] 63.39",
                "73.87",
                "[BOLD] 66.18",
                "[BOLD] 45.27",
                "73.79",
                "71.38",
                "[BOLD] 58.54",
                "[BOLD] 66.49",
                "[BOLD] 45.53"
            ]
        ],
        "question": "Is it true that [CONTINUE] As these models use object detectors pretrained on Pascal-VOC , they have somewhat higher performance on classes that are common to both Flickr30k and Pascal-VOC (\"animals\", \"people\" and \"vehicles\")?",
        "answer_label": "yes"
    },
    {
        "id": "8d3edac1-1144-4e49-ab95-1d0898c2acaf",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 3: Scores on unsupervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Method",
            "STS12",
            "STS13",
            "STS14",
            "STS15",
            "STS16"
        ],
        "table_content_values": [
            [
                "CBOW",
                "43.5",
                "[BOLD] 50.0",
                "[BOLD] 57.7",
                "[BOLD] 63.2",
                "61.0"
            ],
            [
                "CMOW",
                "39.2",
                "31.9",
                "38.7",
                "49.7",
                "52.2"
            ],
            [
                "Hybrid",
                "[BOLD] 49.6",
                "46.0",
                "55.1",
                "62.4",
                "[BOLD] 62.1"
            ],
            [
                "cmp. CBOW",
                "+14.6%",
                "-8%",
                "-4.5%",
                "-1.5%",
                "+1.8%"
            ],
            [
                "cmp. CMOW",
                "+26.5%",
                "+44.2%",
                "+42.4",
                "+25.6%",
                "+19.0%"
            ]
        ],
        "question": "Is it true that The hybrid model is able to repair this deficit, reducing the difference to 8%?",
        "answer_label": "yes"
    },
    {
        "id": "6ff909e7-efc8-4f69-a140-ebb18d859825",
        "table_caption": "The MeMAD Submission to the WMT18 Multimodal Translation Task Table 4: Adding automatic image captions (only the best one or all 5). The table shows BLEU scores in %. All results with Marian Amun.",
        "table_column_names": [
            "en-fr",
            "flickr16",
            "flickr17",
            "mscoco17"
        ],
        "table_content_values": [
            [
                "multi30k",
                "61.4",
                "54.0",
                "43.1"
            ],
            [
                "+autocap (dual attn.)",
                "60.9",
                "52.9",
                "43.3"
            ],
            [
                "+autocap 1 (concat)",
                "61.7",
                "53.7",
                "43.9"
            ],
            [
                "+autocap 1-5 (concat)",
                "[BOLD] 62.2",
                "[BOLD] 54.4",
                "[BOLD] 44.1"
            ],
            [
                "en-de",
                "flickr16",
                "flickr17",
                "mscoco17"
            ],
            [
                "multi30k",
                "38.9",
                "32.0",
                "27.7"
            ],
            [
                "+autocap (dual attn.)",
                "37.8",
                "30.2",
                "27.0"
            ],
            [
                "+autocap 1 (concat)",
                "39.7",
                "[BOLD] 32.2",
                "[BOLD] 28.8"
            ],
            [
                "+autocap 1-5 (concat)",
                "[BOLD] 39.9",
                "32.0",
                "28.7"
            ]
        ],
        "question": "Is it true that We can see that the dual attention model does not work at all and the scores slightly drop?",
        "answer_label": "yes"
    },
    {
        "id": "ccc54cf1-8e27-4c66-868b-a174db35f0bb",
        "table_caption": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations Table 7: Negation classifier performance for scope detection with gold cues and scope.",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Punctuation",
            "[BOLD] BiLSTM",
            "[BOLD] Proposed"
        ],
        "table_content_values": [
            [
                "In-scope (F)",
                "0.66",
                "0.88",
                "0.85"
            ],
            [
                "Out-scope (F)",
                "0.87",
                "0.97",
                "0.97"
            ],
            [
                "PCS",
                "0.52",
                "0.72",
                "0.72"
            ]
        ],
        "question": "Is it true that The results in Table 7 show that the proposed method is not as effective as the state of the art BiLSTM model from (Fancellu et al., 2016) on gold negation cues for scope prediction?",
        "answer_label": "no"
    },
    {
        "id": "8024d1af-5c05-4fe3-8254-ce70860006b0",
        "table_caption": "Entity, Relation, and Event Extraction with Contextualized Span Representations Table 7: In-domain pre-training: SciBERT vs. BERT",
        "table_column_names": [
            "[EMPTY]",
            "SciERC Entity",
            "SciERC Relation",
            "GENIA Entity"
        ],
        "table_content_values": [
            [
                "Best BERT",
                "69.8",
                "41.9",
                "78.4"
            ],
            [
                "Best SciBERT",
                "[BOLD] 72.0",
                "[BOLD] 45.3",
                "[BOLD] 79.5"
            ]
        ],
        "question": "Is it true that SciBERT does not significantly boost performance for scientific datasets including SciERC and GENIA?",
        "answer_label": "no"
    },
    {
        "id": "e0ae18f4-9bba-4ce9-83da-01228c8b4f30",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Type",
            "[BOLD] English-German #P",
            "[BOLD] English-German B",
            "[BOLD] English-German C",
            "[BOLD] English-Czech #P",
            "[BOLD] English-Czech B",
            "[BOLD] English-Czech C"
        ],
        "table_content_values": [
            [
                "BoW+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "12.2",
                "-",
                "-",
                "7.5",
                "-"
            ],
            [
                "CNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "13.7",
                "-",
                "-",
                "8.7",
                "-"
            ],
            [
                "BiRNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "16.1",
                "-",
                "-",
                "9.6",
                "-"
            ],
            [
                "PB-SMT (Beck et al.,  2018 )",
                "Single",
                "-",
                "12.8",
                "43.2",
                "-",
                "8.6",
                "36.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Single",
                "41.4M",
                "15.5",
                "40.8",
                "39.1M",
                "8.9",
                "33.8"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Single",
                "41.2M",
                "16.7",
                "42.4",
                "38.8M",
                "9.8",
                "33.3"
            ],
            [
                "DCGCN (ours)",
                "Single",
                "[BOLD]  29.7M",
                "[BOLD] 19.0",
                "[BOLD] 44.1",
                "[BOLD]  28.3M",
                "[BOLD] 12.1",
                "[BOLD] 37.1"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Ensemble",
                "207M",
                "19.0",
                "44.1",
                "195M",
                "11.3",
                "36.4"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Ensemble",
                "206M",
                "19.6",
                "45.1",
                "194M",
                "11.7",
                "35.9"
            ],
            [
                "DCGCN (ours)",
                "Ensemble",
                "[BOLD]  149M",
                "[BOLD] 20.5",
                "[BOLD] 45.8",
                "[BOLD]  142M",
                "[BOLD] 13.1",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that Our single model is not comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1/6 of theirs?",
        "answer_label": "no"
    },
    {
        "id": "31933281-32c2-4d77-9884-37546c8599f8",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "36.85",
                "5.3782",
                "35.14",
                "55.01",
                "1.6016",
                "00.34",
                "09.81",
                "00.15",
                "10.31"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen",
                "39.23",
                "6.0217",
                "36.97",
                "55.52",
                "1.7623",
                "00.40",
                "03.59",
                "00.07",
                "04.05"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen+",
                "40.25",
                "6.1448",
                "37.50",
                "56.19",
                "1.8181",
                "00.21",
                "01.99",
                "00.05",
                "02.24"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.88",
                "3.9310",
                "32.11",
                "39.90",
                "0.5036",
                "07.73",
                "17.76",
                "09.52",
                "35.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.19",
                "6.0543",
                "37.38",
                "55.88",
                "1.8104",
                "00.17",
                "01.31",
                "00.25",
                "01.72"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen",
                "40.73",
                "6.1711",
                "37.76",
                "56.09",
                "1.8518",
                "00.07",
                "00.72",
                "00.08",
                "00.87"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen+",
                "40.51",
                "6.1226",
                "37.61",
                "55.98",
                "1.8286",
                "00.02",
                "00.63",
                "00.06",
                "00.70"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.66",
                "3.9511",
                "32.93",
                "39.29",
                "0.3855",
                "07.89",
                "15.60",
                "08.44",
                "31.94"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.48",
                "6.0269",
                "37.26",
                "56.19",
                "1.7999",
                "00.43",
                "02.84",
                "00.26",
                "03.52"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen",
                "41.57",
                "6.2830",
                "37.99",
                "56.36",
                "1.8849",
                "00.37",
                "01.40",
                "00.09",
                "01.86"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen+",
                "41.56",
                "6.2700",
                "37.94",
                "56.38",
                "1.8827",
                "00.21",
                "01.04",
                "00.07",
                "01.31"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "35.99",
                "5.0734",
                "34.74",
                "54.79",
                "1.5259",
                "00.02",
                "11.58",
                "00.02",
                "11.62"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen",
                "40.07",
                "6.1243",
                "37.45",
                "55.81",
                "1.8026",
                "00.05",
                "03.23",
                "00.01",
                "03.29"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen+",
                "40.80",
                "6.2197",
                "37.86",
                "56.13",
                "1.8422",
                "00.01",
                "01.87",
                "00.01",
                "01.88"
            ]
        ],
        "question": "Is it true that The results for testing on cleaned data (Table 3, top half) confirm the positive impact of cleaned training data and also show that the cleaned test data is more challenging (cf?",
        "answer_label": "yes"
    },
    {
        "id": "e91d427d-29e5-46b3-a123-ba6111eb0525",
        "table_caption": "Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
        "table_column_names": [
            "[BOLD] ResNet-34",
            "[BOLD] Eval set %",
            "[BOLD] #param"
        ],
        "table_content_values": [
            [
                "Baseline (No SA)Anderson et al. ( 2018 )",
                "55.00",
                "0M"
            ],
            [
                "SA (S: 1,2,3 - B: 1)",
                "55.11",
                "} 0.107M"
            ],
            [
                "SA (S: 1,2,3 - B: 2)",
                "55.17",
                "} 0.107M"
            ],
            [
                "[BOLD] SA (S: 1,2,3 - B: 3)",
                "[BOLD] 55.27",
                "} 0.107M"
            ]
        ],
        "question": "Is it true that We notice no significant improvements relative to the baseline showing that self-attention alone does not improve the VQA task?",
        "answer_label": "no"
    },
    {
        "id": "d0e62762-04dc-4281-b31e-d45d3580665e",
        "table_caption": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 5: Results of the Human Rating on CWC.",
        "table_column_names": [
            "System",
            "Succ. (%)",
            "Smoothness"
        ],
        "table_content_values": [
            [
                "Retrieval-Stgy\u00a0",
                "54.0",
                "2.48"
            ],
            [
                "PMI\u00a0",
                "46.0",
                "2.56"
            ],
            [
                "Neural\u00a0",
                "36.0",
                "2.50"
            ],
            [
                "Kernel\u00a0",
                "58.0",
                "2.48"
            ],
            [
                "DKRN (ours)",
                "[BOLD] 88.0",
                "[BOLD] 3.22"
            ]
        ],
        "question": "Is it true that Our DKRN agent outperforms all other agents with a large margin?",
        "answer_label": "yes"
    },
    {
        "id": "4c635753-fc61-423d-a4b9-cbb47e724697",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
        "table_column_names": [
            "[EMPTY]",
            "Prec.",
            "Rec.",
            "F1"
        ],
        "table_content_values": [
            [
                "(A1) BiLSTM-CNN",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "(A2) Standard attention",
                "0.466",
                "0.638",
                "0.539"
            ],
            [
                "(A3) Window size ( [ITALIC] ws)=5",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "(A4) Window size ( [ITALIC] ws)=10",
                "0.510",
                "0.640",
                "0.568"
            ],
            [
                "(A5) Softmax",
                "0.490",
                "0.658",
                "0.562"
            ],
            [
                "(A6) Max-pool",
                "0.492",
                "0.600",
                "0.541"
            ]
        ],
        "question": "Is it true that When we add multi-factor attention to the baseline BiLSTM-CNN model without the dependency distance-based weight factor in the attention mechanism, we get 0.8% F1 score improvement (A2\u2212A1)?",
        "answer_label": "yes"
    },
    {
        "id": "621a2ffa-a852-4a18-87d4-c5312befefd5",
        "table_caption": "Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
        "table_column_names": [
            "Dataset",
            "Metric",
            "Illinois",
            "IlliCons",
            "rahman2012resolving",
            "KnowFeat",
            "KnowCons",
            "KnowComb"
        ],
        "table_content_values": [
            [
                "[ITALIC] Winograd",
                "Precision",
                "51.48",
                "53.26",
                "73.05",
                "71.81",
                "74.93",
                "[BOLD] 76.41"
            ],
            [
                "[ITALIC] WinoCoref",
                "AntePre",
                "68.37",
                "74.32",
                "\u2014\u2013",
                "88.48",
                "88.95",
                "[BOLD] 89.32"
            ]
        ],
        "question": "Is it true that These results show no significant performance improvement by using Predicate Schemas knowledge on hard coreference problems?",
        "answer_label": "no"
    },
    {
        "id": "65f4f4b5-a857-4cff-a33a-12fbad54d0fd",
        "table_caption": "Solving Hard Coreference Problems Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.",
        "table_column_names": [
            "Schema",
            "AntePre(Test)",
            "AntePre(Train)"
        ],
        "table_content_values": [
            [
                "Type 1",
                "76.67",
                "86.79"
            ],
            [
                "Type 2",
                "79.55",
                "88.86"
            ],
            [
                "Type 1 (Cat1)",
                "90.26",
                "93.64"
            ],
            [
                "Type 2 (Cat2)",
                "83.38",
                "92.49"
            ]
        ],
        "question": "Is it true that Also, the performance drop between Cat1/Cat2 and full data indicates that there is a need to design more complicated knowledge schemas and to refine the knowledge acquisition for further performance improvement?",
        "answer_label": "yes"
    },
    {
        "id": "9f36b848-b4e8-449c-aa79-3a4ed3e10d71",
        "table_caption": "On the difficulty of a distributional semantics of spoken language Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.",
        "table_column_names": [
            "[EMPTY]",
            "Recall@10 (%)",
            "Median rank",
            "RSAimage"
        ],
        "table_content_values": [
            [
                "VGS",
                "27",
                "6",
                "0.4"
            ],
            [
                "SegMatch",
                "[BOLD] 10",
                "[BOLD] 37",
                "[BOLD] 0.5"
            ],
            [
                "Audio2vec-U",
                "5",
                "105",
                "0.0"
            ],
            [
                "Audio2vec-C",
                "2",
                "647",
                "0.0"
            ],
            [
                "Mean MFCC",
                "1",
                "1,414",
                "0.0"
            ],
            [
                "Chance",
                "0",
                "3,955",
                "0.0"
            ]
        ],
        "question": "Is it true that Audio2vec works better than chance and mean MFCC on paraphrase retrieval, but does not correlate with the visual space?",
        "answer_label": "yes"
    },
    {
        "id": "3ccac9a1-709e-4f2a-a485-b6ecf90417cc",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
        "table_column_names": [
            "[BOLD] Method",
            "[BOLD] R-1",
            "[BOLD] R-2",
            "[BOLD] R-SU"
        ],
        "table_content_values": [
            [
                "First-1",
                "26.83",
                "7.25",
                "6.46"
            ],
            [
                "First-2",
                "35.99",
                "10.17",
                "12.06"
            ],
            [
                "First-3",
                "39.41",
                "11.77",
                "14.51"
            ],
            [
                "LexRank Erkan and Radev ( 2004 )",
                "38.27",
                "12.70",
                "13.20"
            ],
            [
                "TextRank Mihalcea and Tarau ( 2004 )",
                "38.44",
                "13.10",
                "13.50"
            ],
            [
                "MMR Carbonell and Goldstein ( 1998 )",
                "38.77",
                "11.98",
                "12.91"
            ],
            [
                "PG-Original Lebanoff et\u00a0al. ( 2018 )",
                "41.85",
                "12.91",
                "16.46"
            ],
            [
                "PG-MMR Lebanoff et\u00a0al. ( 2018 )",
                "40.55",
                "12.36",
                "15.87"
            ],
            [
                "PG-BRNN Gehrmann et\u00a0al. ( 2018 )",
                "42.80",
                "14.19",
                "16.75"
            ],
            [
                "CopyTransformer Gehrmann et\u00a0al. ( 2018 )",
                "[BOLD] 43.57",
                "14.03",
                "17.37"
            ],
            [
                "Hi-MAP (Our Model)",
                "43.47",
                "[BOLD] 14.89",
                "[BOLD] 17.41"
            ]
        ],
        "question": "Is it true that Also, we notice a drop in performance between PG-original, and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model)?",
        "answer_label": "yes"
    },
    {
        "id": "566a37c6-ec13-48a6-a1b3-116b99739810",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold\u2019s folding technique, and TensorFlow\u2019s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Training",
            "Throughput (instances/s) Training",
            "Throughput (instances/s) Training"
        ],
        "table_content_values": [
            [
                "Batch size",
                "Iter",
                "Recur",
                "Fold",
                "Iter",
                "Recur",
                "Fold"
            ],
            [
                "1",
                "19.2",
                "81.4",
                "16.5",
                "2.5",
                "4.8",
                "9.0"
            ],
            [
                "10",
                "49.3",
                "217.9",
                "52.2",
                "4.0",
                "4.2",
                "37.5"
            ],
            [
                "25",
                "72.1",
                "269.9",
                "61.6",
                "5.5",
                "3.6",
                "54.7"
            ]
        ],
        "question": "Is it true that As a result, the recursive approach performs better than the folding technique for the training task?",
        "answer_label": "no"
    },
    {
        "id": "ceeea11f-c920-442d-8462-46a12e693a9c",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 2: Ratings of annotated NLDs by human judges.",
        "table_column_names": [
            "# steps",
            "Reachability",
            "Derivability Step 1",
            "Derivability Step 2",
            "Derivability Step 3"
        ],
        "table_content_values": [
            [
                "1",
                "3.0",
                "3.8",
                "-",
                "-"
            ],
            [
                "2",
                "2.8",
                "3.8",
                "3.7",
                "-"
            ],
            [
                "3",
                "2.3",
                "3.9",
                "3.8",
                "3.8"
            ]
        ],
        "question": "Is it true that The evaluation results shown in Table 2 indicate that the annotated NLDs are of high quality (Reachability), and each NLD is properly derived from supporting documents (Derivability)?",
        "answer_label": "yes"
    },
    {
        "id": "f32c052e-7348-4b5c-86e9-8376b541c61d",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1192",
                "0.0083",
                "0.0137",
                "0.0150",
                "0.0150",
                "0.0445",
                "0.0326"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1022",
                "0.0069",
                "0.0060",
                "0.0092",
                "0.0090",
                "0.0356",
                "0.0162"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5710",
                "0.1948",
                "0.3855",
                "0.5474",
                "0.4485",
                "[BOLD] 0.8052",
                "0.4058"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "[BOLD] 0.6304",
                "0.1870",
                "0.3250",
                "0.5312",
                "0.4576",
                "0.6064",
                "0.3698"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0037",
                "0.3278",
                "0.5941",
                "0.6486",
                "[BOLD] 0.6490",
                "0.0017",
                "0.0003"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0002",
                "0.1486",
                "0.4332",
                "[BOLD] 0.6467",
                "0.6332",
                "0.0967",
                "0.0003"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0002",
                "0.1562",
                "0.5157",
                "[BOLD] 0.7255",
                "0.5932",
                "0.0032",
                "0.0001"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "2.10-5",
                "0.0507",
                "0.4492",
                "[BOLD] 0.7000",
                "0.5887",
                "0.1390",
                "0.0002"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0073",
                "0.0162",
                "0.0268",
                "[BOLD] 0.0293",
                "[BOLD] 0.0293",
                "0.0033",
                "0.0006"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0004",
                "0.0132",
                "0.0118",
                "0.0181",
                "0.0179",
                "[BOLD] 0.0520",
                "0.0005"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0005",
                "0.1733",
                "0.4412",
                "[BOLD] 0.6240",
                "0.5109",
                "0.0064",
                "0.0002"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "4.10-5",
                "0.0798",
                "0.3771",
                "[BOLD] 0.6040",
                "0.5149",
                "0.2261",
                "0.0004"
            ]
        ],
        "question": "Is it true that TF and DF achieved almost the same values of precision, recall and f-measure using the English corpora, achieving the same value of precision (P=0.0150) and f-measure (F=0.0293) when using the Europarl corpus in English?",
        "answer_label": "yes"
    },
    {
        "id": "9dcfd95a-e2f4-426a-8ccc-f49889d05521",
        "table_caption": "Keyphrase Generation for Scientific Articles using GANs Table 2: \u03b1-nDCG@5 metrics",
        "table_column_names": [
            "Model",
            "Inspec",
            "Krapivin",
            "NUS",
            "KP20k"
        ],
        "table_content_values": [
            [
                "Catseq",
                "0.87803",
                "0.781",
                "0.82118",
                "0.804"
            ],
            [
                "Catseq-RL",
                "0.8602",
                "[BOLD] 0.786",
                "0.83",
                "0.809"
            ],
            [
                "GAN",
                "[BOLD] 0.891",
                "0.771",
                "[BOLD] 0.853",
                "[BOLD] 0.85"
            ]
        ],
        "question": "Is it true that Our model obtains the best performance on three out of the four datasets?",
        "answer_label": "yes"
    },
    {
        "id": "7e3de05e-1093-426a-9ad5-e6929654530d",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that The first block in Table 6 shows the performance of our two baseline models: multi-layer GCNs with residual connections (GCN+RC) and multi-layer GCNs with both residual connections and layer aggregations (GCN+RC+LA)?",
        "answer_label": "yes"
    },
    {
        "id": "9ca997f9-4c36-49e9-b0ed-d0b1bcbdcee6",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
        "table_column_names": [
            "[BOLD] Training data",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] Disfl"
        ],
        "table_content_values": [
            [
                "Original",
                "0",
                "22",
                "0",
                "14"
            ],
            [
                "Cleaned added",
                "0",
                "23",
                "0",
                "14"
            ],
            [
                "Cleaned missing",
                "0",
                "1",
                "0",
                "2"
            ],
            [
                "Cleaned",
                "0",
                "0",
                "0",
                "5"
            ]
        ],
        "question": "Is it true that The results in Table 4 confirm the findings of the automatic [CONTINUE] metrics: systems trained on the fully cleaned set or the set with cleaned missing slots have nearperfect performance, with the fully-cleaned one showing a few more slight disfluencies than the other?",
        "answer_label": "yes"
    },
    {
        "id": "5f50a2cd-c027-4ecb-b169-e26fcc59005f",
        "table_caption": "Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
        "table_column_names": [
            "Dataset",
            "Metric",
            "Illinois",
            "IlliCons",
            "rahman2012resolving",
            "KnowFeat",
            "KnowCons",
            "KnowComb"
        ],
        "table_content_values": [
            [
                "[ITALIC] Winograd",
                "Precision",
                "51.48",
                "53.26",
                "73.05",
                "71.81",
                "74.93",
                "[BOLD] 76.41"
            ],
            [
                "[ITALIC] WinoCoref",
                "AntePre",
                "68.37",
                "74.32",
                "\u2014\u2013",
                "88.48",
                "88.95",
                "[BOLD] 89.32"
            ]
        ],
        "question": "Is it true that It does not improve by over 20% over a state-of-art general coreference system on Winograd and also does not outperform Rahman and Ng (2012) by a margin of 3.3%?",
        "answer_label": "no"
    },
    {
        "id": "67b478a8-a83d-4700-8567-f8550bcce109",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 3: ARI and Silhouette coefficient scores.",
        "table_column_names": [
            "Methods",
            "Seanad Abolition ARI",
            "Seanad Abolition  [ITALIC] Sil",
            "Video Games ARI",
            "Video Games  [ITALIC] Sil",
            "Pornography ARI",
            "Pornography  [ITALIC] Sil"
        ],
        "table_content_values": [
            [
                "TF-IDF",
                "0.23",
                "0.02",
                "-0.01",
                "0.01",
                "-0.02",
                "0.01"
            ],
            [
                "WMD",
                "0.09",
                "0.01",
                "0.01",
                "0.01",
                "-0.02",
                "0.01"
            ],
            [
                "Sent2vec",
                "-0.01",
                "-0.01",
                "0.11",
                "0.06",
                "0.01",
                "0.02"
            ],
            [
                "Doc2vec",
                "-0.01",
                "-0.03",
                "-0.01",
                "0.01",
                "0.02",
                "-0.01"
            ],
            [
                "BERT",
                "0.03",
                "-0.04",
                "0.08",
                "0.05",
                "-0.01",
                "0.03"
            ],
            [
                "OD-parse",
                "0.01",
                "-0.04",
                "-0.01",
                "0.02",
                "0.07",
                "0.05"
            ],
            [
                "OD",
                "[BOLD] 0.54",
                "[BOLD] 0.31",
                "[BOLD] 0.56",
                "[BOLD] 0.42",
                "[BOLD] 0.41",
                "[BOLD] 0.41"
            ]
        ],
        "question": "Is it true that among opinions: We see that OD significantly outperforms the baseline methods and the OD-parse variant [CONTINUE] OD achieves high ARI and Sil scores, [CONTINUE] From the above table, we observe that the text-similarity based baselines, such as TF-IDF, WMD and Doc2vec achieving ARI and Silhouette coefficient scores of close to zero on the \"Video Games\" and \"Pornography\" datasets (barely providing a performance improvement over random clustering, i.e., a zero ARI score)?",
        "answer_label": "yes"
    },
    {
        "id": "acab10a0-f7f1-409b-89d5-9e8b256c3c2d",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that In particular, our single DCGCN model does not consistently outperform Seq2Seq models when trained without external resources?",
        "answer_label": "no"
    },
    {
        "id": "ce13f0c9-f310-4589-ab1a-1ab472a1d338",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
        "table_column_names": [
            "<bold>Model</bold>",
            "R",
            "MUC P",
            "<italic>F</italic>1",
            "R",
            "B3 P",
            "<italic>F</italic>1",
            "R",
            "CEAF-<italic>e</italic> P",
            "<italic>F</italic>1",
            "CoNLL <italic>F</italic>1"
        ],
        "table_content_values": [
            [
                "Cluster+Lemma",
                "71.3",
                "83",
                "76.7",
                "53.4",
                "84.9",
                "65.6",
                "70.1",
                "52.5",
                "60",
                "67.4"
            ],
            [
                "Disjoint",
                "76.7",
                "80.8",
                "78.7",
                "63.2",
                "78.2",
                "69.9",
                "65.3",
                "58.3",
                "61.6",
                "70"
            ],
            [
                "Joint",
                "78.6",
                "80.9",
                "79.7",
                "65.5",
                "76.4",
                "70.5",
                "65.4",
                "61.3",
                "63.3",
                "<bold>71.2</bold>"
            ]
        ],
        "question": "Is it true that Our joint model improves upon the strong lemma baseline by 3.8 points in CoNLL F1 score?",
        "answer_label": "yes"
    },
    {
        "id": "8aac774b-9ded-41b0-8070-26614c5200f2",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
        "table_column_names": [
            "Model",
            "Method",
            "Training Data",
            "Overall",
            "Easy",
            "Hard",
            "p-value (%)"
        ],
        "table_content_values": [
            [
                "goodwin-etal-2012-utdhlt",
                "PMI",
                "unsupervised",
                "61.8",
                "64.7",
                "60.0",
                "19.8"
            ],
            [
                "gordon_commonsense_2011-1",
                "PMI",
                "unsupervised",
                "65.4",
                "65.8",
                "65.2",
                "83.5"
            ],
            [
                "sasaki-etal-2017-handling",
                "PMI",
                "unsupervised",
                "71.4",
                "75.3",
                "69.0",
                "4.8\u2217"
            ],
            [
                "Word frequency",
                "wordfreq",
                "COPA",
                "53.5",
                "57.4",
                "51.3",
                "9.8"
            ],
            [
                "BERT-large-FT",
                "LM, NSP",
                "COPA",
                "76.5 (\u00b1 2.7)",
                "83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)",
                "0.0\u2217"
            ],
            [
                "RoBERTa-large-FT",
                "LM",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)",
                "0.0\u2217"
            ]
        ],
        "question": "Is it true that The difference between accuracy on Easy and Hard is less pronounced for RoBERTa, but still suggests some reliance on superficial cues?",
        "answer_label": "yes"
    },
    {
        "id": "4e46bf68-7ddc-4376-b0b7-b5143661ea93",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Original",
                "TGen\u2212",
                "63.37",
                "7.7188",
                "41.99",
                "68.53",
                "1.9355",
                "00.06",
                "15.77",
                "00.11",
                "15.94"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen",
                "66.41",
                "8.5565",
                "45.07",
                "69.17",
                "2.2253",
                "00.14",
                "04.11",
                "00.03",
                "04.27"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen+",
                "67.06",
                "8.5871",
                "45.83",
                "69.73",
                "2.2681",
                "00.04",
                "01.75",
                "00.01",
                "01.80"
            ],
            [
                "Original",
                "[BOLD] Original",
                "SC-LSTM",
                "39.11",
                "5.6704",
                "36.83",
                "50.02",
                "0.6045",
                "02.79",
                "18.90",
                "09.79",
                "31.51"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen\u2212",
                "65.87",
                "8.6400",
                "44.20",
                "67.51",
                "2.1710",
                "00.20",
                "00.56",
                "00.21",
                "00.97"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen",
                "66.24",
                "8.6889",
                "44.66",
                "67.85",
                "2.2181",
                "00.10",
                "00.02",
                "00.00",
                "00.12"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen+",
                "65.97",
                "8.6630",
                "44.45",
                "67.59",
                "2.1855",
                "00.02",
                "00.00",
                "00.00",
                "00.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "SC-LSTM",
                "38.52",
                "5.7125",
                "37.45",
                "48.50",
                "0.4343",
                "03.85",
                "17.39",
                "08.12",
                "29.37"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen\u2212",
                "66.28",
                "8.5202",
                "43.96",
                "67.83",
                "2.1375",
                "00.14",
                "02.26",
                "00.22",
                "02.61"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen",
                "67.00",
                "8.6889",
                "44.97",
                "68.19",
                "2.2228",
                "00.06",
                "00.44",
                "00.03",
                "00.53"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen+",
                "66.74",
                "8.6649",
                "44.84",
                "67.95",
                "2.2018",
                "00.00",
                "00.21",
                "00.03",
                "00.24"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen\u2212",
                "64.40",
                "7.9692",
                "42.81",
                "68.87",
                "2.0563",
                "00.01",
                "13.08",
                "00.00",
                "13.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen",
                "66.23",
                "8.5578",
                "45.12",
                "68.87",
                "2.2548",
                "00.04",
                "03.04",
                "00.00",
                "03.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen+",
                "65.96",
                "8.5238",
                "45.49",
                "68.79",
                "2.2456",
                "00.00",
                "01.44",
                "00.00",
                "01.45"
            ]
        ],
        "question": "Is it true that The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is not effective for both WOMs and SER, whereas the SC-LSTM seems to have difficulty scaling to the E2E dataset?",
        "answer_label": "no"
    },
    {
        "id": "b70cedda-3704-43f2-9ec0-cbf50e7bab85",
        "table_caption": "Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability Table 1: Accuracy obtained by training an standalone classifier, applying the API and the proposed wrapper for each domain",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] BB source acc.",
            "[BOLD] BB target acc.",
            "[BOLD] Non-reject. acc. (10/20/30%)",
            "[BOLD] Class. quality (10/20/30%)",
            "[BOLD] Reject. quality (10/20/30%)"
        ],
        "table_content_values": [
            [
                "[BOLD] Apply Yelp BB to SST-2",
                "89.18\u00b10.08%",
                "77.13\u00b10.52%",
                "82.43\u00b10.22% 88.19\u00b10.50% 93.60\u00b10.16%",
                "80.40\u00b10.39% 83.11\u00b10.80% 83.05\u00b10.23%",
                "6.03\u00b10.45 6.04\u00b10.51 4.97\u00b10.07"
            ],
            [
                "[BOLD] Apply SST-2 BB to Yelp",
                "83.306\u00b10.18%",
                "82.106\u00b10.88%",
                "87,98\u00b10.18% 92.13\u00b10.38% 94.19\u00b10.33%",
                "85.49\u00b10.88% 84.53\u00b10.38% 78.99\u00b10.46%",
                "8.30\u00b11.63 5.72\u00b10.27 3.73\u00b10.10"
            ],
            [
                "[BOLD] Apply Electronics BB to Music",
                "86.39\u00b10.22%",
                "90.38\u00b10.13%",
                "95.04\u00b10.43% 96.45\u00b10.35% 97.26\u00b10.31%",
                "90.67\u00b10.88% 83.93\u00b10.67% 75.77\u00b10.54%",
                "10.7\u00b11.65 4.82\u00b10.35 3.25\u00b10.14"
            ],
            [
                "[BOLD] Apply Music BB to Electronics",
                "93.10\u00b10.02%",
                "79.85\u00b10.0%",
                "83.26\u00b10.41% 87.06\u00b10.55% 90.50\u00b10.29%",
                "79.97\u00b10.74% 79.93\u00b10.87% 76.81\u00b10.41%",
                "4.1\u00b10.55 3.80\u00b10.35 3.32\u00b10.09"
            ]
        ],
        "question": "Is it true that In general terms, the results displayed in table 1 show that the rejection method cannot reduce the error of the output predictions when applying a pre-trained black-box classification system to a new domain?",
        "answer_label": "no"
    },
    {
        "id": "0d4a8de9-4bda-499e-a51c-870a1fed2a55",
        "table_caption": "Argument Generation with Retrieval, Planning, and Realization Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
        "table_column_names": [
            "[EMPTY]",
            "[ITALIC] w/ System Retrieval  [BOLD] B-2",
            "[ITALIC] w/ System Retrieval  [BOLD] B-4",
            "[ITALIC] w/ System Retrieval  [BOLD] R-2",
            "[ITALIC] w/ System Retrieval  [BOLD] MTR",
            "[ITALIC] w/ System Retrieval  [BOLD] #Word",
            "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
        ],
        "table_content_values": [
            [
                "Human",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22"
            ],
            [
                "Retrieval",
                "7.55",
                "1.11",
                "8.64",
                "14.38",
                "123",
                "23",
                "10.97",
                "3.05",
                "23.49",
                "20.08",
                "140",
                "21"
            ],
            [
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Seq2seq",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15"
            ],
            [
                "Seq2seqAug",
                "8.26",
                "2.24",
                "13.79",
                "15.75",
                "78",
                "14",
                "10.98",
                "4.41",
                "22.97",
                "19.62",
                "71",
                "14"
            ],
            [
                "[ITALIC] w/o psg",
                "7.94",
                "2.28",
                "10.13",
                "15.71",
                "75",
                "12",
                "9.89",
                "3.34",
                "14.20",
                "18.40",
                "66",
                "12"
            ],
            [
                "H&W\u00a0Hua and Wang ( 2018 )",
                "3.64",
                "0.92",
                "8.83",
                "11.78",
                "51",
                "12",
                "8.51",
                "2.86",
                "18.89",
                "17.18",
                "58",
                "12"
            ],
            [
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "CANDELA",
                "12.02\u2217",
                "[BOLD] 2.99\u2217",
                "[BOLD] 14.93\u2217",
                "[BOLD] 16.92\u2217",
                "119",
                "22",
                "15.80\u2217",
                "[BOLD] 5.00\u2217",
                "[BOLD] 23.75",
                "[BOLD] 20.18",
                "116",
                "22"
            ],
            [
                "[ITALIC] w/o psg",
                "[BOLD] 12.33\u2217",
                "2.86\u2217",
                "14.53\u2217",
                "16.60\u2217",
                "123",
                "23",
                "[BOLD] 16.33\u2217",
                "4.98\u2217",
                "23.65",
                "19.94",
                "123",
                "23"
            ]
        ],
        "question": "Is it true that Under oracle setup, all models are notably improved due to the higher quality of reranked passages, but our model does not achieve statistically significantly better BLEU scores?",
        "answer_label": "no"
    },
    {
        "id": "d661490a-948e-4b22-ad8e-4d11b28c00cb",
        "table_caption": "Solving Hard Coreference Problems Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.",
        "table_column_names": [
            "System",
            "MUC",
            "BCUB",
            "CEAFe",
            "AVG"
        ],
        "table_content_values": [
            [
                "ACE",
                "ACE",
                "ACE",
                "ACE",
                "ACE"
            ],
            [
                "IlliCons",
                "[BOLD] 78.17",
                "81.64",
                "[BOLD] 78.45",
                "[BOLD] 79.42"
            ],
            [
                "KnowComb",
                "77.51",
                "[BOLD] 81.97",
                "77.44",
                "78.97"
            ],
            [
                "OntoNotes",
                "OntoNotes",
                "OntoNotes",
                "OntoNotes",
                "OntoNotes"
            ],
            [
                "IlliCons",
                "84.10",
                "[BOLD] 78.30",
                "[BOLD] 68.74",
                "[BOLD] 77.05"
            ],
            [
                "KnowComb",
                "[BOLD] 84.33",
                "78.02",
                "67.95",
                "76.76"
            ]
        ],
        "question": "Is it true that Our KnowComb system does not achieve the same level of performance as the state-of-art general coreference system we base it on?",
        "answer_label": "no"
    },
    {
        "id": "45d2802f-6f82-4ee9-96f9-462ec333cbc2",
        "table_caption": "Distant Learning for Entity Linking with Automatic Noise Detection Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)",
        "table_column_names": [
            "System",
            "All LOC",
            "All ORG",
            "All PER",
            "All MISC",
            "In  [ITALIC] E+ LOC",
            "In  [ITALIC] E+ ORG",
            "In  [ITALIC] E+ PER",
            "In  [ITALIC] E+ MISC"
        ],
        "table_content_values": [
            [
                "Name matching",
                "96.26",
                "89.48",
                "57.38",
                "96.60",
                "92.32",
                "76.87",
                "47.40",
                "76.29"
            ],
            [
                "MIL",
                "57.09",
                "[BOLD] 76.30",
                "41.35",
                "93.35",
                "11.90",
                "[BOLD] 47.90",
                "27.60",
                "53.61"
            ],
            [
                "MIL-ND",
                "57.15",
                "77.15",
                "35.95",
                "92.47",
                "12.02",
                "49.77",
                "20.94",
                "47.42"
            ],
            [
                "[ITALIC] \u03c4MIL-ND",
                "[BOLD] 55.15",
                "76.56",
                "[BOLD] 34.03",
                "[BOLD] 92.15",
                "[BOLD] 11.14",
                "51.18",
                "[BOLD] 20.59",
                "[BOLD] 40.00"
            ],
            [
                "Supervised learning",
                "55.58",
                "61.32",
                "24.98",
                "89.96",
                "8.80",
                "14.95",
                "7.40",
                "29.90"
            ]
        ],
        "question": "Is it true that [CONTINUE] For LOC, it turns out that candidate selection is a bottleneck: when candidate selection was flawless, the models made only about 12% errors, down from about 57%?",
        "answer_label": "yes"
    },
    {
        "id": "8c6099dc-368a-44c2-8051-2af00a3c8bdd",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
        "table_column_names": [
            "[EMPTY]",
            "Difference Function",
            "Seanad Abolition",
            "Video Games",
            "Pornography"
        ],
        "table_content_values": [
            [
                "OD-parse",
                "Absolute",
                "0.01",
                "-0.01",
                "0.07"
            ],
            [
                "OD-parse",
                "JS div.",
                "0.01",
                "-0.01",
                "-0.01"
            ],
            [
                "OD-parse",
                "EMD",
                "0.07",
                "0.01",
                "-0.01"
            ],
            [
                "OD",
                "Absolute",
                "[BOLD] 0.54",
                "[BOLD] 0.56",
                "[BOLD] 0.41"
            ],
            [
                "OD",
                "JS div.",
                "0.07",
                "-0.01",
                "-0.02"
            ],
            [
                "OD",
                "EMD",
                "0.26",
                "-0.01",
                "0.01"
            ],
            [
                "OD (no polarity shifters)",
                "Absolute",
                "0.23",
                "0.08",
                "0.04"
            ],
            [
                "OD (no polarity shifters)",
                "JS div.",
                "0.09",
                "-0.01",
                "-0.02"
            ],
            [
                "OD (no polarity shifters)",
                "EMD",
                "0.10",
                "0.01",
                "-0.01"
            ]
        ],
        "question": "Is it true that [CONTINUE] Sentiment polarity shifters have a high impact on clustering performance of opinion distance: We find that not utilizing the sentiment polarity shifters, especially in case of datasets \"Video games\" and \"Pornography\" hurts the Opinion Representation phase, and thereby leads to incorrect computation of opinion distance?",
        "answer_label": "yes"
    },
    {
        "id": "aa9eb3ce-acdd-4019-b442-f8b929d15b84",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
        "table_column_names": [
            "[EMPTY]",
            "Difference Function",
            "Seanad Abolition",
            "Video Games",
            "Pornography"
        ],
        "table_content_values": [
            [
                "OD-parse",
                "Absolute",
                "0.01",
                "-0.01",
                "0.07"
            ],
            [
                "OD-parse",
                "JS div.",
                "0.01",
                "-0.01",
                "-0.01"
            ],
            [
                "OD-parse",
                "EMD",
                "0.07",
                "0.01",
                "-0.01"
            ],
            [
                "OD",
                "Absolute",
                "[BOLD] 0.54",
                "[BOLD] 0.56",
                "[BOLD] 0.41"
            ],
            [
                "OD",
                "JS div.",
                "0.07",
                "-0.01",
                "-0.02"
            ],
            [
                "OD",
                "EMD",
                "0.26",
                "-0.01",
                "0.01"
            ],
            [
                "OD (no polarity shifters)",
                "Absolute",
                "0.23",
                "0.08",
                "0.04"
            ],
            [
                "OD (no polarity shifters)",
                "JS div.",
                "0.09",
                "-0.01",
                "-0.02"
            ],
            [
                "OD (no polarity shifters)",
                "EMD",
                "0.10",
                "0.01",
                "-0.01"
            ]
        ],
        "question": "Is it true that [CONTINUE] OD does not significantly outperform OD-parse: We observe that compared to OD-parse, OD is not significantly more accurate?",
        "answer_label": "no"
    },
    {
        "id": "474a4ee7-88be-4e91-abc9-7f5d22b64f62",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.",
        "table_column_names": [
            "Metric",
            "Method of validation",
            "Yelp",
            "Lit."
        ],
        "table_content_values": [
            [
                "Acc",
                "% of machine and human judgments that match",
                "94",
                "84"
            ],
            [
                "Sim",
                "Spearman\u2019s  [ITALIC] \u03c1 b/w Sim and human ratings of semantic preservation",
                "0.79",
                "0.75"
            ],
            [
                "PP",
                "Spearman\u2019s  [ITALIC] \u03c1 b/w negative PP and human ratings of fluency",
                "0.81",
                "0.67"
            ]
        ],
        "question": "Is it true that To validate Acc, human annotators were asked to judge the style of 150 transferred sentences. We then compute the percentage of machine and human judgments that match?",
        "answer_label": "no"
    },
    {
        "id": "238007ba-b7a6-4b65-9173-b00fdafd9bf2",
        "table_caption": "Sparse and Structured Visual Attention Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
        "table_column_names": [
            "[EMPTY]",
            "MSCOCO spice",
            "MSCOCO cider",
            "MSCOCO rouge [ITALIC] L",
            "MSCOCO bleu4",
            "MSCOCO meteor",
            "MSCOCO rep\u2193",
            "Flickr30k spice",
            "Flickr30k cider",
            "Flickr30k rouge [ITALIC] L",
            "Flickr30k bleu4",
            "Flickr30k meteor",
            "Flickr30k rep\u2193"
        ],
        "table_content_values": [
            [
                "softmax",
                "18.4",
                "0.967",
                "52.9",
                "29.9",
                "24.9",
                "3.76",
                "13.5",
                "0.443",
                "44.2",
                "19.9",
                "19.1",
                "6.09"
            ],
            [
                "sparsemax",
                "[BOLD] 18.9",
                "[BOLD] 0.990",
                "[BOLD] 53.5",
                "[BOLD] 31.5",
                "[BOLD] 25.3",
                "3.69",
                "[BOLD] 13.7",
                "[BOLD] 0.444",
                "[BOLD] 44.3",
                "[BOLD] 20.7",
                "[BOLD] 19.3",
                "5.84"
            ],
            [
                "TVmax",
                "18.5",
                "0.974",
                "53.1",
                "29.9",
                "25.1",
                "[BOLD] 3.17",
                "13.3",
                "0.438",
                "44.2",
                "20.5",
                "19.0",
                "[BOLD] 3.97"
            ]
        ],
        "question": "Is it true that [CONTINUE] Moreover, for TVMAX, automatic metrics results are slightly worse than sparsemax and significantly worse than softmax on MSCOCO and similar on Flickr30k?",
        "answer_label": "no"
    },
    {
        "id": "97d1d7a0-87be-4a17-92c0-da0ee295a374",
        "table_caption": "Argument Generation with Retrieval, Planning, and Realization Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
        "table_column_names": [
            "[EMPTY]",
            "[ITALIC] w/ System Retrieval  [BOLD] B-2",
            "[ITALIC] w/ System Retrieval  [BOLD] B-4",
            "[ITALIC] w/ System Retrieval  [BOLD] R-2",
            "[ITALIC] w/ System Retrieval  [BOLD] MTR",
            "[ITALIC] w/ System Retrieval  [BOLD] #Word",
            "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
        ],
        "table_content_values": [
            [
                "Human",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22"
            ],
            [
                "Retrieval",
                "7.55",
                "1.11",
                "8.64",
                "14.38",
                "123",
                "23",
                "10.97",
                "3.05",
                "23.49",
                "20.08",
                "140",
                "21"
            ],
            [
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Seq2seq",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15"
            ],
            [
                "Seq2seqAug",
                "8.26",
                "2.24",
                "13.79",
                "15.75",
                "78",
                "14",
                "10.98",
                "4.41",
                "22.97",
                "19.62",
                "71",
                "14"
            ],
            [
                "[ITALIC] w/o psg",
                "7.94",
                "2.28",
                "10.13",
                "15.71",
                "75",
                "12",
                "9.89",
                "3.34",
                "14.20",
                "18.40",
                "66",
                "12"
            ],
            [
                "H&W\u00a0Hua and Wang ( 2018 )",
                "3.64",
                "0.92",
                "8.83",
                "11.78",
                "51",
                "12",
                "8.51",
                "2.86",
                "18.89",
                "17.18",
                "58",
                "12"
            ],
            [
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "CANDELA",
                "12.02\u2217",
                "[BOLD] 2.99\u2217",
                "[BOLD] 14.93\u2217",
                "[BOLD] 16.92\u2217",
                "119",
                "22",
                "15.80\u2217",
                "[BOLD] 5.00\u2217",
                "[BOLD] 23.75",
                "[BOLD] 20.18",
                "116",
                "22"
            ],
            [
                "[ITALIC] w/o psg",
                "[BOLD] 12.33\u2217",
                "2.86\u2217",
                "14.53\u2217",
                "16.60\u2217",
                "123",
                "23",
                "[BOLD] 16.33\u2217",
                "4.98\u2217",
                "23.65",
                "19.94",
                "123",
                "23"
            ]
        ],
        "question": "Is it true that Surprisingly, we observe a decrease of BLEU-2, BLEU-4, ROUGE-2, and METEOR when removing passages from our model input?",
        "answer_label": "no"
    },
    {
        "id": "2f5f8672-0cea-486d-bf9b-88ee35183cc5",
        "table_caption": "Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
        "table_column_names": [
            "Dataset",
            "Metric",
            "Illinois",
            "IlliCons",
            "rahman2012resolving",
            "KnowFeat",
            "KnowCons",
            "KnowComb"
        ],
        "table_content_values": [
            [
                "[ITALIC] Winograd",
                "Precision",
                "51.48",
                "53.26",
                "73.05",
                "71.81",
                "74.93",
                "[BOLD] 76.41"
            ],
            [
                "[ITALIC] WinoCoref",
                "AntePre",
                "68.37",
                "74.32",
                "\u2014\u2013",
                "88.48",
                "88.95",
                "[BOLD] 89.32"
            ]
        ],
        "question": "Is it true that The results also show that it is better to compile knowledge into constraints when the knowledge quality is high than add them as features?",
        "answer_label": "yes"
    },
    {
        "id": "715f5a99-6ca2-4117-b4df-a55d7bb5833e",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that Consequently, with an 8% improvement on average, the hybrid model [CONTINUE] Word Content are increased?",
        "answer_label": "yes"
    },
    {
        "id": "bded69dc-786d-4fc5-83d4-d2e766361785",
        "table_caption": "Evaluation of Greek Word Embeddings Table 3: Summary for 3CosAdd and top-1 nearest vectors.",
        "table_column_names": [
            "Category Semantic",
            "Category no oov words",
            "gr_def 58.42%",
            "gr_neg10 59.33%",
            "cc.el.300  [BOLD] 68.80%",
            "wiki.el 27.20%",
            "gr_cbow_def 31.76%",
            "gr_d300_nosub 60.79%",
            "gr_w2v_sg_n5 52.70%"
        ],
        "table_content_values": [
            [
                "[EMPTY]",
                "with oov words",
                "52.97%",
                "55.33%",
                "[BOLD] 64.34%",
                "25.73%",
                "28.80%",
                "55.11%",
                "47.82%"
            ],
            [
                "Syntactic",
                "no oov words",
                "65.73%",
                "61.02%",
                "[BOLD] 69.35%",
                "40.90%",
                "64.02%",
                "53.69%",
                "52.60%"
            ],
            [
                "[EMPTY]",
                "with oov words",
                "[BOLD] 53.95%",
                "48.69%",
                "49.43%",
                "28.42%",
                "52.54%",
                "44.06%",
                "43.13%"
            ],
            [
                "Overall",
                "no oov words",
                "63.02%",
                "59.96%",
                "[BOLD] 68.97%",
                "36.45%",
                "52.04%",
                "56.30%",
                "52.66%"
            ],
            [
                "[EMPTY]",
                "with oov words",
                "53.60%",
                "51.00%",
                "[BOLD] 54.60%",
                "27.50%",
                "44.30%",
                "47.90%",
                "44.80%"
            ]
        ],
        "question": "Is it true that Model wiki.el, trained only on Wikipedia, was the best in the category semantic with no oov words and the overall category with oov words?",
        "answer_label": "no"
    },
    {
        "id": "3e5908f5-1d9d-47a6-8e48-a0822530dbdb",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "Base ACC",
            "Base Time",
            "+LN ACC",
            "+LN Time",
            "+BERT ACC",
            "+BERT Time",
            "+LN+BERT ACC",
            "+LN+BERT Time"
        ],
        "table_content_values": [
            [
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "250K",
                "83.50",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-"
            ],
            [
                "This",
                "LSTM",
                "8.36M",
                "84.27",
                "0.262",
                "86.03",
                "0.432",
                "89.95",
                "0.544",
                "[BOLD] 90.49",
                "0.696"
            ],
            [
                "This",
                "GRU",
                "6.41M",
                "[BOLD] 85.71",
                "0.245",
                "[BOLD] 86.05",
                "0.419",
                "[BOLD] 90.29",
                "0.529",
                "90.10",
                "0.695"
            ],
            [
                "This",
                "ATR",
                "2.87M",
                "84.88",
                "0.210",
                "85.81",
                "0.307",
                "90.00",
                "0.494",
                "90.28",
                "0.580"
            ],
            [
                "Work",
                "SRU",
                "5.48M",
                "84.28",
                "0.258",
                "85.32",
                "0.283",
                "89.98",
                "0.543",
                "90.09",
                "0.555"
            ],
            [
                "[EMPTY]",
                "LRN",
                "4.25M",
                "84.88",
                "[BOLD] 0.209",
                "85.06",
                "[BOLD] 0.223",
                "89.98",
                "[BOLD] 0.488",
                "89.93",
                "[BOLD] 0.506"
            ]
        ],
        "question": "Is it true that [CONTINUE] Results with BERT show that contextual information is valuable for performance improvement?",
        "answer_label": "yes"
    },
    {
        "id": "d39ee230-80e9-4d79-ae14-922b3fc922e4",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 4: Experiment 2, t= \u201cb*tch\u201d",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.010",
                "0.010",
                "-0.632",
                "[EMPTY]",
                "0.978"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.963",
                "0.944",
                "20.064",
                "***",
                "1.020"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.011",
                "0.011",
                "-1.254",
                "[EMPTY]",
                "0.955"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.349",
                "0.290",
                "28.803",
                "***",
                "1.203"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.012",
                "0.012",
                "-0.162",
                "[EMPTY]",
                "0.995"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.017",
                "0.015",
                "4.698",
                "***",
                "1.152"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.988",
                "0.991",
                "-6.289",
                "***",
                "0.997"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.099",
                "0.091",
                "6.273",
                "***",
                "1.091"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.074",
                "0.027",
                "46.054",
                "***",
                "2.728"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.925",
                "0.968",
                "-41.396",
                "***",
                "0.956"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.010",
                "0.010",
                "0.000",
                "[EMPTY]",
                "1.000"
            ]
        ],
        "question": "Is it true that We see different results for Waseem and Hovy (2016) and Waseem (2016)?",
        "answer_label": "no"
    },
    {
        "id": "1563ec9d-c56c-4d98-b401-792e93c5a56d",
        "table_caption": "Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
        "table_column_names": [
            "Dataset",
            "Metric",
            "Illinois",
            "IlliCons",
            "rahman2012resolving",
            "KnowFeat",
            "KnowCons",
            "KnowComb"
        ],
        "table_content_values": [
            [
                "[ITALIC] Winograd",
                "Precision",
                "51.48",
                "53.26",
                "73.05",
                "71.81",
                "74.93",
                "[BOLD] 76.41"
            ],
            [
                "[ITALIC] WinoCoref",
                "AntePre",
                "68.37",
                "74.32",
                "\u2014\u2013",
                "88.48",
                "88.95",
                "[BOLD] 89.32"
            ]
        ],
        "question": "Is it true that It improves by over 20% over a state-of-art general coreference system on Winograd and also outperforms Rahman and Ng (2012) by a margin of 3.3%?",
        "answer_label": "yes"
    },
    {
        "id": "584be382-d99a-4b4e-92c5-bcdb3ef882e9",
        "table_caption": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation Table 4: The ablation study on the WoZ2.0 dataset with the joint goal accuracy on the test set. For \u201c- Hierachical-Attn\u201d, we remove the residual connections between the attention modules in the CMR decoders and all the attention memory access are based on the output from the LSTM. For \u201c- MLP\u201d, we further replace the MLP with a single linear layer with the non-linear activation.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Joint Acc."
        ],
        "table_content_values": [
            [
                "COMER",
                "88.64%"
            ],
            [
                "- Hierachical-Attn",
                "86.69%"
            ],
            [
                "- MLP",
                "83.24%"
            ]
        ],
        "question": "Is it true that [CONTINUE] The effectiveness of our hierarchical attention design is disproved by an accuracy drop of only 1.95% after removing residual connections and the hierarchical stack of our attention modules?",
        "answer_label": "no"
    },
    {
        "id": "e3125f04-5ee3-4b83-957b-861f893cd399",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.",
        "table_column_names": [
            "Uni",
            "POS",
            "0 87.9",
            "1 92.0",
            "2 91.7",
            "3 91.8",
            "4 91.9"
        ],
        "table_content_values": [
            [
                "Uni",
                "SEM",
                "81.8",
                "87.8",
                "87.4",
                "87.6",
                "88.2"
            ],
            [
                "Bi",
                "POS",
                "87.9",
                "93.3",
                "92.9",
                "93.2",
                "92.8"
            ],
            [
                "Bi",
                "SEM",
                "81.9",
                "91.3",
                "90.8",
                "91.9",
                "91.9"
            ],
            [
                "Res",
                "POS",
                "87.9",
                "92.5",
                "91.9",
                "92.0",
                "92.4"
            ],
            [
                "Res",
                "SEM",
                "81.9",
                "88.2",
                "87.5",
                "87.6",
                "88.5"
            ]
        ],
        "question": "Is it true that Comparing POS and SEM tagging (Table 5), we note that higher layer representations do not necessarily improve SEM tagging, while POS tagging does not peak at layer 1. We noticed no improvements in both translation (+0.9 BLEU) and POS and SEM tagging (up to +0.6% accuracy) when using features extracted from an NMT model trained with residual connections (Table 5)?",
        "answer_label": "no"
    },
    {
        "id": "9be65c57-384f-4389-a499-15fd4ac3ff16",
        "table_caption": "Semantic Neural Machine Translation using AMR Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.",
        "table_column_names": [
            "System",
            "NC-v11 BLEU",
            "NC-v11 TER\u2193",
            "NC-v11 Meteor",
            "Full BLEU",
            "Full TER\u2193",
            "Full Meteor"
        ],
        "table_content_values": [
            [
                "OpenNMT-tf",
                "15.1",
                "0.6902",
                "0.3040",
                "24.3",
                "0.5567",
                "0.4225"
            ],
            [
                "Transformer-tf",
                "17.1",
                "0.6647",
                "0.3578",
                "25.1",
                "0.5537",
                "0.4344"
            ],
            [
                "Seq2seq",
                "16.0",
                "0.6695",
                "0.3379",
                "23.7",
                "0.5590",
                "0.4258"
            ],
            [
                "Dual2seq-LinAMR",
                "17.3",
                "0.6530",
                "0.3612",
                "24.0",
                "0.5643",
                "0.4246"
            ],
            [
                "Duel2seq-SRL",
                "17.2",
                "0.6591",
                "0.3644",
                "23.8",
                "0.5626",
                "0.4223"
            ],
            [
                "Dual2seq-Dep",
                "17.8",
                "0.6516",
                "0.3673",
                "25.0",
                "0.5538",
                "0.4328"
            ],
            [
                "Dual2seq",
                "[BOLD] *19.2*",
                "[BOLD] 0.6305",
                "[BOLD] 0.3840",
                "[BOLD] *25.5*",
                "[BOLD] 0.5480",
                "[BOLD] 0.4376"
            ]
        ],
        "question": "Is it true that Dual2seq-LinAMR shows much worse performance than our Dual2seq model and significantly outperforms the Seq2seq baseline?",
        "answer_label": "no"
    },
    {
        "id": "5db7cece-882f-45e8-96c3-82a786c846c2",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that [CONTINUE] Analyzing Table 3, we can observe that all values of precision using the Portuguese corpora have higher scores when compared with the English corpora?",
        "answer_label": "yes"
    },
    {
        "id": "2f553672-527e-49c7-85e8-c13ecb888e56",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
        "table_column_names": [
            "[EMPTY]",
            "WN-N P",
            "WN-N R",
            "WN-N F",
            "WN-V P",
            "WN-V R",
            "WN-V F",
            "VN P",
            "VN R",
            "VN F"
        ],
        "table_content_values": [
            [
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2"
            ],
            [
                "type",
                ".700",
                ".654",
                ".676",
                ".535",
                ".474",
                ".503",
                ".327",
                ".309",
                ".318"
            ],
            [
                "x+POS",
                ".699",
                ".651",
                ".674",
                ".544",
                ".472",
                ".505",
                ".339",
                ".312",
                ".325"
            ],
            [
                "lemma",
                ".706",
                ".660",
                ".682",
                ".576",
                ".520",
                ".547",
                ".384",
                ".360",
                ".371"
            ],
            [
                "x+POS",
                "<bold>.710</bold>",
                "<bold>.662</bold>",
                "<bold>.685</bold>",
                "<bold>.589</bold>",
                "<bold>.529</bold>",
                "<bold>.557</bold>",
                "<bold>.410</bold>",
                "<bold>.389</bold>",
                "<bold>.399</bold>"
            ],
            [
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep"
            ],
            [
                "type",
                ".712",
                ".661",
                ".686",
                ".545",
                ".457",
                ".497",
                ".324",
                ".296",
                ".310"
            ],
            [
                "x+POS",
                ".715",
                ".659",
                ".686",
                ".560",
                ".464",
                ".508",
                ".349",
                ".320",
                ".334"
            ],
            [
                "lemma",
                "<bold>.725</bold>",
                "<bold>.668</bold>",
                "<bold>.696</bold>",
                ".591",
                ".512",
                ".548",
                ".408",
                ".371",
                ".388"
            ],
            [
                "x+POS",
                ".722",
                ".666",
                ".693",
                "<bold>.609</bold>",
                "<bold>.527</bold>",
                "<bold>.565</bold>",
                "<bold>.412</bold>",
                "<bold>.381</bold>",
                "<bold>.396</bold>"
            ]
        ],
        "question": "Is it true that For window-based w2 contexts POS disambiguation yields significantly better F scores on lemmatized targets for VN (p \u2264 .005) with no significant difference for WN-N and WN-V (p \u2248 .05)?",
        "answer_label": "no"
    },
    {
        "id": "5f0079a8-4eb7-4fbd-8bcc-a2e289bc4562",
        "table_caption": "Aligning Vector-spaces with Noisy Supervised Lexicons Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
        "table_column_names": [
            "Method",
            "En\u2192It best",
            "En\u2192It avg",
            "En\u2192It iters",
            "En\u2192De best",
            "En\u2192De avg",
            "En\u2192De iters",
            "En\u2192Fi best",
            "En\u2192Fi avg",
            "En\u2192Fi iters",
            "En\u2192Es best",
            "En\u2192Es avg",
            "En\u2192Es iters"
        ],
        "table_content_values": [
            [
                "Artetxe et\u00a0al., 2018b",
                "[BOLD] 48.53",
                "48.13",
                "573",
                "48.47",
                "48.19",
                "773",
                "33.50",
                "32.63",
                "988",
                "37.60",
                "37.33",
                "808"
            ],
            [
                "Noise-aware Alignment",
                "[BOLD] 48.53",
                "[BOLD] 48.20",
                "471",
                "[BOLD] 49.67",
                "[BOLD] 48.89",
                "568",
                "[BOLD] 33.98",
                "[BOLD] 33.68",
                "502",
                "[BOLD] 38.40",
                "[BOLD] 37.79",
                "551"
            ]
        ],
        "question": "Is it true that In most setups our average case is better than the former best case?",
        "answer_label": "yes"
    },
    {
        "id": "6d64d32d-781d-473e-b478-f646adbed3f4",
        "table_caption": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
        "table_column_names": [
            "Model & Decoding Scheme",
            "Act # w/o",
            "Act # w/",
            "Slot # w/o",
            "Slot # w/"
        ],
        "table_content_values": [
            [
                "Single-Action Baselines",
                "Single-Action Baselines",
                "Single-Action Baselines",
                "Single-Action Baselines",
                "Single-Action Baselines"
            ],
            [
                "DAMD + greedy",
                "[BOLD] 1.00",
                "[BOLD] 1.00",
                "1.95",
                "[BOLD] 2.51"
            ],
            [
                "HDSA + fixed threshold",
                "[BOLD] 1.00",
                "[BOLD] 1.00",
                "2.07",
                "[BOLD] 2.40"
            ],
            [
                "5-Action Generation",
                "5-Action Generation",
                "5-Action Generation",
                "5-Action Generation",
                "5-Action Generation"
            ],
            [
                "DAMD + beam search",
                "2.67",
                "[BOLD] 2.87",
                "3.36",
                "[BOLD] 4.39"
            ],
            [
                "DAMD + diverse beam search",
                "2.68",
                "[BOLD] 2.88",
                "3.41",
                "[BOLD] 4.50"
            ],
            [
                "DAMD + top-k sampling",
                "3.08",
                "[BOLD] 3.43",
                "3.61",
                "[BOLD] 4.91"
            ],
            [
                "DAMD + top-p sampling",
                "3.08",
                "[BOLD] 3.40",
                "3.79",
                "[BOLD] 5.20"
            ],
            [
                "HDSA + sampled threshold",
                "1.32",
                "[BOLD] 1.50",
                "3.08",
                "[BOLD] 3.31"
            ],
            [
                "10-Action Generation",
                "10-Action Generation",
                "10-Action Generation",
                "10-Action Generation",
                "10-Action Generation"
            ],
            [
                "DAMD + beam search",
                "3.06",
                "[BOLD] 3.39",
                "4.06",
                "[BOLD] 5.29"
            ],
            [
                "DAMD + diverse beam search",
                "3.05",
                "[BOLD] 3.39",
                "4.05",
                "[BOLD] 5.31"
            ],
            [
                "DAMD + top-k sampling",
                "3.59",
                "[BOLD] 4.12",
                "4.21",
                "[BOLD] 5.77"
            ],
            [
                "DAMD + top-p sampling",
                "3.53",
                "[BOLD] 4.02",
                "4.41",
                "[BOLD] 6.17"
            ],
            [
                "HDSA + sampled threshold",
                "1.54",
                "[BOLD] 1.83",
                "3.42",
                "[BOLD] 3.92"
            ]
        ],
        "question": "Is it true that [CONTINUE] After applying our data augmentation, both the action and slot diversity are improved consistently, [CONTINUE] HDSA has the worse performance and benefits less from data augmentation comparing to our proposed domain-aware multi-decoder network,?",
        "answer_label": "yes"
    },
    {
        "id": "ba4e9b1e-818d-4a1e-97d9-9d7b71b2e18d",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
        "table_column_names": [
            "Model",
            "Answerability Macro P/R/F",
            "# Answerable",
            "Answer Prec.",
            "Derivation Prec. RG-L (P/R/F)",
            "Derivation Prec. BL-4"
        ],
        "table_content_values": [
            [
                "Shortest Path",
                "54.8/55.5/53.2",
                "976",
                "3.6",
                "56.7/38.5/41.5",
                "31.3"
            ],
            [
                "PRKGC",
                "52.6/51.5/50.7",
                "1,021",
                "45.2",
                "40.7/60.7/44.7",
                "30.9"
            ],
            [
                "PRKGC+NS",
                "53.6/54.1/52.1",
                "980",
                "45.4",
                "42.2/61.6/46.1",
                "33.4"
            ]
        ],
        "question": "Is it true that Supervising path attentions (the PRKGC+NS model) is not effective for improving the human interpretability of generated NLDs?",
        "answer_label": "no"
    },
    {
        "id": "ba0126ce-21f6-46bd-8eff-2d93dfcaa85c",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 2: Averages of rankings of the words in same-gender pairs vs. different-gender pairs for Italian and German, along with their differences. Og stands for the original embeddings, Db for the debiased embeddings, and En for English. Each row presents the averages of pairs with the respective scores in SimLex-999 (0\u20134, 4\u20137, 7\u201310).",
        "table_column_names": [
            "[EMPTY]",
            "Italian Same-gender",
            "Italian Diff-Gender",
            "Italian difference",
            "German Same-gender",
            "German Diff-Gender",
            "German difference"
        ],
        "table_content_values": [
            [
                "7\u201310",
                "Og: 4884",
                "Og: 12947",
                "Og: 8063",
                "Og: 5925",
                "Og: 33604",
                "Og: 27679"
            ],
            [
                "7\u201310",
                "Db: 5523",
                "Db: 7312",
                "Db: 1789",
                "Db: 7653",
                "Db: 26071",
                "Db: 18418"
            ],
            [
                "7\u201310",
                "En: 6978",
                "En: 2467",
                "En: -4511",
                "En: 4517",
                "En: 8666",
                "En: 4149"
            ],
            [
                "4\u20137",
                "Og: 10954",
                "Og: 15838",
                "Og: 4884",
                "Og: 19271",
                "Og: 27256",
                "Og: 7985"
            ],
            [
                "4\u20137",
                "Db: 12037",
                "Db: 12564",
                "Db: 527",
                "Db: 24845",
                "Db: 22970",
                "Db: -1875"
            ],
            [
                "4\u20137",
                "En: 15891",
                "En: 17782",
                "En: 1891",
                "En: 13282",
                "En: 17649",
                "En: 4367"
            ],
            [
                "0\u20134",
                "Og: 23314",
                "Og: 35783",
                "Og: 12469",
                "Og: 50983",
                "Og: 85263",
                "Og: 34280"
            ],
            [
                "0\u20134",
                "Db: 26386",
                "Db: 28067",
                "Db: 1681",
                "Db: 60603",
                "Db: 79081",
                "Db: 18478"
            ],
            [
                "0\u20134",
                "En: 57278",
                "En: 53053",
                "En: -4225",
                "En: 41509",
                "En: 62929",
                "En: 21420"
            ]
        ],
        "question": "Is it true that As expected, the average ranking of samegender pairs is significantly higher than that of different-gender pairs, both for German and Italian, while the difference between the sets in English is much smaller?",
        "answer_label": "no"
    },
    {
        "id": "fecfd170-1f8f-4f70-8b18-e211486982f2",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1038",
                "0.0170",
                "0.0490",
                "0.0641",
                "0.0641",
                "0.0613",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1282",
                "0.0291",
                "0.0410",
                "0.0270",
                "0.0270",
                "0.1154",
                "0.0661"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.6185",
                "0.3744",
                "0.4144",
                "0.4394",
                "0.4394",
                "[BOLD] 0.7553",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.6308",
                "0.4124",
                "0.4404",
                "0.4515",
                "0.4945",
                "[BOLD] 0.8609",
                "0.5295"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "[BOLD] 0.0021",
                "0.0004",
                "0.0011",
                "0.0014",
                "0.0014",
                "0.0013",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0011",
                "0.0008",
                "0.0011",
                "0.0008",
                "0.0008",
                "[BOLD] 0.0030",
                "0.0018"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0012",
                "0.0008",
                "0.0009",
                "0.0010",
                "0.0010",
                "[BOLD] 0.0016",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0003",
                "0.0009",
                "0.0009",
                "0.0010",
                "0.0010",
                "[BOLD] 0.0017",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "[BOLD] 0.0041",
                "0.0007",
                "0.0021",
                "0.0027",
                "0.0027",
                "0.0026",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0022",
                "0.0016",
                "0.0022",
                "0.0015",
                "0.0015",
                "[BOLD] 0.0058",
                "0.0036"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0024",
                "0.0016",
                "0.0018",
                "0.0019",
                "0.0019",
                "[BOLD] 0.0031",
                "0.0023"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0005",
                "0.0018",
                "0.0018",
                "0.0020",
                "0.0021",
                "[BOLD] 0.0034",
                "0.0022"
            ]
        ],
        "question": "Is it true that On the other hand, choosing the best hypernym worked very well for DocSub which obtained the best precision for the Portuguese corpora?",
        "answer_label": "yes"
    },
    {
        "id": "de3034a6-6815-43f6-ab74-408ae82f3718",
        "table_caption": "Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
        "table_column_names": [
            "[BOLD] ResNet-34",
            "[BOLD] Eval set %",
            "[BOLD] #param"
        ],
        "table_content_values": [
            [
                "SA (S: 3 - M: 1)",
                "55.25",
                "} 0.082M"
            ],
            [
                "[BOLD] SA (S: 3 - B: 3)",
                "[BOLD] 55.42",
                "} 0.082M"
            ],
            [
                "SA (S: 3 - B: 4)",
                "55.33",
                "} 0.082M"
            ],
            [
                "SA (S: 3 - B: 6)",
                "55.31",
                "} 0.082M"
            ],
            [
                "SA (S: 3 - B: 1,3,5)",
                "55.45",
                "} 0.245M"
            ],
            [
                "[BOLD] SA (S: 3 - B: 2,4,6)",
                "[BOLD] 55.56",
                "} 0.245M"
            ]
        ],
        "question": "Is it true that The improvement is not significant enough to warrant further research into visual modulation?",
        "answer_label": "no"
    },
    {
        "id": "52f9985b-e7c4-4516-a758-9eebf47cb971",
        "table_caption": "Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
        "table_column_names": [
            "[BOLD] ResNet-34",
            "[BOLD] Eval set %",
            "[BOLD] #param"
        ],
        "table_content_values": [
            [
                "Baseline (No SA)Anderson et al. ( 2018 )",
                "55.00",
                "0M"
            ],
            [
                "SA (S: 1,2,3 - B: 1)",
                "55.11",
                "} 0.107M"
            ],
            [
                "SA (S: 1,2,3 - B: 2)",
                "55.17",
                "} 0.107M"
            ],
            [
                "[BOLD] SA (S: 1,2,3 - B: 3)",
                "[BOLD] 55.27",
                "} 0.107M"
            ]
        ],
        "question": "Is it true that [CONTINUE] We showed that it is possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks?",
        "answer_label": "yes"
    },
    {
        "id": "51c11868-1bbf-466a-b95d-621a87c43768",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that In fact, DocSub had worse results in precision only when using Europarl corpus in English, where DF reached best values of precision and f-measure?",
        "answer_label": "yes"
    },
    {
        "id": "2de15a72-9055-4a7e-897f-cf0c3c3aeb36",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that [CONTINUE] Though ALDM obtains a lower inform F1 and match rate than PPO, it gets a slight improvement [CONTINUE] on task success [CONTINUE] Ablation test is investigated in Table 3?",
        "answer_label": "yes"
    },
    {
        "id": "0f61a515-9f46-4502-a2db-a1ef9a6c2148",
        "table_caption": "Lost in Translation: Loss and Decay of Linguistic Richness in Machine Translation Table 5: Automatic evaluation scores (BLEU and TER) for the REV systems.",
        "table_column_names": [
            "System reference",
            "BLEU\u2191",
            "TER\u2193"
        ],
        "table_content_values": [
            [
                "en-fr-rnn-rev",
                "33.3",
                "50.2"
            ],
            [
                "en-fr-smt-rev",
                "36.5",
                "47.1"
            ],
            [
                "en-fr-trans-rev",
                "[BOLD] 36.8",
                "[BOLD] 46.8"
            ],
            [
                "en-es-rnn-rev",
                "37.8",
                "45.0"
            ],
            [
                "en-es-smt-rev",
                "39.2",
                "44.0"
            ],
            [
                "en-es-trans-rev",
                "[BOLD] 40.4",
                "[BOLD] 42.7"
            ]
        ],
        "question": "Is it true that we present BLEU and TER for the REV systems in Table 5, [CONTINUE] While RNN models are the best ones according to the evaluation metrics,?",
        "answer_label": "no"
    },
    {
        "id": "9b0998ee-9b1d-472a-a9dc-19f4dc5c57a8",
        "table_caption": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.",
        "table_column_names": [
            "Feature",
            "LR P",
            "LR R",
            "LR F1",
            "SVM P",
            "SVM R",
            "SVM F1",
            "ANN P",
            "ANN R",
            "ANN F1"
        ],
        "table_content_values": [
            [
                "+BoW",
                "0.93",
                "0.91",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+BoC (Wiki-PubMed-PMC)",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.91",
                "0.91",
                "[BOLD] 0.91"
            ],
            [
                "+BoC (GloVe)",
                "0.93",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+ASM",
                "0.90",
                "0.85",
                "0.88",
                "0.90",
                "0.86",
                "0.88",
                "0.89",
                "0.89",
                "0.89"
            ],
            [
                "+Sentence Embeddings(SEs)",
                "0.89",
                "0.89",
                "0.89",
                "0.90",
                "0.86",
                "0.88",
                "0.88",
                "0.88",
                "0.88"
            ],
            [
                "+BoC(Wiki-PubMed-PMC)+SEs",
                "0.92",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ]
        ],
        "question": "Is it true that [CONTINUE] Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) outperforms GloVe (Mikolov et al., 2013a) in the extraction of most relation types (Table 1) [CONTINUE] the combination feature of BoC and sentence embeddings outperforms sentence embeddings alone, but do not exceed the upper boundary of BoC feature, in which again demonstrating the competitiveness of BoC feature?",
        "answer_label": "yes"
    },
    {
        "id": "775b4ac6-9478-4306-b388-b0e8203c4ac0",
        "table_caption": "Zero-Shot Grounding of Objects from Natural Language Queries Table 6: Ablation study: BM=Base Model, softmax means we classify only one candidate box as foreground, BCE = Binary Cross Entropy means we classify each candidate box as the foreground or background, FL = Focal Loss, Img-Resize: use images of dimension 600\u00d7600",
        "table_column_names": [
            "Model",
            "Accuracy on RefClef"
        ],
        "table_content_values": [
            [
                "BM + Softmax",
                "48.54"
            ],
            [
                "BM + BCE",
                "55.20"
            ],
            [
                "BM + FL",
                "57.13"
            ],
            [
                "BM + FL + Img-Resize",
                "[BOLD] 61.75"
            ]
        ],
        "question": "Is it true that [CONTINUE] Finally, image resizing gives another 4% increase?",
        "answer_label": "yes"
    },
    {
        "id": "3197bce9-5af1-44bb-ae78-af57c4346c14",
        "table_caption": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM Table 5: Confusion matrix for testing set predictions",
        "table_column_names": [
            "[BOLD] LabelPrediction",
            "[BOLD] C",
            "[BOLD] D",
            "[BOLD] Q",
            "[BOLD] S"
        ],
        "table_content_values": [
            [
                "[BOLD] Commenting",
                "760",
                "0",
                "12",
                "6"
            ],
            [
                "[BOLD] Denying",
                "68",
                "0",
                "1",
                "2"
            ],
            [
                "[BOLD] Querying",
                "69",
                "0",
                "36",
                "1"
            ],
            [
                "[BOLD] Supporting",
                "67",
                "0",
                "1",
                "26"
            ]
        ],
        "question": "Is it true that Most denying instances get misclassified as querying (see Table 5),?",
        "answer_label": "no"
    },
    {
        "id": "919169e5-b0e4-4818-96d5-27895efad28b",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
        "table_column_names": [
            "[BOLD] Emoji alias",
            "[BOLD] N",
            "[BOLD] emoji #",
            "[BOLD] emoji %",
            "[BOLD] no-emoji #",
            "[BOLD] no-emoji %",
            "[BOLD] \u0394%"
        ],
        "table_content_values": [
            [
                "mask",
                "163",
                "154",
                "94.48",
                "134",
                "82.21",
                "- 12.27"
            ],
            [
                "two_hearts",
                "87",
                "81",
                "93.10",
                "77",
                "88.51",
                "- 4.59"
            ],
            [
                "heart_eyes",
                "122",
                "109",
                "89.34",
                "103",
                "84.43",
                "- 4.91"
            ],
            [
                "heart",
                "267",
                "237",
                "88.76",
                "235",
                "88.01",
                "- 0.75"
            ],
            [
                "rage",
                "92",
                "78",
                "84.78",
                "66",
                "71.74",
                "- 13.04"
            ],
            [
                "cry",
                "116",
                "97",
                "83.62",
                "83",
                "71.55",
                "- 12.07"
            ],
            [
                "sob",
                "490",
                "363",
                "74.08",
                "345",
                "70.41",
                "- 3.67"
            ],
            [
                "unamused",
                "167",
                "121",
                "72.46",
                "116",
                "69.46",
                "- 3.00"
            ],
            [
                "weary",
                "204",
                "140",
                "68.63",
                "139",
                "68.14",
                "- 0.49"
            ],
            [
                "joy",
                "978",
                "649",
                "66.36",
                "629",
                "64.31",
                "- 2.05"
            ],
            [
                "sweat_smile",
                "111",
                "73",
                "65.77",
                "75",
                "67.57",
                "1.80"
            ],
            [
                "confused",
                "77",
                "46",
                "59.74",
                "48",
                "62.34",
                "2.60"
            ]
        ],
        "question": "Is it true that [CONTINUE] When removing sweat smile and confused accuracy decreased?",
        "answer_label": "no"
    },
    {
        "id": "65d85bed-5ff2-4954-92c9-f7e7cfd25951",
        "table_caption": "Improving Generalization by Incorporating Coverage in Natural Language Inference Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
        "table_column_names": [
            "[EMPTY]",
            "in-domain MultiNLI",
            "out-of-domain SNLI",
            "out-of-domain Glockner",
            "out-of-domain SICK"
        ],
        "table_content_values": [
            [
                "MQAN",
                "72.30",
                "60.91",
                "41.82",
                "53.95"
            ],
            [
                "+ coverage",
                "<bold>73.84</bold>",
                "<bold>65.38</bold>",
                "<bold>78.69</bold>",
                "<bold>54.55</bold>"
            ],
            [
                "ESIM (ELMO)",
                "80.04",
                "68.70",
                "60.21",
                "51.37"
            ],
            [
                "+ coverage",
                "<bold>80.38</bold>",
                "<bold>70.05</bold>",
                "<bold>67.47</bold>",
                "<bold>52.65</bold>"
            ]
        ],
        "question": "Is it true that The resulting cross-dataset improvements on the SNLI and Glockner datasets are larger than those on the SICK dataset?",
        "answer_label": "yes"
    },
    {
        "id": "ef13c2cf-6271-4c5c-a1ee-17e71aea7566",
        "table_caption": "Entity, Relation, and Event Extraction with Contextualized Span Representations Table 3: F1 scores on Relation.",
        "table_column_names": [
            "[EMPTY]",
            "ACE05",
            "SciERC",
            "WLPC"
        ],
        "table_content_values": [
            [
                "BERT + LSTM",
                "60.6",
                "40.3",
                "65.1"
            ],
            [
                "+RelProp",
                "61.9",
                "41.1",
                "65.3"
            ],
            [
                "+CorefProp",
                "59.7",
                "42.6",
                "-"
            ],
            [
                "BERT FineTune",
                "[BOLD] 62.1",
                "44.3",
                "65.4"
            ],
            [
                "+RelProp",
                "62.0",
                "43.0",
                "[BOLD] 65.5"
            ],
            [
                "+CorefProp",
                "60.0",
                "[BOLD] 45.3",
                "-"
            ]
        ],
        "question": "Is it true that [CONTINUE] Relation propagation (RelProp) improves relation extraction performance over both pretrained and fine-tuned BERT?",
        "answer_label": "no"
    },
    {
        "id": "fe71de2d-b005-4e58-81c6-22d84526ca66",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
        "table_column_names": [
            "[BOLD] Dataset",
            "[BOLD] # pairs",
            "[BOLD] # words (doc)",
            "[BOLD] # sents (docs)",
            "[BOLD] # words (summary)",
            "[BOLD] # sents (summary)",
            "[BOLD] vocab size"
        ],
        "table_content_values": [
            [
                "Multi-News",
                "44,972/5,622/5,622",
                "2,103.49",
                "82.73",
                "263.66",
                "9.97",
                "666,515"
            ],
            [
                "DUC03+04",
                "320",
                "4,636.24",
                "173.15",
                "109.58",
                "2.88",
                "19,734"
            ],
            [
                "TAC 2011",
                "176",
                "4,695.70",
                "188.43",
                "99.70",
                "1.00",
                "24,672"
            ],
            [
                "CNNDM",
                "287,227/13,368/11,490",
                "810.57",
                "39.78",
                "56.20",
                "3.68",
                "717,951"
            ]
        ],
        "question": "Is it true that Our summaries are notably longer than in other works, about 260 words on average?",
        "answer_label": "yes"
    },
    {
        "id": "70361fad-829f-4e8c-af9b-e23d8acd1375",
        "table_caption": "Keyphrase Generation for Scientific Articles using GANs Table 2: \u03b1-nDCG@5 metrics",
        "table_column_names": [
            "Model",
            "Inspec",
            "Krapivin",
            "NUS",
            "KP20k"
        ],
        "table_content_values": [
            [
                "Catseq",
                "0.87803",
                "0.781",
                "0.82118",
                "0.804"
            ],
            [
                "Catseq-RL",
                "0.8602",
                "[BOLD] 0.786",
                "0.83",
                "0.809"
            ],
            [
                "GAN",
                "[BOLD] 0.891",
                "0.771",
                "[BOLD] 0.853",
                "[BOLD] 0.85"
            ]
        ],
        "question": "Is it true that The difference is most prevalent in KP20k, the largest of the four datasets, where our GAN model (at 0.85) is nearly 5% better than both the other baseline models?",
        "answer_label": "yes"
    },
    {
        "id": "6fceb272-19a2-4482-9526-d93a34ee8ba4",
        "table_caption": "Distant Learning for Entity Linking with Automatic Noise Detection Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
        "table_column_names": [
            "System",
            "All P",
            "All R",
            "All F1",
            "In  [ITALIC] E+ P",
            "In  [ITALIC] E+ R",
            "In  [ITALIC] E+ F1"
        ],
        "table_content_values": [
            [
                "Name matching",
                "15.03",
                "15.03",
                "15.03",
                "29.13",
                "29.13",
                "29.13"
            ],
            [
                "MIL (model 1)",
                "35.87",
                "35.87",
                "35.87 \u00b10.72",
                "69.38",
                "69.38",
                "69.38 \u00b11.29"
            ],
            [
                "MIL-ND (model 2)",
                "37.42",
                "[BOLD] 37.42",
                "37.42 \u00b10.35",
                "72.50",
                "[BOLD] 72.50",
                "[BOLD] 72.50 \u00b10.68"
            ],
            [
                "[ITALIC] \u03c4MIL-ND (model 2)",
                "[BOLD] 38.91",
                "36.73",
                "[BOLD] 37.78 \u00b10.26",
                "[BOLD] 73.19",
                "71.15",
                "72.16 \u00b10.48"
            ],
            [
                "Supervised learning",
                "42.90",
                "42.90",
                "42.90 \u00b10.59",
                "83.12",
                "83.12",
                "83.12 \u00b11.15"
            ]
        ],
        "question": "Is it true that The ND classifier had a significant positive effect on F1 for the 'In E+' setting?",
        "answer_label": "no"
    },
    {
        "id": "e9ed6a2c-59be-47fd-815e-3a08898c26c5",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 2: Ablation study results.",
        "table_column_names": [
            "[BOLD] Variation",
            "[BOLD] Accuracy (%)",
            "[BOLD] \u0394%"
        ],
        "table_content_values": [
            [
                "Submitted",
                "[BOLD] 69.23",
                "-"
            ],
            [
                "No emoji",
                "68.36",
                "- 0.87"
            ],
            [
                "No ELMo",
                "65.52",
                "- 3.71"
            ],
            [
                "Concat Pooling",
                "68.47",
                "- 0.76"
            ],
            [
                "LSTM hidden=4096",
                "69.10",
                "- 0.13"
            ],
            [
                "LSTM hidden=1024",
                "68.93",
                "- 0.30"
            ],
            [
                "LSTM hidden=512",
                "68.43",
                "- 0.80"
            ],
            [
                "POS emb dim=100",
                "68.99",
                "- 0.24"
            ],
            [
                "POS emb dim=75",
                "68.61",
                "- 0.62"
            ],
            [
                "POS emb dim=50",
                "69.33",
                "+ 0.10"
            ],
            [
                "POS emb dim=25",
                "69.21",
                "- 0.02"
            ],
            [
                "SGD optim lr=1",
                "64.33",
                "- 4.90"
            ],
            [
                "SGD optim lr=0.1",
                "66.11",
                "- 3.12"
            ],
            [
                "SGD optim lr=0.01",
                "60.72",
                "- 8.51"
            ],
            [
                "SGD optim lr=0.001",
                "30.49",
                "- 38.74"
            ]
        ],
        "question": "Is it true that We performed an ablation study on a single model having obtained 69.23% accuracy on the validation set?",
        "answer_label": "yes"
    },
    {
        "id": "2df5a453-0b55-4883-8f4a-a1486ebbb214",
        "table_caption": "Two Causal Principles for Improving Visual Dialog Table 1: Performance (NDCG%) comparison for the experiments of applying our principles on the validation set of VisDial v1.0. LF is the enhanced version as we mentioned. QT, S and D denote question type, answer score sampling, and hidden dictionary learning, respectively. R0, R1, R2, R3 denote regressive loss, weighted softmax loss, binary sigmoid loss ,and generalized ranking loss, respectively.",
        "table_column_names": [
            "Model",
            "baseline",
            "QT",
            "S  [ITALIC] R0",
            "S  [ITALIC] R1",
            "S  [ITALIC] R2",
            "S  [ITALIC] R3",
            "D"
        ],
        "table_content_values": [
            [
                "LF\u00a0",
                "57.21",
                "58.97",
                "67.82",
                "71.27",
                "72.04",
                "72.36",
                "72.65"
            ],
            [
                "LF +P1",
                "61.88",
                "62.87",
                "69.47",
                "72.16",
                "72.85",
                "73.42",
                "[BOLD] 73.63"
            ]
        ],
        "question": "Is it true that Overall, none of the implementations can improve the performances of base models?",
        "answer_label": "no"
    },
    {
        "id": "d697a74d-c867-4d09-950e-3d3f84fef4c5",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 6: F1 score on CoNLL-2003 English NER task. \u201c#Params\u201d: the parameter number in NER task. LSTM* denotes the reported result\u00a0Lample et\u00a0al. (2016).",
        "table_column_names": [
            "Model",
            "#Params",
            "NER"
        ],
        "table_content_values": [
            [
                "LSTM*",
                "-",
                "90.94"
            ],
            [
                "LSTM",
                "245K",
                "[BOLD] 89.61"
            ],
            [
                "GRU",
                "192K",
                "89.35"
            ],
            [
                "ATR",
                "87K",
                "88.46"
            ],
            [
                "SRU",
                "161K",
                "88.89"
            ],
            [
                "LRN",
                "129K",
                "88.56"
            ]
        ],
        "question": "Is it true that As shown in Table 6, the performance of LRN matches that of ATR and SRU, though LSTM and GRU operate better (+1.05 and +0.79)?",
        "answer_label": "yes"
    },
    {
        "id": "dffdd441-9432-4656-830a-adf397ec3283",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Type",
            "[BOLD] English-German #P",
            "[BOLD] English-German B",
            "[BOLD] English-German C",
            "[BOLD] English-Czech #P",
            "[BOLD] English-Czech B",
            "[BOLD] English-Czech C"
        ],
        "table_content_values": [
            [
                "BoW+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "12.2",
                "-",
                "-",
                "7.5",
                "-"
            ],
            [
                "CNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "13.7",
                "-",
                "-",
                "8.7",
                "-"
            ],
            [
                "BiRNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "16.1",
                "-",
                "-",
                "9.6",
                "-"
            ],
            [
                "PB-SMT (Beck et al.,  2018 )",
                "Single",
                "-",
                "12.8",
                "43.2",
                "-",
                "8.6",
                "36.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Single",
                "41.4M",
                "15.5",
                "40.8",
                "39.1M",
                "8.9",
                "33.8"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Single",
                "41.2M",
                "16.7",
                "42.4",
                "38.8M",
                "9.8",
                "33.3"
            ],
            [
                "DCGCN (ours)",
                "Single",
                "[BOLD]  29.7M",
                "[BOLD] 19.0",
                "[BOLD] 44.1",
                "[BOLD]  28.3M",
                "[BOLD] 12.1",
                "[BOLD] 37.1"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Ensemble",
                "207M",
                "19.0",
                "44.1",
                "195M",
                "11.3",
                "36.4"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Ensemble",
                "206M",
                "19.6",
                "45.1",
                "194M",
                "11.7",
                "35.9"
            ],
            [
                "DCGCN (ours)",
                "Ensemble",
                "[BOLD]  149M",
                "[BOLD] 20.5",
                "[BOLD] 45.8",
                "[BOLD]  142M",
                "[BOLD] 13.1",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that Our models DCGCN(single) and DCGCN(ensemble) do not remove the burden of employing a recurrent encoder to extract non-local contextual information in the bottom layers, as evidenced by the results of BoW+GCN, CNN+GCN, and BiRNN+GCN?",
        "answer_label": "no"
    },
    {
        "id": "a1f1fc3a-e648-4af9-84ce-3b59c2a584d5",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that Consequently, with an 8% decrease on average, the hybrid model [CONTINUE] Word Content are decreased?",
        "answer_label": "no"
    },
    {
        "id": "cd352a89-328c-4845-bf4c-d60c006603a7",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Method",
            "SUBJ",
            "CR",
            "MR",
            "MPQA",
            "MRPC",
            "TREC",
            "SICK-E",
            "SST2",
            "SST5",
            "STS-B",
            "SICK-R"
        ],
        "table_content_values": [
            [
                "CBOW/784",
                "90.0",
                "[BOLD] 79.2",
                "[BOLD] 74.0",
                "87.1",
                "71.6",
                "85.6",
                "78.9",
                "78.5",
                "42.1",
                "61.0",
                "[BOLD] 78.1"
            ],
            [
                "CMOW/784",
                "87.5",
                "73.4",
                "70.6",
                "[BOLD] 87.3",
                "69.6",
                "[BOLD] 88.0",
                "77.2",
                "74.7",
                "37.9",
                "56.5",
                "76.2"
            ],
            [
                "Hybrid",
                "[BOLD] 90.2",
                "78.7",
                "73.7",
                "[BOLD] 87.3",
                "[BOLD] 72.7",
                "87.6",
                "[BOLD] 79.4",
                "[BOLD] 79.6",
                "[BOLD] 43.3",
                "[BOLD] 63.4",
                "77.8"
            ],
            [
                "cmp. CBOW",
                "+0.2%",
                "-0.6%",
                "-0.4%",
                "+0.2%",
                "+1.5%",
                "+2.3%",
                "+0.6%",
                "+1.4%",
                "+2.9%",
                "+3.9%",
                "-0.4%"
            ],
            [
                "cmp. CMOW",
                "+3.1%",
                "+7.2%",
                "+4.4%",
                "+0%",
                "+4.5%",
                "-0.5%",
                "+2.9%",
                "+6.7%",
                "+14.3",
                "+12.2%",
                "+2.1%"
            ]
        ],
        "question": "Is it true that On 7 out of 11 supervised tasks, the joint model even improves upon the better model, and on SST2, SST5, and MRPC the difference is more than 1 point?",
        "answer_label": "yes"
    },
    {
        "id": "362601b1-3bf5-47a7-a8ab-192050bbeea7",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that This suggests that enriching input graphs with the global node and excluding the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations?",
        "answer_label": "no"
    },
    {
        "id": "0522e114-a86c-49ae-a0c1-2291cf0b6e30",
        "table_caption": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 4: Precisions on the Wikidata dataset with different choice of d.",
        "table_column_names": [
            "Recall",
            "0.1",
            "0.2",
            "0.3",
            "AUC",
            "Time"
        ],
        "table_content_values": [
            [
                "[ITALIC] d=1",
                "0.602",
                "0.487",
                "0.403",
                "0.367",
                "4h"
            ],
            [
                "[ITALIC] d=32",
                "0.645",
                "0.501",
                "0.393",
                "0.370",
                "-"
            ],
            [
                "[ITALIC] d=16",
                "0.655",
                "0.518",
                "0.413",
                "0.413",
                "20h"
            ],
            [
                "[ITALIC] d=8",
                "0.650",
                "0.519",
                "0.422",
                "0.405",
                "8h"
            ]
        ],
        "question": "Is it true that As the table 4 depicts, the training time increases with the growth of d?",
        "answer_label": "yes"
    },
    {
        "id": "cb276238-d6d0-427f-9a25-e923b519df9b",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Balanced",
            "Throughput (instances/s) Moderate",
            "Throughput (instances/s) Linear"
        ],
        "table_content_values": [
            [
                "1",
                "46.7",
                "27.3",
                "7.6"
            ],
            [
                "10",
                "125.2",
                "78.2",
                "22.7"
            ],
            [
                "25",
                "129.7",
                "83.1",
                "45.4"
            ]
        ],
        "question": "Is it true that For all batch sizes, the training throughput on the balanced dataset is the highest, while the throughput on the linear dataset is the lowest?",
        "answer_label": "yes"
    },
    {
        "id": "6cd67271-8961-4a05-9ba2-2f3f773359c6",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that Consequently, with an 8% decrease, CMOW is substantially less linguistically informed than CBOW?",
        "answer_label": "no"
    },
    {
        "id": "d0317274-0a8c-4613-b192-9b9d0da2ca72",
        "table_caption": "Semantic Neural Machine Translation using AMR Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.",
        "table_column_names": [
            "System",
            "NC-v11 BLEU",
            "NC-v11 TER\u2193",
            "NC-v11 Meteor",
            "Full BLEU",
            "Full TER\u2193",
            "Full Meteor"
        ],
        "table_content_values": [
            [
                "OpenNMT-tf",
                "15.1",
                "0.6902",
                "0.3040",
                "24.3",
                "0.5567",
                "0.4225"
            ],
            [
                "Transformer-tf",
                "17.1",
                "0.6647",
                "0.3578",
                "25.1",
                "0.5537",
                "0.4344"
            ],
            [
                "Seq2seq",
                "16.0",
                "0.6695",
                "0.3379",
                "23.7",
                "0.5590",
                "0.4258"
            ],
            [
                "Dual2seq-LinAMR",
                "17.3",
                "0.6530",
                "0.3612",
                "24.0",
                "0.5643",
                "0.4246"
            ],
            [
                "Duel2seq-SRL",
                "17.2",
                "0.6591",
                "0.3644",
                "23.8",
                "0.5626",
                "0.4223"
            ],
            [
                "Dual2seq-Dep",
                "17.8",
                "0.6516",
                "0.3673",
                "25.0",
                "0.5538",
                "0.4328"
            ],
            [
                "Dual2seq",
                "[BOLD] *19.2*",
                "[BOLD] 0.6305",
                "[BOLD] 0.3840",
                "[BOLD] *25.5*",
                "[BOLD] 0.5480",
                "[BOLD] 0.4376"
            ]
        ],
        "question": "Is it true that [CONTINUE] Dual2seq is signifi [CONTINUE] cantly better than Seq2seq in both settings, [CONTINUE] In particular, the improvement is much larger under the small-scale setting (+3.2 BLEU) than that under the large-scale setting (+1.7 BLEU)?",
        "answer_label": "yes"
    },
    {
        "id": "74bfcde1-c624-4382-8be4-2c3de804b05a",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that Results also show the linear combination is more effective than the global node?",
        "answer_label": "yes"
    },
    {
        "id": "a5635632-7ec5-4631-bf69-893fe583a701",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1038",
                "0.0170",
                "0.0490",
                "0.0641",
                "0.0641",
                "0.0613",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1282",
                "0.0291",
                "0.0410",
                "0.0270",
                "0.0270",
                "0.1154",
                "0.0661"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.6185",
                "0.3744",
                "0.4144",
                "0.4394",
                "0.4394",
                "[BOLD] 0.7553",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.6308",
                "0.4124",
                "0.4404",
                "0.4515",
                "0.4945",
                "[BOLD] 0.8609",
                "0.5295"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "[BOLD] 0.0021",
                "0.0004",
                "0.0011",
                "0.0014",
                "0.0014",
                "0.0013",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0011",
                "0.0008",
                "0.0011",
                "0.0008",
                "0.0008",
                "[BOLD] 0.0030",
                "0.0018"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0012",
                "0.0008",
                "0.0009",
                "0.0010",
                "0.0010",
                "[BOLD] 0.0016",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0003",
                "0.0009",
                "0.0009",
                "0.0010",
                "0.0010",
                "[BOLD] 0.0017",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "[BOLD] 0.0041",
                "0.0007",
                "0.0021",
                "0.0027",
                "0.0027",
                "0.0026",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0022",
                "0.0016",
                "0.0022",
                "0.0015",
                "0.0015",
                "[BOLD] 0.0058",
                "0.0036"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0024",
                "0.0016",
                "0.0018",
                "0.0019",
                "0.0019",
                "[BOLD] 0.0031",
                "0.0023"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0005",
                "0.0018",
                "0.0018",
                "0.0020",
                "0.0021",
                "[BOLD] 0.0034",
                "0.0022"
            ]
        ],
        "question": "Is it true that [CONTINUE] Analyzing Table 5 we observe that Patt achieves again the best precision values for the English corpora?",
        "answer_label": "yes"
    },
    {
        "id": "d8e62ced-4be2-4ae7-b832-ac7070831958",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "36.85",
                "5.3782",
                "35.14",
                "55.01",
                "1.6016",
                "00.34",
                "09.81",
                "00.15",
                "10.31"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen",
                "39.23",
                "6.0217",
                "36.97",
                "55.52",
                "1.7623",
                "00.40",
                "03.59",
                "00.07",
                "04.05"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen+",
                "40.25",
                "6.1448",
                "37.50",
                "56.19",
                "1.8181",
                "00.21",
                "01.99",
                "00.05",
                "02.24"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.88",
                "3.9310",
                "32.11",
                "39.90",
                "0.5036",
                "07.73",
                "17.76",
                "09.52",
                "35.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.19",
                "6.0543",
                "37.38",
                "55.88",
                "1.8104",
                "00.17",
                "01.31",
                "00.25",
                "01.72"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen",
                "40.73",
                "6.1711",
                "37.76",
                "56.09",
                "1.8518",
                "00.07",
                "00.72",
                "00.08",
                "00.87"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen+",
                "40.51",
                "6.1226",
                "37.61",
                "55.98",
                "1.8286",
                "00.02",
                "00.63",
                "00.06",
                "00.70"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.66",
                "3.9511",
                "32.93",
                "39.29",
                "0.3855",
                "07.89",
                "15.60",
                "08.44",
                "31.94"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.48",
                "6.0269",
                "37.26",
                "56.19",
                "1.7999",
                "00.43",
                "02.84",
                "00.26",
                "03.52"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen",
                "41.57",
                "6.2830",
                "37.99",
                "56.36",
                "1.8849",
                "00.37",
                "01.40",
                "00.09",
                "01.86"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen+",
                "41.56",
                "6.2700",
                "37.94",
                "56.38",
                "1.8827",
                "00.21",
                "01.04",
                "00.07",
                "01.31"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "35.99",
                "5.0734",
                "34.74",
                "54.79",
                "1.5259",
                "00.02",
                "11.58",
                "00.02",
                "11.62"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen",
                "40.07",
                "6.1243",
                "37.45",
                "55.81",
                "1.8026",
                "00.05",
                "03.23",
                "00.01",
                "03.29"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen+",
                "40.80",
                "6.2197",
                "37.86",
                "56.13",
                "1.8422",
                "00.01",
                "01.87",
                "00.01",
                "01.88"
            ]
        ],
        "question": "Is it true that the models more often hallucinate additional information, rather than failing to realise part of the MR?",
        "answer_label": "no"
    },
    {
        "id": "8e524e36-4e8a-4a05-8698-51fa8969dd6e",
        "table_caption": "Solving Hard Coreference Problems Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.",
        "table_column_names": [
            "Schema",
            "AntePre(Test)",
            "AntePre(Train)"
        ],
        "table_content_values": [
            [
                "Type 1",
                "76.67",
                "86.79"
            ],
            [
                "Type 2",
                "79.55",
                "88.86"
            ],
            [
                "Type 1 (Cat1)",
                "90.26",
                "93.64"
            ],
            [
                "Type 2 (Cat2)",
                "83.38",
                "92.49"
            ]
        ],
        "question": "Is it true that The performance increase between Cat1/Cat2 and full data indicates that the existing knowledge schemas and knowledge acquisition are sufficient for further performance improvement?",
        "answer_label": "no"
    },
    {
        "id": "abe5fac5-e3f6-453a-8fbf-4e0cf1ed10d0",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 4: Experiment 2, t= \u201cb*tch\u201d",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.010",
                "0.010",
                "-0.632",
                "[EMPTY]",
                "0.978"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.963",
                "0.944",
                "20.064",
                "***",
                "1.020"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.011",
                "0.011",
                "-1.254",
                "[EMPTY]",
                "0.955"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.349",
                "0.290",
                "28.803",
                "***",
                "1.203"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.012",
                "0.012",
                "-0.162",
                "[EMPTY]",
                "0.995"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.017",
                "0.015",
                "4.698",
                "***",
                "1.152"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.988",
                "0.991",
                "-6.289",
                "***",
                "0.997"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.099",
                "0.091",
                "6.273",
                "***",
                "1.091"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.074",
                "0.027",
                "46.054",
                "***",
                "2.728"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.925",
                "0.968",
                "-41.396",
                "***",
                "0.956"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.010",
                "0.010",
                "0.000",
                "[EMPTY]",
                "1.000"
            ]
        ],
        "question": "Is it true that [CONTINUE] We see similar results for Waseem and Hovy (2016) and Waseem (2016)?",
        "answer_label": "yes"
    },
    {
        "id": "169b9c65-30e6-4fa5-8874-293dc2112cce",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that The largest loss is by 4% on the CoordInv task?",
        "answer_label": "yes"
    },
    {
        "id": "ed4eb2c0-b3d0-4980-bf4b-9425b6ef4eb9",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
        "table_column_names": [
            "Model",
            "Training data",
            "Overall",
            "Easy",
            "Hard"
        ],
        "table_content_values": [
            [
                "BERT-large-FT",
                "B-COPA",
                "74.5 (\u00b1 0.7)",
                "74.7 (\u00b1 0.4)",
                "[BOLD] 74.4 (\u00b1 0.9)"
            ],
            [
                "BERT-large-FT",
                "B-COPA (50%)",
                "74.3 (\u00b1 2.2)",
                "76.8 (\u00b1 1.9)",
                "72.8 (\u00b1 3.1)"
            ],
            [
                "BERT-large-FT",
                "COPA",
                "[BOLD] 76.5 (\u00b1 2.7)",
                "[BOLD] 83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA",
                "[BOLD] 89.0 (\u00b1 0.3)",
                "88.9 (\u00b1 2.1)",
                "[BOLD] 89.0 (\u00b1 0.8)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA (50%)",
                "86.1 (\u00b1 2.2)",
                "87.4 (\u00b1 1.1)",
                "85.4 (\u00b1 2.9)"
            ],
            [
                "RoBERTa-large-FT",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "[BOLD] 91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)"
            ]
        ],
        "question": "Is it true that The smaller performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely less on superficial cues?",
        "answer_label": "yes"
    },
    {
        "id": "bf1a3820-efa1-4b5b-bbe1-7810752e51d9",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that When comparing DF model which takes into account only the number of documents that the word occurs, with DocSub which considers the number of shared documents between two words, DF achieved better values of precision, but lower values of recall?",
        "answer_label": "no"
    },
    {
        "id": "43e1a4df-038c-4b24-9c68-e1455a5a77a7",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Original",
                "TGen\u2212",
                "63.37",
                "7.7188",
                "41.99",
                "68.53",
                "1.9355",
                "00.06",
                "15.77",
                "00.11",
                "15.94"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen",
                "66.41",
                "8.5565",
                "45.07",
                "69.17",
                "2.2253",
                "00.14",
                "04.11",
                "00.03",
                "04.27"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen+",
                "67.06",
                "8.5871",
                "45.83",
                "69.73",
                "2.2681",
                "00.04",
                "01.75",
                "00.01",
                "01.80"
            ],
            [
                "Original",
                "[BOLD] Original",
                "SC-LSTM",
                "39.11",
                "5.6704",
                "36.83",
                "50.02",
                "0.6045",
                "02.79",
                "18.90",
                "09.79",
                "31.51"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen\u2212",
                "65.87",
                "8.6400",
                "44.20",
                "67.51",
                "2.1710",
                "00.20",
                "00.56",
                "00.21",
                "00.97"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen",
                "66.24",
                "8.6889",
                "44.66",
                "67.85",
                "2.2181",
                "00.10",
                "00.02",
                "00.00",
                "00.12"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen+",
                "65.97",
                "8.6630",
                "44.45",
                "67.59",
                "2.1855",
                "00.02",
                "00.00",
                "00.00",
                "00.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "SC-LSTM",
                "38.52",
                "5.7125",
                "37.45",
                "48.50",
                "0.4343",
                "03.85",
                "17.39",
                "08.12",
                "29.37"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen\u2212",
                "66.28",
                "8.5202",
                "43.96",
                "67.83",
                "2.1375",
                "00.14",
                "02.26",
                "00.22",
                "02.61"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen",
                "67.00",
                "8.6889",
                "44.97",
                "68.19",
                "2.2228",
                "00.06",
                "00.44",
                "00.03",
                "00.53"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen+",
                "66.74",
                "8.6649",
                "44.84",
                "67.95",
                "2.2018",
                "00.00",
                "00.21",
                "00.03",
                "00.24"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen\u2212",
                "64.40",
                "7.9692",
                "42.81",
                "68.87",
                "2.0563",
                "00.01",
                "13.08",
                "00.00",
                "13.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen",
                "66.23",
                "8.5578",
                "45.12",
                "68.87",
                "2.2548",
                "00.04",
                "03.04",
                "00.00",
                "03.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen+",
                "65.96",
                "8.5238",
                "45.49",
                "68.79",
                "2.2456",
                "00.00",
                "01.44",
                "00.00",
                "01.45"
            ]
        ],
        "question": "Is it true that However, the main improvement of SER comes from training on cleaned data with up to 97% error reduction with the ranker and 94% without.11 just cleaning the training data has a much more dramatic effect than just using a semantic control mechanism, such as the reranker (0.97% vs. 4.27% SER)?",
        "answer_label": "yes"
    },
    {
        "id": "9602fe65-f85f-4388-81ee-1fd46873e99b",
        "table_caption": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task Table 1: Image-caption ranking results for English (Multi30k)",
        "table_column_names": [
            "[EMPTY]",
            "Image to Text R@1",
            "Image to Text R@5",
            "Image to Text R@10",
            "Image to Text Mr",
            "Text to Image R@1",
            "Text to Image R@5",
            "Text to Image R@10",
            "Text to Image Mr",
            "Alignment"
        ],
        "table_content_values": [
            [
                "[BOLD] symmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Parallel\u00a0gella:17",
                "31.7",
                "62.4",
                "74.1",
                "3",
                "24.7",
                "53.9",
                "65.7",
                "5",
                "-"
            ],
            [
                "UVS\u00a0kiros:15",
                "23.0",
                "50.7",
                "62.9",
                "5",
                "16.8",
                "42.0",
                "56.5",
                "8",
                "-"
            ],
            [
                "EmbeddingNet\u00a0wang:18",
                "40.7",
                "69.7",
                "79.2",
                "-",
                "29.2",
                "59.6",
                "71.7",
                "-",
                "-"
            ],
            [
                "sm-LSTM\u00a0huang:17",
                "42.5",
                "71.9",
                "81.5",
                "2",
                "30.2",
                "60.4",
                "72.3",
                "3",
                "-"
            ],
            [
                "VSE++\u00a0faghri:18",
                "[BOLD] 43.7",
                "71.9",
                "82.1",
                "2",
                "32.3",
                "60.9",
                "72.1",
                "3",
                "-"
            ],
            [
                "Mono",
                "41.4",
                "74.2",
                "84.2",
                "2",
                "32.1",
                "63.0",
                "73.9",
                "3",
                "-"
            ],
            [
                "FME",
                "39.2",
                "71.1",
                "82.1",
                "2",
                "29.7",
                "62.5",
                "74.1",
                "3",
                "76.81%"
            ],
            [
                "AME",
                "43.5",
                "[BOLD] 77.2",
                "[BOLD] 85.3",
                "[BOLD] 2",
                "[BOLD] 34.0",
                "[BOLD] 64.2",
                "[BOLD] 75.4",
                "[BOLD] 3",
                "66.91%"
            ],
            [
                "[BOLD] asymmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Pivot\u00a0gella:17",
                "33.8",
                "62.8",
                "75.2",
                "3",
                "26.2",
                "56.4",
                "68.4",
                "4",
                "-"
            ],
            [
                "Parallel\u00a0gella:17",
                "31.5",
                "61.4",
                "74.7",
                "3",
                "27.1",
                "56.2",
                "66.9",
                "4",
                "-"
            ],
            [
                "Mono",
                "47.7",
                "77.1",
                "86.9",
                "2",
                "35.8",
                "66.6",
                "76.8",
                "3",
                "-"
            ],
            [
                "FME",
                "44.9",
                "76.9",
                "86.4",
                "2",
                "34.2",
                "66.1",
                "77.1",
                "3",
                "76.81%"
            ],
            [
                "AME",
                "[BOLD] 50.5",
                "[BOLD] 79.7",
                "[BOLD] 88.4",
                "[BOLD] 1",
                "[BOLD] 38.0",
                "[BOLD] 68.5",
                "[BOLD] 78.4",
                "[BOLD] 2",
                "73.10%"
            ]
        ],
        "question": "Is it true that FME performs better than AME model on both symmetric and asymmetric modes, which shows the advantage of finetuning word embeddings during training?",
        "answer_label": "no"
    },
    {
        "id": "cc03ab7e-6e0d-45fd-a054-8cf38d14a2a1",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).",
        "table_column_names": [
            "Model",
            "#Params",
            "Base",
            "+Elmo"
        ],
        "table_content_values": [
            [
                "rnet*",
                "-",
                "71.1/79.5",
                "-/-"
            ],
            [
                "LSTM",
                "2.67M",
                "[BOLD] 70.46/78.98",
                "75.17/82.79"
            ],
            [
                "GRU",
                "2.31M",
                "70.41/ [BOLD] 79.15",
                "75.81/83.12"
            ],
            [
                "ATR",
                "1.59M",
                "69.73/78.70",
                "75.06/82.76"
            ],
            [
                "SRU",
                "2.44M",
                "69.27/78.41",
                "74.56/82.50"
            ],
            [
                "LRN",
                "2.14M",
                "70.11/78.83",
                "[BOLD] 76.14/ [BOLD] 83.83"
            ]
        ],
        "question": "Is it true that After integrating Elmo for contextual modeling, the performance of LRN does not reach the best (76.1 EM and 83.83 F1), with GRU and LSTM outperforming it (+0.33EM, +0.71F1)?",
        "answer_label": "no"
    },
    {
        "id": "737a3ba3-f0ff-476c-b995-bb8fc27b877e",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
        "table_column_names": [
            "Cue",
            "App.",
            "Prod.",
            "Cov."
        ],
        "table_content_values": [
            [
                "in",
                "47",
                "55.3",
                "9.40"
            ],
            [
                "was",
                "55",
                "61.8",
                "11.0"
            ],
            [
                "to",
                "82",
                "40.2",
                "16.4"
            ],
            [
                "the",
                "85",
                "38.8",
                "17.0"
            ],
            [
                "a",
                "106",
                "57.5",
                "21.2"
            ]
        ],
        "question": "Is it true that Its productivity of 57.5% expresses that it appears in in correct alternatives 7.5% more often than expected by random chance?",
        "answer_label": "yes"
    },
    {
        "id": "36419694-fbe7-448e-ab77-5a057e25f499",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 4: Scores for different training objectives on the linguistic probing tasks.",
        "table_column_names": [
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "CMOW-C",
                "[BOLD] 36.2",
                "66.0",
                "81.1",
                "78.7",
                "61.7",
                "[BOLD] 83.9",
                "79.1",
                "73.6",
                "50.4",
                "66.8"
            ],
            [
                "CMOW-R",
                "35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "[BOLD] 80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "[BOLD] 74.2",
                "[BOLD] 50.7",
                "[BOLD] 72.9"
            ],
            [
                "CBOW-C",
                "[BOLD] 34.3",
                "[BOLD] 50.5",
                "[BOLD] 79.8",
                "[BOLD] 79.9",
                "53.0",
                "[BOLD] 75.9",
                "[BOLD] 79.8",
                "[BOLD] 72.9",
                "48.6",
                "89.0"
            ],
            [
                "CBOW-R",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "[BOLD] 53.6",
                "74.5",
                "78.6",
                "72.0",
                "[BOLD] 49.6",
                "[BOLD] 89.5"
            ]
        ],
        "question": "Is it true that While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points lower scores on WordContent [CONTINUE] and BigramShift?",
        "answer_label": "yes"
    },
    {
        "id": "a859b35d-7e4b-4d4d-b124-9e84252c100b",
        "table_caption": "Low-supervision urgency detection and transfer in short crisis messages TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal",
        "table_column_names": [
            "System",
            "Accuracy",
            "Precision",
            "Recall",
            "F-Measure"
        ],
        "table_content_values": [
            [
                "Local",
                "63.97%",
                "64.27%",
                "64.50%",
                "63.93%"
            ],
            [
                "Manual",
                "64.25%",
                "[BOLD] 70.84%\u2217\u2217",
                "48.50%",
                "57.11%"
            ],
            [
                "Wiki",
                "67.25%",
                "66.51%",
                "69.50%",
                "67.76%"
            ],
            [
                "Local-Manual",
                "65.75%",
                "67.96%",
                "59.50%",
                "62.96%"
            ],
            [
                "Wiki-Local",
                "67.40%",
                "65.54%",
                "68.50%",
                "66.80%"
            ],
            [
                "Wiki-Manual",
                "67.75%",
                "70.38%",
                "63.00%",
                "65.79%"
            ],
            [
                "[ITALIC] Our Approach",
                "[BOLD] 69.25%\u2217\u2217\u2217",
                "68.76%",
                "[BOLD] 70.50%\u2217\u2217",
                "[BOLD] 69.44%\u2217\u2217\u2217"
            ]
        ],
        "question": "Is it true that The results illustrate the viability of urgency detection in low-supervision settings (with our approach yielding 69.44% F-Measure on Nepal, at 99% significance compared to the Local baseline), with different feature sets contributing differently to the four metrics?",
        "answer_label": "yes"
    },
    {
        "id": "4f42503e-f21e-45b0-83e1-2d986870b0a9",
        "table_caption": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment Table 1: Results on belinkov2014exploring\u2019s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et\u00a0al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Sch\u00fctze (2015) on GloVe.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] Initialization",
            "[BOLD] Embedding",
            "[BOLD] Resources",
            "[BOLD] Test Acc."
        ],
        "table_content_values": [
            [
                "HPCD (full)",
                "Syntactic-SG",
                "Type",
                "WordNet, VerbNet",
                "88.7"
            ],
            [
                "LSTM-PP",
                "GloVe",
                "Type",
                "-",
                "84.3"
            ],
            [
                "LSTM-PP",
                "GloVe-retro",
                "Type",
                "WordNet",
                "84.8"
            ],
            [
                "OntoLSTM-PP",
                "GloVe-extended",
                "Token",
                "WordNet",
                "[BOLD] 89.7"
            ]
        ],
        "question": "Is it true that Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP outperforms the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%?",
        "answer_label": "yes"
    },
    {
        "id": "d3085af2-d938-41fe-8453-0c632cca7716",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Type",
            "[BOLD] English-German #P",
            "[BOLD] English-German B",
            "[BOLD] English-German C",
            "[BOLD] English-Czech #P",
            "[BOLD] English-Czech B",
            "[BOLD] English-Czech C"
        ],
        "table_content_values": [
            [
                "BoW+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "12.2",
                "-",
                "-",
                "7.5",
                "-"
            ],
            [
                "CNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "13.7",
                "-",
                "-",
                "8.7",
                "-"
            ],
            [
                "BiRNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "16.1",
                "-",
                "-",
                "9.6",
                "-"
            ],
            [
                "PB-SMT (Beck et al.,  2018 )",
                "Single",
                "-",
                "12.8",
                "43.2",
                "-",
                "8.6",
                "36.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Single",
                "41.4M",
                "15.5",
                "40.8",
                "39.1M",
                "8.9",
                "33.8"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Single",
                "41.2M",
                "16.7",
                "42.4",
                "38.8M",
                "9.8",
                "33.3"
            ],
            [
                "DCGCN (ours)",
                "Single",
                "[BOLD]  29.7M",
                "[BOLD] 19.0",
                "[BOLD] 44.1",
                "[BOLD]  28.3M",
                "[BOLD] 12.1",
                "[BOLD] 37.1"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Ensemble",
                "207M",
                "19.0",
                "44.1",
                "195M",
                "11.3",
                "36.4"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Ensemble",
                "206M",
                "19.6",
                "45.1",
                "194M",
                "11.7",
                "35.9"
            ],
            [
                "DCGCN (ours)",
                "Ensemble",
                "[BOLD]  149M",
                "[BOLD] 20.5",
                "[BOLD] 45.8",
                "[BOLD]  142M",
                "[BOLD] 13.1",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that BoW+GCN, CNN+GCN and BiRNN+GCN refer to employing the following encoders with a GCN layer on top respectively: 1) a bag-of-words encoder, 2) a one-layer CNN, 3) a bidirectional RNN?",
        "answer_label": "yes"
    },
    {
        "id": "2a32cde4-28fb-4f0d-9e67-1a3ef3871e5b",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1192",
                "0.0083",
                "0.0137",
                "0.0150",
                "0.0150",
                "0.0445",
                "0.0326"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1022",
                "0.0069",
                "0.0060",
                "0.0092",
                "0.0090",
                "0.0356",
                "0.0162"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5710",
                "0.1948",
                "0.3855",
                "0.5474",
                "0.4485",
                "[BOLD] 0.8052",
                "0.4058"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "[BOLD] 0.6304",
                "0.1870",
                "0.3250",
                "0.5312",
                "0.4576",
                "0.6064",
                "0.3698"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0037",
                "0.3278",
                "0.5941",
                "0.6486",
                "[BOLD] 0.6490",
                "0.0017",
                "0.0003"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0002",
                "0.1486",
                "0.4332",
                "[BOLD] 0.6467",
                "0.6332",
                "0.0967",
                "0.0003"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0002",
                "0.1562",
                "0.5157",
                "[BOLD] 0.7255",
                "0.5932",
                "0.0032",
                "0.0001"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "2.10-5",
                "0.0507",
                "0.4492",
                "[BOLD] 0.7000",
                "0.5887",
                "0.1390",
                "0.0002"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0073",
                "0.0162",
                "0.0268",
                "[BOLD] 0.0293",
                "[BOLD] 0.0293",
                "0.0033",
                "0.0006"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0004",
                "0.0132",
                "0.0118",
                "0.0181",
                "0.0179",
                "[BOLD] 0.0520",
                "0.0005"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0005",
                "0.1733",
                "0.4412",
                "[BOLD] 0.6240",
                "0.5109",
                "0.0064",
                "0.0002"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "4.10-5",
                "0.0798",
                "0.3771",
                "[BOLD] 0.6040",
                "0.5149",
                "0.2261",
                "0.0004"
            ]
        ],
        "question": "Is it true that As occurred in the experiment using the top 1,000 words, this experiment also kept TF with the highest values of f-measure for most methods?",
        "answer_label": "yes"
    },
    {
        "id": "b9b60316-88c2-497b-a548-5474b6280198",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE IX: Accuracies (%) for Sentiment Classification Task",
        "table_column_names": [
            "GloVe",
            "Word2Vec",
            "OIWE-IPG",
            "SOV",
            "SPINE",
            "Word2Sense",
            "Proposed"
        ],
        "table_content_values": [
            [
                "77.34",
                "77.91",
                "74.27",
                "78.43",
                "74.13",
                "81.21",
                "78.26"
            ]
        ],
        "question": "Is it true that Pretrained Word2Sense embeddings outperform our method, however it has the advantage of training on a larger corpus?",
        "answer_label": "yes"
    },
    {
        "id": "808f4732-df2d-4930-8f56-448c21b910eb",
        "table_caption": "Evaluation of Greek Word Embeddings Table 3: Summary for 3CosAdd and top-1 nearest vectors.",
        "table_column_names": [
            "Category Semantic",
            "Category no oov words",
            "gr_def 58.42%",
            "gr_neg10 59.33%",
            "cc.el.300  [BOLD] 68.80%",
            "wiki.el 27.20%",
            "gr_cbow_def 31.76%",
            "gr_d300_nosub 60.79%",
            "gr_w2v_sg_n5 52.70%"
        ],
        "table_content_values": [
            [
                "[EMPTY]",
                "with oov words",
                "52.97%",
                "55.33%",
                "[BOLD] 64.34%",
                "25.73%",
                "28.80%",
                "55.11%",
                "47.82%"
            ],
            [
                "Syntactic",
                "no oov words",
                "65.73%",
                "61.02%",
                "[BOLD] 69.35%",
                "40.90%",
                "64.02%",
                "53.69%",
                "52.60%"
            ],
            [
                "[EMPTY]",
                "with oov words",
                "[BOLD] 53.95%",
                "48.69%",
                "49.43%",
                "28.42%",
                "52.54%",
                "44.06%",
                "43.13%"
            ],
            [
                "Overall",
                "no oov words",
                "63.02%",
                "59.96%",
                "[BOLD] 68.97%",
                "36.45%",
                "52.04%",
                "56.30%",
                "52.66%"
            ],
            [
                "[EMPTY]",
                "with oov words",
                "53.60%",
                "51.00%",
                "[BOLD] 54.60%",
                "27.50%",
                "44.30%",
                "47.90%",
                "44.80%"
            ]
        ],
        "question": "Is it true that Model wiki.el, trained only on Wikipedia, was the worst almost in every category (and sub-category)?",
        "answer_label": "yes"
    },
    {
        "id": "73f747e6-bd1f-459a-be5d-557593d0128c",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.",
        "table_column_names": [
            "Model",
            "Encoder",
            "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1",
            "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
            "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
            "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
            "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1",
            "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
            "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
            "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
        ],
        "table_content_values": [
            [
                "MLP",
                "CNN-RNN",
                ".311",
                ".340",
                ".486",
                ".532",
                ".318",
                ".335",
                ".481",
                ".524"
            ],
            [
                "MLP",
                "PMeans-RNN",
                ".313",
                ".331",
                ".489",
                ".536",
                ".354",
                ".375",
                ".502",
                ".556"
            ],
            [
                "MLP",
                "BERT",
                "[BOLD] .487",
                "[BOLD] .526",
                "[BOLD] .544",
                "[BOLD] .597",
                "[BOLD] .505",
                "[BOLD] .531",
                "[BOLD] .556",
                "[BOLD] .608"
            ],
            [
                "SimRed",
                "CNN",
                ".340",
                ".392",
                ".470",
                ".515",
                ".396",
                ".443",
                ".499",
                ".549"
            ],
            [
                "SimRed",
                "PMeans",
                ".354",
                ".393",
                ".493",
                ".541",
                ".370",
                ".374",
                ".507",
                ".551"
            ],
            [
                "SimRed",
                "BERT",
                ".266",
                ".296",
                ".458",
                ".495",
                ".325",
                ".338",
                ".485",
                ".533"
            ],
            [
                "Peyrard and Gurevych ( 2018 )",
                "Peyrard and Gurevych ( 2018 )",
                ".177",
                ".189",
                ".271",
                ".306",
                ".175",
                ".186",
                ".268",
                ".174"
            ]
        ],
        "question": "Is it true that MLP with BERT as en(2018) coder has the best overall performance?",
        "answer_label": "yes"
    },
    {
        "id": "a2ada531-d61d-4735-836c-cbb20e8c7d46",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that When we increase the DCGCN blocks from 1 to 4, the model performance continues increasing on AMR15 development set?",
        "answer_label": "yes"
    },
    {
        "id": "ced2b584-1a28-45df-92fc-3613d7dbcf34",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 7: Comparisons of different DCGCN models under almost the same parameter budget.",
        "table_column_names": [
            "[BOLD] Model",
            "D",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN(1)",
                "300",
                "10.9M",
                "20.9",
                "52.0"
            ],
            [
                "DCGCN(2)",
                "180",
                "10.9M",
                "[BOLD] 22.2",
                "[BOLD] 52.3"
            ],
            [
                "DCGCN(2)",
                "240",
                "11.3M",
                "22.8",
                "52.8"
            ],
            [
                "DCGCN(4)",
                "180",
                "11.4M",
                "[BOLD] 23.4",
                "[BOLD] 53.4"
            ],
            [
                "DCGCN(1)",
                "420",
                "12.6M",
                "22.2",
                "52.4"
            ],
            [
                "DCGCN(2)",
                "300",
                "12.5M",
                "23.8",
                "53.8"
            ],
            [
                "DCGCN(3)",
                "240",
                "12.3M",
                "[BOLD] 23.9",
                "[BOLD] 54.1"
            ],
            [
                "DCGCN(2)",
                "360",
                "14.0M",
                "24.2",
                "[BOLD] 54.4"
            ],
            [
                "DCGCN(3)",
                "300",
                "14.0M",
                "[BOLD] 24.4",
                "54.2"
            ],
            [
                "DCGCN(2)",
                "420",
                "15.6M",
                "24.1",
                "53.7"
            ],
            [
                "DCGCN(4)",
                "300",
                "15.6M",
                "[BOLD] 24.6",
                "[BOLD] 54.8"
            ],
            [
                "DCGCN(3)",
                "420",
                "18.6M",
                "24.5",
                "54.6"
            ],
            [
                "DCGCN(4)",
                "360",
                "18.4M",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that Similarly, when DCGCN3 and DCGCN4 contain 18.6M and 18.4M parameters?",
        "answer_label": "yes"
    },
    {
        "id": "2496440e-30cf-465a-8de6-814a13e9b1f5",
        "table_caption": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment Table 1: Results on belinkov2014exploring\u2019s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et\u00a0al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Sch\u00fctze (2015) on GloVe.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] Initialization",
            "[BOLD] Embedding",
            "[BOLD] Resources",
            "[BOLD] Test Acc."
        ],
        "table_content_values": [
            [
                "HPCD (full)",
                "Syntactic-SG",
                "Type",
                "WordNet, VerbNet",
                "88.7"
            ],
            [
                "LSTM-PP",
                "GloVe",
                "Type",
                "-",
                "84.3"
            ],
            [
                "LSTM-PP",
                "GloVe-retro",
                "Type",
                "WordNet",
                "84.8"
            ],
            [
                "OntoLSTM-PP",
                "GloVe-extended",
                "Token",
                "WordNet",
                "[BOLD] 89.7"
            ]
        ],
        "question": "Is it true that OntoLSTM-PP also outperforms HPCD (full), the previous best result on this dataset?",
        "answer_label": "yes"
    },
    {
        "id": "7643b874-54d1-4b3e-a2f6-d022395c9e6d",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that For example, GCN+RC+LA (10) achieves a BLEU score of 21.2, which is worse than GCN+RC+LA (9)?",
        "answer_label": "yes"
    },
    {
        "id": "0e60f569-bde5-4ada-8fbc-6176240824bf",
        "table_caption": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context Table 1: Multi-action evaluation results. The \u201cw\u201d and \u201cw/o\u201d column denote with and without data augmentation respectively, and the better score between them is in bold. We report the average performance over 5 runs.",
        "table_column_names": [
            "Model & Decoding Scheme",
            "Act # w/o",
            "Act # w/",
            "Slot # w/o",
            "Slot # w/"
        ],
        "table_content_values": [
            [
                "Single-Action Baselines",
                "Single-Action Baselines",
                "Single-Action Baselines",
                "Single-Action Baselines",
                "Single-Action Baselines"
            ],
            [
                "DAMD + greedy",
                "[BOLD] 1.00",
                "[BOLD] 1.00",
                "1.95",
                "[BOLD] 2.51"
            ],
            [
                "HDSA + fixed threshold",
                "[BOLD] 1.00",
                "[BOLD] 1.00",
                "2.07",
                "[BOLD] 2.40"
            ],
            [
                "5-Action Generation",
                "5-Action Generation",
                "5-Action Generation",
                "5-Action Generation",
                "5-Action Generation"
            ],
            [
                "DAMD + beam search",
                "2.67",
                "[BOLD] 2.87",
                "3.36",
                "[BOLD] 4.39"
            ],
            [
                "DAMD + diverse beam search",
                "2.68",
                "[BOLD] 2.88",
                "3.41",
                "[BOLD] 4.50"
            ],
            [
                "DAMD + top-k sampling",
                "3.08",
                "[BOLD] 3.43",
                "3.61",
                "[BOLD] 4.91"
            ],
            [
                "DAMD + top-p sampling",
                "3.08",
                "[BOLD] 3.40",
                "3.79",
                "[BOLD] 5.20"
            ],
            [
                "HDSA + sampled threshold",
                "1.32",
                "[BOLD] 1.50",
                "3.08",
                "[BOLD] 3.31"
            ],
            [
                "10-Action Generation",
                "10-Action Generation",
                "10-Action Generation",
                "10-Action Generation",
                "10-Action Generation"
            ],
            [
                "DAMD + beam search",
                "3.06",
                "[BOLD] 3.39",
                "4.06",
                "[BOLD] 5.29"
            ],
            [
                "DAMD + diverse beam search",
                "3.05",
                "[BOLD] 3.39",
                "4.05",
                "[BOLD] 5.31"
            ],
            [
                "DAMD + top-k sampling",
                "3.59",
                "[BOLD] 4.12",
                "4.21",
                "[BOLD] 5.77"
            ],
            [
                "DAMD + top-p sampling",
                "3.53",
                "[BOLD] 4.02",
                "4.41",
                "[BOLD] 6.17"
            ],
            [
                "HDSA + sampled threshold",
                "1.54",
                "[BOLD] 1.83",
                "3.42",
                "[BOLD] 3.92"
            ]
        ],
        "question": "Is it true that [CONTINUE] After applying our data augmentation, both the action and slot diversity are improved consistently, [CONTINUE] HDSA has the better performance and benefits more from data augmentation comparing to our proposed domain-aware multi-decoder network?",
        "answer_label": "no"
    },
    {
        "id": "a2e66d42-21d8-4914-9262-6b5dac5f738d",
        "table_caption": "Low-supervision urgency detection and transfer in short crisis messages TABLE II: Details on datasets used for experiments.",
        "table_column_names": [
            "Dataset",
            "Unlabeled / Labeled Messages",
            "Urgent / Non-urgent Messages",
            "Unique Tokens",
            "Avg. Tokens / Message",
            "Time Range"
        ],
        "table_content_values": [
            [
                "Nepal",
                "6,063/400",
                "201/199",
                "1,641",
                "14",
                "04/05/2015-05/06/2015"
            ],
            [
                "Macedonia",
                "0/205",
                "92/113",
                "129",
                "18",
                "09/18/2018-09/21/2018"
            ],
            [
                "Kerala",
                "92,046/400",
                "125/275",
                "19,393",
                "15",
                "08/17/2018-08/22/2018"
            ]
        ],
        "question": "Is it true that Table II shows that Nepal is roughly balanced, while Kerala is imbalanced?",
        "answer_label": "yes"
    },
    {
        "id": "2ff59490-1779-47f3-828f-e2c1600f71e9",
        "table_caption": "Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
        "table_column_names": [
            "Dataset",
            "Metric",
            "Illinois",
            "IlliCons",
            "rahman2012resolving",
            "KnowFeat",
            "KnowCons",
            "KnowComb"
        ],
        "table_content_values": [
            [
                "[ITALIC] Winograd",
                "Precision",
                "51.48",
                "53.26",
                "73.05",
                "71.81",
                "74.93",
                "[BOLD] 76.41"
            ],
            [
                "[ITALIC] WinoCoref",
                "AntePre",
                "68.37",
                "74.32",
                "\u2014\u2013",
                "88.48",
                "88.95",
                "[BOLD] 89.32"
            ]
        ],
        "question": "Is it true that On the WinoCoref dataset, KnowComb does not improve by 15%?",
        "answer_label": "no"
    },
    {
        "id": "f658a810-80dc-48f8-b9fc-5445844a411c",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 4: Test BLEU for en-de adaptive training, with sequential adaptation to a third task. EWC-tuned models give the best performance on each domain.",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Training scheme",
            "[BOLD] News",
            "[BOLD] TED",
            "[BOLD] IT"
        ],
        "table_content_values": [
            [
                "1",
                "News",
                "37.8",
                "25.3",
                "35.3"
            ],
            [
                "2",
                "TED",
                "23.7",
                "24.1",
                "14.4"
            ],
            [
                "3",
                "IT",
                "1.6",
                "1.8",
                "39.6"
            ],
            [
                "4",
                "News and TED",
                "38.2",
                "25.5",
                "35.4"
            ],
            [
                "5",
                "1 then TED, No-reg",
                "30.6",
                "[BOLD] 27.0",
                "22.1"
            ],
            [
                "6",
                "1 then TED, L2",
                "37.9",
                "26.7",
                "31.8"
            ],
            [
                "7",
                "1 then TED, EWC",
                "[BOLD] 38.3",
                "[BOLD] 27.0",
                "33.1"
            ],
            [
                "8",
                "5 then IT, No-reg",
                "8.0",
                "6.9",
                "56.3"
            ],
            [
                "9",
                "6 then IT, L2",
                "32.3",
                "22.6",
                "56.9"
            ],
            [
                "10",
                "7 then IT, EWC",
                "35.8",
                "24.6",
                "[BOLD] 57.0"
            ]
        ],
        "question": "Is it true that However, EWC does not outperform no-reg and L2 on News, as it only gives a 0.5 BLEU improvement over the baseline News model?",
        "answer_label": "no"
    },
    {
        "id": "7fe42729-fa71-4447-b95a-5048ec662bce",
        "table_caption": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 2: F1 score results per relation type of the best performing models.",
        "table_column_names": [
            "Relation type",
            "Count",
            "Intra-sentential co-occ.  [ITALIC] \u03c1=0",
            "Intra-sentential co-occ.  [ITALIC] \u03c1=5",
            "Intra-sentential co-occ.  [ITALIC] \u03c1=10",
            "BoC(Wiki-PubMed-PMC) LR",
            "BoC(Wiki-PubMed-PMC) SVM",
            "BoC(Wiki-PubMed-PMC) ANN"
        ],
        "table_content_values": [
            [
                "TherapyTiming(TP,TD)",
                "428",
                "[BOLD] 0.84",
                "0.59",
                "0.47",
                "0.78",
                "0.81",
                "0.78"
            ],
            [
                "NextReview(Followup,TP)",
                "164",
                "[BOLD] 0.90",
                "0.83",
                "0.63",
                "0.86",
                "0.88",
                "0.84"
            ],
            [
                "Toxicity(TP,CF/TR)",
                "163",
                "[BOLD] 0.91",
                "0.77",
                "0.55",
                "0.85",
                "0.86",
                "0.86"
            ],
            [
                "TestTiming(TN,TD/TP)",
                "184",
                "0.90",
                "0.81",
                "0.42",
                "0.96",
                "[BOLD] 0.97",
                "0.95"
            ],
            [
                "TestFinding(TN,TR)",
                "136",
                "0.76",
                "0.60",
                "0.44",
                "[BOLD] 0.82",
                "0.79",
                "0.78"
            ],
            [
                "Threat(O,CF/TR)",
                "32",
                "0.85",
                "0.69",
                "0.54",
                "[BOLD] 0.95",
                "[BOLD] 0.95",
                "0.92"
            ],
            [
                "Intervention(TP,YR)",
                "5",
                "[BOLD] 0.88",
                "0.65",
                "0.47",
                "-",
                "-",
                "-"
            ],
            [
                "EffectOf(Com,CF)",
                "3",
                "[BOLD] 0.92",
                "0.62",
                "0.23",
                "-",
                "-",
                "-"
            ],
            [
                "Severity(CF,CS)",
                "75",
                "[BOLD] 0.61",
                "0.53",
                "0.47",
                "0.52",
                "0.55",
                "0.51"
            ],
            [
                "RecurLink(YR,YR/CF)",
                "7",
                "[BOLD] 1.0",
                "[BOLD] 1.0",
                "0.64",
                "-",
                "-",
                "-"
            ],
            [
                "RecurInfer(NR/YR,TR)",
                "51",
                "0.97",
                "0.69",
                "0.43",
                "[BOLD] 0.99",
                "[BOLD] 0.99",
                "0.98"
            ],
            [
                "GetOpinion(Referral,CF/other)",
                "4",
                "[BOLD] 0.75",
                "[BOLD] 0.75",
                "0.5",
                "-",
                "-",
                "-"
            ],
            [
                "Context(Dis,DisCont)",
                "40",
                "[BOLD] 0.70",
                "0.63",
                "0.53",
                "0.60",
                "0.41",
                "0.57"
            ],
            [
                "TestToAssess(TN,CF/TR)",
                "36",
                "0.76",
                "0.66",
                "0.36",
                "[BOLD] 0.92",
                "[BOLD] 0.92",
                "0.91"
            ],
            [
                "TimeStamp(TD,TP)",
                "221",
                "[BOLD] 0.88",
                "0.83",
                "0.50",
                "0.86",
                "0.85",
                "0.83"
            ],
            [
                "TimeLink(TP,TP)",
                "20",
                "[BOLD] 0.92",
                "0.85",
                "0.45",
                "0.91",
                "[BOLD] 0.92",
                "0.90"
            ],
            [
                "Overall",
                "1569",
                "0.90",
                "0.73",
                "0.45",
                "0.92",
                "[BOLD] 0.93",
                "0.91"
            ]
        ],
        "question": "Is it true that [CONTINUE] As the results of applying the co-occurrence baseline (\u03c1 = 0) shows (Table 2), the semantic relations in this data are not strongly concentrated within a sentence boundary, as evidenced by the relatively low F1 scores for the relation of TestTiming (0.90) and TestFinding (0.76)?",
        "answer_label": "no"
    },
    {
        "id": "8dd82b54-3d4d-4100-a866-be3f6c35160f",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
        "table_column_names": [
            "<bold>Model</bold>",
            "R",
            "MUC P",
            "<italic>F</italic>1",
            "R",
            "B3 P",
            "<italic>F</italic>1",
            "R",
            "CEAF-<italic>e</italic> P",
            "<italic>F</italic>1",
            "CoNLL <italic>F</italic>1"
        ],
        "table_content_values": [
            [
                "Cluster+Lemma",
                "71.3",
                "83",
                "76.7",
                "53.4",
                "84.9",
                "65.6",
                "70.1",
                "52.5",
                "60",
                "67.4"
            ],
            [
                "Disjoint",
                "76.7",
                "80.8",
                "78.7",
                "63.2",
                "78.2",
                "69.9",
                "65.3",
                "58.3",
                "61.6",
                "70"
            ],
            [
                "Joint",
                "78.6",
                "80.9",
                "79.7",
                "65.5",
                "76.4",
                "70.5",
                "65.4",
                "61.3",
                "63.3",
                "<bold>71.2</bold>"
            ]
        ],
        "question": "Is it true that Our joint model does not improve upon the strong lemma baseline by 3.8 points in CoNLL F1 score?",
        "answer_label": "no"
    },
    {
        "id": "2a9eb10f-7dee-4dc5-8c9a-663a90cb7c87",
        "table_caption": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 6: Results of the Human Rating on CWC.",
        "table_column_names": [
            "[EMPTY]",
            "Ours Better(%)",
            "No Prefer(%)",
            "Ours Worse(%)"
        ],
        "table_content_values": [
            [
                "Retrieval-Stgy\u00a0",
                "[BOLD] 62",
                "22",
                "16"
            ],
            [
                "PMI\u00a0",
                "[BOLD] 54",
                "32",
                "14"
            ],
            [
                "Neural\u00a0",
                "[BOLD] 60",
                "22",
                "18"
            ],
            [
                "Kernel\u00a0",
                "[BOLD] 62",
                "26",
                "12"
            ]
        ],
        "question": "Is it true that Our agent does not outperform the comparison agents with a large margin?",
        "answer_label": "no"
    },
    {
        "id": "fb607d1d-2d8e-4b05-ac94-895e601e5682",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE VII: Precision scores for the Analogy Test",
        "table_column_names": [
            "Methods",
            "# dims",
            "Analg. (sem)",
            "Analg. (syn)",
            "Total"
        ],
        "table_content_values": [
            [
                "GloVe",
                "300",
                "78.94",
                "64.12",
                "70.99"
            ],
            [
                "Word2Vec",
                "300",
                "81.03",
                "66.11",
                "73.03"
            ],
            [
                "OIWE-IPG",
                "300",
                "19.99",
                "23.44",
                "21.84"
            ],
            [
                "SOV",
                "3000",
                "64.09",
                "46.26",
                "54.53"
            ],
            [
                "SPINE",
                "1000",
                "17.07",
                "8.68",
                "12.57"
            ],
            [
                "Word2Sense",
                "2250",
                "12.94",
                "19.44",
                "5.84"
            ],
            [
                "Proposed",
                "300",
                "79.96",
                "63.52",
                "71.15"
            ]
        ],
        "question": "Is it true that However, our proposed method has comparable performance with the original GloVe embeddings?",
        "answer_label": "yes"
    },
    {
        "id": "ab2c2f1d-1b29-4c35-81bd-e2ea6c8073c8",
        "table_caption": "Two Causal Principles for Improving Visual Dialog Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table\u00a01. Note that only applying P2 is implemented by the implementations in Section\u00a05 with the history shortcut.",
        "table_column_names": [
            "Model",
            "LF\u00a0",
            "HCIAE\u00a0",
            "CoAtt\u00a0",
            "RvA\u00a0"
        ],
        "table_content_values": [
            [
                "baseline",
                "57.21",
                "56.98",
                "56.46",
                "56.74"
            ],
            [
                "+P1",
                "61.88",
                "60.12",
                "60.27",
                "61.02"
            ],
            [
                "+P2",
                "72.65",
                "71.50",
                "71.41",
                "71.44"
            ],
            [
                "+P1+P2",
                "[BOLD] 73.63",
                "71.99",
                "71.87",
                "72.88"
            ]
        ],
        "question": "Is it true that In general, our principle P2 can improve all the models in any ablative condition (i.e., P1, P2, P1+P2), while P1 does not always lead to an improvement?",
        "answer_label": "no"
    },
    {
        "id": "ca69f7e8-9e0e-4870-859c-649e8f88eceb",
        "table_caption": "What do Deep Networks Like to Read? Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.",
        "table_column_names": [
            "[EMPTY]",
            "<bold>RNN</bold>",
            "<bold>CNN</bold>",
            "<bold>DAN</bold>"
        ],
        "table_content_values": [
            [
                "Positive",
                "+9.7",
                "+4.3",
                "+<bold>23.6</bold>"
            ],
            [
                "Negative",
                "+6.9",
                "+5.5",
                "+<bold>16.1</bold>"
            ],
            [
                "Flipped to Positive",
                "+20.2",
                "+24.9",
                "+27.4"
            ],
            [
                "Flipped to Negative",
                "+31.5",
                "+28.6",
                "+19.3"
            ]
        ],
        "question": "Is it true that By considering only adjectives, we obtain a measure of the positive and negative score for each sentence before and after fine-tuning?",
        "answer_label": "yes"
    },
    {
        "id": "f30d3d9e-a1e3-4e50-a778-882897039098",
        "table_caption": "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns Table 6: Performance of our baselines on the development set. Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.",
        "table_column_names": [
            "[EMPTY]",
            "M",
            "F",
            "B",
            "O"
        ],
        "table_content_values": [
            [
                "Random",
                "43.6",
                "39.3",
                "[ITALIC] 0.90",
                "41.5"
            ],
            [
                "Token Distance",
                "50.1",
                "42.4",
                "[ITALIC] 0.85",
                "46.4"
            ],
            [
                "Topical Entity",
                "51.5",
                "43.7",
                "[ITALIC] 0.85",
                "47.7"
            ],
            [
                "Syntactic Distance",
                "63.0",
                "56.2",
                "[ITALIC] 0.89",
                "59.7"
            ],
            [
                "Parallelism",
                "[BOLD] 67.1",
                "[BOLD] 63.1",
                "[ITALIC]  [BOLD] 0.94",
                "[BOLD] 65.2"
            ],
            [
                "Parallelism+URL",
                "[BOLD] 71.1",
                "[BOLD] 66.9",
                "[ITALIC]  [BOLD] 0.94",
                "[BOLD] 69.0"
            ],
            [
                "Transformer-Single",
                "58.6",
                "51.2",
                "[ITALIC] 0.87",
                "55.0"
            ],
            [
                "Transformer-Multi",
                "59.3",
                "52.9",
                "[ITALIC] 0.89",
                "56.2"
            ]
        ],
        "question": "Is it true that [CONTINUE] TRANSFORMER-MULTI is stronger than TRANSFORMER-SINGLE [CONTINUE] .2% overall improvement over TRANSFORMER-SINGLE for the goldtwo-mention task?",
        "answer_label": "yes"
    },
    {
        "id": "5bf06f43-9f2d-4b94-a6da-8fc836362016",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Original",
                "TGen\u2212",
                "63.37",
                "7.7188",
                "41.99",
                "68.53",
                "1.9355",
                "00.06",
                "15.77",
                "00.11",
                "15.94"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen",
                "66.41",
                "8.5565",
                "45.07",
                "69.17",
                "2.2253",
                "00.14",
                "04.11",
                "00.03",
                "04.27"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen+",
                "67.06",
                "8.5871",
                "45.83",
                "69.73",
                "2.2681",
                "00.04",
                "01.75",
                "00.01",
                "01.80"
            ],
            [
                "Original",
                "[BOLD] Original",
                "SC-LSTM",
                "39.11",
                "5.6704",
                "36.83",
                "50.02",
                "0.6045",
                "02.79",
                "18.90",
                "09.79",
                "31.51"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen\u2212",
                "65.87",
                "8.6400",
                "44.20",
                "67.51",
                "2.1710",
                "00.20",
                "00.56",
                "00.21",
                "00.97"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen",
                "66.24",
                "8.6889",
                "44.66",
                "67.85",
                "2.2181",
                "00.10",
                "00.02",
                "00.00",
                "00.12"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen+",
                "65.97",
                "8.6630",
                "44.45",
                "67.59",
                "2.1855",
                "00.02",
                "00.00",
                "00.00",
                "00.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "SC-LSTM",
                "38.52",
                "5.7125",
                "37.45",
                "48.50",
                "0.4343",
                "03.85",
                "17.39",
                "08.12",
                "29.37"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen\u2212",
                "66.28",
                "8.5202",
                "43.96",
                "67.83",
                "2.1375",
                "00.14",
                "02.26",
                "00.22",
                "02.61"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen",
                "67.00",
                "8.6889",
                "44.97",
                "68.19",
                "2.2228",
                "00.06",
                "00.44",
                "00.03",
                "00.53"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen+",
                "66.74",
                "8.6649",
                "44.84",
                "67.95",
                "2.2018",
                "00.00",
                "00.21",
                "00.03",
                "00.24"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen\u2212",
                "64.40",
                "7.9692",
                "42.81",
                "68.87",
                "2.0563",
                "00.01",
                "13.08",
                "00.00",
                "13.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen",
                "66.23",
                "8.5578",
                "45.12",
                "68.87",
                "2.2548",
                "00.04",
                "03.04",
                "00.00",
                "03.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen+",
                "65.96",
                "8.5238",
                "45.49",
                "68.79",
                "2.2456",
                "00.00",
                "01.44",
                "00.00",
                "01.45"
            ]
        ],
        "question": "Is it true that In other words, [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe no significant effect on SER from cleaning the missed slots?",
        "answer_label": "no"
    },
    {
        "id": "aad0d5bd-5420-41cc-88eb-af2eaa32aac1",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks but WC and SOMO?",
        "answer_label": "yes"
    },
    {
        "id": "dfb953a4-cb91-45be-98c4-5977336f6d6c",
        "table_caption": "Towards Universal Dialogue State Tracking Table 1: Joint goal accuracy on DSTC2 and WOZ 2.0 test set vs.\u00a0various approaches as reported in the literature.",
        "table_column_names": [
            "[BOLD] DST Models",
            "[BOLD] Joint Acc. DSTC2",
            "[BOLD] Joint Acc. WOZ 2.0"
        ],
        "table_content_values": [
            [
                "Delexicalisation-Based (DB) Model Mrk\u0161i\u0107 et\u00a0al. ( 2017 )",
                "69.1",
                "70.8"
            ],
            [
                "DB Model + Semantic Dictionary Mrk\u0161i\u0107 et\u00a0al. ( 2017 )",
                "72.9",
                "83.7"
            ],
            [
                "Scalable Multi-domain DST Rastogi et\u00a0al. ( 2017 )",
                "70.3",
                "-"
            ],
            [
                "MemN2N Perez and Liu ( 2017 )",
                "74.0",
                "-"
            ],
            [
                "PtrNet Xu and Hu ( 2018 )",
                "72.1",
                "-"
            ],
            [
                "Neural Belief Tracker: NBT-DNN Mrk\u0161i\u0107 et\u00a0al. ( 2017 )",
                "72.6",
                "84.4"
            ],
            [
                "Neural Belief Tracker: NBT-CNN Mrk\u0161i\u0107 et\u00a0al. ( 2017 )",
                "73.4",
                "84.2"
            ],
            [
                "Belief Tracking: Bi-LSTM Ramadan et\u00a0al. ( 2018 )",
                "-",
                "85.1"
            ],
            [
                "Belief Tracking: CNN Ramadan et\u00a0al. ( 2018 )",
                "-",
                "85.5"
            ],
            [
                "GLAD Zhong et\u00a0al. ( 2018 )",
                "74.5",
                "88.1"
            ],
            [
                "StateNet",
                "74.1",
                "87.8"
            ],
            [
                "StateNet_PS",
                "74.5",
                "88.2"
            ],
            [
                "[BOLD] StateNet_PSI",
                "[BOLD] 75.5",
                "[BOLD] 88.9"
            ]
        ],
        "question": "Is it true that StateNet PSI does not outperform StateNet, and StateNet PS performs best among all 3 models?",
        "answer_label": "no"
    },
    {
        "id": "6468a0b4-715d-495e-9627-1f859a1198cb",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).",
        "table_column_names": [
            "Model",
            "#Params",
            "Base",
            "+Elmo"
        ],
        "table_content_values": [
            [
                "rnet*",
                "-",
                "71.1/79.5",
                "-/-"
            ],
            [
                "LSTM",
                "2.67M",
                "[BOLD] 70.46/78.98",
                "75.17/82.79"
            ],
            [
                "GRU",
                "2.31M",
                "70.41/ [BOLD] 79.15",
                "75.81/83.12"
            ],
            [
                "ATR",
                "1.59M",
                "69.73/78.70",
                "75.06/82.76"
            ],
            [
                "SRU",
                "2.44M",
                "69.27/78.41",
                "74.56/82.50"
            ],
            [
                "LRN",
                "2.14M",
                "70.11/78.83",
                "[BOLD] 76.14/ [BOLD] 83.83"
            ]
        ],
        "question": "Is it true that In this task, ATR and SRU outperform LRN in terms of both EM and F1 score?",
        "answer_label": "no"
    },
    {
        "id": "a8cb7bdd-dbe4-4991-8ecd-99ee870ab569",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
        "table_column_names": [
            "Model",
            "Method",
            "Training Data",
            "Overall",
            "Easy",
            "Hard",
            "p-value (%)"
        ],
        "table_content_values": [
            [
                "goodwin-etal-2012-utdhlt",
                "PMI",
                "unsupervised",
                "61.8",
                "64.7",
                "60.0",
                "19.8"
            ],
            [
                "gordon_commonsense_2011-1",
                "PMI",
                "unsupervised",
                "65.4",
                "65.8",
                "65.2",
                "83.5"
            ],
            [
                "sasaki-etal-2017-handling",
                "PMI",
                "unsupervised",
                "71.4",
                "75.3",
                "69.0",
                "4.8\u2217"
            ],
            [
                "Word frequency",
                "wordfreq",
                "COPA",
                "53.5",
                "57.4",
                "51.3",
                "9.8"
            ],
            [
                "BERT-large-FT",
                "LM, NSP",
                "COPA",
                "76.5 (\u00b1 2.7)",
                "83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)",
                "0.0\u2217"
            ],
            [
                "RoBERTa-large-FT",
                "LM",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)",
                "0.0\u2217"
            ]
        ],
        "question": "Is it true that We then compare BERT and RoBERTa with previous models on the Easy and Hard subsets. As Table 4 shows, previous models perform significantly better on the Easy subset than on the Hard subset, with the exception of Sasaki et al?",
        "answer_label": "no"
    },
    {
        "id": "d244fa95-6080-43f6-869e-02dcacc260ce",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Feature",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Feature",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams"
            ],
            [
                "not",
                ".154",
                "[URL]",
                ".150"
            ],
            [
                "my",
                ".131",
                "!",
                ".082"
            ],
            [
                "working",
                ".124",
                "he",
                ".069"
            ],
            [
                "still",
                ".123",
                "thank",
                ".067"
            ],
            [
                "on",
                ".119",
                ",",
                ".064"
            ],
            [
                "can\u2019t",
                ".113",
                "love",
                ".064"
            ],
            [
                "service",
                ".112",
                "lol",
                ".061"
            ],
            [
                "customer",
                ".109",
                "you",
                ".060"
            ],
            [
                "why",
                ".108",
                "great",
                ".058"
            ],
            [
                "website",
                ".107",
                "win",
                ".058"
            ],
            [
                "no",
                ".104",
                "\u2019",
                ".058"
            ],
            [
                "?",
                ".098",
                "she",
                ".054"
            ],
            [
                "fix",
                ".093",
                ":",
                ".053"
            ],
            [
                "won\u2019t",
                ".092",
                "that",
                ".053"
            ],
            [
                "been",
                ".090",
                "more",
                ".052"
            ],
            [
                "issue",
                ".089",
                "it",
                ".052"
            ],
            [
                "days",
                ".088",
                "would",
                ".051"
            ],
            [
                "error",
                ".087",
                "him",
                ".047"
            ],
            [
                "is",
                ".084",
                "life",
                ".046"
            ],
            [
                "charged",
                ".083",
                "good",
                ".046"
            ],
            [
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)"
            ],
            [
                "VBN",
                ".141",
                "UH",
                ".104"
            ],
            [
                "$",
                ".118",
                "NNP",
                ".098"
            ],
            [
                "VBZ",
                ".114",
                "PRP",
                ".076"
            ],
            [
                "NN_VBZ",
                ".114",
                "HT",
                ".076"
            ],
            [
                "PRP$",
                ".107",
                "PRP_.",
                ".076"
            ],
            [
                "PRP$_NN",
                ".105",
                "PRP_RB",
                ".067"
            ],
            [
                "VBG",
                ".093",
                "NNP_NNP",
                ".062"
            ],
            [
                "CD",
                ".092",
                "VBP_PRP",
                ".054"
            ],
            [
                "WRB_VBZ",
                ".084",
                "JJ",
                ".053"
            ],
            [
                "VBZ_VBN",
                ".084",
                "DT_JJ",
                ".051"
            ]
        ],
        "question": "Is it true that [CONTINUE] Negations are uncovered through unigrams (not, no, won't) [CONTINUE] Several unigrams (error, issue, working, fix) [CONTINUE] Words regularly describing negative sentiment or emotions (such as 'not', 'my', and 'can't') are among the most distinctive features for complaints?",
        "answer_label": "no"
    },
    {
        "id": "4e1bee90-35f0-4df8-ba48-b2a758e2d9d6",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. \u201cEn\u201d column is an English autoencoder. BLEU scores are given for reference.",
        "table_column_names": [
            "[ITALIC] k",
            "Ar",
            "Es",
            "Fr",
            "Ru",
            "Zh",
            "En"
        ],
        "table_content_values": [
            [
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy"
            ],
            [
                "0",
                "88.0",
                "87.9",
                "87.9",
                "87.8",
                "87.7",
                "87.4"
            ],
            [
                "1",
                "92.4",
                "91.9",
                "92.1",
                "92.1",
                "91.5",
                "89.4"
            ],
            [
                "2",
                "91.9",
                "91.8",
                "91.8",
                "91.8",
                "91.3",
                "88.3"
            ],
            [
                "3",
                "92.0",
                "92.3",
                "92.1",
                "91.6",
                "91.2",
                "87.9"
            ],
            [
                "4",
                "92.1",
                "92.4",
                "92.5",
                "92.0",
                "90.5",
                "86.9"
            ],
            [
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy"
            ],
            [
                "0",
                "81.9",
                "81.9",
                "81.8",
                "81.8",
                "81.8",
                "81.2"
            ],
            [
                "1",
                "87.9",
                "87.7",
                "87.8",
                "87.9",
                "87.7",
                "84.5"
            ],
            [
                "2",
                "87.4",
                "87.5",
                "87.4",
                "87.3",
                "87.2",
                "83.2"
            ],
            [
                "3",
                "87.8",
                "87.9",
                "87.9",
                "87.3",
                "87.3",
                "82.9"
            ],
            [
                "4",
                "88.3",
                "88.6",
                "88.4",
                "88.1",
                "87.7",
                "82.1"
            ],
            [
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU"
            ],
            [
                "[EMPTY]",
                "32.7",
                "49.1",
                "38.5",
                "34.2",
                "32.1",
                "96.6"
            ]
        ],
        "question": "Is it true that [CONTINUE] Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 only marginally boost the performance to around 87-88%, [CONTINUE] which is not significantly higher than the UnsupEmb and MFT baselines?",
        "answer_label": "no"
    },
    {
        "id": "59e2114d-5576-4698-9ac1-bea4da38592d",
        "table_caption": "Semantic Neural Machine Translation using AMR Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.",
        "table_column_names": [
            "System",
            "NC-v11 BLEU",
            "NC-v11 TER\u2193",
            "NC-v11 Meteor",
            "Full BLEU",
            "Full TER\u2193",
            "Full Meteor"
        ],
        "table_content_values": [
            [
                "OpenNMT-tf",
                "15.1",
                "0.6902",
                "0.3040",
                "24.3",
                "0.5567",
                "0.4225"
            ],
            [
                "Transformer-tf",
                "17.1",
                "0.6647",
                "0.3578",
                "25.1",
                "0.5537",
                "0.4344"
            ],
            [
                "Seq2seq",
                "16.0",
                "0.6695",
                "0.3379",
                "23.7",
                "0.5590",
                "0.4258"
            ],
            [
                "Dual2seq-LinAMR",
                "17.3",
                "0.6530",
                "0.3612",
                "24.0",
                "0.5643",
                "0.4246"
            ],
            [
                "Duel2seq-SRL",
                "17.2",
                "0.6591",
                "0.3644",
                "23.8",
                "0.5626",
                "0.4223"
            ],
            [
                "Dual2seq-Dep",
                "17.8",
                "0.6516",
                "0.3673",
                "25.0",
                "0.5538",
                "0.4328"
            ],
            [
                "Dual2seq",
                "[BOLD] *19.2*",
                "[BOLD] 0.6305",
                "[BOLD] 0.3840",
                "[BOLD] *25.5*",
                "[BOLD] 0.5480",
                "[BOLD] 0.4376"
            ]
        ],
        "question": "Is it true that Dual2seq is not consistently better than the other systems under all three metrics, [CONTINUE] as OpenNMT-tf and Transformer-tf both outperform Dual2seq in terms of BLEU and Meteor scores?",
        "answer_label": "no"
    },
    {
        "id": "78f1241f-560b-413e-aeeb-e7d909a8b4fd",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Original",
                "TGen\u2212",
                "63.37",
                "7.7188",
                "41.99",
                "68.53",
                "1.9355",
                "00.06",
                "15.77",
                "00.11",
                "15.94"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen",
                "66.41",
                "8.5565",
                "45.07",
                "69.17",
                "2.2253",
                "00.14",
                "04.11",
                "00.03",
                "04.27"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen+",
                "67.06",
                "8.5871",
                "45.83",
                "69.73",
                "2.2681",
                "00.04",
                "01.75",
                "00.01",
                "01.80"
            ],
            [
                "Original",
                "[BOLD] Original",
                "SC-LSTM",
                "39.11",
                "5.6704",
                "36.83",
                "50.02",
                "0.6045",
                "02.79",
                "18.90",
                "09.79",
                "31.51"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen\u2212",
                "65.87",
                "8.6400",
                "44.20",
                "67.51",
                "2.1710",
                "00.20",
                "00.56",
                "00.21",
                "00.97"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen",
                "66.24",
                "8.6889",
                "44.66",
                "67.85",
                "2.2181",
                "00.10",
                "00.02",
                "00.00",
                "00.12"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen+",
                "65.97",
                "8.6630",
                "44.45",
                "67.59",
                "2.1855",
                "00.02",
                "00.00",
                "00.00",
                "00.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "SC-LSTM",
                "38.52",
                "5.7125",
                "37.45",
                "48.50",
                "0.4343",
                "03.85",
                "17.39",
                "08.12",
                "29.37"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen\u2212",
                "66.28",
                "8.5202",
                "43.96",
                "67.83",
                "2.1375",
                "00.14",
                "02.26",
                "00.22",
                "02.61"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen",
                "67.00",
                "8.6889",
                "44.97",
                "68.19",
                "2.2228",
                "00.06",
                "00.44",
                "00.03",
                "00.53"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen+",
                "66.74",
                "8.6649",
                "44.84",
                "67.95",
                "2.2018",
                "00.00",
                "00.21",
                "00.03",
                "00.24"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen\u2212",
                "64.40",
                "7.9692",
                "42.81",
                "68.87",
                "2.0563",
                "00.01",
                "13.08",
                "00.00",
                "13.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen",
                "66.23",
                "8.5578",
                "45.12",
                "68.87",
                "2.2548",
                "00.04",
                "03.04",
                "00.00",
                "03.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen+",
                "65.96",
                "8.5238",
                "45.49",
                "68.79",
                "2.2456",
                "00.00",
                "01.44",
                "00.00",
                "01.45"
            ]
        ],
        "question": "Is it true that WOMs are slightly lower for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams?",
        "answer_label": "yes"
    },
    {
        "id": "cca89ac1-e731-42b7-b5c8-0b1ba49eae32",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 3: Full-length ROUGE F-scores of some recent RL-based (upper) and supervised (middle) extractive summarisation systems, as well as our system with learned rewards (bottom). R-1/2/L stands for ROUGE-1/2/L. Our system maximises the learned reward instead of ROUGE, hence receives lower ROUGE scores.",
        "table_column_names": [
            "System",
            "Reward",
            "R-1",
            "R-2",
            "R-L"
        ],
        "table_content_values": [
            [
                "Kryscinski et\u00a0al. ( 2018 )",
                "R-L",
                "40.2",
                "17.4",
                "37.5"
            ],
            [
                "Narayan et\u00a0al. ( 2018b )",
                "R-1,2,L",
                "40.0",
                "18.2",
                "36.6"
            ],
            [
                "Chen and Bansal ( 2018 )",
                "R-L",
                "41.5",
                "18.7",
                "37.8"
            ],
            [
                "Dong et\u00a0al. ( 2018 )",
                "R-1,2,L",
                "41.5",
                "18.7",
                "37.6"
            ],
            [
                "Zhang et\u00a0al. ( 2018 )",
                "[EMPTY]",
                "41.1",
                "18.8",
                "37.5"
            ],
            [
                "Zhou et\u00a0al. ( 2018 )",
                "[EMPTY]",
                "41.6",
                "19.0",
                "38.0"
            ],
            [
                "Kedzie et\u00a0al. ( 2018 )",
                "[EMPTY]",
                "39.1",
                "17.9",
                "35.9"
            ],
            [
                "(ours) NeuralTD",
                "Learned",
                "39.6",
                "18.1",
                "36.5"
            ]
        ],
        "question": "Is it true that The summaries generated by our system receive decent ROUGE metrics, but are lower than most of the recent systems, because our learned reward is optimised towards high correlation with human judgement instead of ROUGE metrics?",
        "answer_label": "yes"
    },
    {
        "id": "7cb9f68a-d66f-4adf-b12d-35a2fd47dd67",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 3: ARI and Silhouette coefficient scores.",
        "table_column_names": [
            "Methods",
            "Seanad Abolition ARI",
            "Seanad Abolition  [ITALIC] Sil",
            "Video Games ARI",
            "Video Games  [ITALIC] Sil",
            "Pornography ARI",
            "Pornography  [ITALIC] Sil"
        ],
        "table_content_values": [
            [
                "TF-IDF",
                "0.23",
                "0.02",
                "-0.01",
                "0.01",
                "-0.02",
                "0.01"
            ],
            [
                "WMD",
                "0.09",
                "0.01",
                "0.01",
                "0.01",
                "-0.02",
                "0.01"
            ],
            [
                "Sent2vec",
                "-0.01",
                "-0.01",
                "0.11",
                "0.06",
                "0.01",
                "0.02"
            ],
            [
                "Doc2vec",
                "-0.01",
                "-0.03",
                "-0.01",
                "0.01",
                "0.02",
                "-0.01"
            ],
            [
                "BERT",
                "0.03",
                "-0.04",
                "0.08",
                "0.05",
                "-0.01",
                "0.03"
            ],
            [
                "OD-parse",
                "0.01",
                "-0.04",
                "-0.01",
                "0.02",
                "0.07",
                "0.05"
            ],
            [
                "OD",
                "[BOLD] 0.54",
                "[BOLD] 0.31",
                "[BOLD] 0.56",
                "[BOLD] 0.42",
                "[BOLD] 0.41",
                "[BOLD] 0.41"
            ]
        ],
        "question": "Is it true that among opinions: We see that OD significantly outperforms the baseline methods and the OD-parse variant [CONTINUE] OD achieves high ARI and Sil scores, [CONTINUE] From the above table, we observe that the text-similarity based baselines, such as TF-IDF, WMD and Doc2vec do not achieve high ARI and Silhouette coefficient scores on the \"Video Games\" and \"Pornography\" datasets?",
        "answer_label": "no"
    },
    {
        "id": "dad6a4ed-cf24-42d0-9293-ac3ed0d9efcf",
        "table_caption": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set",
        "table_column_names": [
            "Method",
            "WER (%)",
            "DCE"
        ],
        "table_content_values": [
            [
                "No enhancement",
                "38.4",
                "0.958"
            ],
            [
                "Wiener filter",
                "41.0",
                "0.775"
            ],
            [
                "Minimizing DCE",
                "31.1",
                "[BOLD] 0.392"
            ],
            [
                "FSEGAN",
                "29.1",
                "0.421"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
                "27.7",
                "0.476"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
                "[BOLD] 26.1",
                "0.462"
            ],
            [
                "Clean speech",
                "9.3",
                "0.0"
            ]
        ],
        "question": "Is it true that The AAS method with weights wAC=1 and wAD=105 shows the lowest WER and DCE?",
        "answer_label": "no"
    },
    {
        "id": "f0edce30-c35c-41ad-b5f3-719e7e112bb0",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 2: Ratings of annotated NLDs by human judges.",
        "table_column_names": [
            "# steps",
            "Reachability",
            "Derivability Step 1",
            "Derivability Step 2",
            "Derivability Step 3"
        ],
        "table_content_values": [
            [
                "1",
                "3.0",
                "3.8",
                "-",
                "-"
            ],
            [
                "2",
                "2.8",
                "3.8",
                "3.7",
                "-"
            ],
            [
                "3",
                "2.3",
                "3.9",
                "3.8",
                "3.8"
            ]
        ],
        "question": "Is it true that [CONTINUE] On the other hand, we found the quality of 3-step NLDs is relatively lower than the others?",
        "answer_label": "yes"
    },
    {
        "id": "7099710f-edb1-4afd-ab16-47381f257b0d",
        "table_caption": "The MeMAD Submission to the WMT18 Multimodal Translation Task Table 6: Ablation experiments (BLEU% scores). The row subs3MLM detectron shows our best single model. Individual components or data choices are varied one by one. + stands for adding a component, and \u2212 for removing a component or data set. Multiple modifications are indicated by increasing the indentation.",
        "table_column_names": [
            "en-fr",
            "flickr16",
            "flickr17",
            "mscoco17"
        ],
        "table_content_values": [
            [
                "subs3M [ITALIC]  [ITALIC] LM detectron",
                "68.30",
                "62.45",
                "52.86"
            ],
            [
                "+ensemble-of-3",
                "68.72",
                "62.70",
                "53.06"
            ],
            [
                "\u2212visual features",
                "[BOLD] 68.74",
                "[BOLD] 62.71",
                "53.14"
            ],
            [
                "\u2212MS-COCO",
                "67.13",
                "61.17",
                "[BOLD] 53.34"
            ],
            [
                "\u2212multi-lingual",
                "68.21",
                "61.99",
                "52.40"
            ],
            [
                "subs6M [ITALIC]  [ITALIC] LM detectron",
                "68.29",
                "61.73",
                "53.05"
            ],
            [
                "subs3M [ITALIC]  [ITALIC] LM gn2048",
                "67.74",
                "61.78",
                "52.76"
            ],
            [
                "subs3M [ITALIC]  [ITALIC] LM text-only",
                "67.72",
                "61.75",
                "53.02"
            ],
            [
                "en-de",
                "flickr16",
                "flickr17",
                "mscoco17"
            ],
            [
                "subs3M [ITALIC]  [ITALIC] LM detectron",
                "45.09",
                "40.81",
                "36.94"
            ],
            [
                "+ensemble-of-3",
                "45.52",
                "[BOLD] 41.84",
                "[BOLD] 37.49"
            ],
            [
                "\u2212visual features",
                "[BOLD] 45.59",
                "41.75",
                "37.43"
            ],
            [
                "\u2212MS-COCO",
                "45.11",
                "40.52",
                "36.47"
            ],
            [
                "\u2212multi-lingual",
                "44.95",
                "40.09",
                "35.28"
            ],
            [
                "subs6M [ITALIC]  [ITALIC] LM detectron",
                "45.50",
                "41.01",
                "36.81"
            ],
            [
                "subs3M [ITALIC]  [ITALIC] LM gn2048",
                "45.38",
                "40.07",
                "36.82"
            ],
            [
                "subs3M [ITALIC]  [ITALIC] LM text-only",
                "44.87",
                "41.27",
                "36.59"
            ],
            [
                "+multi-modal finetune",
                "44.56",
                "41.61",
                "36.93"
            ]
        ],
        "question": "Is it true that When the experiment was repeated so that the finetuning phase included the text-only data, the performance did not return to approximately the same level as without tuning (+multi-modal finetune row in Table 6)?",
        "answer_label": "no"
    },
    {
        "id": "1b8acf49-27ee-44b2-a81b-2ea143aa00f0",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 2: Experiment 1",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.001",
                "0.003",
                "-20.818",
                "***",
                "0.505"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.083",
                "0.048",
                "101.636",
                "***",
                "1.724"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.001",
                "0.001",
                "0.035",
                "[EMPTY]",
                "1.001"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.023",
                "0.012",
                "64.418",
                "***",
                "1.993"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.002",
                "0.001",
                "4.047",
                "***",
                "1.120"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.049",
                "0.019",
                "120.986",
                "***",
                "2.573"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.173",
                "0.065",
                "243.285",
                "***",
                "2.653"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.032",
                "0.023",
                "39.483",
                "***",
                "1.396"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.111",
                "0.061",
                "122.707",
                "***",
                "1.812"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.178",
                "0.080",
                "211.319",
                "***",
                "2.239"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.028",
                "0.015",
                "63.131",
                "***",
                "1.854"
            ]
        ],
        "question": "Is it true that (2017), we find large disparities, with around 5% of tweets in the black-aligned corpus classified as hate speech compared to 2% of those in the white-aligned set?",
        "answer_label": "yes"
    },
    {
        "id": "74e9434e-0d7c-4c20-8f65-b02bf1e43667",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
        "table_column_names": [
            "[BOLD] Dataset",
            "[BOLD] # pairs",
            "[BOLD] # words (doc)",
            "[BOLD] # sents (docs)",
            "[BOLD] # words (summary)",
            "[BOLD] # sents (summary)",
            "[BOLD] vocab size"
        ],
        "table_content_values": [
            [
                "Multi-News",
                "44,972/5,622/5,622",
                "2,103.49",
                "82.73",
                "263.66",
                "9.97",
                "666,515"
            ],
            [
                "DUC03+04",
                "320",
                "4,636.24",
                "173.15",
                "109.58",
                "2.88",
                "19,734"
            ],
            [
                "TAC 2011",
                "176",
                "4,695.70",
                "188.43",
                "99.70",
                "1.00",
                "24,672"
            ],
            [
                "CNNDM",
                "287,227/13,368/11,490",
                "810.57",
                "39.78",
                "56.20",
                "3.68",
                "717,951"
            ]
        ],
        "question": "Is it true that The number of examples in our Multi-News dataset is two orders of magnitude larger than previous MDS news data?",
        "answer_label": "yes"
    },
    {
        "id": "52c61ca7-ec3c-4313-bd66-c792ef9bf414",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
        "table_column_names": [
            "Model",
            "#Params",
            "BLEU",
            "Train",
            "Decode"
        ],
        "table_content_values": [
            [
                "GNMT",
                "-",
                "24.61",
                "-",
                "-"
            ],
            [
                "GRU",
                "206M",
                "26.28",
                "2.67",
                "45.35"
            ],
            [
                "ATR",
                "122M",
                "25.70",
                "1.33",
                "[BOLD] 34.40"
            ],
            [
                "SRU",
                "170M",
                "25.91",
                "1.34",
                "42.84"
            ],
            [
                "LRN",
                "143M",
                "26.26",
                "[BOLD] 0.99",
                "36.50"
            ],
            [
                "oLRN",
                "164M",
                "[BOLD] 26.73",
                "1.15",
                "40.19"
            ]
        ],
        "question": "Is it true that Despite LRN and oLRN having faster training times than SRU (+15%/+6%), SRU still achieves a higher BLEU score?",
        "answer_label": "no"
    },
    {
        "id": "88f1cf27-946a-4442-98dc-02d34534f76e",
        "table_caption": "Sparse and Structured Visual Attention Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
        "table_column_names": [
            "[EMPTY]",
            "Att. to image",
            "Att. to bounding boxes",
            "Test-Dev Yes/No",
            "Test-Dev Number",
            "Test-Dev Other",
            "Test-Dev Overall",
            "Test-Standard Yes/No",
            "Test-Standard Number",
            "Test-Standard Other",
            "Test-Standard Overall"
        ],
        "table_content_values": [
            [
                "softmax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "42.65",
                "55.74",
                "65.52",
                "83.55",
                "42.68",
                "56.01",
                "65.97"
            ],
            [
                "sparsemax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "43.19",
                "55.79",
                "65.60",
                "83.33",
                "42.99",
                "56.06",
                "65.94"
            ],
            [
                "soft-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.13",
                "43.53",
                "56.01",
                "65.76",
                "83.63",
                "43.24",
                "56.10",
                "66.11"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.10",
                "43.30",
                "56.14",
                "65.79",
                "83.66",
                "43.18",
                "56.21",
                "66.17"
            ],
            [
                "softmax",
                "[EMPTY]",
                "\u2713",
                "85.14",
                "49.59",
                "58.72",
                "68.57",
                "85.56",
                "49.54",
                "59.11",
                "69.04"
            ],
            [
                "sparsemax",
                "[EMPTY]",
                "\u2713",
                "[BOLD] 85.40",
                "[BOLD] 50.87",
                "58.67",
                "68.79",
                "[BOLD] 85.80",
                "50.18",
                "59.08",
                "69.19"
            ],
            [
                "softmax",
                "\u2713",
                "\u2713",
                "85.33",
                "50.49",
                "58.88",
                "68.82",
                "85.58",
                "50.42",
                "59.18",
                "69.17"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "\u2713",
                "85.35",
                "50.52",
                "[BOLD] 59.15",
                "[BOLD] 68.96",
                "85.72",
                "[BOLD] 50.66",
                "[BOLD] 59.22",
                "[BOLD] 69.28"
            ]
        ],
        "question": "Is it true that Additionally, when using bounding box features, sparsemax outperforms softmax, showing that selecting only the bounding boxes of the relevant objects leads to a better answering capability?",
        "answer_label": "yes"
    },
    {
        "id": "b90516c0-5d54-4636-b318-cdc83d2fce12",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 2: POS and SEM tagging accuracy with baselines and an upper bound. MFT: most frequent tag; UnsupEmb: classifier using unsupervised word embeddings; Word2Tag: upper bound encoder-decoder.",
        "table_column_names": [
            "[EMPTY]",
            "MFT",
            "UnsupEmb",
            "Word2Tag"
        ],
        "table_content_values": [
            [
                "POS",
                "91.95",
                "87.06",
                "95.55"
            ],
            [
                "SEM",
                "82.00",
                "81.11",
                "91.41"
            ]
        ],
        "question": "Is it true that The UnsupEmb baseline performs comparably to the Word2Tag upper bound on both POS and SEM tagging?",
        "answer_label": "no"
    },
    {
        "id": "406069b0-825f-4f0b-b982-048d1e765fcf",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 4: Lexicon member coverage (%)",
        "table_column_names": [
            "target",
            "VN",
            "WN-V",
            "WN-N"
        ],
        "table_content_values": [
            [
                "type",
                "81",
                "66",
                "47"
            ],
            [
                "x+POS",
                "54",
                "39",
                "43"
            ],
            [
                "lemma",
                "88",
                "76",
                "53"
            ],
            [
                "x+POS",
                "79",
                "63",
                "50"
            ],
            [
                "shared",
                "54",
                "39",
                "41"
            ]
        ],
        "question": "Is it true that POS-disambiguation does not fragment the vocabulary and consistently increases the coverage with the effect being more pronounced for lemmatized targets?",
        "answer_label": "no"
    },
    {
        "id": "c9a67956-a20f-488b-bd85-062a6fc04a01",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 2: Experiment 1",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.001",
                "0.003",
                "-20.818",
                "***",
                "0.505"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.083",
                "0.048",
                "101.636",
                "***",
                "1.724"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.001",
                "0.001",
                "0.035",
                "[EMPTY]",
                "1.001"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.023",
                "0.012",
                "64.418",
                "***",
                "1.993"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.002",
                "0.001",
                "4.047",
                "***",
                "1.120"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.049",
                "0.019",
                "120.986",
                "***",
                "2.573"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.173",
                "0.065",
                "243.285",
                "***",
                "2.653"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.032",
                "0.023",
                "39.483",
                "***",
                "1.396"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.111",
                "0.061",
                "122.707",
                "***",
                "1.812"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.178",
                "0.080",
                "211.319",
                "***",
                "2.239"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.028",
                "0.015",
                "63.131",
                "***",
                "1.854"
            ]
        ],
        "question": "Is it true that In most cases the racial disparities persist, and are generally larger in magnitude than the disparities for other classes?",
        "answer_label": "no"
    },
    {
        "id": "c67da597-da7b-4c75-99a6-5184dfb0c485",
        "table_caption": "Entity, Relation, and Event Extraction with Contextualized Span Representations Table 3: F1 scores on Relation.",
        "table_column_names": [
            "[EMPTY]",
            "ACE05",
            "SciERC",
            "WLPC"
        ],
        "table_content_values": [
            [
                "BERT + LSTM",
                "60.6",
                "40.3",
                "65.1"
            ],
            [
                "+RelProp",
                "61.9",
                "41.1",
                "65.3"
            ],
            [
                "+CorefProp",
                "59.7",
                "42.6",
                "-"
            ],
            [
                "BERT FineTune",
                "[BOLD] 62.1",
                "44.3",
                "65.4"
            ],
            [
                "+RelProp",
                "62.0",
                "43.0",
                "[BOLD] 65.5"
            ],
            [
                "+CorefProp",
                "60.0",
                "[BOLD] 45.3",
                "-"
            ]
        ],
        "question": "Is it true that CorefProp also improves relation extraction on SciERC?",
        "answer_label": "yes"
    },
    {
        "id": "fede62ad-8591-411a-974e-a263d0e6dd91",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "Base ACC",
            "Base Time",
            "+LN ACC",
            "+LN Time",
            "+BERT ACC",
            "+BERT Time",
            "+LN+BERT ACC",
            "+LN+BERT Time"
        ],
        "table_content_values": [
            [
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "250K",
                "83.50",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-"
            ],
            [
                "This",
                "LSTM",
                "8.36M",
                "84.27",
                "0.262",
                "86.03",
                "0.432",
                "89.95",
                "0.544",
                "[BOLD] 90.49",
                "0.696"
            ],
            [
                "This",
                "GRU",
                "6.41M",
                "[BOLD] 85.71",
                "0.245",
                "[BOLD] 86.05",
                "0.419",
                "[BOLD] 90.29",
                "0.529",
                "90.10",
                "0.695"
            ],
            [
                "This",
                "ATR",
                "2.87M",
                "84.88",
                "0.210",
                "85.81",
                "0.307",
                "90.00",
                "0.494",
                "90.28",
                "0.580"
            ],
            [
                "Work",
                "SRU",
                "5.48M",
                "84.28",
                "0.258",
                "85.32",
                "0.283",
                "89.98",
                "0.543",
                "90.09",
                "0.555"
            ],
            [
                "[EMPTY]",
                "LRN",
                "4.25M",
                "84.88",
                "[BOLD] 0.209",
                "85.06",
                "[BOLD] 0.223",
                "89.98",
                "[BOLD] 0.488",
                "89.93",
                "[BOLD] 0.506"
            ]
        ],
        "question": "Is it true that LRN obtains an accuracy of 90.49 with BERT, the highest among all models?",
        "answer_label": "no"
    },
    {
        "id": "dafe0889-31b8-48ce-a240-cad9ffdebc2e",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
        "table_column_names": [
            "<bold>Model</bold>",
            "REF \u21d2 GEN <bold>ENT</bold>",
            "REF \u21d2 GEN <bold>CON</bold>",
            "REF \u21d2 GEN <bold>NEU</bold>"
        ],
        "table_content_values": [
            [
                "S2S",
                "38.45",
                "11.17",
                "50.38"
            ],
            [
                "G2S-GIN",
                "49.78",
                "9.80",
                "40.42"
            ],
            [
                "G2S-GAT",
                "49.48",
                "8.09",
                "42.43"
            ],
            [
                "G2S-GGNN",
                "51.32",
                "8.82",
                "39.86"
            ],
            [
                "[EMPTY]",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF"
            ],
            [
                "<bold>Model</bold>",
                "<bold>ENT</bold>",
                "<bold>CON</bold>",
                "<bold>NEU</bold>"
            ],
            [
                "S2S",
                "73.79",
                "12.75",
                "13.46"
            ],
            [
                "G2S-GIN",
                "76.27",
                "10.65",
                "13.08"
            ],
            [
                "G2S-GAT",
                "77.54",
                "8.54",
                "13.92"
            ],
            [
                "G2S-GGNN",
                "77.64",
                "9.64",
                "12.72"
            ]
        ],
        "question": "Is it true that G2S-GGNN has 33.5% and 5.2% better entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively?",
        "answer_label": "yes"
    },
    {
        "id": "96bf0f6b-f429-4648-bab7-cf3759539016",
        "table_caption": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).",
        "table_column_names": [
            "[EMPTY]",
            "dev CS",
            "dev mono",
            "test CS",
            "test mono"
        ],
        "table_content_values": [
            [
                "CS-only-LM",
                "45.20",
                "65.87",
                "43.20",
                "62.80"
            ],
            [
                "Fine-Tuned-LM",
                "49.60",
                "72.67",
                "47.60",
                "71.33"
            ],
            [
                "CS-only-disc",
                "[BOLD] 75.60",
                "70.40",
                "70.80",
                "70.53"
            ],
            [
                "Fine-Tuned-disc",
                "70.80",
                "[BOLD] 74.40",
                "[BOLD] 75.33",
                "[BOLD] 75.87"
            ]
        ],
        "question": "Is it true that Table 5 breaks down the results of the different models according to two conditions: when the gold sentence is code-switched, and when the gold sentence is monolingual?",
        "answer_label": "yes"
    },
    {
        "id": "03375542-1aaf-400c-9743-0e332dd4183b",
        "table_caption": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
        "table_column_names": [
            "Recall",
            "0.1",
            "0.2",
            "0.3",
            "AUC"
        ],
        "table_content_values": [
            [
                "-Word-ATT",
                "0.648",
                "0.515",
                "0.395",
                "0.389"
            ],
            [
                "-Capsule",
                "0.635",
                "0.507",
                "0.413",
                "0.386"
            ],
            [
                "Our Model",
                "0.650",
                "0.519",
                "0.422",
                "0.405"
            ]
        ],
        "question": "Is it true that According to the table, the drop of precision demonstrates that the word-level attention is quite useful?",
        "answer_label": "yes"
    },
    {
        "id": "966e1252-7bc6-49de-8cde-ddf9dd9771a0",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 4: Scores for different training objectives on the linguistic probing tasks.",
        "table_column_names": [
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "CMOW-C",
                "[BOLD] 36.2",
                "66.0",
                "81.1",
                "78.7",
                "61.7",
                "[BOLD] 83.9",
                "79.1",
                "73.6",
                "50.4",
                "66.8"
            ],
            [
                "CMOW-R",
                "35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "[BOLD] 80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "[BOLD] 74.2",
                "[BOLD] 50.7",
                "[BOLD] 72.9"
            ],
            [
                "CBOW-C",
                "[BOLD] 34.3",
                "[BOLD] 50.5",
                "[BOLD] 79.8",
                "[BOLD] 79.9",
                "53.0",
                "[BOLD] 75.9",
                "[BOLD] 79.8",
                "[BOLD] 72.9",
                "48.6",
                "89.0"
            ],
            [
                "CBOW-R",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "[BOLD] 53.6",
                "74.5",
                "78.6",
                "72.0",
                "[BOLD] 49.6",
                "[BOLD] 89.5"
            ]
        ],
        "question": "Is it true that While CMOW-R and CMOW-C perform comparably on most probing tasks, CMOW-C yields 5 points higher scores on WordContent and BigramShift?",
        "answer_label": "no"
    },
    {
        "id": "07e54d6a-f1de-49d1-8249-96055ac0191a",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.",
        "table_column_names": [
            "Uni",
            "POS",
            "0 87.9",
            "1 92.0",
            "2 91.7",
            "3 91.8",
            "4 91.9"
        ],
        "table_content_values": [
            [
                "Uni",
                "SEM",
                "81.8",
                "87.8",
                "87.4",
                "87.6",
                "88.2"
            ],
            [
                "Bi",
                "POS",
                "87.9",
                "93.3",
                "92.9",
                "93.2",
                "92.8"
            ],
            [
                "Bi",
                "SEM",
                "81.9",
                "91.3",
                "90.8",
                "91.9",
                "91.9"
            ],
            [
                "Res",
                "POS",
                "87.9",
                "92.5",
                "91.9",
                "92.0",
                "92.4"
            ],
            [
                "Res",
                "SEM",
                "81.9",
                "88.2",
                "87.5",
                "87.6",
                "88.5"
            ]
        ],
        "question": "Is it true that We also observe similar trends as before: POS tagging does not benefit from features from the upper layers, while SEM tagging improves with layer 4 representations?",
        "answer_label": "yes"
    },
    {
        "id": "63fe7961-86a5-4ceb-9556-1f4592c15b2c",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that After removing the graph attention module, our model gives 24.9 BLEU points?",
        "answer_label": "yes"
    },
    {
        "id": "9df0b311-fa64-4aaa-b766-568444a3f1a9",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 3: Performance comparison of our model with different values of m on the two datasets.",
        "table_column_names": [
            "[ITALIC] m",
            "NYT10 Prec.",
            "NYT10 Rec.",
            "NYT10 F1",
            "NYT11 Prec.",
            "NYT11 Rec.",
            "NYT11 F1"
        ],
        "table_content_values": [
            [
                "1",
                "0.541",
                "0.595",
                "[BOLD] 0.566",
                "0.495",
                "0.621",
                "0.551"
            ],
            [
                "2",
                "0.521",
                "0.597",
                "0.556",
                "0.482",
                "0.656",
                "0.555"
            ],
            [
                "3",
                "0.490",
                "0.617",
                "0.547",
                "0.509",
                "0.633",
                "0.564"
            ],
            [
                "4",
                "0.449",
                "0.623",
                "0.522",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "5",
                "0.467",
                "0.609",
                "0.529",
                "0.488",
                "0.677",
                "0.567"
            ]
        ],
        "question": "Is it true that We observe that for the NYT10 dataset, m = {1, 2, 3} gives good performance with m = 1 achieving the highest F1 score?",
        "answer_label": "yes"
    },
    {
        "id": "3ce2ce6f-939a-4a30-b4ec-06b484e1ae64",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
        "table_column_names": [
            "Model",
            "Method",
            "Training Data",
            "Overall",
            "Easy",
            "Hard",
            "p-value (%)"
        ],
        "table_content_values": [
            [
                "goodwin-etal-2012-utdhlt",
                "PMI",
                "unsupervised",
                "61.8",
                "64.7",
                "60.0",
                "19.8"
            ],
            [
                "gordon_commonsense_2011-1",
                "PMI",
                "unsupervised",
                "65.4",
                "65.8",
                "65.2",
                "83.5"
            ],
            [
                "sasaki-etal-2017-handling",
                "PMI",
                "unsupervised",
                "71.4",
                "75.3",
                "69.0",
                "4.8\u2217"
            ],
            [
                "Word frequency",
                "wordfreq",
                "COPA",
                "53.5",
                "57.4",
                "51.3",
                "9.8"
            ],
            [
                "BERT-large-FT",
                "LM, NSP",
                "COPA",
                "76.5 (\u00b1 2.7)",
                "83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)",
                "0.0\u2217"
            ],
            [
                "RoBERTa-large-FT",
                "LM",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)",
                "0.0\u2217"
            ]
        ],
        "question": "Is it true that (2017).8 Overall both BERT (76.5%) and [CONTINUE] RoBERTa (87.7%) considerably outperform the best previous model (71.4%)?",
        "answer_label": "yes"
    },
    {
        "id": "dfff7b52-8d27-4ac2-8e6a-fb62361453d3",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that In particular, our single DCGCN model consistently outperforms Seq2Seq models by a significant margin when trained without external resources?",
        "answer_label": "yes"
    },
    {
        "id": "8a6a5782-c6a0-428a-aadb-4813c0a9d2ae",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
        "table_column_names": [
            "Model",
            "Training data",
            "Overall",
            "Easy",
            "Hard"
        ],
        "table_content_values": [
            [
                "BERT-large-FT",
                "B-COPA",
                "74.5 (\u00b1 0.7)",
                "74.7 (\u00b1 0.4)",
                "[BOLD] 74.4 (\u00b1 0.9)"
            ],
            [
                "BERT-large-FT",
                "B-COPA (50%)",
                "74.3 (\u00b1 2.2)",
                "76.8 (\u00b1 1.9)",
                "72.8 (\u00b1 3.1)"
            ],
            [
                "BERT-large-FT",
                "COPA",
                "[BOLD] 76.5 (\u00b1 2.7)",
                "[BOLD] 83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA",
                "[BOLD] 89.0 (\u00b1 0.3)",
                "88.9 (\u00b1 2.1)",
                "[BOLD] 89.0 (\u00b1 0.8)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA (50%)",
                "86.1 (\u00b1 2.2)",
                "87.4 (\u00b1 1.1)",
                "85.4 (\u00b1 2.9)"
            ],
            [
                "RoBERTa-large-FT",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "[BOLD] 91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)"
            ]
        ],
        "question": "Is it true that The larger performance gap between Easy and Hard subsets indicates that training on BCOPA encourages BERT and RoBERTa to rely more on superficial cues?",
        "answer_label": "no"
    },
    {
        "id": "4ca84e7f-8b85-4b40-8506-e06ed09099af",
        "table_caption": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task Table 2: Image-caption ranking results for German (Multi30k)",
        "table_column_names": [
            "[EMPTY]",
            "Image to Text R@1",
            "Image to Text R@5",
            "Image to Text R@10",
            "Image to Text Mr",
            "Text to Image R@1",
            "Text to Image R@5",
            "Text to Image R@10",
            "Text to Image Mr",
            "Alignment"
        ],
        "table_content_values": [
            [
                "[BOLD] symmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Parallel\u00a0gella:17",
                "28.2",
                "57.7",
                "71.3",
                "4",
                "20.9",
                "46.9",
                "59.3",
                "6",
                "-"
            ],
            [
                "Mono",
                "34.2",
                "67.5",
                "79.6",
                "3",
                "26.5",
                "54.7",
                "66.2",
                "4",
                "-"
            ],
            [
                "FME",
                "36.8",
                "69.4",
                "80.8",
                "2",
                "26.6",
                "56.2",
                "68.5",
                "4",
                "76.81%"
            ],
            [
                "AME",
                "[BOLD] 39.6",
                "[BOLD] 72.7",
                "[BOLD] 82.7",
                "[BOLD] 2",
                "[BOLD] 28.9",
                "[BOLD] 58.0",
                "[BOLD] 68.7",
                "[BOLD] 4",
                "66.91%"
            ],
            [
                "[BOLD] asymmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Pivot\u00a0gella:17",
                "28.2",
                "61.9",
                "73.4",
                "3",
                "22.5",
                "49.3",
                "61.7",
                "6",
                "-"
            ],
            [
                "Parallel\u00a0gella:17",
                "30.2",
                "60.4",
                "72.8",
                "3",
                "21.8",
                "50.5",
                "62.3",
                "5",
                "-"
            ],
            [
                "Mono",
                "[BOLD] 42.0",
                "72.5",
                "83.0",
                "2",
                "29.6",
                "58.4",
                "69.6",
                "4",
                "-"
            ],
            [
                "FME",
                "40.5",
                "73.3",
                "83.4",
                "2",
                "29.6",
                "59.2",
                "[BOLD] 72.1",
                "3",
                "76.81%"
            ],
            [
                "AME",
                "40.5",
                "[BOLD] 74.3",
                "[BOLD] 83.4",
                "[BOLD] 2",
                "[BOLD] 31.0",
                "[BOLD] 60.5",
                "70.6",
                "[BOLD] 3",
                "73.10%"
            ]
        ],
        "question": "Is it true that For German descriptions, The results are 11.05% worse on average compared to (Gella et al., 2017) in symmetric mode?",
        "answer_label": "no"
    },
    {
        "id": "8a3ce3da-5db7-48f2-929f-2d27febabd8a",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.",
        "table_column_names": [
            "[ITALIC] Block",
            "[ITALIC] n",
            "[ITALIC] m",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "1",
                "1",
                "1",
                "17.6",
                "48.3"
            ],
            [
                "1",
                "1",
                "2",
                "19.2",
                "50.3"
            ],
            [
                "1",
                "2",
                "1",
                "18.4",
                "49.1"
            ],
            [
                "1",
                "1",
                "3",
                "19.6",
                "49.4"
            ],
            [
                "1",
                "3",
                "1",
                "20.0",
                "50.5"
            ],
            [
                "1",
                "3",
                "3",
                "21.4",
                "51.0"
            ],
            [
                "1",
                "3",
                "6",
                "21.8",
                "51.7"
            ],
            [
                "1",
                "6",
                "3",
                "21.7",
                "51.5"
            ],
            [
                "1",
                "6",
                "6",
                "22.0",
                "52.1"
            ],
            [
                "2",
                "3",
                "6",
                "[BOLD] 23.5",
                "53.3"
            ],
            [
                "2",
                "6",
                "3",
                "23.3",
                "[BOLD] 53.4"
            ],
            [
                "2",
                "6",
                "6",
                "22.0",
                "52.1"
            ]
        ],
        "question": "Is it true that In general, the performance increases when we gradually enlarge n and m. For example, when n=1 and m=1, the BLEU score is 17.6; when n=6 and m=6, the BLEU score becomes 22.0?",
        "answer_label": "yes"
    },
    {
        "id": "37eee057-4b1f-43f6-9889-4aa72045c882",
        "table_caption": "Syntactic Dependency Representations in Neural Relation Classification Table 1: Effect of using the shortest dependency path on each relation type.",
        "table_column_names": [
            "[BOLD] Relation",
            "[BOLD] best F1 (in 5-fold) without sdp",
            "[BOLD] best F1 (in 5-fold) with sdp",
            "[BOLD] Diff."
        ],
        "table_content_values": [
            [
                "USAGE",
                "60.34",
                "80.24",
                "+ 19.90"
            ],
            [
                "MODEL-FEATURE",
                "48.89",
                "70.00",
                "+ 21.11"
            ],
            [
                "PART_WHOLE",
                "29.51",
                "70.27",
                "+40.76"
            ],
            [
                "TOPIC",
                "45.80",
                "91.26",
                "+45.46"
            ],
            [
                "RESULT",
                "54.35",
                "81.58",
                "+27.23"
            ],
            [
                "COMPARE",
                "20.00",
                "61.82",
                "+ 41.82"
            ],
            [
                "macro-averaged",
                "50.10",
                "76.10",
                "+26.00"
            ]
        ],
        "question": "Is it true that However, the sdp information does not have a clear positive impact on all the relation types (Table 1)?",
        "answer_label": "no"
    },
    {
        "id": "cbc48b80-a560-4ead-aeb2-98b1dcb907eb",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] External",
            "B"
        ],
        "table_content_values": [
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "-",
                "22.0"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "-",
                "23.3"
            ],
            [
                "GCNSEQ (Damonte and Cohen,  2019 )",
                "-",
                "24.4"
            ],
            [
                "DCGCN(single)",
                "-",
                "25.9"
            ],
            [
                "DCGCN(ensemble)",
                "-",
                "[BOLD] 28.2"
            ],
            [
                "TSP (Song et al.,  2016 )",
                "ALL",
                "22.4"
            ],
            [
                "PBMT (Pourdamghani et al.,  2016 )",
                "ALL",
                "26.9"
            ],
            [
                "Tree2Str (Flanigan et al.,  2016 )",
                "ALL",
                "23.0"
            ],
            [
                "SNRG (Song et al.,  2017 )",
                "ALL",
                "25.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "0.2M",
                "27.4"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "0.2M",
                "28.2"
            ],
            [
                "DCGCN(single)",
                "0.1M",
                "29.0"
            ],
            [
                "DCGCN(single)",
                "0.2M",
                "[BOLD] 31.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "2M",
                "32.3"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "2M",
                "33.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "20M",
                "33.8"
            ],
            [
                "DCGCN(single)",
                "0.3M",
                "33.2"
            ],
            [
                "DCGCN(ensemble)",
                "0.3M",
                "[BOLD] 35.3"
            ]
        ],
        "question": "Is it true that DCGCN model is able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a score of 33.6 by using 2M data and Seq2SeqK achieves a score of 33.8 by using 20M data?",
        "answer_label": "yes"
    },
    {
        "id": "bd1eba72-ce56-4f45-a304-cb354ff75544",
        "table_caption": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set",
        "table_column_names": [
            "Method",
            "WER (%)",
            "DCE"
        ],
        "table_content_values": [
            [
                "No enhancement",
                "38.4",
                "0.958"
            ],
            [
                "Wiener filter",
                "41.0",
                "0.775"
            ],
            [
                "Minimizing DCE",
                "31.1",
                "[BOLD] 0.392"
            ],
            [
                "FSEGAN",
                "29.1",
                "0.421"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
                "27.7",
                "0.476"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
                "[BOLD] 26.1",
                "0.462"
            ],
            [
                "Clean speech",
                "9.3",
                "0.0"
            ]
        ],
        "question": "Is it true that acoustic supervision (27.7%) and multi-task learning (26.1%) show higher WER than minimizing DCE (31.1%) and FSEGAN (29.1%)?",
        "answer_label": "no"
    },
    {
        "id": "1f607985-944c-457a-878b-94f8d36c7b28",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that In addition, our single DCGCN model obtains better results than previous ensemble models?",
        "answer_label": "yes"
    },
    {
        "id": "2df13a17-6c33-4cd4-8cf3-7c85fc97cb01",
        "table_caption": "Evaluation of Greek Word Embeddings Table 4: Word similarity.",
        "table_column_names": [
            "Model",
            "Pearson",
            "p-value",
            "Pairs (unknown)"
        ],
        "table_content_values": [
            [
                "gr_def",
                "[BOLD] 0.6042",
                "3.1E-35",
                "2.3%"
            ],
            [
                "gr_neg10",
                "0.5973",
                "2.9E-34",
                "2.3%"
            ],
            [
                "cc.el.300",
                "0.5311",
                "1.7E-25",
                "4.9%"
            ],
            [
                "wiki.el",
                "0.5812",
                "2.2E-31",
                "4.5%"
            ],
            [
                "gr_cbow_def",
                "0.5232",
                "2.7E-25",
                "2.3%"
            ],
            [
                "gr_d300_nosub",
                "0.5889",
                "3.8E-33",
                "2.3%"
            ],
            [
                "gr_w2v_sg_n5",
                "0.5879",
                "4.4E-33",
                "2.3%"
            ]
        ],
        "question": "Is it true that According to Pearson correlation, gr def model had the highest correlation with human ratings of similarity?",
        "answer_label": "yes"
    },
    {
        "id": "f273252e-5941-436d-aaf0-23e946eaca18",
        "table_caption": "Entity, Relation, and Event Extraction with Contextualized Span Representations Table 7: In-domain pre-training: SciBERT vs. BERT",
        "table_column_names": [
            "[EMPTY]",
            "SciERC Entity",
            "SciERC Relation",
            "GENIA Entity"
        ],
        "table_content_values": [
            [
                "Best BERT",
                "69.8",
                "41.9",
                "78.4"
            ],
            [
                "Best SciBERT",
                "[BOLD] 72.0",
                "[BOLD] 45.3",
                "[BOLD] 79.5"
            ]
        ],
        "question": "Is it true that SciBERT significantly boosts performance for scientific datasets including SciERC and GENIA?",
        "answer_label": "yes"
    },
    {
        "id": "02999797-f0ae-4c27-b7bd-8c4a44e60537",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.",
        "table_column_names": [
            "Metric",
            "Method of validation",
            "Yelp",
            "Lit."
        ],
        "table_content_values": [
            [
                "Acc",
                "% of machine and human judgments that match",
                "94",
                "84"
            ],
            [
                "Sim",
                "Spearman\u2019s  [ITALIC] \u03c1 b/w Sim and human ratings of semantic preservation",
                "0.79",
                "0.75"
            ],
            [
                "PP",
                "Spearman\u2019s  [ITALIC] \u03c1 b/w negative PP and human ratings of fluency",
                "0.81",
                "0.67"
            ]
        ],
        "question": "Is it true that [CONTINUE] We validate Sim and PP by computing sentence-level Spearman's \u03c1 between the metric and human judgments [CONTINUE] From Table 5, all validations show strong correlations on the Yelp dataset and reasonable correlations on Literature?",
        "answer_label": "yes"
    },
    {
        "id": "07c4b243-00fb-4439-94ae-89bb5c1641f5",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Feature",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Feature",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams"
            ],
            [
                "not",
                ".154",
                "[URL]",
                ".150"
            ],
            [
                "my",
                ".131",
                "!",
                ".082"
            ],
            [
                "working",
                ".124",
                "he",
                ".069"
            ],
            [
                "still",
                ".123",
                "thank",
                ".067"
            ],
            [
                "on",
                ".119",
                ",",
                ".064"
            ],
            [
                "can\u2019t",
                ".113",
                "love",
                ".064"
            ],
            [
                "service",
                ".112",
                "lol",
                ".061"
            ],
            [
                "customer",
                ".109",
                "you",
                ".060"
            ],
            [
                "why",
                ".108",
                "great",
                ".058"
            ],
            [
                "website",
                ".107",
                "win",
                ".058"
            ],
            [
                "no",
                ".104",
                "\u2019",
                ".058"
            ],
            [
                "?",
                ".098",
                "she",
                ".054"
            ],
            [
                "fix",
                ".093",
                ":",
                ".053"
            ],
            [
                "won\u2019t",
                ".092",
                "that",
                ".053"
            ],
            [
                "been",
                ".090",
                "more",
                ".052"
            ],
            [
                "issue",
                ".089",
                "it",
                ".052"
            ],
            [
                "days",
                ".088",
                "would",
                ".051"
            ],
            [
                "error",
                ".087",
                "him",
                ".047"
            ],
            [
                "is",
                ".084",
                "life",
                ".046"
            ],
            [
                "charged",
                ".083",
                "good",
                ".046"
            ],
            [
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)"
            ],
            [
                "VBN",
                ".141",
                "UH",
                ".104"
            ],
            [
                "$",
                ".118",
                "NNP",
                ".098"
            ],
            [
                "VBZ",
                ".114",
                "PRP",
                ".076"
            ],
            [
                "NN_VBZ",
                ".114",
                "HT",
                ".076"
            ],
            [
                "PRP$",
                ".107",
                "PRP_.",
                ".076"
            ],
            [
                "PRP$_NN",
                ".105",
                "PRP_RB",
                ".067"
            ],
            [
                "VBG",
                ".093",
                "NNP_NNP",
                ".062"
            ],
            [
                "CD",
                ".092",
                "VBP_PRP",
                ".054"
            ],
            [
                "WRB_VBZ",
                ".084",
                "JJ",
                ".053"
            ],
            [
                "VBZ_VBN",
                ".084",
                "DT_JJ",
                ".051"
            ]
        ],
        "question": "Is it true that [CONTINUE] Negations are uncovered through unigrams (not, no, won't) [CONTINUE] Several unigrams (error, issue, working, fix) [CONTINUE] However, words regularly describing negative sentiment or emotions are not one of the most distinctive features for complaints?",
        "answer_label": "yes"
    },
    {
        "id": "16827e79-dd19-40b9-b474-1a6e305ece38",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 7: Sensitivity of BERT-large to superficial cues identified in \u00a72 (unit: 10\u22122). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.",
        "table_column_names": [
            "Cue",
            "[ITALIC] SCOPA",
            "[ITALIC] SB_COPA",
            "Diff.",
            "Prod."
        ],
        "table_content_values": [
            [
                "woman",
                "7.98",
                "4.84",
                "-3.14",
                "0.25"
            ],
            [
                "mother",
                "5.16",
                "3.95",
                "-1.21",
                "0.75"
            ],
            [
                "went",
                "6.00",
                "5.15",
                "-0.85",
                "0.73"
            ],
            [
                "down",
                "5.52",
                "4.93",
                "-0.58",
                "0.71"
            ],
            [
                "into",
                "4.07",
                "3.51",
                "-0.56",
                "0.40"
            ]
        ],
        "question": "Is it true that We observe that BERT trained on Balanced COPA is more sensitive to a few highly productive superficial cues than BERT trained on original COPA?",
        "answer_label": "no"
    },
    {
        "id": "0785d05a-87ed-4bb1-ac80-372c08eb76c8",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that For example, DCGCN4 contains 36 layers?",
        "answer_label": "yes"
    },
    {
        "id": "387a041a-727b-42a7-a6b1-1c83ae902c61",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that We do not have competitive results to Guo et al?",
        "answer_label": "no"
    },
    {
        "id": "35e1aff5-32e2-45a6-bc1d-ecc90f105f47",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 5: Group text features associated with tweets that are complaints and not complaints. Features are sorted by Pearson correlation (r) between their each feature\u2019s normalized frequency and the outcome. We restrict to only the top six categories for each feature type. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Label",
            "[BOLD] Complaints  [BOLD] Words",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Label",
            "[BOLD] Not Complaints  [BOLD] Words",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] LIWC Features",
                "[BOLD] LIWC Features",
                "[BOLD] LIWC Features",
                "[BOLD] LIWC Features",
                "[BOLD] LIWC Features",
                "[BOLD] LIWC Features"
            ],
            [
                "NEGATE",
                "not, no, can\u2019t, don\u2019t, never, nothing, doesn\u2019t, won\u2019t",
                ".271",
                "POSEMO",
                "thanks, love, thank, good, great, support, lol, win",
                ".185"
            ],
            [
                "RELATIV",
                "in, on, when, at, out, still, now, up, back, new",
                ".225",
                "AFFECT",
                "thanks, love, thank, good, great, support, lol",
                ".111"
            ],
            [
                "FUNCTION",
                "the, i, to, a, my, and, you, for, is, in",
                ".204",
                "SHEHE",
                "he, his, she, her, him, he\u2019s, himself",
                ".105"
            ],
            [
                "TIME",
                "when, still, now, back, new, never, after, then, waiting",
                ".186",
                "MALE",
                "he, his, man, him, sir, he\u2019s, son",
                ".086"
            ],
            [
                "DIFFER",
                "not, but, if, or, can\u2019t, really, than, other, haven\u2019t",
                ".169",
                "FEMALE",
                "she, her, girl, mom, ma, lady, mother, female, mrs",
                ".084"
            ],
            [
                "COGPROC",
                "not, but, how, if, all, why, or, any, need",
                ".132",
                "ASSENT",
                "yes, ok, awesome, okay, yeah, cool, absolutely, agree",
                ".080"
            ],
            [
                "[BOLD] Word2Vec Clusters",
                "[BOLD] Word2Vec Clusters",
                "[BOLD] Word2Vec Clusters",
                "[BOLD] Word2Vec Clusters",
                "[BOLD] Word2Vec Clusters",
                "[BOLD] Word2Vec Clusters"
            ],
            [
                "Cust. Service",
                "service, customer, contact, job, staff, assist, agent",
                ".136",
                "Gratitude",
                "thanks, thank, good, great, support, everyone, huge, proud",
                ".089"
            ],
            [
                "Order",
                "order, store, buy, free, delivery, available, package",
                ".128",
                "Family",
                "old, friend, family, mom, wife, husband, younger",
                ".063"
            ],
            [
                "Issues",
                "delayed, closed, between, outage, delay, road, accident",
                ".122",
                "Voting",
                "favorite, part, stars, model, vote, models, represent",
                ".060"
            ],
            [
                "Time Ref.",
                "been, yet, haven\u2019t, long, happened, yesterday, took",
                ".122",
                "Contests",
                "Christmas, gift, receive, entered, giveaway, enter, cards",
                ".058"
            ],
            [
                "Tech Parts",
                "battery, laptop, screen, warranty, desktop, printer",
                ".100",
                "Pets",
                "dogs, cat, dog, pet, shepherd, fluffy, treats",
                ".054"
            ],
            [
                "Access",
                "use, using, error, password, access, automatically, reset",
                ".098",
                "Christian",
                "god, shall, heaven, spirit, lord, belongs, soul, believers",
                ".053"
            ]
        ],
        "question": "Is it true that Several groups of words are much more likely to appear in a complaint, and are used to express complaints per se: about orders or deliveries (in the retail domain), about access (in complaints to service providers) and about parts of tech products (in tech)?",
        "answer_label": "no"
    },
    {
        "id": "144aa87d-c757-4945-bf5f-39149c5ba574",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "-{4} dense block",
                "24.8",
                "54.9"
            ],
            [
                "-{3, 4} dense blocks",
                "23.8",
                "54.1"
            ],
            [
                "-{2, 3, 4} dense blocks",
                "23.2",
                "53.1"
            ]
        ],
        "question": "Is it true that These results indicate dense connections do play a significant role in our model?",
        "answer_label": "yes"
    },
    {
        "id": "d3fb5a11-bfc1-4394-ad0d-3d78e82880e7",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that This empirically shows that compared to recurrent graph encoders, DCGCNs do not necessarily learn better representations for graphs?",
        "answer_label": "no"
    },
    {
        "id": "4158cd57-dd27-47fd-b14e-d3df4b88c3aa",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
        "table_column_names": [
            "[BOLD] Emoji alias",
            "[BOLD] N",
            "[BOLD] emoji #",
            "[BOLD] emoji %",
            "[BOLD] no-emoji #",
            "[BOLD] no-emoji %",
            "[BOLD] \u0394%"
        ],
        "table_content_values": [
            [
                "mask",
                "163",
                "154",
                "94.48",
                "134",
                "82.21",
                "- 12.27"
            ],
            [
                "two_hearts",
                "87",
                "81",
                "93.10",
                "77",
                "88.51",
                "- 4.59"
            ],
            [
                "heart_eyes",
                "122",
                "109",
                "89.34",
                "103",
                "84.43",
                "- 4.91"
            ],
            [
                "heart",
                "267",
                "237",
                "88.76",
                "235",
                "88.01",
                "- 0.75"
            ],
            [
                "rage",
                "92",
                "78",
                "84.78",
                "66",
                "71.74",
                "- 13.04"
            ],
            [
                "cry",
                "116",
                "97",
                "83.62",
                "83",
                "71.55",
                "- 12.07"
            ],
            [
                "sob",
                "490",
                "363",
                "74.08",
                "345",
                "70.41",
                "- 3.67"
            ],
            [
                "unamused",
                "167",
                "121",
                "72.46",
                "116",
                "69.46",
                "- 3.00"
            ],
            [
                "weary",
                "204",
                "140",
                "68.63",
                "139",
                "68.14",
                "- 0.49"
            ],
            [
                "joy",
                "978",
                "649",
                "66.36",
                "629",
                "64.31",
                "- 2.05"
            ],
            [
                "sweat_smile",
                "111",
                "73",
                "65.77",
                "75",
                "67.57",
                "1.80"
            ],
            [
                "confused",
                "77",
                "46",
                "59.74",
                "48",
                "62.34",
                "2.60"
            ]
        ],
        "question": "Is it true that [CONTINUE] The most interesting ones are mask, rage, and cry, which significantly increase accuracy?",
        "answer_label": "yes"
    },
    {
        "id": "88a1e00a-fb2f-47ef-b350-3f22f3214735",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "Base ACC",
            "Base Time",
            "+LN ACC",
            "+LN Time",
            "+BERT ACC",
            "+BERT Time",
            "+LN+BERT ACC",
            "+LN+BERT Time"
        ],
        "table_content_values": [
            [
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "250K",
                "83.50",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-"
            ],
            [
                "This",
                "LSTM",
                "8.36M",
                "84.27",
                "0.262",
                "86.03",
                "0.432",
                "89.95",
                "0.544",
                "[BOLD] 90.49",
                "0.696"
            ],
            [
                "This",
                "GRU",
                "6.41M",
                "[BOLD] 85.71",
                "0.245",
                "[BOLD] 86.05",
                "0.419",
                "[BOLD] 90.29",
                "0.529",
                "90.10",
                "0.695"
            ],
            [
                "This",
                "ATR",
                "2.87M",
                "84.88",
                "0.210",
                "85.81",
                "0.307",
                "90.00",
                "0.494",
                "90.28",
                "0.580"
            ],
            [
                "Work",
                "SRU",
                "5.48M",
                "84.28",
                "0.258",
                "85.32",
                "0.283",
                "89.98",
                "0.543",
                "90.09",
                "0.555"
            ],
            [
                "[EMPTY]",
                "LRN",
                "4.25M",
                "84.88",
                "[BOLD] 0.209",
                "85.06",
                "[BOLD] 0.223",
                "89.98",
                "[BOLD] 0.488",
                "89.93",
                "[BOLD] 0.506"
            ]
        ],
        "question": "Is it true that Results with BERT show that contextual information is not always valuable for performance improvement?",
        "answer_label": "no"
    },
    {
        "id": "bfe65751-16ee-43e3-9cc4-8301d4625a8e",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
        "table_column_names": [
            "[BOLD] Method",
            "[BOLD] R-1",
            "[BOLD] R-2",
            "[BOLD] R-SU"
        ],
        "table_content_values": [
            [
                "First-1",
                "26.83",
                "7.25",
                "6.46"
            ],
            [
                "First-2",
                "35.99",
                "10.17",
                "12.06"
            ],
            [
                "First-3",
                "39.41",
                "11.77",
                "14.51"
            ],
            [
                "LexRank Erkan and Radev ( 2004 )",
                "38.27",
                "12.70",
                "13.20"
            ],
            [
                "TextRank Mihalcea and Tarau ( 2004 )",
                "38.44",
                "13.10",
                "13.50"
            ],
            [
                "MMR Carbonell and Goldstein ( 1998 )",
                "38.77",
                "11.98",
                "12.91"
            ],
            [
                "PG-Original Lebanoff et\u00a0al. ( 2018 )",
                "41.85",
                "12.91",
                "16.46"
            ],
            [
                "PG-MMR Lebanoff et\u00a0al. ( 2018 )",
                "40.55",
                "12.36",
                "15.87"
            ],
            [
                "PG-BRNN Gehrmann et\u00a0al. ( 2018 )",
                "42.80",
                "14.19",
                "16.75"
            ],
            [
                "CopyTransformer Gehrmann et\u00a0al. ( 2018 )",
                "[BOLD] 43.57",
                "14.03",
                "17.37"
            ],
            [
                "Hi-MAP (Our Model)",
                "43.47",
                "[BOLD] 14.89",
                "[BOLD] 17.41"
            ]
        ],
        "question": "Is it true that The Transformer performs best in terms of R-1 while Hi-MAP outperforms it on R-2 and R-SU?",
        "answer_label": "yes"
    },
    {
        "id": "7020ecb6-4c9f-47c3-9983-05d987388d83",
        "table_caption": "Evaluation of Greek Word Embeddings Table 3: Summary for 3CosAdd and top-1 nearest vectors.",
        "table_column_names": [
            "Category Semantic",
            "Category no oov words",
            "gr_def 58.42%",
            "gr_neg10 59.33%",
            "cc.el.300  [BOLD] 68.80%",
            "wiki.el 27.20%",
            "gr_cbow_def 31.76%",
            "gr_d300_nosub 60.79%",
            "gr_w2v_sg_n5 52.70%"
        ],
        "table_content_values": [
            [
                "[EMPTY]",
                "with oov words",
                "52.97%",
                "55.33%",
                "[BOLD] 64.34%",
                "25.73%",
                "28.80%",
                "55.11%",
                "47.82%"
            ],
            [
                "Syntactic",
                "no oov words",
                "65.73%",
                "61.02%",
                "[BOLD] 69.35%",
                "40.90%",
                "64.02%",
                "53.69%",
                "52.60%"
            ],
            [
                "[EMPTY]",
                "with oov words",
                "[BOLD] 53.95%",
                "48.69%",
                "49.43%",
                "28.42%",
                "52.54%",
                "44.06%",
                "43.13%"
            ],
            [
                "Overall",
                "no oov words",
                "63.02%",
                "59.96%",
                "[BOLD] 68.97%",
                "36.45%",
                "52.04%",
                "56.30%",
                "52.66%"
            ],
            [
                "[EMPTY]",
                "with oov words",
                "53.60%",
                "51.00%",
                "[BOLD] 54.60%",
                "27.50%",
                "44.30%",
                "47.90%",
                "44.80%"
            ]
        ],
        "question": "Is it true that Considering the two aggregated categories of syntactic and semantic word analogies respectively and both 3CosAdd and 3CosMul metrics, model gr def had the best performance in both cases, even when we included the out-of-vocabulary (oov) terms?",
        "answer_label": "no"
    },
    {
        "id": "4c2dc82f-365f-43a9-b795-daccfa505e9c",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "Base ACC",
            "Base Time",
            "+LN ACC",
            "+LN Time",
            "+BERT ACC",
            "+BERT Time",
            "+LN+BERT ACC",
            "+LN+BERT Time"
        ],
        "table_content_values": [
            [
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "250K",
                "83.50",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-"
            ],
            [
                "This",
                "LSTM",
                "8.36M",
                "84.27",
                "0.262",
                "86.03",
                "0.432",
                "89.95",
                "0.544",
                "[BOLD] 90.49",
                "0.696"
            ],
            [
                "This",
                "GRU",
                "6.41M",
                "[BOLD] 85.71",
                "0.245",
                "[BOLD] 86.05",
                "0.419",
                "[BOLD] 90.29",
                "0.529",
                "90.10",
                "0.695"
            ],
            [
                "This",
                "ATR",
                "2.87M",
                "84.88",
                "0.210",
                "85.81",
                "0.307",
                "90.00",
                "0.494",
                "90.28",
                "0.580"
            ],
            [
                "Work",
                "SRU",
                "5.48M",
                "84.28",
                "0.258",
                "85.32",
                "0.283",
                "89.98",
                "0.543",
                "90.09",
                "0.555"
            ],
            [
                "[EMPTY]",
                "LRN",
                "4.25M",
                "84.88",
                "[BOLD] 0.209",
                "85.06",
                "[BOLD] 0.223",
                "89.98",
                "[BOLD] 0.488",
                "89.93",
                "[BOLD] 0.506"
            ]
        ],
        "question": "Is it true that LRN is not the fastest model, with ATR outperforming it by 8%\u223c27%?",
        "answer_label": "no"
    },
    {
        "id": "e41db642-41b4-4b36-846a-3e019d6ab36a",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Balanced",
            "Throughput (instances/s) Moderate",
            "Throughput (instances/s) Linear"
        ],
        "table_content_values": [
            [
                "1",
                "46.7",
                "27.3",
                "7.6"
            ],
            [
                "10",
                "125.2",
                "78.2",
                "22.7"
            ],
            [
                "25",
                "129.7",
                "83.1",
                "45.4"
            ]
        ],
        "question": "Is it true that [CONTINUE] On the contrary, for the linear dataset, the recursive implementation fails to efficiently make use of CPU resources and thus the performance gain provided by increasing the batch size is relatively high?",
        "answer_label": "yes"
    },
    {
        "id": "b2b4fcf5-0b84-4720-ae2c-90e27e36dcef",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. \u2020 denotes the previous best state-of-the-art model.",
        "table_column_names": [
            "Model",
            "NYT10 Prec.",
            "NYT10 Rec.",
            "NYT10 F1",
            "NYT11 Prec.",
            "NYT11 Rec.",
            "NYT11 F1"
        ],
        "table_content_values": [
            [
                "CNN zeng2014relation",
                "0.413",
                "0.591",
                "0.486",
                "0.444",
                "0.625",
                "0.519"
            ],
            [
                "PCNN zeng2015distant",
                "0.380",
                "[BOLD] 0.642",
                "0.477",
                "0.446",
                "0.679",
                "0.538\u2020"
            ],
            [
                "EA huang2016attention",
                "0.443",
                "0.638",
                "0.523\u2020",
                "0.419",
                "0.677",
                "0.517"
            ],
            [
                "BGWA jat2018attention",
                "0.364",
                "0.632",
                "0.462",
                "0.417",
                "[BOLD] 0.692",
                "0.521"
            ],
            [
                "BiLSTM-CNN",
                "0.490",
                "0.507",
                "0.498",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "Our model",
                "[BOLD] 0.541",
                "0.595",
                "[BOLD] 0.566*",
                "[BOLD] 0.507",
                "0.652",
                "[BOLD] 0.571*"
            ]
        ],
        "question": "Is it true that Our model outperforms the previous stateof-the-art models on both datasets in terms of F1 score?",
        "answer_label": "yes"
    },
    {
        "id": "8c6fd9a3-be98-4473-be7b-5f21d24b48ad",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Method",
            "SUBJ",
            "CR",
            "MR",
            "MPQA",
            "MRPC",
            "TREC",
            "SICK-E",
            "SST2",
            "SST5",
            "STS-B",
            "SICK-R"
        ],
        "table_content_values": [
            [
                "CBOW/784",
                "90.0",
                "[BOLD] 79.2",
                "[BOLD] 74.0",
                "87.1",
                "71.6",
                "85.6",
                "78.9",
                "78.5",
                "42.1",
                "61.0",
                "[BOLD] 78.1"
            ],
            [
                "CMOW/784",
                "87.5",
                "73.4",
                "70.6",
                "[BOLD] 87.3",
                "69.6",
                "[BOLD] 88.0",
                "77.2",
                "74.7",
                "37.9",
                "56.5",
                "76.2"
            ],
            [
                "Hybrid",
                "[BOLD] 90.2",
                "78.7",
                "73.7",
                "[BOLD] 87.3",
                "[BOLD] 72.7",
                "87.6",
                "[BOLD] 79.4",
                "[BOLD] 79.6",
                "[BOLD] 43.3",
                "[BOLD] 63.4",
                "77.8"
            ],
            [
                "cmp. CBOW",
                "+0.2%",
                "-0.6%",
                "-0.4%",
                "+0.2%",
                "+1.5%",
                "+2.3%",
                "+0.6%",
                "+1.4%",
                "+2.9%",
                "+3.9%",
                "-0.4%"
            ],
            [
                "cmp. CMOW",
                "+3.1%",
                "+7.2%",
                "+4.4%",
                "+0%",
                "+4.5%",
                "-0.5%",
                "+2.9%",
                "+6.7%",
                "+14.3",
                "+12.2%",
                "+2.1%"
            ]
        ],
        "question": "Is it true that Comparing the 784-dimensional models, CBOW and CMOW do not seem to complement each other?",
        "answer_label": "no"
    },
    {
        "id": "69e1dea0-ae41-4ea6-85e2-1976da7f370e",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that In conclusion, these results above can show the ineffectiveness of our DCGCN models?",
        "answer_label": "no"
    },
    {
        "id": "d6fd61c7-8f3c-4a96-90ac-3b84b96b5c00",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that This indicates that GINs can be employed in tasks where the distribution of node degrees has a long tail?",
        "answer_label": "yes"
    },
    {
        "id": "3c2a6721-2498-4f45-9f29-d3ebf070edc9",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "Base ACC",
            "Base Time",
            "+LN ACC",
            "+LN Time",
            "+BERT ACC",
            "+BERT Time",
            "+LN+BERT ACC",
            "+LN+BERT Time"
        ],
        "table_content_values": [
            [
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "250K",
                "83.50",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-"
            ],
            [
                "This",
                "LSTM",
                "8.36M",
                "84.27",
                "0.262",
                "86.03",
                "0.432",
                "89.95",
                "0.544",
                "[BOLD] 90.49",
                "0.696"
            ],
            [
                "This",
                "GRU",
                "6.41M",
                "[BOLD] 85.71",
                "0.245",
                "[BOLD] 86.05",
                "0.419",
                "[BOLD] 90.29",
                "0.529",
                "90.10",
                "0.695"
            ],
            [
                "This",
                "ATR",
                "2.87M",
                "84.88",
                "0.210",
                "85.81",
                "0.307",
                "90.00",
                "0.494",
                "90.28",
                "0.580"
            ],
            [
                "Work",
                "SRU",
                "5.48M",
                "84.28",
                "0.258",
                "85.32",
                "0.283",
                "89.98",
                "0.543",
                "90.09",
                "0.555"
            ],
            [
                "[EMPTY]",
                "LRN",
                "4.25M",
                "84.88",
                "[BOLD] 0.209",
                "85.06",
                "[BOLD] 0.223",
                "89.98",
                "[BOLD] 0.488",
                "89.93",
                "[BOLD] 0.506"
            ]
        ],
        "question": "Is it true that Although LSTM and GRU outperform LRN by 0.3\u223c0.9 in terms of accuracy, these recurrent units do not sacrifice running efficiency (about 7%\u223c48%) depending on whether LN and BERT are applied?",
        "answer_label": "no"
    },
    {
        "id": "932c9362-088b-48c2-b6bc-63df4037f765",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Feature",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Feature",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams"
            ],
            [
                "not",
                ".154",
                "[URL]",
                ".150"
            ],
            [
                "my",
                ".131",
                "!",
                ".082"
            ],
            [
                "working",
                ".124",
                "he",
                ".069"
            ],
            [
                "still",
                ".123",
                "thank",
                ".067"
            ],
            [
                "on",
                ".119",
                ",",
                ".064"
            ],
            [
                "can\u2019t",
                ".113",
                "love",
                ".064"
            ],
            [
                "service",
                ".112",
                "lol",
                ".061"
            ],
            [
                "customer",
                ".109",
                "you",
                ".060"
            ],
            [
                "why",
                ".108",
                "great",
                ".058"
            ],
            [
                "website",
                ".107",
                "win",
                ".058"
            ],
            [
                "no",
                ".104",
                "\u2019",
                ".058"
            ],
            [
                "?",
                ".098",
                "she",
                ".054"
            ],
            [
                "fix",
                ".093",
                ":",
                ".053"
            ],
            [
                "won\u2019t",
                ".092",
                "that",
                ".053"
            ],
            [
                "been",
                ".090",
                "more",
                ".052"
            ],
            [
                "issue",
                ".089",
                "it",
                ".052"
            ],
            [
                "days",
                ".088",
                "would",
                ".051"
            ],
            [
                "error",
                ".087",
                "him",
                ".047"
            ],
            [
                "is",
                ".084",
                "life",
                ".046"
            ],
            [
                "charged",
                ".083",
                "good",
                ".046"
            ],
            [
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)"
            ],
            [
                "VBN",
                ".141",
                "UH",
                ".104"
            ],
            [
                "$",
                ".118",
                "NNP",
                ".098"
            ],
            [
                "VBZ",
                ".114",
                "PRP",
                ".076"
            ],
            [
                "NN_VBZ",
                ".114",
                "HT",
                ".076"
            ],
            [
                "PRP$",
                ".107",
                "PRP_.",
                ".076"
            ],
            [
                "PRP$_NN",
                ".105",
                "PRP_RB",
                ".067"
            ],
            [
                "VBG",
                ".093",
                "NNP_NNP",
                ".062"
            ],
            [
                "CD",
                ".092",
                "VBP_PRP",
                ".054"
            ],
            [
                "WRB_VBZ",
                ".084",
                "JJ",
                ".053"
            ],
            [
                "VBZ_VBN",
                ".084",
                "DT_JJ",
                ".051"
            ]
        ],
        "question": "Is it true that [CONTINUE] On the other hand, the presence of terms that show positive sentiment or emotions (good, great, win, POSEMO, AFFECT, ASSENT) are among the top most distinctive features for a tweet not being labeled as a complaint?",
        "answer_label": "yes"
    },
    {
        "id": "8246391e-06c9-4bb7-a7c8-749c2940f735",
        "table_caption": "Turing at SemEval-2017 Task 8: Sequential Approach to Rumour Stance Classification with Branch-LSTM Table 5: Confusion matrix for testing set predictions",
        "table_column_names": [
            "[BOLD] LabelPrediction",
            "[BOLD] C",
            "[BOLD] D",
            "[BOLD] Q",
            "[BOLD] S"
        ],
        "table_content_values": [
            [
                "[BOLD] Commenting",
                "760",
                "0",
                "12",
                "6"
            ],
            [
                "[BOLD] Denying",
                "68",
                "0",
                "1",
                "2"
            ],
            [
                "[BOLD] Querying",
                "69",
                "0",
                "36",
                "1"
            ],
            [
                "[BOLD] Supporting",
                "67",
                "0",
                "1",
                "26"
            ]
        ],
        "question": "Is it true that Most denying instances get misclassified as commenting (see Table 5),?",
        "answer_label": "yes"
    },
    {
        "id": "0ab283cb-2c6e-451e-84c0-791df4822e8c",
        "table_caption": "Sparse and Structured Visual Attention Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
        "table_column_names": [
            "[EMPTY]",
            "MSCOCO spice",
            "MSCOCO cider",
            "MSCOCO rouge [ITALIC] L",
            "MSCOCO bleu4",
            "MSCOCO meteor",
            "MSCOCO rep\u2193",
            "Flickr30k spice",
            "Flickr30k cider",
            "Flickr30k rouge [ITALIC] L",
            "Flickr30k bleu4",
            "Flickr30k meteor",
            "Flickr30k rep\u2193"
        ],
        "table_content_values": [
            [
                "softmax",
                "18.4",
                "0.967",
                "52.9",
                "29.9",
                "24.9",
                "3.76",
                "13.5",
                "0.443",
                "44.2",
                "19.9",
                "19.1",
                "6.09"
            ],
            [
                "sparsemax",
                "[BOLD] 18.9",
                "[BOLD] 0.990",
                "[BOLD] 53.5",
                "[BOLD] 31.5",
                "[BOLD] 25.3",
                "3.69",
                "[BOLD] 13.7",
                "[BOLD] 0.444",
                "[BOLD] 44.3",
                "[BOLD] 20.7",
                "[BOLD] 19.3",
                "5.84"
            ],
            [
                "TVmax",
                "18.5",
                "0.974",
                "53.1",
                "29.9",
                "25.1",
                "[BOLD] 3.17",
                "13.3",
                "0.438",
                "44.2",
                "20.5",
                "19.0",
                "[BOLD] 3.97"
            ]
        ],
        "question": "Is it true that As can be seen in Table 1, sparsemax and TVMAX achieve better results overall when compared with softmax, indicating that the use of selective attention leads to better captions?",
        "answer_label": "yes"
    },
    {
        "id": "0d2c4c12-d580-4a42-88a2-1abeada7b180",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 3: Test BLEU for es-en adaptive training. EWC reduces forgetting compared to other fine-tuning methods, while offering the greatest improvement on the new domain.",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Training scheme",
            "[BOLD] Health",
            "[BOLD] Bio"
        ],
        "table_content_values": [
            [
                "1",
                "Health",
                "[BOLD] 35.9",
                "33.1"
            ],
            [
                "2",
                "Bio",
                "29.6",
                "36.1"
            ],
            [
                "3",
                "Health and Bio",
                "35.8",
                "37.2"
            ],
            [
                "4",
                "1 then Bio, No-reg",
                "30.3",
                "36.6"
            ],
            [
                "5",
                "1 then Bio, L2",
                "35.1",
                "37.3"
            ],
            [
                "6",
                "1 then Bio, EWC",
                "35.2",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that We find EWC does not outperform the L2 approach?",
        "answer_label": "no"
    },
    {
        "id": "b9b1312c-bfee-4f66-b10d-cedc62a7476a",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that On the other side, H-CMOW shows, among others, no improvements at BShift?",
        "answer_label": "no"
    },
    {
        "id": "9eb04d72-35f4-42a2-84cb-8233924128b3",
        "table_caption": "Sparse and Structured Visual Attention Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
        "table_column_names": [
            "[EMPTY]",
            "MSCOCO spice",
            "MSCOCO cider",
            "MSCOCO rouge [ITALIC] L",
            "MSCOCO bleu4",
            "MSCOCO meteor",
            "MSCOCO rep\u2193",
            "Flickr30k spice",
            "Flickr30k cider",
            "Flickr30k rouge [ITALIC] L",
            "Flickr30k bleu4",
            "Flickr30k meteor",
            "Flickr30k rep\u2193"
        ],
        "table_content_values": [
            [
                "softmax",
                "18.4",
                "0.967",
                "52.9",
                "29.9",
                "24.9",
                "3.76",
                "13.5",
                "0.443",
                "44.2",
                "19.9",
                "19.1",
                "6.09"
            ],
            [
                "sparsemax",
                "[BOLD] 18.9",
                "[BOLD] 0.990",
                "[BOLD] 53.5",
                "[BOLD] 31.5",
                "[BOLD] 25.3",
                "3.69",
                "[BOLD] 13.7",
                "[BOLD] 0.444",
                "[BOLD] 44.3",
                "[BOLD] 20.7",
                "[BOLD] 19.3",
                "5.84"
            ],
            [
                "TVmax",
                "18.5",
                "0.974",
                "53.1",
                "29.9",
                "25.1",
                "[BOLD] 3.17",
                "13.3",
                "0.438",
                "44.2",
                "20.5",
                "19.0",
                "[BOLD] 3.97"
            ]
        ],
        "question": "Is it true that [CONTINUE] Moreover, for TVMAX, automatic metrics results are slightly worse than sparsemax but still superior to softmax on MSCOCO and similar on Flickr30k?",
        "answer_label": "yes"
    },
    {
        "id": "ee5d43ff-0f49-4fe4-8d83-d0e9571e46d3",
        "table_caption": "Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
        "table_column_names": [
            "Dataset",
            "Metric",
            "Illinois",
            "IlliCons",
            "rahman2012resolving",
            "KnowFeat",
            "KnowCons",
            "KnowComb"
        ],
        "table_content_values": [
            [
                "[ITALIC] Winograd",
                "Precision",
                "51.48",
                "53.26",
                "73.05",
                "71.81",
                "74.93",
                "[BOLD] 76.41"
            ],
            [
                "[ITALIC] WinoCoref",
                "AntePre",
                "68.37",
                "74.32",
                "\u2014\u2013",
                "88.48",
                "88.95",
                "[BOLD] 89.32"
            ]
        ],
        "question": "Is it true that The best performing system is KnowComb?",
        "answer_label": "yes"
    },
    {
        "id": "e8fa28c5-0b0f-4d5e-a963-f0867a4b2f2a",
        "table_caption": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 4: Precisions on the Wikidata dataset with different choice of d.",
        "table_column_names": [
            "Recall",
            "0.1",
            "0.2",
            "0.3",
            "AUC",
            "Time"
        ],
        "table_content_values": [
            [
                "[ITALIC] d=1",
                "0.602",
                "0.487",
                "0.403",
                "0.367",
                "4h"
            ],
            [
                "[ITALIC] d=32",
                "0.645",
                "0.501",
                "0.393",
                "0.370",
                "-"
            ],
            [
                "[ITALIC] d=16",
                "0.655",
                "0.518",
                "0.413",
                "0.413",
                "20h"
            ],
            [
                "[ITALIC] d=8",
                "0.650",
                "0.519",
                "0.422",
                "0.405",
                "8h"
            ]
        ],
        "question": "Is it true that As the table 4 depicts, the precision increases with the growth of d, but the training time also increases?",
        "answer_label": "no"
    },
    {
        "id": "555cfc65-dbb1-41e2-8b2c-ca1d6db747d1",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that This is unexpected as encoding a bigger graph (containing more information) should be easier than encoding smaller graphs?",
        "answer_label": "no"
    },
    {
        "id": "736bdd5b-3280-45f8-ba57-7a23885bf208",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain high degree nodes?",
        "answer_label": "yes"
    },
    {
        "id": "83e691c8-43ce-46c6-bbc7-2b3d400d8c58",
        "table_caption": "Aligning Vector-spaces with Noisy Supervised Lexicons Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
        "table_column_names": [
            "Method",
            "En\u2192It best",
            "En\u2192It avg",
            "En\u2192It iters",
            "En\u2192De best",
            "En\u2192De avg",
            "En\u2192De iters",
            "En\u2192Fi best",
            "En\u2192Fi avg",
            "En\u2192Fi iters",
            "En\u2192Es best",
            "En\u2192Es avg",
            "En\u2192Es iters"
        ],
        "table_content_values": [
            [
                "Artetxe et\u00a0al., 2018b",
                "[BOLD] 48.53",
                "48.13",
                "573",
                "48.47",
                "48.19",
                "773",
                "33.50",
                "32.63",
                "988",
                "37.60",
                "37.33",
                "808"
            ],
            [
                "Noise-aware Alignment",
                "[BOLD] 48.53",
                "[BOLD] 48.20",
                "471",
                "[BOLD] 49.67",
                "[BOLD] 48.89",
                "568",
                "[BOLD] 33.98",
                "[BOLD] 33.68",
                "502",
                "[BOLD] 38.40",
                "[BOLD] 37.79",
                "551"
            ]
        ],
        "question": "Is it true that Our model does not improve the results in the translation tasks?",
        "answer_label": "no"
    },
    {
        "id": "d9f56bd3-ead2-44b1-8f61-415c79ef85fa",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 3: Number of tweets annotated as complaints across the nine domains.",
        "table_column_names": [
            "[BOLD] Category",
            "[BOLD] Complaints",
            "[BOLD] Not Complaints"
        ],
        "table_content_values": [
            [
                "Food & Beverage",
                "95",
                "35"
            ],
            [
                "Apparel",
                "141",
                "117"
            ],
            [
                "Retail",
                "124",
                "75"
            ],
            [
                "Cars",
                "67",
                "25"
            ],
            [
                "Services",
                "207",
                "130"
            ],
            [
                "Software & Online Services",
                "189",
                "103"
            ],
            [
                "Transport",
                "139",
                "109"
            ],
            [
                "Electronics",
                "174",
                "112"
            ],
            [
                "Other",
                "96",
                "33"
            ],
            [
                "Total",
                "1232",
                "739"
            ]
        ],
        "question": "Is it true that In total, 1,232 tweets (62.4%) are complaints and 739 are not complaints (37.6%)?",
        "answer_label": "yes"
    },
    {
        "id": "5d51c21b-4d94-4b64-bb07-a899563e5b9a",
        "table_caption": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set",
        "table_column_names": [
            "Method",
            "WER (%)",
            "DCE"
        ],
        "table_content_values": [
            [
                "No enhancement",
                "17.3",
                "0.828"
            ],
            [
                "Wiener filter",
                "19.5",
                "0.722"
            ],
            [
                "Minimizing DCE",
                "15.8",
                "[BOLD] 0.269"
            ],
            [
                "FSEGAN",
                "14.9",
                "0.291"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
                "15.6",
                "0.330"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
                "[BOLD] 14.4",
                "0.303"
            ],
            [
                "Clean speech",
                "5.7",
                "0.0"
            ]
        ],
        "question": "Is it true that The Wiener filtering method shows lower DCE, but higher WER than no enhancement?",
        "answer_label": "yes"
    },
    {
        "id": "7f0dcd5c-173e-4858-a85d-6f861623a0d4",
        "table_caption": "Assessing Gender Bias in Machine Translation \u2013 A Case Study with Google Translate Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table",
        "table_column_names": [
            "Category",
            "Female (%)",
            "Male (%)",
            "Neutral (%)"
        ],
        "table_content_values": [
            [
                "Office and administrative support",
                "11.015",
                "58.812",
                "16.954"
            ],
            [
                "Architecture and engineering",
                "2.299",
                "72.701",
                "10.92"
            ],
            [
                "Farming, fishing, and forestry",
                "12.179",
                "62.179",
                "14.744"
            ],
            [
                "Management",
                "11.232",
                "66.667",
                "12.681"
            ],
            [
                "Community and social service",
                "20.238",
                "62.5",
                "10.119"
            ],
            [
                "Healthcare support",
                "25.0",
                "43.75",
                "17.188"
            ],
            [
                "Sales and related",
                "8.929",
                "62.202",
                "16.964"
            ],
            [
                "Installation, maintenance, and repair",
                "5.22",
                "58.333",
                "17.125"
            ],
            [
                "Transportation and material moving",
                "8.81",
                "62.976",
                "17.5"
            ],
            [
                "Legal",
                "11.905",
                "72.619",
                "10.714"
            ],
            [
                "Business and financial operations",
                "7.065",
                "67.935",
                "15.58"
            ],
            [
                "Life, physical, and social science",
                "5.882",
                "73.284",
                "10.049"
            ],
            [
                "Arts, design, entertainment, sports, and media",
                "10.36",
                "67.342",
                "11.486"
            ],
            [
                "Education, training, and library",
                "23.485",
                "53.03",
                "9.091"
            ],
            [
                "Building and grounds cleaning and maintenance",
                "12.5",
                "68.333",
                "11.667"
            ],
            [
                "Personal care and service",
                "18.939",
                "49.747",
                "18.434"
            ],
            [
                "Healthcare practitioners and technical",
                "22.674",
                "51.744",
                "15.116"
            ],
            [
                "Production",
                "14.331",
                "51.199",
                "18.245"
            ],
            [
                "Computer and mathematical",
                "4.167",
                "66.146",
                "14.062"
            ],
            [
                "Construction and extraction",
                "8.578",
                "61.887",
                "17.525"
            ],
            [
                "Protective service",
                "8.631",
                "65.179",
                "12.5"
            ],
            [
                "Food preparation and serving related",
                "21.078",
                "58.333",
                "17.647"
            ],
            [
                "Total",
                "11.76",
                "58.93",
                "15.939"
            ]
        ],
        "question": "Is it true that Furthermore, this bias is seemingly aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics ?",
        "answer_label": "yes"
    },
    {
        "id": "511ea10f-bd33-40f4-a2ab-40fe6c582bbd",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
        "table_column_names": [
            "[EMPTY]",
            "WN-N P",
            "WN-N R",
            "WN-N F",
            "WN-V P",
            "WN-V R",
            "WN-V F",
            "VN P",
            "VN R",
            "VN F"
        ],
        "table_content_values": [
            [
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2"
            ],
            [
                "type",
                ".700",
                ".654",
                ".676",
                ".535",
                ".474",
                ".503",
                ".327",
                ".309",
                ".318"
            ],
            [
                "x+POS",
                ".699",
                ".651",
                ".674",
                ".544",
                ".472",
                ".505",
                ".339",
                ".312",
                ".325"
            ],
            [
                "lemma",
                ".706",
                ".660",
                ".682",
                ".576",
                ".520",
                ".547",
                ".384",
                ".360",
                ".371"
            ],
            [
                "x+POS",
                "<bold>.710</bold>",
                "<bold>.662</bold>",
                "<bold>.685</bold>",
                "<bold>.589</bold>",
                "<bold>.529</bold>",
                "<bold>.557</bold>",
                "<bold>.410</bold>",
                "<bold>.389</bold>",
                "<bold>.399</bold>"
            ],
            [
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep"
            ],
            [
                "type",
                ".712",
                ".661",
                ".686",
                ".545",
                ".457",
                ".497",
                ".324",
                ".296",
                ".310"
            ],
            [
                "x+POS",
                ".715",
                ".659",
                ".686",
                ".560",
                ".464",
                ".508",
                ".349",
                ".320",
                ".334"
            ],
            [
                "lemma",
                "<bold>.725</bold>",
                "<bold>.668</bold>",
                "<bold>.696</bold>",
                ".591",
                ".512",
                ".548",
                ".408",
                ".371",
                ".388"
            ],
            [
                "x+POS",
                ".722",
                ".666",
                ".693",
                "<bold>.609</bold>",
                "<bold>.527</bold>",
                "<bold>.565</bold>",
                "<bold>.412</bold>",
                "<bold>.381</bold>",
                "<bold>.396</bold>"
            ]
        ],
        "question": "Is it true that Still, lemma-based targets significantly7 (p \u2264 .005) outperform type-based targets in terms of F-measure in all cases?",
        "answer_label": "yes"
    },
    {
        "id": "44c10fb7-e496-4663-8f4c-d71ccd7dd67d",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
        "table_column_names": [
            "[EMPTY]",
            "Acc",
            "Sim",
            "PP",
            "GM"
        ],
        "table_content_values": [
            [
                "M0: shen-1",
                "0.818",
                "0.719",
                "37.3",
                "10.0"
            ],
            [
                "M1: M0 [ITALIC] +para",
                "0.819",
                "0.734",
                "26.3",
                "14.2"
            ],
            [
                "M2: M0 [ITALIC] +cyc",
                "0.813",
                "0.770",
                "36.4",
                "18.8"
            ],
            [
                "M3: M0 [ITALIC] +cyc+lang",
                "0.807",
                "0.796",
                "28.4",
                "21.5"
            ],
            [
                "M4: M0 [ITALIC] +cyc+para",
                "0.798",
                "0.783",
                "39.7",
                "19.2"
            ],
            [
                "M5: M0 [ITALIC] +cyc+para+lang",
                "0.804",
                "0.785",
                "27.1",
                "20.3"
            ],
            [
                "M6: M0 [ITALIC] +cyc+2d",
                "0.805",
                "[BOLD] 0.817",
                "43.3",
                "21.6"
            ],
            [
                "M7: M6+ [ITALIC] para+lang",
                "0.818",
                "0.805",
                "[BOLD] 29.0",
                "[BOLD] 22.8"
            ]
        ],
        "question": "Is it true that Table 2 shows that the model with cyclic loss (M2) and the model with cyclic loss, paraphrase loss, and language model loss (M5) both have lower Sim than M0 on both datasets under similar Acc?",
        "answer_label": "no"
    },
    {
        "id": "ecd566bf-ff78-4522-a91d-986557c5748c",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE IX: Accuracies (%) for Sentiment Classification Task",
        "table_column_names": [
            "GloVe",
            "Word2Vec",
            "OIWE-IPG",
            "SOV",
            "SPINE",
            "Word2Sense",
            "Proposed"
        ],
        "table_content_values": [
            [
                "77.34",
                "77.91",
                "74.27",
                "78.43",
                "74.13",
                "81.21",
                "78.26"
            ]
        ],
        "question": "Is it true that The proposed method outperforms the original embeddings and performs on par with the SOV?",
        "answer_label": "yes"
    },
    {
        "id": "7b76da1e-3531-49c1-ad27-dfe176882449",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
        "table_column_names": [
            "Model",
            "#Params",
            "BLEU",
            "Train",
            "Decode"
        ],
        "table_content_values": [
            [
                "GNMT",
                "-",
                "24.61",
                "-",
                "-"
            ],
            [
                "GRU",
                "206M",
                "26.28",
                "2.67",
                "45.35"
            ],
            [
                "ATR",
                "122M",
                "25.70",
                "1.33",
                "[BOLD] 34.40"
            ],
            [
                "SRU",
                "170M",
                "25.91",
                "1.34",
                "42.84"
            ],
            [
                "LRN",
                "143M",
                "26.26",
                "[BOLD] 0.99",
                "36.50"
            ],
            [
                "oLRN",
                "164M",
                "[BOLD] 26.73",
                "1.15",
                "40.19"
            ]
        ],
        "question": "Is it true that In addition, the training time results in Table 3 confirm the computational disadvantage of LRN over all other recurrent units, where LRN slows down compared to ATR and SRU by approximately 25%?",
        "answer_label": "no"
    },
    {
        "id": "7ee1b8c8-3a20-4d62-9344-ec5f5a542ba9",
        "table_caption": "Variational Self-attention Model for Sentence Representation Table 2: Performance comparison with the state-of-art algorithms on the FNC-1 test dataset.",
        "table_column_names": [
            "Model",
            "Accuracy (%) agree",
            "Accuracy (%) disagree",
            "Accuracy (%) discuss",
            "Accuracy (%) unrelated",
            "Micro F1(%)"
        ],
        "table_content_values": [
            [
                "Average of Word2vec Embedding",
                "12.43",
                "01.30",
                "43.32",
                "74.24",
                "45.53"
            ],
            [
                "CNN-based Sentence Embedding",
                "24.54",
                "05.06",
                "53.24",
                "79.53",
                "81.72"
            ],
            [
                "RNN-based Sentence Embedding",
                "24.42",
                "05.42",
                "69.05",
                "65.34",
                "78.70"
            ],
            [
                "Self-attention Sentence Embedding",
                "23.53",
                "04.63",
                "63.59",
                "80.34",
                "80.11"
            ],
            [
                "Our model",
                "28.53",
                "10.43",
                "65.43",
                "82.43",
                "[BOLD] 83.54"
            ]
        ],
        "question": "Is it true that As for the micro F1 evaluation metric, our model does not achieve the highest performance (83.54%) on the FNC-1 testing subset?",
        "answer_label": "no"
    },
    {
        "id": "e55f868d-c3b3-42e4-91ae-3e87cab511f3",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
        "table_column_names": [
            "# of Heads",
            "Accuracy",
            "Val. Loss",
            "Effect"
        ],
        "table_content_values": [
            [
                "1",
                "89.44%",
                "0.2811",
                "-6.84%"
            ],
            [
                "2",
                "91.20%",
                "0.2692",
                "-5.08%"
            ],
            [
                "4",
                "93.85%",
                "0.2481",
                "-2.43%"
            ],
            [
                "8",
                "96.02%",
                "0.2257",
                "-0.26%"
            ],
            [
                "10",
                "96.28%",
                "0.2197",
                "[EMPTY]"
            ],
            [
                "16",
                "96.32%",
                "0.2190",
                "+0.04"
            ]
        ],
        "question": "Is it true that This shows that more attention heads, thereby attending to multiple different contexts at once, is important to boosting performance to state-of-the-art results?",
        "answer_label": "yes"
    },
    {
        "id": "8ca9d3e5-cce5-41f9-ada1-afa9e75a6cf1",
        "table_caption": "Solving Hard Coreference Problems Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.",
        "table_column_names": [
            "System",
            "MUC",
            "BCUB",
            "CEAFe",
            "AVG"
        ],
        "table_content_values": [
            [
                "ACE",
                "ACE",
                "ACE",
                "ACE",
                "ACE"
            ],
            [
                "IlliCons",
                "[BOLD] 78.17",
                "81.64",
                "[BOLD] 78.45",
                "[BOLD] 79.42"
            ],
            [
                "KnowComb",
                "77.51",
                "[BOLD] 81.97",
                "77.44",
                "78.97"
            ],
            [
                "OntoNotes",
                "OntoNotes",
                "OntoNotes",
                "OntoNotes",
                "OntoNotes"
            ],
            [
                "IlliCons",
                "84.10",
                "[BOLD] 78.30",
                "[BOLD] 68.74",
                "[BOLD] 77.05"
            ],
            [
                "KnowComb",
                "[BOLD] 84.33",
                "78.02",
                "67.95",
                "76.76"
            ]
        ],
        "question": "Is it true that Despite our system achieving the same level of performance compared to a state-of-art general coreference system, we still observe significant performance improvement on the ACE and OntoNotes datasets?",
        "answer_label": "no"
    },
    {
        "id": "5ea07570-7f39-4790-b562-22fd697fb6ef",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that Results also show the global node is more effective than the linear combination?",
        "answer_label": "no"
    },
    {
        "id": "ebaa19a5-b7cf-4f3b-8b04-b4304e779ff8",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
        "table_column_names": [
            "[BOLD] Training data",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] Disfl"
        ],
        "table_content_values": [
            [
                "Original",
                "0",
                "22",
                "0",
                "14"
            ],
            [
                "Cleaned added",
                "0",
                "23",
                "0",
                "14"
            ],
            [
                "Cleaned missing",
                "0",
                "1",
                "0",
                "2"
            ],
            [
                "Cleaned",
                "0",
                "0",
                "0",
                "5"
            ]
        ],
        "question": "Is it true that All fluency problems we found were very slight and no added or wrong-valued slots were found, so missed slots are the main problem?",
        "answer_label": "yes"
    },
    {
        "id": "36fa8fb5-8f19-4084-8cbb-4c3195e16bc3",
        "table_caption": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.",
        "table_column_names": [
            "Feature",
            "LR P",
            "LR R",
            "LR F1",
            "SVM P",
            "SVM R",
            "SVM F1",
            "ANN P",
            "ANN R",
            "ANN F1"
        ],
        "table_content_values": [
            [
                "+BoW",
                "0.93",
                "0.91",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+BoC (Wiki-PubMed-PMC)",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.91",
                "0.91",
                "[BOLD] 0.91"
            ],
            [
                "+BoC (GloVe)",
                "0.93",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+ASM",
                "0.90",
                "0.85",
                "0.88",
                "0.90",
                "0.86",
                "0.88",
                "0.89",
                "0.89",
                "0.89"
            ],
            [
                "+Sentence Embeddings(SEs)",
                "0.89",
                "0.89",
                "0.89",
                "0.90",
                "0.86",
                "0.88",
                "0.88",
                "0.88",
                "0.88"
            ],
            [
                "+BoC(Wiki-PubMed-PMC)+SEs",
                "0.92",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ]
        ],
        "question": "Is it true that The models using BoC do not outperform models using BoW as well as ASM features?",
        "answer_label": "no"
    },
    {
        "id": "0b7152d7-54db-4a1f-b5cb-18c3b4b17aa2",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 7: Test accuracy on SNLI task with Base+LN setting and test perplexity on PTB task with Base setting.",
        "table_column_names": [
            "Model",
            "SNLI",
            "PTB"
        ],
        "table_content_values": [
            [
                "LRN",
                "[BOLD] 85.06",
                "[BOLD] 61.26"
            ],
            [
                "gLRN",
                "84.72",
                "92.49"
            ],
            [
                "eLRN",
                "83.56",
                "169.81"
            ]
        ],
        "question": "Is it true that Results in Table 7 show that although the accuracy on SNLI is acceptable, gLRN and eLRN perform significantly worse on the PTB task?",
        "answer_label": "yes"
    },
    {
        "id": "15ca12b8-56ac-4053-8fe7-3b00a97b9a07",
        "table_caption": "UKP TU-DA at GermEval 2017:Deep Learning for Aspect Based Sentiment Detection Table 5: Task B results",
        "table_column_names": [
            "[EMPTY]",
            "Micro F1"
        ],
        "table_content_values": [
            [
                "Baseline",
                "0.709"
            ],
            [
                "W2V (<italic>d</italic>=50)",
                "0.736"
            ],
            [
                "W2V (<italic>d</italic>=500)",
                "0.753"
            ],
            [
                "S2V",
                "0.748"
            ],
            [
                "S2V + W2V (<italic>d</italic>=50)",
                "0.744"
            ],
            [
                "S2V + K + W2V(<italic>d</italic>=50)",
                "0.749"
            ],
            [
                "SIF (DE)",
                "0.759"
            ],
            [
                "SIF (DE-EN)",
                "<bold>0.765</bold>"
            ]
        ],
        "question": "Is it true that For Task B, the baseline model outperformed all models trained on the stacked learner when using only plain averaged word embeddings?",
        "answer_label": "no"
    },
    {
        "id": "05f252e0-8409-4a35-8596-dc70f2f2b281",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
        "table_column_names": [
            "[EMPTY]",
            "Difference Function",
            "Seanad Abolition",
            "Video Games",
            "Pornography"
        ],
        "table_content_values": [
            [
                "OD-parse",
                "Absolute",
                "0.01",
                "-0.01",
                "0.07"
            ],
            [
                "OD-parse",
                "JS div.",
                "0.01",
                "-0.01",
                "-0.01"
            ],
            [
                "OD-parse",
                "EMD",
                "0.07",
                "0.01",
                "-0.01"
            ],
            [
                "OD",
                "Absolute",
                "[BOLD] 0.54",
                "[BOLD] 0.56",
                "[BOLD] 0.41"
            ],
            [
                "OD",
                "JS div.",
                "0.07",
                "-0.01",
                "-0.02"
            ],
            [
                "OD",
                "EMD",
                "0.26",
                "-0.01",
                "0.01"
            ],
            [
                "OD (no polarity shifters)",
                "Absolute",
                "0.23",
                "0.08",
                "0.04"
            ],
            [
                "OD (no polarity shifters)",
                "JS div.",
                "0.09",
                "-0.01",
                "-0.02"
            ],
            [
                "OD (no polarity shifters)",
                "EMD",
                "0.10",
                "0.01",
                "-0.01"
            ]
        ],
        "question": "Is it true that This is evident from the significant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters?",
        "answer_label": "yes"
    },
    {
        "id": "73c20a83-ae30-4b98-b2fc-1ba2d74977b6",
        "table_caption": "Lost in Translation: Loss and Decay of Linguistic Richness in Machine Translation Table 5: Automatic evaluation scores (BLEU and TER) for the REV systems.",
        "table_column_names": [
            "System reference",
            "BLEU\u2191",
            "TER\u2193"
        ],
        "table_content_values": [
            [
                "en-fr-rnn-rev",
                "33.3",
                "50.2"
            ],
            [
                "en-fr-smt-rev",
                "36.5",
                "47.1"
            ],
            [
                "en-fr-trans-rev",
                "[BOLD] 36.8",
                "[BOLD] 46.8"
            ],
            [
                "en-es-rnn-rev",
                "37.8",
                "45.0"
            ],
            [
                "en-es-smt-rev",
                "39.2",
                "44.0"
            ],
            [
                "en-es-trans-rev",
                "[BOLD] 40.4",
                "[BOLD] 42.7"
            ]
        ],
        "question": "Is it true that we present BLEU and TER for the REV systems in Table 5, [CONTINUE] While Transformer models are the best ones according to the evaluation metrics,?",
        "answer_label": "yes"
    },
    {
        "id": "6e0cec48-2365-4b12-a062-d9a7a42c0fa8",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
        "table_column_names": [
            "[BOLD] Model",
            "R",
            "MUC P",
            "[ITALIC] F1",
            "R",
            "B3 P",
            "[ITALIC] F1",
            "R",
            "CEAF- [ITALIC] e P",
            "[ITALIC] F1",
            "CoNLL  [ITALIC] F1"
        ],
        "table_content_values": [
            [
                "[BOLD] Baselines",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Cluster+Lemma",
                "76.5",
                "79.9",
                "78.1",
                "71.7",
                "85",
                "77.8",
                "75.5",
                "71.7",
                "73.6",
                "76.5"
            ],
            [
                "CV Cybulska and Vossen ( 2015a )",
                "71",
                "75",
                "73",
                "71",
                "78",
                "74",
                "-",
                "-",
                "64",
                "73"
            ],
            [
                "KCP Kenyon-Dean et\u00a0al. ( 2018 )",
                "67",
                "71",
                "69",
                "71",
                "67",
                "69",
                "71",
                "67",
                "69",
                "69"
            ],
            [
                "Cluster+KCP",
                "68.4",
                "79.3",
                "73.4",
                "67.2",
                "87.2",
                "75.9",
                "77.4",
                "66.4",
                "71.5",
                "73.6"
            ],
            [
                "[BOLD] Model Variants",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Disjoint",
                "75.5",
                "83.6",
                "79.4",
                "75.4",
                "86",
                "80.4",
                "80.3",
                "71.9",
                "75.9",
                "78.5"
            ],
            [
                "Joint",
                "77.6",
                "84.5",
                "80.9",
                "76.1",
                "85.1",
                "80.3",
                "81",
                "73.8",
                "77.3",
                "[BOLD] 79.5"
            ]
        ],
        "question": "Is it true that Our joint model does not outperform all the base lines, with a gap of only 10.5 CoNLL F1 points from the last published results (KCP), and only surpassing our strong lemma baseline by 3 points?",
        "answer_label": "no"
    },
    {
        "id": "b3ea990d-54c0-4078-a9fc-75898eb8965c",
        "table_caption": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] ROUGE-1  [BOLD] R (%)",
            "[BOLD] ROUGE-1  [BOLD] P (%)",
            "[BOLD] ROUGE-1  [BOLD] F (%)",
            "[BOLD] ROUGE-2  [BOLD] R (%)",
            "[BOLD] ROUGE-2  [BOLD] P (%)",
            "[BOLD] ROUGE-2  [BOLD] F (%)",
            "[BOLD] Sentence-Level  [BOLD] R (%)",
            "[BOLD] Sentence-Level  [BOLD] P (%)",
            "[BOLD] Sentence-Level  [BOLD] F (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] ILP",
                "24.5",
                "41.1",
                "29.3\u00b10.5",
                "7.9",
                "15.0",
                "9.9\u00b10.5",
                "13.6",
                "22.6",
                "15.6\u00b10.4"
            ],
            [
                "[BOLD] Sum-Basic",
                "28.4",
                "44.4",
                "33.1\u00b10.5",
                "8.5",
                "15.6",
                "10.4\u00b10.4",
                "14.7",
                "22.9",
                "16.7\u00b10.5"
            ],
            [
                "[BOLD] KL-Sum",
                "39.5",
                "34.6",
                "35.5\u00b10.5",
                "13.0",
                "12.7",
                "12.3\u00b10.5",
                "15.2",
                "21.1",
                "16.3\u00b10.5"
            ],
            [
                "[BOLD] LexRank",
                "42.1",
                "39.5",
                "38.7\u00b10.5",
                "14.7",
                "15.3",
                "14.2\u00b10.5",
                "14.3",
                "21.5",
                "16.0\u00b10.5"
            ],
            [
                "[BOLD] MEAD",
                "45.5",
                "36.5",
                "38.5\u00b1 0.5",
                "17.9",
                "14.9",
                "15.4\u00b10.5",
                "27.8",
                "29.2",
                "26.8\u00b10.5"
            ],
            [
                "[BOLD] SVM",
                "19.0",
                "48.8",
                "24.7\u00b10.8",
                "7.5",
                "21.1",
                "10.0\u00b10.5",
                "32.7",
                "34.3",
                "31.4\u00b10.4"
            ],
            [
                "[BOLD] LogReg",
                "26.9",
                "34.5",
                "28.7\u00b10.6",
                "6.4",
                "9.9",
                "7.3\u00b10.4",
                "12.2",
                "14.9",
                "12.7\u00b10.5"
            ],
            [
                "[BOLD] LogReg [ITALIC] r",
                "28.0",
                "34.8",
                "29.4\u00b10.6",
                "6.9",
                "10.4",
                "7.8\u00b10.4",
                "12.1",
                "14.5",
                "12.5\u00b10.5"
            ],
            [
                "[BOLD] HAN",
                "31.0",
                "42.8",
                "33.7\u00b10.7",
                "11.2",
                "17.8",
                "12.7\u00b10.5",
                "26.9",
                "34.1",
                "32.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT",
                "32.2",
                "42.4",
                "34.4\u00b10.7",
                "11.5",
                "17.5",
                "12.9\u00b10.5",
                "29.6",
                "35.8",
                "32.2\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU",
                "32.1",
                "42.1",
                "33.8\u00b10.7",
                "11.6",
                "17.6",
                "12.9\u00b10.5",
                "30.1",
                "35.6",
                "32.3\u00b10.5"
            ],
            [
                "[BOLD] HAN [ITALIC] r",
                "38.1",
                "40.5",
                "[BOLD] 37.8\u00b10.5",
                "14.0",
                "17.1",
                "[BOLD] 14.7\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.5",
                "16.8",
                "[BOLD] 14.4\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.6",
                "16.9",
                "[BOLD] 14.4\u00b10.5",
                "33.9",
                "33.8",
                "[BOLD] 33.8\u00b10.5"
            ]
        ],
        "question": "Is it true that When redundancy removal was applied to LogReg, it produces only marginal improvement?",
        "answer_label": "yes"
    },
    {
        "id": "a4aa23f5-33df-4432-aa1a-31d016705823",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Acc",
            "[BOLD] F1",
            "[BOLD] AUC"
        ],
        "table_content_values": [
            [
                "Most Frequent Class",
                "64.2",
                "39.1",
                "0.500"
            ],
            [
                "Logistic Regression",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Sentiment \u2013 MPQA",
                "64.2",
                "39.1",
                "0.499"
            ],
            [
                "Sentiment \u2013 NRC",
                "63.9",
                "42.2",
                "0.599"
            ],
            [
                "Sentiment \u2013 V&B",
                "68.9",
                "60.0",
                "0.696"
            ],
            [
                "Sentiment \u2013 VADER",
                "66.0",
                "54.2",
                "0.654"
            ],
            [
                "Sentiment \u2013 Stanford",
                "68.0",
                "55.6",
                "0.696"
            ],
            [
                "Complaint Specific (all)",
                "65.7",
                "55.2",
                "0.634"
            ],
            [
                "Request",
                "64.2",
                "39.1",
                "0.583"
            ],
            [
                "Intensifiers",
                "64.5",
                "47.3",
                "0.639"
            ],
            [
                "Downgraders",
                "65.4",
                "49.8",
                "0.615"
            ],
            [
                "Temporal References",
                "64.2",
                "43.7",
                "0.535"
            ],
            [
                "Pronoun Types",
                "64.1",
                "39.1",
                "0.545"
            ],
            [
                "POS Bigrams",
                "72.2",
                "66.8",
                "0.756"
            ],
            [
                "LIWC",
                "71.6",
                "65.8",
                "0.784"
            ],
            [
                "Word2Vec Clusters",
                "67.7",
                "58.3",
                "0.738"
            ],
            [
                "Bag-of-Words",
                "79.8",
                "77.5",
                "0.866"
            ],
            [
                "All Features",
                "[BOLD] 80.5",
                "[BOLD] 78.0",
                "[BOLD] 0.873"
            ],
            [
                "Neural Networks",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "MLP",
                "78.3",
                "76.2",
                "0.845"
            ],
            [
                "LSTM",
                "80.2",
                "77.0",
                "0.864"
            ]
        ],
        "question": "Is it true that Syntactic part-ofspeech features alone obtain higher performance than any sentiment or complaint feature group, showing the syntactic patterns discussed in the previous section hold high predictive accuracy for the task?",
        "answer_label": "yes"
    },
    {
        "id": "83f16698-7b03-47a0-8716-3ab3a76eb741",
        "table_caption": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
        "table_column_names": [
            "Classifier",
            "Positive Sentiment Precision",
            "Positive Sentiment Recall",
            "Positive Sentiment Fscore"
        ],
        "table_content_values": [
            [
                "SVM-w/o neg.",
                "0.57",
                "0.72",
                "0.64"
            ],
            [
                "SVM-Punct. neg.",
                "0.58",
                "0.70",
                "0.63"
            ],
            [
                "SVM-our-neg.",
                "0.58",
                "0.73",
                "0.65"
            ],
            [
                "CNN",
                "0.63",
                "0.83",
                "0.72"
            ],
            [
                "CNN-LSTM",
                "0.71",
                "0.72",
                "0.72"
            ],
            [
                "CNN-LSTM-Our-neg-Ant",
                "[BOLD] 0.78",
                "[BOLD] 0.77",
                "[BOLD] 0.78"
            ],
            [
                "[EMPTY]",
                "Negative Sentiment",
                "Negative Sentiment",
                "Negative Sentiment"
            ],
            [
                "[EMPTY]",
                "Precision",
                "Recall",
                "Fscore"
            ],
            [
                "SVM-w/o neg.",
                "0.78",
                "0.86",
                "0.82"
            ],
            [
                "SVM-Punct. neg.",
                "0.78",
                "0.87",
                "0.83"
            ],
            [
                "SVM-Our neg.",
                "0.80",
                "0.87",
                "0.83"
            ],
            [
                "CNN",
                "0.88",
                "0.72",
                "0.79"
            ],
            [
                "CNN-LSTM.",
                "0.83",
                "0.83",
                "0.83"
            ],
            [
                "CNN-LSTM-our-neg-Ant",
                "[BOLD] 0.87",
                "[BOLD] 0.87",
                "[BOLD] 0.87"
            ],
            [
                "[EMPTY]",
                "Train",
                "[EMPTY]",
                "Test"
            ],
            [
                "Positive tweets",
                "5121",
                "[EMPTY]",
                "1320"
            ],
            [
                "Negative tweets",
                "9094",
                "[EMPTY]",
                "2244"
            ]
        ],
        "question": "Is it true that The proposed CNN-LSTMOur-neg-Ant improves upon the simple CNNLSTM-w/o neg?",
        "answer_label": "yes"
    },
    {
        "id": "ef59728f-b28b-4481-b3b7-3aac7d96ac9d",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is lower than Guo et al. (2019), a state-of-the-art model that does not employ external information?",
        "answer_label": "no"
    },
    {
        "id": "4c4e4ede-1cec-4827-b374-11f06872dac4",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Balanced",
            "Throughput (instances/s) Moderate",
            "Throughput (instances/s) Linear"
        ],
        "table_content_values": [
            [
                "1",
                "46.7",
                "27.3",
                "7.6"
            ],
            [
                "10",
                "125.2",
                "78.2",
                "22.7"
            ],
            [
                "25",
                "129.7",
                "83.1",
                "45.4"
            ]
        ],
        "question": "Is it true that [CONTINUE] As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees?",
        "answer_label": "yes"
    },
    {
        "id": "0206cb33-81d5-41f7-8a9b-65f54e67f75e",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
        "table_column_names": [
            "Model",
            "Method",
            "Training Data",
            "Overall",
            "Easy",
            "Hard",
            "p-value (%)"
        ],
        "table_content_values": [
            [
                "goodwin-etal-2012-utdhlt",
                "PMI",
                "unsupervised",
                "61.8",
                "64.7",
                "60.0",
                "19.8"
            ],
            [
                "gordon_commonsense_2011-1",
                "PMI",
                "unsupervised",
                "65.4",
                "65.8",
                "65.2",
                "83.5"
            ],
            [
                "sasaki-etal-2017-handling",
                "PMI",
                "unsupervised",
                "71.4",
                "75.3",
                "69.0",
                "4.8\u2217"
            ],
            [
                "Word frequency",
                "wordfreq",
                "COPA",
                "53.5",
                "57.4",
                "51.3",
                "9.8"
            ],
            [
                "BERT-large-FT",
                "LM, NSP",
                "COPA",
                "76.5 (\u00b1 2.7)",
                "83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)",
                "0.0\u2217"
            ],
            [
                "RoBERTa-large-FT",
                "LM",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)",
                "0.0\u2217"
            ]
        ],
        "question": "Is it true that However, BERT's improvements over previous work can be almost entirely attributed to high accuracy on the Easy subset: on this subset, finetuned BERT-large improves 8.6 percent over the model by (Sasaki et al., 2017) (83.9% vs. 75.3%), but on the Hard subset, the improvement is actually 4.8 percent (71.9% vs. 69.0%)?",
        "answer_label": "no"
    },
    {
        "id": "a24c8c0a-c398-4600-8503-17bec62989ed",
        "table_caption": "A context sensitive real-time Spell Checker with language adaptability TABLE II: Synthetic Data Performance results",
        "table_column_names": [
            "[BOLD] Language",
            "[BOLD] # Test",
            "[BOLD] P@1",
            "[BOLD] P@3",
            "[BOLD] P@5",
            "[BOLD] P@10",
            "[BOLD] MRR"
        ],
        "table_content_values": [
            [
                "[BOLD] Language",
                "[BOLD] Samples",
                "[BOLD] P@1",
                "[BOLD] P@3",
                "[BOLD] P@5",
                "[BOLD] P@10",
                "[BOLD] MRR"
            ],
            [
                "Bengali",
                "140000",
                "91.30",
                "97.83",
                "98.94",
                "99.65",
                "94.68"
            ],
            [
                "Czech",
                "94205",
                "95.84",
                "98.72",
                "99.26",
                "99.62",
                "97.37"
            ],
            [
                "Danish",
                "140000",
                "85.84",
                "95.19",
                "97.28",
                "98.83",
                "90.85"
            ],
            [
                "Dutch",
                "140000",
                "86.83",
                "95.01",
                "97.04",
                "98.68",
                "91.32"
            ],
            [
                "English",
                "140000",
                "97.08",
                "99.39",
                "99.67",
                "99.86",
                "98.27"
            ],
            [
                "Finnish",
                "140000",
                "97.77",
                "99.58",
                "99.79",
                "99.90",
                "98.69"
            ],
            [
                "French",
                "140000",
                "86.52",
                "95.66",
                "97.52",
                "98.83",
                "91.38"
            ],
            [
                "German",
                "140000",
                "87.58",
                "96.16",
                "97.86",
                "99.05",
                "92.10"
            ],
            [
                "Greek",
                "30022",
                "84.95",
                "94.99",
                "96.88",
                "98.44",
                "90.27"
            ],
            [
                "Hebrew",
                "132596",
                "94.00",
                "98.26",
                "99.05",
                "99.62",
                "96.24"
            ],
            [
                "Hindi",
                "140000",
                "82.19",
                "93.71",
                "96.28",
                "98.30",
                "88.40"
            ],
            [
                "Indonesian",
                "140000",
                "95.01",
                "98.98",
                "99.50",
                "99.84",
                "97.04"
            ],
            [
                "Italian",
                "140000",
                "89.93",
                "97.31",
                "98.54",
                "99.38",
                "93.76"
            ],
            [
                "Marathi",
                "140000",
                "93.01",
                "98.16",
                "99.06",
                "99.66",
                "95.69"
            ],
            [
                "Polish",
                "140000",
                "95.65",
                "99.17",
                "99.62",
                "99.86",
                "97.44"
            ],
            [
                "Portuguese",
                "140000",
                "86.73",
                "96.29",
                "97.94",
                "99.10",
                "91.74"
            ],
            [
                "Romanian",
                "140000",
                "95.52",
                "98.79",
                "99.32",
                "99.68",
                "97.22"
            ],
            [
                "Russian",
                "140000",
                "94.85",
                "98.74",
                "99.33",
                "99.71",
                "96.86"
            ],
            [
                "Spanish",
                "140000",
                "85.91",
                "95.35",
                "97.18",
                "98.57",
                "90.92"
            ],
            [
                "Swedish",
                "140000",
                "88.86",
                "96.40",
                "98.00",
                "99.14",
                "92.87"
            ],
            [
                "Tamil",
                "140000",
                "98.05",
                "99.70",
                "99.88",
                "99.98",
                "98.88"
            ],
            [
                "Telugu",
                "140000",
                "97.11",
                "99.68",
                "99.92",
                "99.99",
                "98.38"
            ],
            [
                "Thai",
                "12403",
                "98.73",
                "99.71",
                "99.78",
                "99.85",
                "99.22"
            ],
            [
                "Turkish",
                "140000",
                "97.13",
                "99.51",
                "99.78",
                "99.92",
                "98.33"
            ]
        ],
        "question": "Is it true that The system performs well on synthetic dataset with a minimum of 80% P@1 and 98% P@10?",
        "answer_label": "yes"
    },
    {
        "id": "aded4603-ff26-4698-8185-8b0236c4f926",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 3: Results on LDC2015E86 test set when models are trained with additional Gigaword data.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>External</bold>",
            "<bold>BLEU</bold>"
        ],
        "table_content_values": [
            [
                "Konstas et al. (2017)",
                "200K",
                "27.40"
            ],
            [
                "Song et al. (2018)",
                "200K",
                "28.20"
            ],
            [
                "Guo et al. (2019)",
                "200K",
                "31.60"
            ],
            [
                "G2S-GGNN",
                "200K",
                "<bold>32.23</bold>"
            ]
        ],
        "question": "Is it true that G2S-GGNN does not outperform others with the same amount of Gigaword sentences (200K), as shown in Table 3, with a BLEU score of 32.23?",
        "answer_label": "no"
    },
    {
        "id": "2f40f1d6-68bd-422d-8d22-55e0bef9a42c",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1192",
                "0.0083",
                "0.0137",
                "0.0150",
                "0.0150",
                "0.0445",
                "0.0326"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1022",
                "0.0069",
                "0.0060",
                "0.0092",
                "0.0090",
                "0.0356",
                "0.0162"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5710",
                "0.1948",
                "0.3855",
                "0.5474",
                "0.4485",
                "[BOLD] 0.8052",
                "0.4058"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "[BOLD] 0.6304",
                "0.1870",
                "0.3250",
                "0.5312",
                "0.4576",
                "0.6064",
                "0.3698"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0037",
                "0.3278",
                "0.5941",
                "0.6486",
                "[BOLD] 0.6490",
                "0.0017",
                "0.0003"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0002",
                "0.1486",
                "0.4332",
                "[BOLD] 0.6467",
                "0.6332",
                "0.0967",
                "0.0003"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0002",
                "0.1562",
                "0.5157",
                "[BOLD] 0.7255",
                "0.5932",
                "0.0032",
                "0.0001"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "2.10-5",
                "0.0507",
                "0.4492",
                "[BOLD] 0.7000",
                "0.5887",
                "0.1390",
                "0.0002"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0073",
                "0.0162",
                "0.0268",
                "[BOLD] 0.0293",
                "[BOLD] 0.0293",
                "0.0033",
                "0.0006"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0004",
                "0.0132",
                "0.0118",
                "0.0181",
                "0.0179",
                "[BOLD] 0.0520",
                "0.0005"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0005",
                "0.1733",
                "0.4412",
                "[BOLD] 0.6240",
                "0.5109",
                "0.0064",
                "0.0002"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "4.10-5",
                "0.0798",
                "0.3771",
                "[BOLD] 0.6040",
                "0.5149",
                "0.2261",
                "0.0004"
            ]
        ],
        "question": "Is it true that TF and DF achieved different values of precision, recall and f-measure using the English corpora, with TF achieving a higher precision (P=0.0150) and f-measure (F=0.0293) than DF when using the Europarl corpus in English?",
        "answer_label": "no"
    },
    {
        "id": "e0ea0f70-dd7a-44e6-af9c-6da48ffb8b11",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 3: Literature results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
        "table_column_names": [
            "[EMPTY]",
            "Acc",
            "Sim",
            "PP",
            "GM"
        ],
        "table_content_values": [
            [
                "M0: shen-1",
                "0.694",
                "0.728",
                "[BOLD] 22.3",
                "8.81"
            ],
            [
                "M1: M0 [ITALIC] +para",
                "0.702",
                "0.747",
                "23.6",
                "11.7"
            ],
            [
                "M2: M0 [ITALIC] +cyc",
                "0.692",
                "0.781",
                "49.9",
                "[BOLD] 12.8"
            ],
            [
                "M3: M0 [ITALIC] +cyc+lang",
                "0.698",
                "0.754",
                "39.2",
                "12.0"
            ],
            [
                "M4: M0 [ITALIC] +cyc+para",
                "0.702",
                "0.757",
                "33.9",
                "[BOLD] 12.8"
            ],
            [
                "M5: M0 [ITALIC] +cyc+para+lang",
                "0.688",
                "0.753",
                "28.6",
                "11.8"
            ],
            [
                "M6: M0 [ITALIC] +cyc+2d",
                "0.704",
                "[BOLD] 0.794",
                "63.2",
                "[BOLD] 12.8"
            ],
            [
                "M7: M6+ [ITALIC] para+lang",
                "0.706",
                "0.768",
                "49.0",
                "[BOLD] 12.8"
            ]
        ],
        "question": "Is it true that [CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss reduces PP, sometimes at a slight cost of semantic preservation?",
        "answer_label": "yes"
    },
    {
        "id": "81166961-1a5a-4aec-8f8b-554e7059779c",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that [CONTINUE] ACER and PPO obtain high performance in inform F1 and match rate as well?",
        "answer_label": "yes"
    },
    {
        "id": "b7aee9bb-1d0f-4611-9d2c-3e1f6f0b870f",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 2: Ablation study results.",
        "table_column_names": [
            "[BOLD] Variation",
            "[BOLD] Accuracy (%)",
            "[BOLD] \u0394%"
        ],
        "table_content_values": [
            [
                "Submitted",
                "[BOLD] 69.23",
                "-"
            ],
            [
                "No emoji",
                "68.36",
                "- 0.87"
            ],
            [
                "No ELMo",
                "65.52",
                "- 3.71"
            ],
            [
                "Concat Pooling",
                "68.47",
                "- 0.76"
            ],
            [
                "LSTM hidden=4096",
                "69.10",
                "- 0.13"
            ],
            [
                "LSTM hidden=1024",
                "68.93",
                "- 0.30"
            ],
            [
                "LSTM hidden=512",
                "68.43",
                "- 0.80"
            ],
            [
                "POS emb dim=100",
                "68.99",
                "- 0.24"
            ],
            [
                "POS emb dim=75",
                "68.61",
                "- 0.62"
            ],
            [
                "POS emb dim=50",
                "69.33",
                "+ 0.10"
            ],
            [
                "POS emb dim=25",
                "69.21",
                "- 0.02"
            ],
            [
                "SGD optim lr=1",
                "64.33",
                "- 4.90"
            ],
            [
                "SGD optim lr=0.1",
                "66.11",
                "- 3.12"
            ],
            [
                "SGD optim lr=0.01",
                "60.72",
                "- 8.51"
            ],
            [
                "SGD optim lr=0.001",
                "30.49",
                "- 38.74"
            ]
        ],
        "question": "Is it true that [CONTINUE] Using a greater BiLSTM hidden size did not help the model, [CONTINUE] We found that using 25-dimensional part-ofspeech embeddings slightly improved results, [CONTINUE] Regarding optimization strategies, we also tried using SGD with different learning rates and a stepwise learning rate schedule as described by Conneau et al?",
        "answer_label": "no"
    },
    {
        "id": "154f0d35-a7d3-4bc4-94c3-9f8a040f2fe9",
        "table_caption": "Semantic Neural Machine Translation using AMR Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.",
        "table_column_names": [
            "AMR Anno.",
            "BLEU"
        ],
        "table_content_values": [
            [
                "Automatic",
                "16.8"
            ],
            [
                "Gold",
                "[BOLD] *17.5*"
            ]
        ],
        "question": "Is it true that [CONTINUE] The improvement from automatic AMR to gold AMR (+0.7 BLEU) is significant, which shows that the translation quality of our model can be further improved with an increase of AMR parsing accuracy?",
        "answer_label": "yes"
    },
    {
        "id": "bd51e05d-94ea-4aaf-8572-7fff74309537",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that This empirically shows that compared to recurrent graph encoders, DCGCNs can learn better representations for graphs?",
        "answer_label": "yes"
    },
    {
        "id": "306fb06d-a734-46af-9d03-b1fd6bdb10a2",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that The performance of each approach that interacts with the agenda-based user simulator is shown in [CONTINUE] Table 3?",
        "answer_label": "yes"
    },
    {
        "id": "d48f4021-6614-4e16-96c9-f4fad3f39106",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
        "table_column_names": [
            "Topic Name",
            "Size",
            "TF-IDF ARI",
            "WMD ARI",
            "Sent2vec ARI",
            "Doc2vec ARI",
            "BERT ARI",
            "[ITALIC] OD-w2v ARI",
            "[ITALIC] OD-d2v ARI",
            "TF-IDF  [ITALIC] Sil.",
            "WMD  [ITALIC] Sil.",
            "Sent2vec  [ITALIC] Sil.",
            "Doc2vec  [ITALIC] Sil.",
            "BERT  [ITALIC] Sil.",
            "[ITALIC] OD-w2v  [ITALIC] Sil.",
            "[ITALIC] OD-d2v  [ITALIC] Sil."
        ],
        "table_content_values": [
            [
                "Affirmative Action",
                "81",
                "-0.07",
                "-0.02",
                "0.03",
                "-0.01",
                "-0.02",
                "[BOLD] 0.14",
                "[ITALIC] 0.02",
                "0.01",
                "0.01",
                "-0.01",
                "-0.02",
                "-0.04",
                "[BOLD] 0.06",
                "[ITALIC] 0.01"
            ],
            [
                "Atheism",
                "116",
                "[BOLD] 0.19",
                "0.07",
                "0.00",
                "0.03",
                "-0.01",
                "0.11",
                "[ITALIC] 0.16",
                "0.02",
                "0.01",
                "0.02",
                "0.01",
                "0.01",
                "[ITALIC] 0.05",
                "[BOLD] 0.07"
            ],
            [
                "Austerity Measures",
                "20",
                "[ITALIC] 0.04",
                "[ITALIC] 0.04",
                "-0.01",
                "-0.05",
                "0.04",
                "[BOLD] 0.21",
                "-0.01",
                "0.06",
                "0.07",
                "0.05",
                "-0.03",
                "0.10",
                "[BOLD] 0.19",
                "0.1"
            ],
            [
                "Democratization",
                "76",
                "0.02",
                "-0.01",
                "0.00",
                "[ITALIC] 0.09",
                "-0.01",
                "[BOLD] 0.11",
                "0.07",
                "0.01",
                "0.01",
                "0.02",
                "0.02",
                "0.03",
                "[BOLD] 0.16",
                "[ITALIC] 0.11"
            ],
            [
                "Education Voucher Scheme",
                "30",
                "[BOLD] 0.25",
                "0.12",
                "0.08",
                "-0.02",
                "0.04",
                "0.13",
                "[ITALIC] 0.19",
                "0.01",
                "0.01",
                "0.01",
                "-0.01",
                "0.02",
                "[ITALIC] 0.38",
                "[BOLD] 0.40"
            ],
            [
                "Gambling",
                "60",
                "-0.06",
                "-0.01",
                "-0.02",
                "0.04",
                "0.09",
                "[ITALIC] 0.35",
                "[BOLD] 0.39",
                "0.01",
                "0.02",
                "0.03",
                "0.01",
                "0.09",
                "[BOLD] 0.30",
                "[ITALIC] 0.22"
            ],
            [
                "Housing",
                "30",
                "0.01",
                "-0.01",
                "-0.01",
                "-0.02",
                "0.08",
                "[BOLD] 0.27",
                "0.01",
                "0.02",
                "0.03",
                "0.03",
                "0.01",
                "0.11",
                "[BOLD] 0.13",
                "[ITALIC] 0.13"
            ],
            [
                "Hydroelectric Dams",
                "110",
                "[BOLD] 0.47",
                "[ITALIC] 0.45",
                "[ITALIC] 0.45",
                "-0.01",
                "0.38",
                "0.35",
                "0.14",
                "0.04",
                "0.08",
                "0.12",
                "0.01",
                "0.19",
                "[BOLD] 0.26",
                "[ITALIC] 0.09"
            ],
            [
                "Intellectual Property",
                "66",
                "0.01",
                "0.01",
                "0.00",
                "0.03",
                "0.03",
                "[ITALIC] 0.05",
                "[BOLD] 0.14",
                "0.01",
                "[ITALIC] 0.04",
                "0.03",
                "0.01",
                "0.03",
                "[ITALIC] 0.04",
                "[BOLD] 0.12"
            ],
            [
                "Keystone pipeline",
                "18",
                "0.01",
                "0.01",
                "0.00",
                "-0.13",
                "[BOLD] 0.07",
                "-0.01",
                "[BOLD] 0.07",
                "-0.01",
                "-0.03",
                "-0.03",
                "-0.07",
                "0.03",
                "[BOLD] 0.05",
                "[ITALIC] 0.02"
            ],
            [
                "Monarchy",
                "61",
                "-0.04",
                "0.01",
                "0.00",
                "0.03",
                "-0.02",
                "[BOLD] 0.15",
                "[BOLD] 0.15",
                "0.01",
                "0.02",
                "0.02",
                "0.01",
                "0.01",
                "[BOLD] 0.11",
                "[ITALIC] 0.09"
            ],
            [
                "National Service",
                "33",
                "0.14",
                "-0.03",
                "-0.01",
                "0.02",
                "0.01",
                "[ITALIC] 0.31",
                "[BOLD] 0.39",
                "0.02",
                "0.04",
                "0.02",
                "0.01",
                "0.02",
                "[BOLD] 0.25",
                "[BOLD] 0.25"
            ],
            [
                "One-child policy China",
                "67",
                "-0.05",
                "0.01",
                "[BOLD] 0.11",
                "-0.02",
                "0.02",
                "[BOLD] 0.11",
                "0.01",
                "0.01",
                "0.02",
                "[ITALIC] 0.04",
                "-0.01",
                "0.03",
                "[BOLD] 0.07",
                "-0.02"
            ],
            [
                "Open-source Software",
                "48",
                "-0.02",
                "-0.01",
                "[ITALIC] 0.05",
                "0.01",
                "0.12",
                "[BOLD] 0.09",
                "-0.02",
                "0.01",
                "-0.01",
                "0.00",
                "-0.02",
                "0.03",
                "[BOLD] 0.18",
                "0.01"
            ],
            [
                "Pornography",
                "52",
                "-0.02",
                "0.01",
                "0.01",
                "-0.02",
                "-0.01",
                "[BOLD] 0.41",
                "[BOLD] 0.41",
                "0.01",
                "0.01",
                "0.02",
                "-0.01",
                "0.03",
                "[BOLD] 0.47",
                "[ITALIC] 0.41"
            ],
            [
                "Seanad Abolition",
                "25",
                "0.23",
                "0.09",
                "-0.01",
                "-0.01",
                "0.03",
                "[ITALIC] 0.32",
                "[BOLD] 0.54",
                "0.02",
                "0.01",
                "-0.01",
                "-0.03",
                "-0.04",
                "[ITALIC] 0.15",
                "[BOLD] 0.31"
            ],
            [
                "Trades Unions",
                "19",
                "[ITALIC] 0.44",
                "[ITALIC] 0.44",
                "[BOLD] 0.60",
                "-0.05",
                "0.44",
                "[ITALIC] 0.44",
                "0.29",
                "0.1",
                "0.17",
                "0.21",
                "0.01",
                "0.26",
                "[BOLD] 0.48",
                "[ITALIC] 0.32"
            ],
            [
                "Video Games",
                "72",
                "-0.01",
                "0.01",
                "0.12",
                "0.01",
                "0.08",
                "[ITALIC] 0.40",
                "[BOLD] 0.56",
                "0.01",
                "0.01",
                "0.06",
                "0.01",
                "0.05",
                "[ITALIC] 0.32",
                "[BOLD] 0.42"
            ],
            [
                "Average",
                "54.67",
                "0.09",
                "0.07",
                "0.08",
                "0.01",
                "0.08",
                "[BOLD] 0.22",
                "[ITALIC] 0.20",
                "0.02",
                "0.03",
                "0.04",
                "-0.01",
                "0.05",
                "[BOLD] 0.20",
                "[ITALIC] 0.17"
            ]
        ],
        "question": "Is it true that [CONTINUE] Opinion distance methods generally outperform the competition on both ARI and Silhouette coefficient?",
        "answer_label": "yes"
    },
    {
        "id": "59840d7f-df7c-4eb3-8f66-0f78d22aac52",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 4: Results of the ablation study on the LDC2017T10 development set.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>",
            "<bold>Size</bold>"
        ],
        "table_content_values": [
            [
                "biLSTM",
                "22.50",
                "30.42",
                "57.6M"
            ],
            [
                "<italic>GEt</italic> + biLSTM",
                "26.33",
                "32.62",
                "59.6M"
            ],
            [
                "<italic>GEb</italic> + biLSTM",
                "26.12",
                "32.49",
                "59.6M"
            ],
            [
                "<italic>GEt</italic> + <italic>GEb</italic> + biLSTM",
                "27.37",
                "33.30",
                "61.7M"
            ]
        ],
        "question": "Is it true that The complete model has significantly more parameters than the model without graph encoders (57.6M vs 61.7M)?",
        "answer_label": "no"
    },
    {
        "id": "e8525ca4-669e-4b02-829f-c5aeedc8e85f",
        "table_caption": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection Table 3: Relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair (content-content), either the reparandum or repair (content-function) or in neither. Percentages in parentheses show the fraction of tokens belong to each category.",
        "table_column_names": [
            "[BOLD] Type",
            "[BOLD] Reparandum Length  [BOLD] 1-2",
            "[BOLD] Reparandum Length  [BOLD] 3-5"
        ],
        "table_content_values": [
            [
                "content-content",
                "0.61 (30%)",
                "0.58 (52%)"
            ],
            [
                "content-function",
                "0.77 (20%)",
                "0.66 (17%)"
            ],
            [
                "function-function",
                "0.83 (50%)",
                "0.80 (32%)"
            ]
        ],
        "question": "Is it true that We found that rephrase disfluencies that contain content words are easier for the model to detect, compared to rephrases with function words only, and error decreases for longer disfluencies?",
        "answer_label": "no"
    },
    {
        "id": "3010d663-a981-41d4-8f6c-555026bb0257",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc\u2217: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.",
        "table_column_names": [
            "Model",
            "BLEU",
            "Acc\u2217"
        ],
        "table_content_values": [
            [
                "fu-1",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Multi-decoder",
                "7.6",
                "0.792"
            ],
            [
                "Style embed.",
                "15.4",
                "0.095"
            ],
            [
                "simple-transfer",
                "simple-transfer",
                "simple-transfer"
            ],
            [
                "Template",
                "18.0",
                "0.867"
            ],
            [
                "Delete/Retrieve",
                "12.6",
                "0.909"
            ],
            [
                "yang2018unsupervised",
                "yang2018unsupervised",
                "yang2018unsupervised"
            ],
            [
                "LM",
                "13.4",
                "0.854"
            ],
            [
                "LM + classifier",
                "[BOLD] 22.3",
                "0.900"
            ],
            [
                "Untransferred",
                "[BOLD] 31.4",
                "0.024"
            ]
        ],
        "question": "Is it true that We additionally find that supervised BLEU does not show a trade-off with Acc: for a single model type, higher Acc does not necessarily correspond to lower BLEU?",
        "answer_label": "no"
    },
    {
        "id": "8bc5a9aa-47c8-40c0-917e-6659a847af46",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Original",
                "TGen\u2212",
                "63.37",
                "7.7188",
                "41.99",
                "68.53",
                "1.9355",
                "00.06",
                "15.77",
                "00.11",
                "15.94"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen",
                "66.41",
                "8.5565",
                "45.07",
                "69.17",
                "2.2253",
                "00.14",
                "04.11",
                "00.03",
                "04.27"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen+",
                "67.06",
                "8.5871",
                "45.83",
                "69.73",
                "2.2681",
                "00.04",
                "01.75",
                "00.01",
                "01.80"
            ],
            [
                "Original",
                "[BOLD] Original",
                "SC-LSTM",
                "39.11",
                "5.6704",
                "36.83",
                "50.02",
                "0.6045",
                "02.79",
                "18.90",
                "09.79",
                "31.51"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen\u2212",
                "65.87",
                "8.6400",
                "44.20",
                "67.51",
                "2.1710",
                "00.20",
                "00.56",
                "00.21",
                "00.97"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen",
                "66.24",
                "8.6889",
                "44.66",
                "67.85",
                "2.2181",
                "00.10",
                "00.02",
                "00.00",
                "00.12"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen+",
                "65.97",
                "8.6630",
                "44.45",
                "67.59",
                "2.1855",
                "00.02",
                "00.00",
                "00.00",
                "00.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "SC-LSTM",
                "38.52",
                "5.7125",
                "37.45",
                "48.50",
                "0.4343",
                "03.85",
                "17.39",
                "08.12",
                "29.37"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen\u2212",
                "66.28",
                "8.5202",
                "43.96",
                "67.83",
                "2.1375",
                "00.14",
                "02.26",
                "00.22",
                "02.61"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen",
                "67.00",
                "8.6889",
                "44.97",
                "68.19",
                "2.2228",
                "00.06",
                "00.44",
                "00.03",
                "00.53"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen+",
                "66.74",
                "8.6649",
                "44.84",
                "67.95",
                "2.2018",
                "00.00",
                "00.21",
                "00.03",
                "00.24"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen\u2212",
                "64.40",
                "7.9692",
                "42.81",
                "68.87",
                "2.0563",
                "00.01",
                "13.08",
                "00.00",
                "13.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen",
                "66.23",
                "8.5578",
                "45.12",
                "68.87",
                "2.2548",
                "00.04",
                "03.04",
                "00.00",
                "03.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen+",
                "65.96",
                "8.5238",
                "45.49",
                "68.79",
                "2.2456",
                "00.00",
                "01.44",
                "00.00",
                "01.45"
            ]
        ],
        "question": "Is it true that In other words, [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions?",
        "answer_label": "yes"
    },
    {
        "id": "b6bc24e5-272f-4803-863d-d4d8ace3af9d",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE VI: Correlations for Word Similarity Tests",
        "table_column_names": [
            "Dataset (EN-)",
            "GloVe",
            "Word2Vec",
            "OIWE-IPG",
            "SOV",
            "SPINE",
            "Word2Sense",
            "Proposed"
        ],
        "table_content_values": [
            [
                "WS-353-ALL",
                "0.612",
                "0.7156",
                "0.634",
                "0.622",
                "0.173",
                "0.690",
                "0.657"
            ],
            [
                "SIMLEX-999",
                "0.359",
                "0.3939",
                "0.295",
                "0.355",
                "0.090",
                "0.380",
                "0.381"
            ],
            [
                "VERB-143",
                "0.326",
                "0.4430",
                "0.255",
                "0.271",
                "0.293",
                "0.271",
                "0.348"
            ],
            [
                "SimVerb-3500",
                "0.193",
                "0.2856",
                "0.184",
                "0.197",
                "0.035",
                "0.234",
                "0.245"
            ],
            [
                "WS-353-REL",
                "0.578",
                "0.6457",
                "0.595",
                "0.578",
                "0.134",
                "0.695",
                "0.619"
            ],
            [
                "RW-STANF.",
                "0.378",
                "0.4858",
                "0.316",
                "0.373",
                "0.122",
                "0.390",
                "0.382"
            ],
            [
                "YP-130",
                "0.524",
                "0.5211",
                "0.353",
                "0.482",
                "0.169",
                "0.420",
                "0.589"
            ],
            [
                "MEN-TR-3k",
                "0.710",
                "0.7528",
                "0.684",
                "0.696",
                "0.298",
                "0.769",
                "0.725"
            ],
            [
                "RG-65",
                "0.768",
                "0.8051",
                "0.736",
                "0.732",
                "0.338",
                "0.761",
                "0.774"
            ],
            [
                "MTurk-771",
                "0.650",
                "0.6712",
                "0.593",
                "0.623",
                "0.199",
                "0.665",
                "0.671"
            ],
            [
                "WS-353-SIM",
                "0.682",
                "0.7883",
                "0.713",
                "0.702",
                "0.220",
                "0.720",
                "0.720"
            ],
            [
                "MC-30",
                "0.749",
                "0.8112",
                "0.799",
                "0.726",
                "0.330",
                "0.735",
                "0.776"
            ],
            [
                "MTurk-287",
                "0.649",
                "0.6645",
                "0.591",
                "0.631",
                "0.295",
                "0.674",
                "0.634"
            ],
            [
                "Average",
                "0.552",
                "0.6141",
                "0.519",
                "0.538",
                "0.207",
                "0.570",
                "0.579"
            ]
        ],
        "question": "Is it true that We observe that, let alone a reduction in performance, the obtained scores indicate an almost uniform improvement in the correlation values for the proposed algorithm, outperforming all the alternatives except Word2Vec baseline on average?",
        "answer_label": "yes"
    },
    {
        "id": "e581bff7-189f-47c9-bc6e-ad38451ed188",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
        "table_column_names": [
            "[BOLD] Method",
            "[BOLD] R-1",
            "[BOLD] R-2",
            "[BOLD] R-SU"
        ],
        "table_content_values": [
            [
                "First-1",
                "26.83",
                "7.25",
                "6.46"
            ],
            [
                "First-2",
                "35.99",
                "10.17",
                "12.06"
            ],
            [
                "First-3",
                "39.41",
                "11.77",
                "14.51"
            ],
            [
                "LexRank Erkan and Radev ( 2004 )",
                "38.27",
                "12.70",
                "13.20"
            ],
            [
                "TextRank Mihalcea and Tarau ( 2004 )",
                "38.44",
                "13.10",
                "13.50"
            ],
            [
                "MMR Carbonell and Goldstein ( 1998 )",
                "38.77",
                "11.98",
                "12.91"
            ],
            [
                "PG-Original Lebanoff et\u00a0al. ( 2018 )",
                "41.85",
                "12.91",
                "16.46"
            ],
            [
                "PG-MMR Lebanoff et\u00a0al. ( 2018 )",
                "40.55",
                "12.36",
                "15.87"
            ],
            [
                "PG-BRNN Gehrmann et\u00a0al. ( 2018 )",
                "42.80",
                "14.19",
                "16.75"
            ],
            [
                "CopyTransformer Gehrmann et\u00a0al. ( 2018 )",
                "[BOLD] 43.57",
                "14.03",
                "17.37"
            ],
            [
                "Hi-MAP (Our Model)",
                "43.47",
                "[BOLD] 14.89",
                "[BOLD] 17.41"
            ]
        ],
        "question": "Is it true that We observe an improvement in performance between PG-original and PG-MMR (which takes the pre-trained PG-original and applies MMR on top of the model)?",
        "answer_label": "no"
    },
    {
        "id": "1a799142-09bf-4cb2-a6c1-1c85df1089a8",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 2: Experiment 1",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.001",
                "0.003",
                "-20.818",
                "***",
                "0.505"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.083",
                "0.048",
                "101.636",
                "***",
                "1.724"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.001",
                "0.001",
                "0.035",
                "[EMPTY]",
                "1.001"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.023",
                "0.012",
                "64.418",
                "***",
                "1.993"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.002",
                "0.001",
                "4.047",
                "***",
                "1.120"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.049",
                "0.019",
                "120.986",
                "***",
                "2.573"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.173",
                "0.065",
                "243.285",
                "***",
                "2.653"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.032",
                "0.023",
                "39.483",
                "***",
                "1.396"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.111",
                "0.061",
                "122.707",
                "***",
                "1.812"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.178",
                "0.080",
                "211.319",
                "***",
                "2.239"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.028",
                "0.015",
                "63.131",
                "***",
                "1.854"
            ]
        ],
        "question": "Is it true that Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus?",
        "answer_label": "yes"
    },
    {
        "id": "9df6dedc-23bf-468f-a0c5-7df7dd75c9e3",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 5: Performance of different agents on the neural user simulator.",
        "table_column_names": [
            "Method",
            "VHUS Turns",
            "VHUS Inform",
            "VHUS Match",
            "VHUS Success"
        ],
        "table_content_values": [
            [
                "ACER",
                "22.35",
                "55.13",
                "33.08",
                "18.6"
            ],
            [
                "PPO",
                "[BOLD] 19.23",
                "[BOLD] 56.31",
                "33.08",
                "18.3"
            ],
            [
                "ALDM",
                "26.90",
                "54.37",
                "24.15",
                "16.4"
            ],
            [
                "GDPL",
                "22.43",
                "52.58",
                "[BOLD] 36.21",
                "[BOLD] 19.7"
            ]
        ],
        "question": "Is it true that In comparison, GDPL is still comparable with ACER and PPO, but does not obtain a better match rate, and even achieves lower task success?",
        "answer_label": "no"
    },
    {
        "id": "a5addb02-2aad-498e-99b8-e2e4dd7cf0b0",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that For example, GCN+RC+LA (10) achieves a BLEU score of 52.9, which is better than GCN+RC+LA (9)?",
        "answer_label": "no"
    },
    {
        "id": "352ed084-1079-4116-b4ec-37b4d6ebe790",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
        "table_column_names": [
            "<bold>Model</bold>",
            "REF \u21d2 GEN <bold>ENT</bold>",
            "REF \u21d2 GEN <bold>CON</bold>",
            "REF \u21d2 GEN <bold>NEU</bold>"
        ],
        "table_content_values": [
            [
                "S2S",
                "38.45",
                "11.17",
                "50.38"
            ],
            [
                "G2S-GIN",
                "49.78",
                "9.80",
                "40.42"
            ],
            [
                "G2S-GAT",
                "49.48",
                "8.09",
                "42.43"
            ],
            [
                "G2S-GGNN",
                "51.32",
                "8.82",
                "39.86"
            ],
            [
                "[EMPTY]",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF"
            ],
            [
                "<bold>Model</bold>",
                "<bold>ENT</bold>",
                "<bold>CON</bold>",
                "<bold>NEU</bold>"
            ],
            [
                "S2S",
                "73.79",
                "12.75",
                "13.46"
            ],
            [
                "G2S-GIN",
                "76.27",
                "10.65",
                "13.08"
            ],
            [
                "G2S-GAT",
                "77.54",
                "8.54",
                "13.92"
            ],
            [
                "G2S-GGNN",
                "77.64",
                "9.64",
                "12.72"
            ]
        ],
        "question": "Is it true that All G2S models have lower entailment compared to S2S?",
        "answer_label": "no"
    },
    {
        "id": "20f5e50c-4725-4274-bdff-c605884d6206",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
        "table_column_names": [
            "[EMPTY]",
            "Prec.",
            "Rec.",
            "F1"
        ],
        "table_content_values": [
            [
                "(A1) BiLSTM-CNN",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "(A2) Standard attention",
                "0.466",
                "0.638",
                "0.539"
            ],
            [
                "(A3) Window size ( [ITALIC] ws)=5",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "(A4) Window size ( [ITALIC] ws)=10",
                "0.510",
                "0.640",
                "0.568"
            ],
            [
                "(A5) Softmax",
                "0.490",
                "0.658",
                "0.562"
            ],
            [
                "(A6) Max-pool",
                "0.492",
                "0.600",
                "0.541"
            ]
        ],
        "question": "Is it true that When we add multi-factor attention to the baseline BiLSTM-CNN model without the dependency distance-based weight factor in the attention mechanism, we get 0.4% F1 score decrease (A2\u2212A1)?",
        "answer_label": "no"
    },
    {
        "id": "4a18f8e5-71c1-4cf1-8638-bed82a865841",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
        "table_column_names": [
            "Model",
            "Method",
            "Training Data",
            "Overall",
            "Easy",
            "Hard",
            "p-value (%)"
        ],
        "table_content_values": [
            [
                "goodwin-etal-2012-utdhlt",
                "PMI",
                "unsupervised",
                "61.8",
                "64.7",
                "60.0",
                "19.8"
            ],
            [
                "gordon_commonsense_2011-1",
                "PMI",
                "unsupervised",
                "65.4",
                "65.8",
                "65.2",
                "83.5"
            ],
            [
                "sasaki-etal-2017-handling",
                "PMI",
                "unsupervised",
                "71.4",
                "75.3",
                "69.0",
                "4.8\u2217"
            ],
            [
                "Word frequency",
                "wordfreq",
                "COPA",
                "53.5",
                "57.4",
                "51.3",
                "9.8"
            ],
            [
                "BERT-large-FT",
                "LM, NSP",
                "COPA",
                "76.5 (\u00b1 2.7)",
                "83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)",
                "0.0\u2217"
            ],
            [
                "RoBERTa-large-FT",
                "LM",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)",
                "0.0\u2217"
            ]
        ],
        "question": "Is it true that The difference between accuracy on Easy and Hard is more pronounced for RoBERTa, suggesting a reliance on superficial cues?",
        "answer_label": "no"
    },
    {
        "id": "f245ef3f-7ca3-4a2e-9b70-5f42ca63476c",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 7: Comparisons of different DCGCN models under almost the same parameter budget.",
        "table_column_names": [
            "[BOLD] Model",
            "D",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN(1)",
                "300",
                "10.9M",
                "20.9",
                "52.0"
            ],
            [
                "DCGCN(2)",
                "180",
                "10.9M",
                "[BOLD] 22.2",
                "[BOLD] 52.3"
            ],
            [
                "DCGCN(2)",
                "240",
                "11.3M",
                "22.8",
                "52.8"
            ],
            [
                "DCGCN(4)",
                "180",
                "11.4M",
                "[BOLD] 23.4",
                "[BOLD] 53.4"
            ],
            [
                "DCGCN(1)",
                "420",
                "12.6M",
                "22.2",
                "52.4"
            ],
            [
                "DCGCN(2)",
                "300",
                "12.5M",
                "23.8",
                "53.8"
            ],
            [
                "DCGCN(3)",
                "240",
                "12.3M",
                "[BOLD] 23.9",
                "[BOLD] 54.1"
            ],
            [
                "DCGCN(2)",
                "360",
                "14.0M",
                "24.2",
                "[BOLD] 54.4"
            ],
            [
                "DCGCN(3)",
                "300",
                "14.0M",
                "[BOLD] 24.4",
                "54.2"
            ],
            [
                "DCGCN(2)",
                "420",
                "15.6M",
                "24.1",
                "53.7"
            ],
            [
                "DCGCN(4)",
                "300",
                "15.6M",
                "[BOLD] 24.6",
                "[BOLD] 54.8"
            ],
            [
                "DCGCN(3)",
                "420",
                "18.6M",
                "24.5",
                "54.6"
            ],
            [
                "DCGCN(4)",
                "360",
                "18.4M",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that In general, we found when the parameter budget is the same, shallower DCGCN models can obtain better results than the deeper ones?",
        "answer_label": "no"
    },
    {
        "id": "a5a4fd4a-41ee-417e-a619-d1398be5d04a",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Method",
            "SUBJ",
            "CR",
            "MR",
            "MPQA",
            "MRPC",
            "TREC",
            "SICK-E",
            "SST2",
            "SST5",
            "STS-B",
            "SICK-R"
        ],
        "table_content_values": [
            [
                "CBOW/784",
                "90.0",
                "[BOLD] 79.2",
                "[BOLD] 74.0",
                "87.1",
                "71.6",
                "85.6",
                "78.9",
                "78.5",
                "42.1",
                "61.0",
                "[BOLD] 78.1"
            ],
            [
                "CMOW/784",
                "87.5",
                "73.4",
                "70.6",
                "[BOLD] 87.3",
                "69.6",
                "[BOLD] 88.0",
                "77.2",
                "74.7",
                "37.9",
                "56.5",
                "76.2"
            ],
            [
                "Hybrid",
                "[BOLD] 90.2",
                "78.7",
                "73.7",
                "[BOLD] 87.3",
                "[BOLD] 72.7",
                "87.6",
                "[BOLD] 79.4",
                "[BOLD] 79.6",
                "[BOLD] 43.3",
                "[BOLD] 63.4",
                "77.8"
            ],
            [
                "cmp. CBOW",
                "+0.2%",
                "-0.6%",
                "-0.4%",
                "+0.2%",
                "+1.5%",
                "+2.3%",
                "+0.6%",
                "+1.4%",
                "+2.9%",
                "+3.9%",
                "-0.4%"
            ],
            [
                "cmp. CMOW",
                "+3.1%",
                "+7.2%",
                "+4.4%",
                "+0%",
                "+4.5%",
                "-0.5%",
                "+2.9%",
                "+6.7%",
                "+14.3",
                "+12.2%",
                "+2.1%"
            ]
        ],
        "question": "Is it true that Comparing the 784-dimensional models, again, CBOW and CMOW seem to complement each other?",
        "answer_label": "yes"
    },
    {
        "id": "ea7b772f-808f-4add-97cf-6a2b10dfe9a9",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that The hybrid model does not yield scores close to or even above the better model of the two on all tasks?",
        "answer_label": "no"
    },
    {
        "id": "74e89463-6301-450d-97e1-7cdefad17983",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that [CONTINUE] The performances of all models decrease as the diameters of the graphs increase?",
        "answer_label": "yes"
    },
    {
        "id": "c815f80d-4626-4775-bfff-d8b3630c274d",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
        "table_column_names": [
            "[EMPTY]",
            "Acc",
            "Sim",
            "PP",
            "GM"
        ],
        "table_content_values": [
            [
                "M0: shen-1",
                "0.818",
                "0.719",
                "37.3",
                "10.0"
            ],
            [
                "M1: M0 [ITALIC] +para",
                "0.819",
                "0.734",
                "26.3",
                "14.2"
            ],
            [
                "M2: M0 [ITALIC] +cyc",
                "0.813",
                "0.770",
                "36.4",
                "18.8"
            ],
            [
                "M3: M0 [ITALIC] +cyc+lang",
                "0.807",
                "0.796",
                "28.4",
                "21.5"
            ],
            [
                "M4: M0 [ITALIC] +cyc+para",
                "0.798",
                "0.783",
                "39.7",
                "19.2"
            ],
            [
                "M5: M0 [ITALIC] +cyc+para+lang",
                "0.804",
                "0.785",
                "27.1",
                "20.3"
            ],
            [
                "M6: M0 [ITALIC] +cyc+2d",
                "0.805",
                "[BOLD] 0.817",
                "43.3",
                "21.6"
            ],
            [
                "M7: M6+ [ITALIC] para+lang",
                "0.818",
                "0.805",
                "[BOLD] 29.0",
                "[BOLD] 22.8"
            ]
        ],
        "question": "Is it true that [CONTINUE] Table 2 shows that the model with paraphrase loss (M1) slightly improves Sim over M0 on both datasets under similar Acc?",
        "answer_label": "yes"
    },
    {
        "id": "8bea0857-41f9-4d7c-82b3-1bd70e74a1b4",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that The models have worse results when handling sentences with 20 or fewer tokens?",
        "answer_label": "no"
    },
    {
        "id": "023aa496-aafd-4f83-aa81-5c76da94e3e0",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that [CONTINUE] Due to joint training, our hybrid model learns to pick up the best features from CBOW and CMOW simultaneously?",
        "answer_label": "yes"
    },
    {
        "id": "52d13569-4080-411e-a922-9afd23f2b1b1",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1038",
                "0.0170",
                "0.0490",
                "0.0641",
                "0.0641",
                "0.0613",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1282",
                "0.0291",
                "0.0410",
                "0.0270",
                "0.0270",
                "0.1154",
                "0.0661"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.6185",
                "0.3744",
                "0.4144",
                "0.4394",
                "0.4394",
                "[BOLD] 0.7553",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.6308",
                "0.4124",
                "0.4404",
                "0.4515",
                "0.4945",
                "[BOLD] 0.8609",
                "0.5295"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "[BOLD] 0.0021",
                "0.0004",
                "0.0011",
                "0.0014",
                "0.0014",
                "0.0013",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0011",
                "0.0008",
                "0.0011",
                "0.0008",
                "0.0008",
                "[BOLD] 0.0030",
                "0.0018"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0012",
                "0.0008",
                "0.0009",
                "0.0010",
                "0.0010",
                "[BOLD] 0.0016",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0003",
                "0.0009",
                "0.0009",
                "0.0010",
                "0.0010",
                "[BOLD] 0.0017",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "[BOLD] 0.0041",
                "0.0007",
                "0.0021",
                "0.0027",
                "0.0027",
                "0.0026",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0022",
                "0.0016",
                "0.0022",
                "0.0015",
                "0.0015",
                "[BOLD] 0.0058",
                "0.0036"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0024",
                "0.0016",
                "0.0018",
                "0.0019",
                "0.0019",
                "[BOLD] 0.0031",
                "0.0023"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0005",
                "0.0018",
                "0.0018",
                "0.0020",
                "0.0021",
                "[BOLD] 0.0034",
                "0.0022"
            ]
        ],
        "question": "Is it true that Despite filtering out multiple hypernyms, the recall values for the Portuguese corpora are still relatively high?",
        "answer_label": "no"
    },
    {
        "id": "bc61ec91-f1d8-4416-9fdb-87ade3bbd1e8",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "Base ACC",
            "Base Time",
            "+LN ACC",
            "+LN Time",
            "+BERT ACC",
            "+BERT Time",
            "+LN+BERT ACC",
            "+LN+BERT Time"
        ],
        "table_content_values": [
            [
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "250K",
                "83.50",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-"
            ],
            [
                "This",
                "LSTM",
                "8.36M",
                "84.27",
                "0.262",
                "86.03",
                "0.432",
                "89.95",
                "0.544",
                "[BOLD] 90.49",
                "0.696"
            ],
            [
                "This",
                "GRU",
                "6.41M",
                "[BOLD] 85.71",
                "0.245",
                "[BOLD] 86.05",
                "0.419",
                "[BOLD] 90.29",
                "0.529",
                "90.10",
                "0.695"
            ],
            [
                "This",
                "ATR",
                "2.87M",
                "84.88",
                "0.210",
                "85.81",
                "0.307",
                "90.00",
                "0.494",
                "90.28",
                "0.580"
            ],
            [
                "Work",
                "SRU",
                "5.48M",
                "84.28",
                "0.258",
                "85.32",
                "0.283",
                "89.98",
                "0.543",
                "90.09",
                "0.555"
            ],
            [
                "[EMPTY]",
                "LRN",
                "4.25M",
                "84.88",
                "[BOLD] 0.209",
                "85.06",
                "[BOLD] 0.223",
                "89.98",
                "[BOLD] 0.488",
                "89.93",
                "[BOLD] 0.506"
            ]
        ],
        "question": "Is it true that LRN obtains additional 4 percentage points gain with BERT and reaches an accuracy of around 89.9?",
        "answer_label": "yes"
    },
    {
        "id": "b3f85fb2-384a-484e-9e52-d0bc2a0efac5",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Feature",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Feature",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams"
            ],
            [
                "not",
                ".154",
                "[URL]",
                ".150"
            ],
            [
                "my",
                ".131",
                "!",
                ".082"
            ],
            [
                "working",
                ".124",
                "he",
                ".069"
            ],
            [
                "still",
                ".123",
                "thank",
                ".067"
            ],
            [
                "on",
                ".119",
                ",",
                ".064"
            ],
            [
                "can\u2019t",
                ".113",
                "love",
                ".064"
            ],
            [
                "service",
                ".112",
                "lol",
                ".061"
            ],
            [
                "customer",
                ".109",
                "you",
                ".060"
            ],
            [
                "why",
                ".108",
                "great",
                ".058"
            ],
            [
                "website",
                ".107",
                "win",
                ".058"
            ],
            [
                "no",
                ".104",
                "\u2019",
                ".058"
            ],
            [
                "?",
                ".098",
                "she",
                ".054"
            ],
            [
                "fix",
                ".093",
                ":",
                ".053"
            ],
            [
                "won\u2019t",
                ".092",
                "that",
                ".053"
            ],
            [
                "been",
                ".090",
                "more",
                ".052"
            ],
            [
                "issue",
                ".089",
                "it",
                ".052"
            ],
            [
                "days",
                ".088",
                "would",
                ".051"
            ],
            [
                "error",
                ".087",
                "him",
                ".047"
            ],
            [
                "is",
                ".084",
                "life",
                ".046"
            ],
            [
                "charged",
                ".083",
                "good",
                ".046"
            ],
            [
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)"
            ],
            [
                "VBN",
                ".141",
                "UH",
                ".104"
            ],
            [
                "$",
                ".118",
                "NNP",
                ".098"
            ],
            [
                "VBZ",
                ".114",
                "PRP",
                ".076"
            ],
            [
                "NN_VBZ",
                ".114",
                "HT",
                ".076"
            ],
            [
                "PRP$",
                ".107",
                "PRP_.",
                ".076"
            ],
            [
                "PRP$_NN",
                ".105",
                "PRP_RB",
                ".067"
            ],
            [
                "VBG",
                ".093",
                "NNP_NNP",
                ".062"
            ],
            [
                "CD",
                ".092",
                "VBP_PRP",
                ".054"
            ],
            [
                "WRB_VBZ",
                ".084",
                "JJ",
                ".053"
            ],
            [
                "VBZ_VBN",
                ".084",
                "DT_JJ",
                ".051"
            ]
        ],
        "question": "Is it true that [CONTINUE] However, words and clusters expressing positive states such as gratitude (thank, great, love) or laughter (lol) are not significantly associated with tweets that are not complaints?",
        "answer_label": "no"
    },
    {
        "id": "1e1d2baf-44da-4531-9601-26027fdd859a",
        "table_caption": "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns Table 7: Performance of our baselines on the development set in the gold-two-mention task (access to the two candidate name spans). Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.",
        "table_column_names": [
            "[EMPTY]",
            "M",
            "F",
            "B",
            "O"
        ],
        "table_content_values": [
            [
                "Random",
                "47.5",
                "50.5",
                "[ITALIC] 1.06",
                "49.0"
            ],
            [
                "Token Distance",
                "50.6",
                "47.5",
                "[ITALIC] 0.94",
                "49.1"
            ],
            [
                "Topical Entity",
                "50.2",
                "47.3",
                "[ITALIC] 0.94",
                "48.8"
            ],
            [
                "Syntactic Distance",
                "66.7",
                "66.7",
                "[ITALIC]  [BOLD] 1.00",
                "66.7"
            ],
            [
                "Parallelism",
                "[BOLD] 69.3",
                "[BOLD] 69.2",
                "[ITALIC]  [BOLD] 1.00",
                "[BOLD] 69.2"
            ],
            [
                "Parallelism+URL",
                "[BOLD] 74.2",
                "[BOLD] 71.6",
                "[ITALIC]  [BOLD] 0.96",
                "[BOLD] 72.9"
            ],
            [
                "Transformer-Single",
                "59.6",
                "56.6",
                "[ITALIC] 0.95",
                "58.1"
            ],
            [
                "Transformer-Multi",
                "62.9",
                "61.7",
                "[ITALIC] 0.98",
                "62.3"
            ]
        ],
        "question": "Is it true that RANDOM is indeed closer here to the expected 50% and other baselines are closer to gender-parity?",
        "answer_label": "yes"
    },
    {
        "id": "18c4bae1-104d-4e43-bcbc-da4b289a9d62",
        "table_caption": "Assessing Gender Bias in Machine Translation \u2013 A Case Study with Google Translate Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table",
        "table_column_names": [
            "Category",
            "Female (%)",
            "Male (%)",
            "Neutral (%)"
        ],
        "table_content_values": [
            [
                "Office and administrative support",
                "11.015",
                "58.812",
                "16.954"
            ],
            [
                "Architecture and engineering",
                "2.299",
                "72.701",
                "10.92"
            ],
            [
                "Farming, fishing, and forestry",
                "12.179",
                "62.179",
                "14.744"
            ],
            [
                "Management",
                "11.232",
                "66.667",
                "12.681"
            ],
            [
                "Community and social service",
                "20.238",
                "62.5",
                "10.119"
            ],
            [
                "Healthcare support",
                "25.0",
                "43.75",
                "17.188"
            ],
            [
                "Sales and related",
                "8.929",
                "62.202",
                "16.964"
            ],
            [
                "Installation, maintenance, and repair",
                "5.22",
                "58.333",
                "17.125"
            ],
            [
                "Transportation and material moving",
                "8.81",
                "62.976",
                "17.5"
            ],
            [
                "Legal",
                "11.905",
                "72.619",
                "10.714"
            ],
            [
                "Business and financial operations",
                "7.065",
                "67.935",
                "15.58"
            ],
            [
                "Life, physical, and social science",
                "5.882",
                "73.284",
                "10.049"
            ],
            [
                "Arts, design, entertainment, sports, and media",
                "10.36",
                "67.342",
                "11.486"
            ],
            [
                "Education, training, and library",
                "23.485",
                "53.03",
                "9.091"
            ],
            [
                "Building and grounds cleaning and maintenance",
                "12.5",
                "68.333",
                "11.667"
            ],
            [
                "Personal care and service",
                "18.939",
                "49.747",
                "18.434"
            ],
            [
                "Healthcare practitioners and technical",
                "22.674",
                "51.744",
                "15.116"
            ],
            [
                "Production",
                "14.331",
                "51.199",
                "18.245"
            ],
            [
                "Computer and mathematical",
                "4.167",
                "66.146",
                "14.062"
            ],
            [
                "Construction and extraction",
                "8.578",
                "61.887",
                "17.525"
            ],
            [
                "Protective service",
                "8.631",
                "65.179",
                "12.5"
            ],
            [
                "Food preparation and serving related",
                "21.078",
                "58.333",
                "17.647"
            ],
            [
                "Total",
                "11.76",
                "58.93",
                "15.939"
            ]
        ],
        "question": "Is it true that What we have found is that Google Translate does indeed translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, in general?",
        "answer_label": "yes"
    },
    {
        "id": "edeea560-96b7-4321-b28d-aa8e670d56ca",
        "table_caption": "Deriving Machine Attention from Human Rationales Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.",
        "table_column_names": [
            "Source",
            "Target",
            "Svm",
            "Ra-Svm\u2021",
            "Ra-Cnn\u2021",
            "Trans\u2020",
            "Ra-Trans\u2021\u2020",
            "Ours\u2021\u2020",
            "Oracle\u2020"
        ],
        "table_content_values": [
            [
                "Beer aroma+palate",
                "Beer look",
                "74.41",
                "74.83",
                "74.94",
                "72.75",
                "76.41",
                "[BOLD] 79.53",
                "80.29"
            ],
            [
                "Beer look+palate",
                "Beer aroma",
                "68.57",
                "69.23",
                "67.55",
                "69.92",
                "76.45",
                "[BOLD] 77.94",
                "78.11"
            ],
            [
                "Beer look+aroma",
                "Beer palate",
                "63.88",
                "67.82",
                "65.72",
                "74.66",
                "73.40",
                "[BOLD] 75.24",
                "75.50"
            ]
        ],
        "question": "Is it true that We observe that the transfer baseline that directly uses rationale as augmented supervision (RA-TRANS) underperforms ORACLE by a large margin?",
        "answer_label": "yes"
    },
    {
        "id": "aa63576f-129c-4fdd-a6d2-2f7410459284",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
        "table_column_names": [
            "VS.",
            "Efficiency W",
            "Efficiency D",
            "Efficiency L",
            "Quality W",
            "Quality D",
            "Quality L",
            "Success W",
            "Success D",
            "Success L"
        ],
        "table_content_values": [
            [
                "ACER",
                "55",
                "25",
                "20",
                "44",
                "32",
                "24",
                "52",
                "30",
                "18"
            ],
            [
                "PPO",
                "74",
                "13",
                "13",
                "56",
                "26",
                "18",
                "59",
                "31",
                "10"
            ],
            [
                "ALDM",
                "69",
                "19",
                "12",
                "49",
                "25",
                "26",
                "61",
                "24",
                "15"
            ]
        ],
        "question": "Is it true that GDPL outperforms three baselines significantly in all aspects (sign test, p-value < 0.01) except for the quality compared with ACER?",
        "answer_label": "yes"
    },
    {
        "id": "c6f524cd-84be-41ec-9b1e-61695d024d5c",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
        "table_column_names": [
            "[EMPTY]",
            "WN-N P",
            "WN-N R",
            "WN-N F",
            "WN-V P",
            "WN-V R",
            "WN-V F",
            "VN P",
            "VN R",
            "VN F"
        ],
        "table_content_values": [
            [
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2"
            ],
            [
                "type",
                ".700",
                ".654",
                ".676",
                ".535",
                ".474",
                ".503",
                ".327",
                ".309",
                ".318"
            ],
            [
                "x+POS",
                ".699",
                ".651",
                ".674",
                ".544",
                ".472",
                ".505",
                ".339",
                ".312",
                ".325"
            ],
            [
                "lemma",
                ".706",
                ".660",
                ".682",
                ".576",
                ".520",
                ".547",
                ".384",
                ".360",
                ".371"
            ],
            [
                "x+POS",
                "<bold>.710</bold>",
                "<bold>.662</bold>",
                "<bold>.685</bold>",
                "<bold>.589</bold>",
                "<bold>.529</bold>",
                "<bold>.557</bold>",
                "<bold>.410</bold>",
                "<bold>.389</bold>",
                "<bold>.399</bold>"
            ],
            [
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep"
            ],
            [
                "type",
                ".712",
                ".661",
                ".686",
                ".545",
                ".457",
                ".497",
                ".324",
                ".296",
                ".310"
            ],
            [
                "x+POS",
                ".715",
                ".659",
                ".686",
                ".560",
                ".464",
                ".508",
                ".349",
                ".320",
                ".334"
            ],
            [
                "lemma",
                "<bold>.725</bold>",
                "<bold>.668</bold>",
                "<bold>.696</bold>",
                ".591",
                ".512",
                ".548",
                ".408",
                ".371",
                ".388"
            ],
            [
                "x+POS",
                ".722",
                ".666",
                ".693",
                "<bold>.609</bold>",
                "<bold>.527</bold>",
                "<bold>.565</bold>",
                "<bold>.412</bold>",
                "<bold>.381</bold>",
                "<bold>.396</bold>"
            ]
        ],
        "question": "Is it true that [CONTINUE] Lemma-based targets without POS disambiguation perform best on WN-N when dependency-based contexts are used; however, the difference to lemmatized and disambiguated targets is not statistically significant (p > .1)?",
        "answer_label": "yes"
    },
    {
        "id": "4301bd18-8256-4555-b14e-6fa07d64c262",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Balanced",
            "Throughput (instances/s) Moderate",
            "Throughput (instances/s) Linear"
        ],
        "table_content_values": [
            [
                "1",
                "46.7",
                "27.3",
                "7.6"
            ],
            [
                "10",
                "125.2",
                "78.2",
                "22.7"
            ],
            [
                "25",
                "129.7",
                "83.1",
                "45.4"
            ]
        ],
        "question": "Is it true that Another interesting fact in Table 1 is that the training throughput on the linear dataset does not scale better than the throughput on the balanced dataset, as the batch size increases?",
        "answer_label": "no"
    },
    {
        "id": "e2bf9a66-bb8d-4c47-b276-a33bcb67117c",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.",
        "table_column_names": [
            "GP-MBCM",
            "ACER",
            "PPO",
            "ALDM",
            "GDPL"
        ],
        "table_content_values": [
            [
                "1.666",
                "0.775",
                "0.639",
                "1.069",
                "[BOLD] 0.238"
            ]
        ],
        "question": "Is it true that Table 4 shows that GDPL has the smallest KL-divergence to the human on the number of dialog turns over the baselines, which implies that GDPL behaves more like the human?",
        "answer_label": "yes"
    },
    {
        "id": "298f453e-23a7-4de3-a6b1-be249400fc27",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that This suggests that enriching input graphs with the global node and including the linear combination can facilitate GCNs to learn better information aggregations, producing more expressive graph representations?",
        "answer_label": "yes"
    },
    {
        "id": "bbea621d-e96a-41a8-ad30-309fb0ef0a51",
        "table_caption": "Distant Learning for Entity Linking with Automatic Noise Detection Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
        "table_column_names": [
            "System",
            "All P",
            "All R",
            "All F1",
            "In  [ITALIC] E+ P",
            "In  [ITALIC] E+ R",
            "In  [ITALIC] E+ F1"
        ],
        "table_content_values": [
            [
                "Name matching",
                "15.03",
                "15.03",
                "15.03",
                "29.13",
                "29.13",
                "29.13"
            ],
            [
                "MIL (model 1)",
                "35.87",
                "35.87",
                "35.87 \u00b10.72",
                "69.38",
                "69.38",
                "69.38 \u00b11.29"
            ],
            [
                "MIL-ND (model 2)",
                "37.42",
                "[BOLD] 37.42",
                "37.42 \u00b10.35",
                "72.50",
                "[BOLD] 72.50",
                "[BOLD] 72.50 \u00b10.68"
            ],
            [
                "[ITALIC] \u03c4MIL-ND (model 2)",
                "[BOLD] 38.91",
                "36.73",
                "[BOLD] 37.78 \u00b10.26",
                "[BOLD] 73.19",
                "71.15",
                "72.16 \u00b10.48"
            ],
            [
                "Supervised learning",
                "42.90",
                "42.90",
                "42.90 \u00b10.59",
                "83.12",
                "83.12",
                "83.12 \u00b11.15"
            ]
        ],
        "question": "Is it true that MIL-ND does not achieve higher precision, recall, and F1 than MIL, and using its confidence at test time (\u03c4 MIL-ND, 'All' setting) was not beneficial in terms of precision and F1?",
        "answer_label": "no"
    },
    {
        "id": "086ef478-1afa-472e-b71f-ca7b784c40fb",
        "table_caption": "UKP TU-DA at GermEval 2017:Deep Learning for Aspect Based Sentiment Detection Table 5: Task B results",
        "table_column_names": [
            "[EMPTY]",
            "Micro F1"
        ],
        "table_content_values": [
            [
                "Baseline",
                "0.709"
            ],
            [
                "W2V (<italic>d</italic>=50)",
                "0.736"
            ],
            [
                "W2V (<italic>d</italic>=500)",
                "0.753"
            ],
            [
                "S2V",
                "0.748"
            ],
            [
                "S2V + W2V (<italic>d</italic>=50)",
                "0.744"
            ],
            [
                "S2V + K + W2V(<italic>d</italic>=50)",
                "0.749"
            ],
            [
                "SIF (DE)",
                "0.759"
            ],
            [
                "SIF (DE-EN)",
                "<bold>0.765</bold>"
            ]
        ],
        "question": "Is it true that For Task B, all models trained on the stacked learner beat the baseline substantially even when using only plain averaged word embeddings?",
        "answer_label": "yes"
    },
    {
        "id": "091c653e-d166-4f19-a914-0f0a73b1b51e",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE IX: Accuracies (%) for Sentiment Classification Task",
        "table_column_names": [
            "GloVe",
            "Word2Vec",
            "OIWE-IPG",
            "SOV",
            "SPINE",
            "Word2Sense",
            "Proposed"
        ],
        "table_content_values": [
            [
                "77.34",
                "77.91",
                "74.27",
                "78.43",
                "74.13",
                "81.21",
                "78.26"
            ]
        ],
        "question": "Is it true that Our proposed method outperforms Pretrained Word2Sense embeddings, despite the latter having the advantage of training on a larger corpus?",
        "answer_label": "no"
    },
    {
        "id": "036aebad-b965-470d-beea-378f90f6e122",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
        "table_column_names": [
            "Topic Name",
            "Size",
            "TF-IDF ARI",
            "WMD ARI",
            "Sent2vec ARI",
            "Doc2vec ARI",
            "BERT ARI",
            "[ITALIC] OD-w2v ARI",
            "[ITALIC] OD-d2v ARI",
            "TF-IDF  [ITALIC] Sil.",
            "WMD  [ITALIC] Sil.",
            "Sent2vec  [ITALIC] Sil.",
            "Doc2vec  [ITALIC] Sil.",
            "BERT  [ITALIC] Sil.",
            "[ITALIC] OD-w2v  [ITALIC] Sil.",
            "[ITALIC] OD-d2v  [ITALIC] Sil."
        ],
        "table_content_values": [
            [
                "Affirmative Action",
                "81",
                "-0.07",
                "-0.02",
                "0.03",
                "-0.01",
                "-0.02",
                "[BOLD] 0.14",
                "[ITALIC] 0.02",
                "0.01",
                "0.01",
                "-0.01",
                "-0.02",
                "-0.04",
                "[BOLD] 0.06",
                "[ITALIC] 0.01"
            ],
            [
                "Atheism",
                "116",
                "[BOLD] 0.19",
                "0.07",
                "0.00",
                "0.03",
                "-0.01",
                "0.11",
                "[ITALIC] 0.16",
                "0.02",
                "0.01",
                "0.02",
                "0.01",
                "0.01",
                "[ITALIC] 0.05",
                "[BOLD] 0.07"
            ],
            [
                "Austerity Measures",
                "20",
                "[ITALIC] 0.04",
                "[ITALIC] 0.04",
                "-0.01",
                "-0.05",
                "0.04",
                "[BOLD] 0.21",
                "-0.01",
                "0.06",
                "0.07",
                "0.05",
                "-0.03",
                "0.10",
                "[BOLD] 0.19",
                "0.1"
            ],
            [
                "Democratization",
                "76",
                "0.02",
                "-0.01",
                "0.00",
                "[ITALIC] 0.09",
                "-0.01",
                "[BOLD] 0.11",
                "0.07",
                "0.01",
                "0.01",
                "0.02",
                "0.02",
                "0.03",
                "[BOLD] 0.16",
                "[ITALIC] 0.11"
            ],
            [
                "Education Voucher Scheme",
                "30",
                "[BOLD] 0.25",
                "0.12",
                "0.08",
                "-0.02",
                "0.04",
                "0.13",
                "[ITALIC] 0.19",
                "0.01",
                "0.01",
                "0.01",
                "-0.01",
                "0.02",
                "[ITALIC] 0.38",
                "[BOLD] 0.40"
            ],
            [
                "Gambling",
                "60",
                "-0.06",
                "-0.01",
                "-0.02",
                "0.04",
                "0.09",
                "[ITALIC] 0.35",
                "[BOLD] 0.39",
                "0.01",
                "0.02",
                "0.03",
                "0.01",
                "0.09",
                "[BOLD] 0.30",
                "[ITALIC] 0.22"
            ],
            [
                "Housing",
                "30",
                "0.01",
                "-0.01",
                "-0.01",
                "-0.02",
                "0.08",
                "[BOLD] 0.27",
                "0.01",
                "0.02",
                "0.03",
                "0.03",
                "0.01",
                "0.11",
                "[BOLD] 0.13",
                "[ITALIC] 0.13"
            ],
            [
                "Hydroelectric Dams",
                "110",
                "[BOLD] 0.47",
                "[ITALIC] 0.45",
                "[ITALIC] 0.45",
                "-0.01",
                "0.38",
                "0.35",
                "0.14",
                "0.04",
                "0.08",
                "0.12",
                "0.01",
                "0.19",
                "[BOLD] 0.26",
                "[ITALIC] 0.09"
            ],
            [
                "Intellectual Property",
                "66",
                "0.01",
                "0.01",
                "0.00",
                "0.03",
                "0.03",
                "[ITALIC] 0.05",
                "[BOLD] 0.14",
                "0.01",
                "[ITALIC] 0.04",
                "0.03",
                "0.01",
                "0.03",
                "[ITALIC] 0.04",
                "[BOLD] 0.12"
            ],
            [
                "Keystone pipeline",
                "18",
                "0.01",
                "0.01",
                "0.00",
                "-0.13",
                "[BOLD] 0.07",
                "-0.01",
                "[BOLD] 0.07",
                "-0.01",
                "-0.03",
                "-0.03",
                "-0.07",
                "0.03",
                "[BOLD] 0.05",
                "[ITALIC] 0.02"
            ],
            [
                "Monarchy",
                "61",
                "-0.04",
                "0.01",
                "0.00",
                "0.03",
                "-0.02",
                "[BOLD] 0.15",
                "[BOLD] 0.15",
                "0.01",
                "0.02",
                "0.02",
                "0.01",
                "0.01",
                "[BOLD] 0.11",
                "[ITALIC] 0.09"
            ],
            [
                "National Service",
                "33",
                "0.14",
                "-0.03",
                "-0.01",
                "0.02",
                "0.01",
                "[ITALIC] 0.31",
                "[BOLD] 0.39",
                "0.02",
                "0.04",
                "0.02",
                "0.01",
                "0.02",
                "[BOLD] 0.25",
                "[BOLD] 0.25"
            ],
            [
                "One-child policy China",
                "67",
                "-0.05",
                "0.01",
                "[BOLD] 0.11",
                "-0.02",
                "0.02",
                "[BOLD] 0.11",
                "0.01",
                "0.01",
                "0.02",
                "[ITALIC] 0.04",
                "-0.01",
                "0.03",
                "[BOLD] 0.07",
                "-0.02"
            ],
            [
                "Open-source Software",
                "48",
                "-0.02",
                "-0.01",
                "[ITALIC] 0.05",
                "0.01",
                "0.12",
                "[BOLD] 0.09",
                "-0.02",
                "0.01",
                "-0.01",
                "0.00",
                "-0.02",
                "0.03",
                "[BOLD] 0.18",
                "0.01"
            ],
            [
                "Pornography",
                "52",
                "-0.02",
                "0.01",
                "0.01",
                "-0.02",
                "-0.01",
                "[BOLD] 0.41",
                "[BOLD] 0.41",
                "0.01",
                "0.01",
                "0.02",
                "-0.01",
                "0.03",
                "[BOLD] 0.47",
                "[ITALIC] 0.41"
            ],
            [
                "Seanad Abolition",
                "25",
                "0.23",
                "0.09",
                "-0.01",
                "-0.01",
                "0.03",
                "[ITALIC] 0.32",
                "[BOLD] 0.54",
                "0.02",
                "0.01",
                "-0.01",
                "-0.03",
                "-0.04",
                "[ITALIC] 0.15",
                "[BOLD] 0.31"
            ],
            [
                "Trades Unions",
                "19",
                "[ITALIC] 0.44",
                "[ITALIC] 0.44",
                "[BOLD] 0.60",
                "-0.05",
                "0.44",
                "[ITALIC] 0.44",
                "0.29",
                "0.1",
                "0.17",
                "0.21",
                "0.01",
                "0.26",
                "[BOLD] 0.48",
                "[ITALIC] 0.32"
            ],
            [
                "Video Games",
                "72",
                "-0.01",
                "0.01",
                "0.12",
                "0.01",
                "0.08",
                "[ITALIC] 0.40",
                "[BOLD] 0.56",
                "0.01",
                "0.01",
                "0.06",
                "0.01",
                "0.05",
                "[ITALIC] 0.32",
                "[BOLD] 0.42"
            ],
            [
                "Average",
                "54.67",
                "0.09",
                "0.07",
                "0.08",
                "0.01",
                "0.08",
                "[BOLD] 0.22",
                "[ITALIC] 0.20",
                "0.02",
                "0.03",
                "0.04",
                "-0.01",
                "0.05",
                "[BOLD] 0.20",
                "[ITALIC] 0.17"
            ]
        ],
        "question": "Is it true that The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005?",
        "answer_label": "yes"
    },
    {
        "id": "a48fe54a-935d-4004-97f5-8dbf683c3567",
        "table_caption": "Argument Generation with Retrieval, Planning, and Realization Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
        "table_column_names": [
            "[EMPTY]",
            "[ITALIC] w/ System Retrieval  [BOLD] B-2",
            "[ITALIC] w/ System Retrieval  [BOLD] B-4",
            "[ITALIC] w/ System Retrieval  [BOLD] R-2",
            "[ITALIC] w/ System Retrieval  [BOLD] MTR",
            "[ITALIC] w/ System Retrieval  [BOLD] #Word",
            "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
        ],
        "table_content_values": [
            [
                "Human",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22"
            ],
            [
                "Retrieval",
                "7.55",
                "1.11",
                "8.64",
                "14.38",
                "123",
                "23",
                "10.97",
                "3.05",
                "23.49",
                "20.08",
                "140",
                "21"
            ],
            [
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Seq2seq",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15"
            ],
            [
                "Seq2seqAug",
                "8.26",
                "2.24",
                "13.79",
                "15.75",
                "78",
                "14",
                "10.98",
                "4.41",
                "22.97",
                "19.62",
                "71",
                "14"
            ],
            [
                "[ITALIC] w/o psg",
                "7.94",
                "2.28",
                "10.13",
                "15.71",
                "75",
                "12",
                "9.89",
                "3.34",
                "14.20",
                "18.40",
                "66",
                "12"
            ],
            [
                "H&W\u00a0Hua and Wang ( 2018 )",
                "3.64",
                "0.92",
                "8.83",
                "11.78",
                "51",
                "12",
                "8.51",
                "2.86",
                "18.89",
                "17.18",
                "58",
                "12"
            ],
            [
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "CANDELA",
                "12.02\u2217",
                "[BOLD] 2.99\u2217",
                "[BOLD] 14.93\u2217",
                "[BOLD] 16.92\u2217",
                "119",
                "22",
                "15.80\u2217",
                "[BOLD] 5.00\u2217",
                "[BOLD] 23.75",
                "[BOLD] 20.18",
                "116",
                "22"
            ],
            [
                "[ITALIC] w/o psg",
                "[BOLD] 12.33\u2217",
                "2.86\u2217",
                "14.53\u2217",
                "16.60\u2217",
                "123",
                "23",
                "[BOLD] 16.33\u2217",
                "4.98\u2217",
                "23.65",
                "19.94",
                "123",
                "23"
            ]
        ],
        "question": "Is it true that [CONTINUE] Under system setup, our model CANDELA statistically significantly outperforms all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < [CONTINUE] .0005)?",
        "answer_label": "yes"
    },
    {
        "id": "3197a3a2-dcb8-4bbb-8be5-a2e8eee69365",
        "table_caption": "Syntactic Dependency Representations in Neural Relation Classification Table 1: Effect of using the shortest dependency path on each relation type.",
        "table_column_names": [
            "[BOLD] Relation",
            "[BOLD] best F1 (in 5-fold) without sdp",
            "[BOLD] best F1 (in 5-fold) with sdp",
            "[BOLD] Diff."
        ],
        "table_content_values": [
            [
                "USAGE",
                "60.34",
                "80.24",
                "+ 19.90"
            ],
            [
                "MODEL-FEATURE",
                "48.89",
                "70.00",
                "+ 21.11"
            ],
            [
                "PART_WHOLE",
                "29.51",
                "70.27",
                "+40.76"
            ],
            [
                "TOPIC",
                "45.80",
                "91.26",
                "+45.46"
            ],
            [
                "RESULT",
                "54.35",
                "81.58",
                "+27.23"
            ],
            [
                "COMPARE",
                "20.00",
                "61.82",
                "+ 41.82"
            ],
            [
                "macro-averaged",
                "50.10",
                "76.10",
                "+26.00"
            ]
        ],
        "question": "Is it true that However, the sdp information has a clear positive impact on all the relation types (Table 1)?",
        "answer_label": "yes"
    },
    {
        "id": "a8be9400-0253-4cda-ad4a-06b707c381b5",
        "table_caption": "Neural End-to-End Learning for Computational Argumentation Mining Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by \u201c:\u201d. Layers from which tasks feed are indicated by respective numbers.",
        "table_column_names": [
            "[EMPTY]",
            "C-F1 100%",
            "C-F1 50%",
            "R-F1 100%",
            "R-F1 50%",
            "F1 100%",
            "F1 50%"
        ],
        "table_content_values": [
            [
                "Y-3",
                "49.59",
                "65.37",
                "26.28",
                "37.00",
                "34.35",
                "47.25"
            ],
            [
                "Y-3:Y<italic>C</italic>-1",
                "54.71",
                "66.84",
                "28.44",
                "37.35",
                "37.40",
                "47.92"
            ],
            [
                "Y-3:Y<italic>R</italic>-1",
                "51.32",
                "66.49",
                "26.92",
                "37.18",
                "35.31",
                "47.69"
            ],
            [
                "Y-3:Y<italic>C</italic>-3",
                "<bold>54.58</bold>",
                "67.66",
                "<bold>30.22</bold>",
                "<bold>40.30</bold>",
                "<bold>38.90</bold>",
                "<bold>50.51</bold>"
            ],
            [
                "Y-3:Y<italic>R</italic>-3",
                "53.31",
                "66.71",
                "26.65",
                "35.86",
                "35.53",
                "46.64"
            ],
            [
                "Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2",
                "52.95",
                "<bold>67.84</bold>",
                "27.90",
                "39.71",
                "36.54",
                "50.09"
            ],
            [
                "Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3",
                "54.55",
                "67.60",
                "28.30",
                "38.26",
                "37.26",
                "48.86"
            ]
        ],
        "question": "Is it true that Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally stronger: [CONTINUE] as in Eq?",
        "answer_label": "no"
    },
    {
        "id": "9e0ce9f6-6eb5-4eda-97f7-ec6948db9bf5",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 8: Fraction of elements in the output that are not present in the input (ADDED) and the fraction of elements in the input graph that are missing in the generated sentence (MISS), for the test set of LDC2017T10. The token lemmas are used in the comparison. GOLD refers to the reference sentences.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>ADDED</bold>",
            "<bold>MISS</bold>"
        ],
        "table_content_values": [
            [
                "S2S",
                "47.34",
                "37.14"
            ],
            [
                "G2S-GIN",
                "48.67",
                "33.64"
            ],
            [
                "G2S-GAT",
                "48.24",
                "33.73"
            ],
            [
                "G2S-GGNN",
                "48.66",
                "34.06"
            ],
            [
                "GOLD",
                "50.77",
                "28.35"
            ],
            [
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ]
        ],
        "question": "Is it true that As shown in Table 8, the S2S baseline outperforms the G2S approaches?",
        "answer_label": "no"
    },
    {
        "id": "bfb26805-c8f6-4854-9e1c-14b7534b396e",
        "table_caption": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] ROUGE-1  [BOLD] R (%)",
            "[BOLD] ROUGE-1  [BOLD] P (%)",
            "[BOLD] ROUGE-1  [BOLD] F (%)",
            "[BOLD] ROUGE-2  [BOLD] R (%)",
            "[BOLD] ROUGE-2  [BOLD] P (%)",
            "[BOLD] ROUGE-2  [BOLD] F (%)",
            "[BOLD] Sentence-Level  [BOLD] R (%)",
            "[BOLD] Sentence-Level  [BOLD] P (%)",
            "[BOLD] Sentence-Level  [BOLD] F (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] ILP",
                "24.5",
                "41.1",
                "29.3\u00b10.5",
                "7.9",
                "15.0",
                "9.9\u00b10.5",
                "13.6",
                "22.6",
                "15.6\u00b10.4"
            ],
            [
                "[BOLD] Sum-Basic",
                "28.4",
                "44.4",
                "33.1\u00b10.5",
                "8.5",
                "15.6",
                "10.4\u00b10.4",
                "14.7",
                "22.9",
                "16.7\u00b10.5"
            ],
            [
                "[BOLD] KL-Sum",
                "39.5",
                "34.6",
                "35.5\u00b10.5",
                "13.0",
                "12.7",
                "12.3\u00b10.5",
                "15.2",
                "21.1",
                "16.3\u00b10.5"
            ],
            [
                "[BOLD] LexRank",
                "42.1",
                "39.5",
                "38.7\u00b10.5",
                "14.7",
                "15.3",
                "14.2\u00b10.5",
                "14.3",
                "21.5",
                "16.0\u00b10.5"
            ],
            [
                "[BOLD] MEAD",
                "45.5",
                "36.5",
                "38.5\u00b1 0.5",
                "17.9",
                "14.9",
                "15.4\u00b10.5",
                "27.8",
                "29.2",
                "26.8\u00b10.5"
            ],
            [
                "[BOLD] SVM",
                "19.0",
                "48.8",
                "24.7\u00b10.8",
                "7.5",
                "21.1",
                "10.0\u00b10.5",
                "32.7",
                "34.3",
                "31.4\u00b10.4"
            ],
            [
                "[BOLD] LogReg",
                "26.9",
                "34.5",
                "28.7\u00b10.6",
                "6.4",
                "9.9",
                "7.3\u00b10.4",
                "12.2",
                "14.9",
                "12.7\u00b10.5"
            ],
            [
                "[BOLD] LogReg [ITALIC] r",
                "28.0",
                "34.8",
                "29.4\u00b10.6",
                "6.9",
                "10.4",
                "7.8\u00b10.4",
                "12.1",
                "14.5",
                "12.5\u00b10.5"
            ],
            [
                "[BOLD] HAN",
                "31.0",
                "42.8",
                "33.7\u00b10.7",
                "11.2",
                "17.8",
                "12.7\u00b10.5",
                "26.9",
                "34.1",
                "32.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT",
                "32.2",
                "42.4",
                "34.4\u00b10.7",
                "11.5",
                "17.5",
                "12.9\u00b10.5",
                "29.6",
                "35.8",
                "32.2\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU",
                "32.1",
                "42.1",
                "33.8\u00b10.7",
                "11.6",
                "17.6",
                "12.9\u00b10.5",
                "30.1",
                "35.6",
                "32.3\u00b10.5"
            ],
            [
                "[BOLD] HAN [ITALIC] r",
                "38.1",
                "40.5",
                "[BOLD] 37.8\u00b10.5",
                "14.0",
                "17.1",
                "[BOLD] 14.7\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.5",
                "16.8",
                "[BOLD] 14.4\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.6",
                "16.9",
                "[BOLD] 14.4\u00b10.5",
                "33.9",
                "33.8",
                "[BOLD] 33.8\u00b10.5"
            ]
        ],
        "question": "Is it true that We suspect that there are not enough data to pretrain the models and that the thread classification task used to pretrain the HAN models may not be sophisticated enough to learn effective thread vectors?",
        "answer_label": "yes"
    },
    {
        "id": "87943fea-34a2-4928-b643-cc2d1a2e32fd",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that We also have competitive results to Guo et al?",
        "answer_label": "yes"
    },
    {
        "id": "64086c42-77ce-41fd-8af0-b264671ca83c",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that [CONTINUE] When comparing DF model which takes into account only the number of documents that the word occurs, with DocSub which considers the number of shared documents between two words, DocSub achieved better values of precision, but lower values of recall?",
        "answer_label": "yes"
    },
    {
        "id": "e7aa3eec-8b70-4c8f-820e-a6c49ccaf595",
        "table_caption": "Sparse and Structured Visual Attention Table 2: Human evaluation results on MSCOCO.",
        "table_column_names": [
            "[EMPTY]",
            "caption",
            "attention relevance"
        ],
        "table_content_values": [
            [
                "softmax",
                "3.50",
                "3.38"
            ],
            [
                "sparsemax",
                "3.71",
                "3.89"
            ],
            [
                "TVmax",
                "[BOLD] 3.87",
                "[BOLD] 4.10"
            ]
        ],
        "question": "Is it true that Despite performing slightly worse than sparsemax under automatic metrics, TVMAX outperforms sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation, reported in Table 2?",
        "answer_label": "yes"
    },
    {
        "id": "56156f9c-7163-4650-9091-2240dfc00b55",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
        "table_column_names": [
            "Model",
            "#Params",
            "BLEU",
            "Train",
            "Decode"
        ],
        "table_content_values": [
            [
                "GNMT",
                "-",
                "24.61",
                "-",
                "-"
            ],
            [
                "GRU",
                "206M",
                "26.28",
                "2.67",
                "45.35"
            ],
            [
                "ATR",
                "122M",
                "25.70",
                "1.33",
                "[BOLD] 34.40"
            ],
            [
                "SRU",
                "170M",
                "25.91",
                "1.34",
                "42.84"
            ],
            [
                "LRN",
                "143M",
                "26.26",
                "[BOLD] 0.99",
                "36.50"
            ],
            [
                "oLRN",
                "164M",
                "[BOLD] 26.73",
                "1.15",
                "40.19"
            ]
        ],
        "question": "Is it true that The results in Table 3 show that translation quality of LRN is slightly worse than that of GRU (-0.02 BLEU)?",
        "answer_label": "yes"
    },
    {
        "id": "1af207f8-66bc-4bfc-b6a3-c651e12783f0",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that Without the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores?",
        "answer_label": "yes"
    },
    {
        "id": "d1da902e-3754-4ed7-afcf-cc1642e09b62",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Acc",
            "[BOLD] F1",
            "[BOLD] AUC"
        ],
        "table_content_values": [
            [
                "Most Frequent Class",
                "64.2",
                "39.1",
                "0.500"
            ],
            [
                "Logistic Regression",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Sentiment \u2013 MPQA",
                "64.2",
                "39.1",
                "0.499"
            ],
            [
                "Sentiment \u2013 NRC",
                "63.9",
                "42.2",
                "0.599"
            ],
            [
                "Sentiment \u2013 V&B",
                "68.9",
                "60.0",
                "0.696"
            ],
            [
                "Sentiment \u2013 VADER",
                "66.0",
                "54.2",
                "0.654"
            ],
            [
                "Sentiment \u2013 Stanford",
                "68.0",
                "55.6",
                "0.696"
            ],
            [
                "Complaint Specific (all)",
                "65.7",
                "55.2",
                "0.634"
            ],
            [
                "Request",
                "64.2",
                "39.1",
                "0.583"
            ],
            [
                "Intensifiers",
                "64.5",
                "47.3",
                "0.639"
            ],
            [
                "Downgraders",
                "65.4",
                "49.8",
                "0.615"
            ],
            [
                "Temporal References",
                "64.2",
                "43.7",
                "0.535"
            ],
            [
                "Pronoun Types",
                "64.1",
                "39.1",
                "0.545"
            ],
            [
                "POS Bigrams",
                "72.2",
                "66.8",
                "0.756"
            ],
            [
                "LIWC",
                "71.6",
                "65.8",
                "0.784"
            ],
            [
                "Word2Vec Clusters",
                "67.7",
                "58.3",
                "0.738"
            ],
            [
                "Bag-of-Words",
                "79.8",
                "77.5",
                "0.866"
            ],
            [
                "All Features",
                "[BOLD] 80.5",
                "[BOLD] 78.0",
                "[BOLD] 0.873"
            ],
            [
                "Neural Networks",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "MLP",
                "78.3",
                "76.2",
                "0.845"
            ],
            [
                "LSTM",
                "80.2",
                "77.0",
                "0.864"
            ]
        ],
        "question": "Is it true that However, models trained using linguistic features on the training data obtain significantly higher predictive accuracy?",
        "answer_label": "yes"
    },
    {
        "id": "4c5f3d6e-0a82-4654-8825-545bffd70dd2",
        "table_caption": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 2: F1 score results per relation type of the best performing models.",
        "table_column_names": [
            "Relation type",
            "Count",
            "Intra-sentential co-occ.  [ITALIC] \u03c1=0",
            "Intra-sentential co-occ.  [ITALIC] \u03c1=5",
            "Intra-sentential co-occ.  [ITALIC] \u03c1=10",
            "BoC(Wiki-PubMed-PMC) LR",
            "BoC(Wiki-PubMed-PMC) SVM",
            "BoC(Wiki-PubMed-PMC) ANN"
        ],
        "table_content_values": [
            [
                "TherapyTiming(TP,TD)",
                "428",
                "[BOLD] 0.84",
                "0.59",
                "0.47",
                "0.78",
                "0.81",
                "0.78"
            ],
            [
                "NextReview(Followup,TP)",
                "164",
                "[BOLD] 0.90",
                "0.83",
                "0.63",
                "0.86",
                "0.88",
                "0.84"
            ],
            [
                "Toxicity(TP,CF/TR)",
                "163",
                "[BOLD] 0.91",
                "0.77",
                "0.55",
                "0.85",
                "0.86",
                "0.86"
            ],
            [
                "TestTiming(TN,TD/TP)",
                "184",
                "0.90",
                "0.81",
                "0.42",
                "0.96",
                "[BOLD] 0.97",
                "0.95"
            ],
            [
                "TestFinding(TN,TR)",
                "136",
                "0.76",
                "0.60",
                "0.44",
                "[BOLD] 0.82",
                "0.79",
                "0.78"
            ],
            [
                "Threat(O,CF/TR)",
                "32",
                "0.85",
                "0.69",
                "0.54",
                "[BOLD] 0.95",
                "[BOLD] 0.95",
                "0.92"
            ],
            [
                "Intervention(TP,YR)",
                "5",
                "[BOLD] 0.88",
                "0.65",
                "0.47",
                "-",
                "-",
                "-"
            ],
            [
                "EffectOf(Com,CF)",
                "3",
                "[BOLD] 0.92",
                "0.62",
                "0.23",
                "-",
                "-",
                "-"
            ],
            [
                "Severity(CF,CS)",
                "75",
                "[BOLD] 0.61",
                "0.53",
                "0.47",
                "0.52",
                "0.55",
                "0.51"
            ],
            [
                "RecurLink(YR,YR/CF)",
                "7",
                "[BOLD] 1.0",
                "[BOLD] 1.0",
                "0.64",
                "-",
                "-",
                "-"
            ],
            [
                "RecurInfer(NR/YR,TR)",
                "51",
                "0.97",
                "0.69",
                "0.43",
                "[BOLD] 0.99",
                "[BOLD] 0.99",
                "0.98"
            ],
            [
                "GetOpinion(Referral,CF/other)",
                "4",
                "[BOLD] 0.75",
                "[BOLD] 0.75",
                "0.5",
                "-",
                "-",
                "-"
            ],
            [
                "Context(Dis,DisCont)",
                "40",
                "[BOLD] 0.70",
                "0.63",
                "0.53",
                "0.60",
                "0.41",
                "0.57"
            ],
            [
                "TestToAssess(TN,CF/TR)",
                "36",
                "0.76",
                "0.66",
                "0.36",
                "[BOLD] 0.92",
                "[BOLD] 0.92",
                "0.91"
            ],
            [
                "TimeStamp(TD,TP)",
                "221",
                "[BOLD] 0.88",
                "0.83",
                "0.50",
                "0.86",
                "0.85",
                "0.83"
            ],
            [
                "TimeLink(TP,TP)",
                "20",
                "[BOLD] 0.92",
                "0.85",
                "0.45",
                "0.91",
                "[BOLD] 0.92",
                "0.90"
            ],
            [
                "Overall",
                "1569",
                "0.90",
                "0.73",
                "0.45",
                "0.92",
                "[BOLD] 0.93",
                "0.91"
            ]
        ],
        "question": "Is it true that [CONTINUE] As the results of applying the co-occurrence baseline (\u03c1 = 0) shows (Table 2), the semantic relations in this data are strongly concentrated within a sentence boundary, especially for the relation of RecurLink, with an F1 of 1.0?",
        "answer_label": "yes"
    },
    {
        "id": "9faf0fb8-7f04-487b-8c21-c849d0edd997",
        "table_caption": "Argument Generation with Retrieval, Planning, and Realization Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
        "table_column_names": [
            "[EMPTY]",
            "[ITALIC] w/ System Retrieval  [BOLD] B-2",
            "[ITALIC] w/ System Retrieval  [BOLD] B-4",
            "[ITALIC] w/ System Retrieval  [BOLD] R-2",
            "[ITALIC] w/ System Retrieval  [BOLD] MTR",
            "[ITALIC] w/ System Retrieval  [BOLD] #Word",
            "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
        ],
        "table_content_values": [
            [
                "Human",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22"
            ],
            [
                "Retrieval",
                "7.55",
                "1.11",
                "8.64",
                "14.38",
                "123",
                "23",
                "10.97",
                "3.05",
                "23.49",
                "20.08",
                "140",
                "21"
            ],
            [
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Seq2seq",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15"
            ],
            [
                "Seq2seqAug",
                "8.26",
                "2.24",
                "13.79",
                "15.75",
                "78",
                "14",
                "10.98",
                "4.41",
                "22.97",
                "19.62",
                "71",
                "14"
            ],
            [
                "[ITALIC] w/o psg",
                "7.94",
                "2.28",
                "10.13",
                "15.71",
                "75",
                "12",
                "9.89",
                "3.34",
                "14.20",
                "18.40",
                "66",
                "12"
            ],
            [
                "H&W\u00a0Hua and Wang ( 2018 )",
                "3.64",
                "0.92",
                "8.83",
                "11.78",
                "51",
                "12",
                "8.51",
                "2.86",
                "18.89",
                "17.18",
                "58",
                "12"
            ],
            [
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "CANDELA",
                "12.02\u2217",
                "[BOLD] 2.99\u2217",
                "[BOLD] 14.93\u2217",
                "[BOLD] 16.92\u2217",
                "119",
                "22",
                "15.80\u2217",
                "[BOLD] 5.00\u2217",
                "[BOLD] 23.75",
                "[BOLD] 20.18",
                "116",
                "22"
            ],
            [
                "[ITALIC] w/o psg",
                "[BOLD] 12.33\u2217",
                "2.86\u2217",
                "14.53\u2217",
                "16.60\u2217",
                "123",
                "23",
                "[BOLD] 16.33\u2217",
                "4.98\u2217",
                "23.65",
                "19.94",
                "123",
                "23"
            ]
        ],
        "question": "Is it true that Interestingly, we observe a decrease of ROUGE and METEOR, but a marginal increase of BLEU-2 by removing passages from our model input?",
        "answer_label": "yes"
    },
    {
        "id": "54edbc96-7e26-4732-9e3c-08ce9c75397e",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
        "table_column_names": [
            "[EMPTY]",
            "Acc",
            "Sim",
            "PP",
            "GM"
        ],
        "table_content_values": [
            [
                "M0: shen-1",
                "0.818",
                "0.719",
                "37.3",
                "10.0"
            ],
            [
                "M1: M0 [ITALIC] +para",
                "0.819",
                "0.734",
                "26.3",
                "14.2"
            ],
            [
                "M2: M0 [ITALIC] +cyc",
                "0.813",
                "0.770",
                "36.4",
                "18.8"
            ],
            [
                "M3: M0 [ITALIC] +cyc+lang",
                "0.807",
                "0.796",
                "28.4",
                "21.5"
            ],
            [
                "M4: M0 [ITALIC] +cyc+para",
                "0.798",
                "0.783",
                "39.7",
                "19.2"
            ],
            [
                "M5: M0 [ITALIC] +cyc+para+lang",
                "0.804",
                "0.785",
                "27.1",
                "20.3"
            ],
            [
                "M6: M0 [ITALIC] +cyc+2d",
                "0.805",
                "[BOLD] 0.817",
                "43.3",
                "21.6"
            ],
            [
                "M7: M6+ [ITALIC] para+lang",
                "0.818",
                "0.805",
                "[BOLD] 29.0",
                "[BOLD] 22.8"
            ]
        ],
        "question": "Is it true that For Yelp, M0 has better Acc and PP than M1 at comparable semantic similarity?",
        "answer_label": "no"
    },
    {
        "id": "be06cdae-15fd-48da-8cf2-f7558b2216da",
        "table_caption": "Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability Table 1: Accuracy obtained by training an standalone classifier, applying the API and the proposed wrapper for each domain",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] BB source acc.",
            "[BOLD] BB target acc.",
            "[BOLD] Non-reject. acc. (10/20/30%)",
            "[BOLD] Class. quality (10/20/30%)",
            "[BOLD] Reject. quality (10/20/30%)"
        ],
        "table_content_values": [
            [
                "[BOLD] Apply Yelp BB to SST-2",
                "89.18\u00b10.08%",
                "77.13\u00b10.52%",
                "82.43\u00b10.22% 88.19\u00b10.50% 93.60\u00b10.16%",
                "80.40\u00b10.39% 83.11\u00b10.80% 83.05\u00b10.23%",
                "6.03\u00b10.45 6.04\u00b10.51 4.97\u00b10.07"
            ],
            [
                "[BOLD] Apply SST-2 BB to Yelp",
                "83.306\u00b10.18%",
                "82.106\u00b10.88%",
                "87,98\u00b10.18% 92.13\u00b10.38% 94.19\u00b10.33%",
                "85.49\u00b10.88% 84.53\u00b10.38% 78.99\u00b10.46%",
                "8.30\u00b11.63 5.72\u00b10.27 3.73\u00b10.10"
            ],
            [
                "[BOLD] Apply Electronics BB to Music",
                "86.39\u00b10.22%",
                "90.38\u00b10.13%",
                "95.04\u00b10.43% 96.45\u00b10.35% 97.26\u00b10.31%",
                "90.67\u00b10.88% 83.93\u00b10.67% 75.77\u00b10.54%",
                "10.7\u00b11.65 4.82\u00b10.35 3.25\u00b10.14"
            ],
            [
                "[BOLD] Apply Music BB to Electronics",
                "93.10\u00b10.02%",
                "79.85\u00b10.0%",
                "83.26\u00b10.41% 87.06\u00b10.55% 90.50\u00b10.29%",
                "79.97\u00b10.74% 79.93\u00b10.87% 76.81\u00b10.41%",
                "4.1\u00b10.55 3.80\u00b10.35 3.32\u00b10.09"
            ]
        ],
        "question": "Is it true that [CONTINUE] In general terms, the results displayed in table 1 show that the rejection method can reduce the error of the output predictions when applying a pre-trained black-box classification system to a new domain?",
        "answer_label": "yes"
    },
    {
        "id": "438c0d33-4a57-47d3-9bbc-d767c13a8119",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. \u201cEn\u201d column is an English autoencoder. BLEU scores are given for reference.",
        "table_column_names": [
            "[ITALIC] k",
            "Ar",
            "Es",
            "Fr",
            "Ru",
            "Zh",
            "En"
        ],
        "table_content_values": [
            [
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy"
            ],
            [
                "0",
                "88.0",
                "87.9",
                "87.9",
                "87.8",
                "87.7",
                "87.4"
            ],
            [
                "1",
                "92.4",
                "91.9",
                "92.1",
                "92.1",
                "91.5",
                "89.4"
            ],
            [
                "2",
                "91.9",
                "91.8",
                "91.8",
                "91.8",
                "91.3",
                "88.3"
            ],
            [
                "3",
                "92.0",
                "92.3",
                "92.1",
                "91.6",
                "91.2",
                "87.9"
            ],
            [
                "4",
                "92.1",
                "92.4",
                "92.5",
                "92.0",
                "90.5",
                "86.9"
            ],
            [
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy"
            ],
            [
                "0",
                "81.9",
                "81.9",
                "81.8",
                "81.8",
                "81.8",
                "81.2"
            ],
            [
                "1",
                "87.9",
                "87.7",
                "87.8",
                "87.9",
                "87.7",
                "84.5"
            ],
            [
                "2",
                "87.4",
                "87.5",
                "87.4",
                "87.3",
                "87.2",
                "83.2"
            ],
            [
                "3",
                "87.8",
                "87.9",
                "87.9",
                "87.3",
                "87.3",
                "82.9"
            ],
            [
                "4",
                "88.3",
                "88.6",
                "88.4",
                "88.1",
                "87.7",
                "82.1"
            ],
            [
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU"
            ],
            [
                "[EMPTY]",
                "32.7",
                "49.1",
                "38.5",
                "34.2",
                "32.1",
                "96.6"
            ]
        ],
        "question": "Is it true that [CONTINUE] Comparing layers 1 through 4, we see that in 3/5 target languages (Ar, Ru, Zh), POS tagging accuracy peaks at layer 1 and does not improve at higher layers, with some drops at layers 2 and 3?",
        "answer_label": "yes"
    },
    {
        "id": "f0eebd6c-646f-409a-8ea7-e68af50007a0",
        "table_caption": "Deriving Machine Attention from Human Rationales Table 4: Accuracy of transferring between domains. Models with \u2020 use labeled data from source domains and unlabeled data from the target domain. Models with \u2021 use human rationales on the target task.",
        "table_column_names": [
            "Source",
            "Target",
            "Svm",
            "Ra-Svm\u2021",
            "Ra-Cnn\u2021",
            "Trans\u2020",
            "Ra-Trans\u2021\u2020",
            "Ours\u2021\u2020",
            "Oracle\u2020"
        ],
        "table_content_values": [
            [
                "Beer look + Beer aroma + Beer palate",
                "Hotel location",
                "78.65",
                "79.09",
                "79.28",
                "80.42",
                "82.10",
                "[BOLD] 84.52",
                "85.43"
            ],
            [
                "Beer look + Beer aroma + Beer palate",
                "Hotel cleanliness",
                "86.44",
                "86.68",
                "89.01",
                "86.95",
                "87.15",
                "[BOLD] 90.66",
                "92.09"
            ],
            [
                "Beer look + Beer aroma + Beer palate",
                "Hotel service",
                "85.34",
                "86.61",
                "87.91",
                "87.37",
                "86.40",
                "[BOLD] 89.93",
                "92.42"
            ]
        ],
        "question": "Is it true that The error reduction over the best baseline is 15.08% on average?",
        "answer_label": "yes"
    },
    {
        "id": "31b8b6fe-df87-467d-9695-321d94ad69f9",
        "table_caption": "A context sensitive real-time Spell Checker with language adaptability TABLE II: Synthetic Data Performance results",
        "table_column_names": [
            "[BOLD] Language",
            "[BOLD] # Test",
            "[BOLD] P@1",
            "[BOLD] P@3",
            "[BOLD] P@5",
            "[BOLD] P@10",
            "[BOLD] MRR"
        ],
        "table_content_values": [
            [
                "[BOLD] Language",
                "[BOLD] Samples",
                "[BOLD] P@1",
                "[BOLD] P@3",
                "[BOLD] P@5",
                "[BOLD] P@10",
                "[BOLD] MRR"
            ],
            [
                "Bengali",
                "140000",
                "91.30",
                "97.83",
                "98.94",
                "99.65",
                "94.68"
            ],
            [
                "Czech",
                "94205",
                "95.84",
                "98.72",
                "99.26",
                "99.62",
                "97.37"
            ],
            [
                "Danish",
                "140000",
                "85.84",
                "95.19",
                "97.28",
                "98.83",
                "90.85"
            ],
            [
                "Dutch",
                "140000",
                "86.83",
                "95.01",
                "97.04",
                "98.68",
                "91.32"
            ],
            [
                "English",
                "140000",
                "97.08",
                "99.39",
                "99.67",
                "99.86",
                "98.27"
            ],
            [
                "Finnish",
                "140000",
                "97.77",
                "99.58",
                "99.79",
                "99.90",
                "98.69"
            ],
            [
                "French",
                "140000",
                "86.52",
                "95.66",
                "97.52",
                "98.83",
                "91.38"
            ],
            [
                "German",
                "140000",
                "87.58",
                "96.16",
                "97.86",
                "99.05",
                "92.10"
            ],
            [
                "Greek",
                "30022",
                "84.95",
                "94.99",
                "96.88",
                "98.44",
                "90.27"
            ],
            [
                "Hebrew",
                "132596",
                "94.00",
                "98.26",
                "99.05",
                "99.62",
                "96.24"
            ],
            [
                "Hindi",
                "140000",
                "82.19",
                "93.71",
                "96.28",
                "98.30",
                "88.40"
            ],
            [
                "Indonesian",
                "140000",
                "95.01",
                "98.98",
                "99.50",
                "99.84",
                "97.04"
            ],
            [
                "Italian",
                "140000",
                "89.93",
                "97.31",
                "98.54",
                "99.38",
                "93.76"
            ],
            [
                "Marathi",
                "140000",
                "93.01",
                "98.16",
                "99.06",
                "99.66",
                "95.69"
            ],
            [
                "Polish",
                "140000",
                "95.65",
                "99.17",
                "99.62",
                "99.86",
                "97.44"
            ],
            [
                "Portuguese",
                "140000",
                "86.73",
                "96.29",
                "97.94",
                "99.10",
                "91.74"
            ],
            [
                "Romanian",
                "140000",
                "95.52",
                "98.79",
                "99.32",
                "99.68",
                "97.22"
            ],
            [
                "Russian",
                "140000",
                "94.85",
                "98.74",
                "99.33",
                "99.71",
                "96.86"
            ],
            [
                "Spanish",
                "140000",
                "85.91",
                "95.35",
                "97.18",
                "98.57",
                "90.92"
            ],
            [
                "Swedish",
                "140000",
                "88.86",
                "96.40",
                "98.00",
                "99.14",
                "92.87"
            ],
            [
                "Tamil",
                "140000",
                "98.05",
                "99.70",
                "99.88",
                "99.98",
                "98.88"
            ],
            [
                "Telugu",
                "140000",
                "97.11",
                "99.68",
                "99.92",
                "99.99",
                "98.38"
            ],
            [
                "Thai",
                "12403",
                "98.73",
                "99.71",
                "99.78",
                "99.85",
                "99.22"
            ],
            [
                "Turkish",
                "140000",
                "97.13",
                "99.51",
                "99.78",
                "99.92",
                "98.33"
            ]
        ],
        "question": "Is it true that The system does not perform well on synthetic dataset with a minimum of 80% P@1 and 98% P@10?",
        "answer_label": "no"
    },
    {
        "id": "6369cebf-b9ca-4fd0-9d72-621681d2ebdf",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 3: Performance comparison of our model with different values of m on the two datasets.",
        "table_column_names": [
            "[ITALIC] m",
            "NYT10 Prec.",
            "NYT10 Rec.",
            "NYT10 F1",
            "NYT11 Prec.",
            "NYT11 Rec.",
            "NYT11 F1"
        ],
        "table_content_values": [
            [
                "1",
                "0.541",
                "0.595",
                "[BOLD] 0.566",
                "0.495",
                "0.621",
                "0.551"
            ],
            [
                "2",
                "0.521",
                "0.597",
                "0.556",
                "0.482",
                "0.656",
                "0.555"
            ],
            [
                "3",
                "0.490",
                "0.617",
                "0.547",
                "0.509",
                "0.633",
                "0.564"
            ],
            [
                "4",
                "0.449",
                "0.623",
                "0.522",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "5",
                "0.467",
                "0.609",
                "0.529",
                "0.488",
                "0.677",
                "0.567"
            ]
        ],
        "question": "Is it true that These experiments show that the number of factors giving the best performance does not vary depending on the underlying data distribution?",
        "answer_label": "no"
    },
    {
        "id": "8c45d451-b086-4e3f-a4ec-3f686a647811",
        "table_caption": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model Mrksic et al. (2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ Budzianowski et al. (2018).",
        "table_column_names": [
            "[BOLD] DST Models",
            "[BOLD] Joint Acc. WoZ 2.0",
            "[BOLD] Joint Acc. MultiWoZ",
            "[BOLD] ITC"
        ],
        "table_content_values": [
            [
                "Baselines Mrksic et al. ( 2017 )",
                "70.8%",
                "25.83%",
                "[ITALIC] O( [ITALIC] mn)"
            ],
            [
                "NBT-CNN Mrksic et al. ( 2017 )",
                "84.2%",
                "-",
                "[ITALIC] O( [ITALIC] mn)"
            ],
            [
                "StateNet_PSI Ren et al. ( 2018 )",
                "[BOLD] 88.9%",
                "-",
                "[ITALIC] O( [ITALIC] n)"
            ],
            [
                "GLAD Nouri and Hosseini-Asl ( 2018 )",
                "88.5%",
                "35.58%",
                "[ITALIC] O( [ITALIC] mn)"
            ],
            [
                "HyST (ensemble) Goel et al. ( 2019 )",
                "-",
                "44.22%",
                "[ITALIC] O( [ITALIC] n)"
            ],
            [
                "DSTRead (ensemble) Gao et al. ( 2019 )",
                "-",
                "42.12%",
                "[ITALIC] O( [ITALIC] n)"
            ],
            [
                "TRADE Wu et al. ( 2019 )",
                "-",
                "48.62%",
                "[ITALIC] O( [ITALIC] n)"
            ],
            [
                "COMER",
                "88.6%",
                "[BOLD] 48.79%",
                "[ITALIC] O(1)"
            ]
        ],
        "question": "Is it true that On the muli-domain dataset, MultiWoZ, our model achieves a joint goal accuracy of 48.79%, which is lower than the previous state-of-the-art?",
        "answer_label": "no"
    },
    {
        "id": "690d5d5d-b552-4cdf-b858-ccefaf6ddfbc",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.",
        "table_column_names": [
            "[BOLD] Language pair",
            "[BOLD] Model type",
            "[BOLD] Oracle model",
            "[BOLD] Decoder configuration  [BOLD] Uniform",
            "[BOLD] Decoder configuration  [BOLD] BI + IS"
        ],
        "table_content_values": [
            [
                "es-en",
                "Unadapted",
                "36.4",
                "34.7",
                "36.6"
            ],
            [
                "es-en",
                "No-reg",
                "36.6",
                "34.8",
                "-"
            ],
            [
                "es-en",
                "EWC",
                "37.0",
                "36.3",
                "[BOLD] 37.2"
            ],
            [
                "en-de",
                "Unadapted",
                "36.4",
                "26.8",
                "38.8"
            ],
            [
                "en-de",
                "No-reg",
                "41.7",
                "31.8",
                "-"
            ],
            [
                "en-de",
                "EWC",
                "42.1",
                "38.6",
                "[BOLD] 42.0"
            ]
        ],
        "question": "Is it true that BI+IS with EWC-adapted models gives a 0.9 / 3.4 BLEU gain over the strong uniform EWC ensemble, and a 2.4 / 10.2 overall BLEU gain over the approach described in Freitag and Al-Onaizan (2016)?",
        "answer_label": "yes"
    },
    {
        "id": "ce07043c-e05f-4ac4-bf96-7de4531fb8f8",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
        "table_column_names": [
            "[EMPTY]",
            "Acc",
            "Sim",
            "PP",
            "GM"
        ],
        "table_content_values": [
            [
                "M0: shen-1",
                "0.818",
                "0.719",
                "37.3",
                "10.0"
            ],
            [
                "M1: M0 [ITALIC] +para",
                "0.819",
                "0.734",
                "26.3",
                "14.2"
            ],
            [
                "M2: M0 [ITALIC] +cyc",
                "0.813",
                "0.770",
                "36.4",
                "18.8"
            ],
            [
                "M3: M0 [ITALIC] +cyc+lang",
                "0.807",
                "0.796",
                "28.4",
                "21.5"
            ],
            [
                "M4: M0 [ITALIC] +cyc+para",
                "0.798",
                "0.783",
                "39.7",
                "19.2"
            ],
            [
                "M5: M0 [ITALIC] +cyc+para+lang",
                "0.804",
                "0.785",
                "27.1",
                "20.3"
            ],
            [
                "M6: M0 [ITALIC] +cyc+2d",
                "0.805",
                "[BOLD] 0.817",
                "43.3",
                "21.6"
            ],
            [
                "M7: M6+ [ITALIC] para+lang",
                "0.818",
                "0.805",
                "[BOLD] 29.0",
                "[BOLD] 22.8"
            ]
        ],
        "question": "Is it true that [CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss increases PP, sometimes at a slight cost of semantic preservation?",
        "answer_label": "no"
    },
    {
        "id": "8667413d-d331-4b2e-bae7-01f3a106b373",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1192",
                "0.0083",
                "0.0137",
                "0.0150",
                "0.0150",
                "0.0445",
                "0.0326"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1022",
                "0.0069",
                "0.0060",
                "0.0092",
                "0.0090",
                "0.0356",
                "0.0162"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5710",
                "0.1948",
                "0.3855",
                "0.5474",
                "0.4485",
                "[BOLD] 0.8052",
                "0.4058"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "[BOLD] 0.6304",
                "0.1870",
                "0.3250",
                "0.5312",
                "0.4576",
                "0.6064",
                "0.3698"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0037",
                "0.3278",
                "0.5941",
                "0.6486",
                "[BOLD] 0.6490",
                "0.0017",
                "0.0003"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0002",
                "0.1486",
                "0.4332",
                "[BOLD] 0.6467",
                "0.6332",
                "0.0967",
                "0.0003"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0002",
                "0.1562",
                "0.5157",
                "[BOLD] 0.7255",
                "0.5932",
                "0.0032",
                "0.0001"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "2.10-5",
                "0.0507",
                "0.4492",
                "[BOLD] 0.7000",
                "0.5887",
                "0.1390",
                "0.0002"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0073",
                "0.0162",
                "0.0268",
                "[BOLD] 0.0293",
                "[BOLD] 0.0293",
                "0.0033",
                "0.0006"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0004",
                "0.0132",
                "0.0118",
                "0.0181",
                "0.0179",
                "[BOLD] 0.0520",
                "0.0005"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0005",
                "0.1733",
                "0.4412",
                "[BOLD] 0.6240",
                "0.5109",
                "0.0064",
                "0.0002"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "4.10-5",
                "0.0798",
                "0.3771",
                "[BOLD] 0.6040",
                "0.5149",
                "0.2261",
                "0.0004"
            ]
        ],
        "question": "Is it true that As occurred in the experiment using the top 1,000 words, this experiment also kept TF with the highest values of f-measure for most methods, except for the Portuguese Europarl corpus, where DocSub had the highest value?",
        "answer_label": "no"
    },
    {
        "id": "447eeb5c-f007-4096-b630-ce70255d1c14",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
        "table_column_names": [
            "<bold>Model</bold>",
            "R",
            "MUC P",
            "<italic>F</italic>1",
            "R",
            "B3 P",
            "<italic>F</italic>1",
            "R",
            "CEAF-<italic>e</italic> P",
            "<italic>F</italic>1",
            "CoNLL <italic>F</italic>1"
        ],
        "table_content_values": [
            [
                "<bold>Baselines</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Cluster+Lemma",
                "76.5",
                "79.9",
                "78.1",
                "71.7",
                "85",
                "77.8",
                "75.5",
                "71.7",
                "73.6",
                "76.5"
            ],
            [
                "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
                "71",
                "75",
                "73",
                "71",
                "78",
                "74",
                "-",
                "-",
                "64",
                "73"
            ],
            [
                "KCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>)",
                "67",
                "71",
                "69",
                "71",
                "67",
                "69",
                "71",
                "67",
                "69",
                "69"
            ],
            [
                "Cluster+KCP",
                "68.4",
                "79.3",
                "73.4",
                "67.2",
                "87.2",
                "75.9",
                "77.4",
                "66.4",
                "71.5",
                "73.6"
            ],
            [
                "<bold>Model Variants</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Disjoint",
                "75.5",
                "83.6",
                "79.4",
                "75.4",
                "86",
                "80.4",
                "80.3",
                "71.9",
                "75.9",
                "78.5"
            ],
            [
                "Joint",
                "77.6",
                "84.5",
                "80.9",
                "76.1",
                "85.1",
                "80.3",
                "81",
                "73.8",
                "77.3",
                "<bold>79.5</bold>"
            ]
        ],
        "question": "Is it true that [CONTINUE] The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points, though still performing substantially worse than our joint model?",
        "answer_label": "yes"
    },
    {
        "id": "b920ceff-a2e2-4cc1-877c-73c2b3404336",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.",
        "table_column_names": [
            "[EMPTY]",
            "Ours",
            "Refresh",
            "ExtAbsRL"
        ],
        "table_content_values": [
            [
                "Avg. Human Rating",
                "[BOLD] 2.52",
                "2.27",
                "1.66"
            ],
            [
                "Best%",
                "[BOLD] 70.0",
                "33.3",
                "6.7"
            ]
        ],
        "question": "Is it true that Also, the average human rating for Refresh is not significantly higher (p (cid:28) 0.01) than ExtAbsRL?",
        "answer_label": "no"
    },
    {
        "id": "80fcba83-634a-4705-a314-22a36d228ec5",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 2: Yelp results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
        "table_column_names": [
            "[EMPTY]",
            "Acc",
            "Sim",
            "PP",
            "GM"
        ],
        "table_content_values": [
            [
                "M0: shen-1",
                "0.818",
                "0.719",
                "37.3",
                "10.0"
            ],
            [
                "M1: M0 [ITALIC] +para",
                "0.819",
                "0.734",
                "26.3",
                "14.2"
            ],
            [
                "M2: M0 [ITALIC] +cyc",
                "0.813",
                "0.770",
                "36.4",
                "18.8"
            ],
            [
                "M3: M0 [ITALIC] +cyc+lang",
                "0.807",
                "0.796",
                "28.4",
                "21.5"
            ],
            [
                "M4: M0 [ITALIC] +cyc+para",
                "0.798",
                "0.783",
                "39.7",
                "19.2"
            ],
            [
                "M5: M0 [ITALIC] +cyc+para+lang",
                "0.804",
                "0.785",
                "27.1",
                "20.3"
            ],
            [
                "M6: M0 [ITALIC] +cyc+2d",
                "0.805",
                "[BOLD] 0.817",
                "43.3",
                "21.6"
            ],
            [
                "M7: M6+ [ITALIC] para+lang",
                "0.818",
                "0.805",
                "[BOLD] 29.0",
                "[BOLD] 22.8"
            ]
        ],
        "question": "Is it true that For Yelp, M1 has better Acc and PP than M0 at comparable semantic similarity?",
        "answer_label": "yes"
    },
    {
        "id": "5ccbb007-877f-4e17-8513-9731641554db",
        "table_caption": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
        "table_column_names": [
            "ID LSTM-800",
            "5-fold CV 70.56",
            "\u0394 0.66",
            "Single model 67.54",
            "\u0394 0.78",
            "Ensemble 67.65",
            "\u0394 0.30"
        ],
        "table_content_values": [
            [
                "LSTM-400",
                "70.50",
                "0.60",
                "[BOLD] 67.59",
                "0.83",
                "[BOLD] 68.00",
                "0.65"
            ],
            [
                "IN-TITLE",
                "70.11",
                "0.21",
                "[EMPTY]",
                "[EMPTY]",
                "67.52",
                "0.17"
            ],
            [
                "[BOLD] SUBMISSION",
                "69.90",
                "\u2013",
                "66.76",
                "\u2013",
                "67.35",
                "\u2013"
            ],
            [
                "NO-HIGHWAY",
                "69.72",
                "\u22120.18",
                "66.42",
                "\u22120.34",
                "66.64",
                "\u22120.71"
            ],
            [
                "NO-OVERLAPS",
                "69.46",
                "\u22120.44",
                "65.07",
                "\u22121.69",
                "66.47",
                "\u22120.88"
            ],
            [
                "LSTM-400-DROPOUT",
                "69.45",
                "\u22120.45",
                "65.53",
                "\u22121.23",
                "67.28",
                "\u22120.07"
            ],
            [
                "NO-TRANSLATIONS",
                "69.42",
                "\u22120.48",
                "65.92",
                "\u22120.84",
                "67.23",
                "\u22120.12"
            ],
            [
                "NO-ELMO-FINETUNING",
                "67.71",
                "\u22122.19",
                "65.16",
                "\u22121.60",
                "65.42",
                "\u22121.93"
            ]
        ],
        "question": "Is it true that [CONTINUE] However, our data augmentation technique (NO-TRANSLATIONS) had a significant impact on the final score, reducing it by 0.84 points?",
        "answer_label": "no"
    },
    {
        "id": "7a281d71-ec5a-4086-bb9e-b1a04976b77b",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "-{4} dense block",
                "24.8",
                "54.9"
            ],
            [
                "-{3, 4} dense blocks",
                "23.8",
                "54.1"
            ],
            [
                "-{2, 3, 4} dense blocks",
                "23.2",
                "53.1"
            ]
        ],
        "question": "Is it true that The full model gives 25.5 BLEU points on the AMR15 dev set?",
        "answer_label": "yes"
    },
    {
        "id": "b87e736b-b577-4883-9cd3-271efb940ee7",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
        "table_column_names": [
            "Topic Name",
            "Size",
            "TF-IDF ARI",
            "WMD ARI",
            "Sent2vec ARI",
            "Doc2vec ARI",
            "BERT ARI",
            "[ITALIC] OD-w2v ARI",
            "[ITALIC] OD-d2v ARI",
            "TF-IDF  [ITALIC] Sil.",
            "WMD  [ITALIC] Sil.",
            "Sent2vec  [ITALIC] Sil.",
            "Doc2vec  [ITALIC] Sil.",
            "BERT  [ITALIC] Sil.",
            "[ITALIC] OD-w2v  [ITALIC] Sil.",
            "[ITALIC] OD-d2v  [ITALIC] Sil."
        ],
        "table_content_values": [
            [
                "Affirmative Action",
                "81",
                "-0.07",
                "-0.02",
                "0.03",
                "-0.01",
                "-0.02",
                "[BOLD] 0.14",
                "[ITALIC] 0.02",
                "0.01",
                "0.01",
                "-0.01",
                "-0.02",
                "-0.04",
                "[BOLD] 0.06",
                "[ITALIC] 0.01"
            ],
            [
                "Atheism",
                "116",
                "[BOLD] 0.19",
                "0.07",
                "0.00",
                "0.03",
                "-0.01",
                "0.11",
                "[ITALIC] 0.16",
                "0.02",
                "0.01",
                "0.02",
                "0.01",
                "0.01",
                "[ITALIC] 0.05",
                "[BOLD] 0.07"
            ],
            [
                "Austerity Measures",
                "20",
                "[ITALIC] 0.04",
                "[ITALIC] 0.04",
                "-0.01",
                "-0.05",
                "0.04",
                "[BOLD] 0.21",
                "-0.01",
                "0.06",
                "0.07",
                "0.05",
                "-0.03",
                "0.10",
                "[BOLD] 0.19",
                "0.1"
            ],
            [
                "Democratization",
                "76",
                "0.02",
                "-0.01",
                "0.00",
                "[ITALIC] 0.09",
                "-0.01",
                "[BOLD] 0.11",
                "0.07",
                "0.01",
                "0.01",
                "0.02",
                "0.02",
                "0.03",
                "[BOLD] 0.16",
                "[ITALIC] 0.11"
            ],
            [
                "Education Voucher Scheme",
                "30",
                "[BOLD] 0.25",
                "0.12",
                "0.08",
                "-0.02",
                "0.04",
                "0.13",
                "[ITALIC] 0.19",
                "0.01",
                "0.01",
                "0.01",
                "-0.01",
                "0.02",
                "[ITALIC] 0.38",
                "[BOLD] 0.40"
            ],
            [
                "Gambling",
                "60",
                "-0.06",
                "-0.01",
                "-0.02",
                "0.04",
                "0.09",
                "[ITALIC] 0.35",
                "[BOLD] 0.39",
                "0.01",
                "0.02",
                "0.03",
                "0.01",
                "0.09",
                "[BOLD] 0.30",
                "[ITALIC] 0.22"
            ],
            [
                "Housing",
                "30",
                "0.01",
                "-0.01",
                "-0.01",
                "-0.02",
                "0.08",
                "[BOLD] 0.27",
                "0.01",
                "0.02",
                "0.03",
                "0.03",
                "0.01",
                "0.11",
                "[BOLD] 0.13",
                "[ITALIC] 0.13"
            ],
            [
                "Hydroelectric Dams",
                "110",
                "[BOLD] 0.47",
                "[ITALIC] 0.45",
                "[ITALIC] 0.45",
                "-0.01",
                "0.38",
                "0.35",
                "0.14",
                "0.04",
                "0.08",
                "0.12",
                "0.01",
                "0.19",
                "[BOLD] 0.26",
                "[ITALIC] 0.09"
            ],
            [
                "Intellectual Property",
                "66",
                "0.01",
                "0.01",
                "0.00",
                "0.03",
                "0.03",
                "[ITALIC] 0.05",
                "[BOLD] 0.14",
                "0.01",
                "[ITALIC] 0.04",
                "0.03",
                "0.01",
                "0.03",
                "[ITALIC] 0.04",
                "[BOLD] 0.12"
            ],
            [
                "Keystone pipeline",
                "18",
                "0.01",
                "0.01",
                "0.00",
                "-0.13",
                "[BOLD] 0.07",
                "-0.01",
                "[BOLD] 0.07",
                "-0.01",
                "-0.03",
                "-0.03",
                "-0.07",
                "0.03",
                "[BOLD] 0.05",
                "[ITALIC] 0.02"
            ],
            [
                "Monarchy",
                "61",
                "-0.04",
                "0.01",
                "0.00",
                "0.03",
                "-0.02",
                "[BOLD] 0.15",
                "[BOLD] 0.15",
                "0.01",
                "0.02",
                "0.02",
                "0.01",
                "0.01",
                "[BOLD] 0.11",
                "[ITALIC] 0.09"
            ],
            [
                "National Service",
                "33",
                "0.14",
                "-0.03",
                "-0.01",
                "0.02",
                "0.01",
                "[ITALIC] 0.31",
                "[BOLD] 0.39",
                "0.02",
                "0.04",
                "0.02",
                "0.01",
                "0.02",
                "[BOLD] 0.25",
                "[BOLD] 0.25"
            ],
            [
                "One-child policy China",
                "67",
                "-0.05",
                "0.01",
                "[BOLD] 0.11",
                "-0.02",
                "0.02",
                "[BOLD] 0.11",
                "0.01",
                "0.01",
                "0.02",
                "[ITALIC] 0.04",
                "-0.01",
                "0.03",
                "[BOLD] 0.07",
                "-0.02"
            ],
            [
                "Open-source Software",
                "48",
                "-0.02",
                "-0.01",
                "[ITALIC] 0.05",
                "0.01",
                "0.12",
                "[BOLD] 0.09",
                "-0.02",
                "0.01",
                "-0.01",
                "0.00",
                "-0.02",
                "0.03",
                "[BOLD] 0.18",
                "0.01"
            ],
            [
                "Pornography",
                "52",
                "-0.02",
                "0.01",
                "0.01",
                "-0.02",
                "-0.01",
                "[BOLD] 0.41",
                "[BOLD] 0.41",
                "0.01",
                "0.01",
                "0.02",
                "-0.01",
                "0.03",
                "[BOLD] 0.47",
                "[ITALIC] 0.41"
            ],
            [
                "Seanad Abolition",
                "25",
                "0.23",
                "0.09",
                "-0.01",
                "-0.01",
                "0.03",
                "[ITALIC] 0.32",
                "[BOLD] 0.54",
                "0.02",
                "0.01",
                "-0.01",
                "-0.03",
                "-0.04",
                "[ITALIC] 0.15",
                "[BOLD] 0.31"
            ],
            [
                "Trades Unions",
                "19",
                "[ITALIC] 0.44",
                "[ITALIC] 0.44",
                "[BOLD] 0.60",
                "-0.05",
                "0.44",
                "[ITALIC] 0.44",
                "0.29",
                "0.1",
                "0.17",
                "0.21",
                "0.01",
                "0.26",
                "[BOLD] 0.48",
                "[ITALIC] 0.32"
            ],
            [
                "Video Games",
                "72",
                "-0.01",
                "0.01",
                "0.12",
                "0.01",
                "0.08",
                "[ITALIC] 0.40",
                "[BOLD] 0.56",
                "0.01",
                "0.01",
                "0.06",
                "0.01",
                "0.05",
                "[ITALIC] 0.32",
                "[BOLD] 0.42"
            ],
            [
                "Average",
                "54.67",
                "0.09",
                "0.07",
                "0.08",
                "0.01",
                "0.08",
                "[BOLD] 0.22",
                "[ITALIC] 0.20",
                "0.02",
                "0.03",
                "0.04",
                "-0.01",
                "0.05",
                "[BOLD] 0.20",
                "[ITALIC] 0.17"
            ]
        ],
        "question": "Is it true that The semantic threshold for OD-d2v is set at 0.3 while for OD-w2v is set at 0.6?",
        "answer_label": "yes"
    },
    {
        "id": "5a893e75-277d-48d6-abe4-2d1757a04b02",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that This suggests that graph encoders based on gating mechanisms are not as effective as other models in text generation models?",
        "answer_label": "no"
    },
    {
        "id": "bb0c415e-fa38-4ace-aa92-8f8480f1b2ab",
        "table_caption": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment Table 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] Full UAS",
            "[BOLD] PPA Acc."
        ],
        "table_content_values": [
            [
                "RBG",
                "94.17",
                "88.51"
            ],
            [
                "RBG + HPCD (full)",
                "94.19",
                "89.59"
            ],
            [
                "RBG + LSTM-PP",
                "94.14",
                "86.35"
            ],
            [
                "RBG + OntoLSTM-PP",
                "94.30",
                "90.11"
            ],
            [
                "RBG + Oracle PP",
                "94.60",
                "98.97"
            ]
        ],
        "question": "Is it true that However, when gold PP attachment are used, we note a large potential improve [CONTINUE] ment of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which confirms that adding PP predictions as features is an effective approach?",
        "answer_label": "yes"
    },
    {
        "id": "05fde2b1-5561-41b9-b324-49eb1967cf32",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.",
        "table_column_names": [
            "Dataset",
            "Accuracy",
            "Fleiss\u2019 kappa  [ITALIC] k"
        ],
        "table_content_values": [
            [
                "Original COPA",
                "100.0",
                "0.973"
            ],
            [
                "Balanced COPA",
                "97.0",
                "0.798"
            ]
        ],
        "question": "Is it true that The human evaluation shows that our mirrored instances are not as difficult as the original ones (see Table 3)?",
        "answer_label": "no"
    },
    {
        "id": "0fdec0b4-dd5c-40da-b23e-43bf04c01a24",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
        "table_column_names": [
            "Model",
            "Answerability Macro P/R/F",
            "# Answerable",
            "Answer Prec.",
            "Derivation Prec. RG-L (P/R/F)",
            "Derivation Prec. BL-4"
        ],
        "table_content_values": [
            [
                "Shortest Path",
                "54.8/55.5/53.2",
                "976",
                "3.6",
                "56.7/38.5/41.5",
                "31.3"
            ],
            [
                "PRKGC",
                "52.6/51.5/50.7",
                "1,021",
                "45.2",
                "40.7/60.7/44.7",
                "30.9"
            ],
            [
                "PRKGC+NS",
                "53.6/54.1/52.1",
                "980",
                "45.4",
                "42.2/61.6/46.1",
                "33.4"
            ]
        ],
        "question": "Is it true that The PRKGC model gives considerably good results, which indicates the non-triviality of RC-QEDE?",
        "answer_label": "no"
    },
    {
        "id": "cb23a665-93d6-4923-befd-01b39b00e674",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that We hypothesize that the gating mechanism cannot better capture long-distance dependencies between nodes far apart in the graph?",
        "answer_label": "no"
    },
    {
        "id": "6241fc50-d27d-46d7-9be1-a96d6b384e0e",
        "table_caption": "Semantic Neural Machine Translation using AMR Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.",
        "table_column_names": [
            "System",
            "NC-v11 BLEU",
            "NC-v11 TER\u2193",
            "NC-v11 Meteor",
            "Full BLEU",
            "Full TER\u2193",
            "Full Meteor"
        ],
        "table_content_values": [
            [
                "OpenNMT-tf",
                "15.1",
                "0.6902",
                "0.3040",
                "24.3",
                "0.5567",
                "0.4225"
            ],
            [
                "Transformer-tf",
                "17.1",
                "0.6647",
                "0.3578",
                "25.1",
                "0.5537",
                "0.4344"
            ],
            [
                "Seq2seq",
                "16.0",
                "0.6695",
                "0.3379",
                "23.7",
                "0.5590",
                "0.4258"
            ],
            [
                "Dual2seq-LinAMR",
                "17.3",
                "0.6530",
                "0.3612",
                "24.0",
                "0.5643",
                "0.4246"
            ],
            [
                "Duel2seq-SRL",
                "17.2",
                "0.6591",
                "0.3644",
                "23.8",
                "0.5626",
                "0.4223"
            ],
            [
                "Dual2seq-Dep",
                "17.8",
                "0.6516",
                "0.3673",
                "25.0",
                "0.5538",
                "0.4328"
            ],
            [
                "Dual2seq",
                "[BOLD] *19.2*",
                "[BOLD] 0.6305",
                "[BOLD] 0.3840",
                "[BOLD] *25.5*",
                "[BOLD] 0.5480",
                "[BOLD] 0.4376"
            ]
        ],
        "question": "Is it true that When trained on the NC-v11 subset, the gap between Seq2seq and Dual2seq under BLEU (around 3 points) is greater than that under Meteor (around 5 points)?",
        "answer_label": "no"
    },
    {
        "id": "e042f4df-12c4-467c-95bd-5043fd5e178e",
        "table_caption": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).",
        "table_column_names": [
            "[EMPTY]",
            "dev CS",
            "dev mono",
            "test CS",
            "test mono"
        ],
        "table_content_values": [
            [
                "CS-only-LM",
                "45.20",
                "65.87",
                "43.20",
                "62.80"
            ],
            [
                "Fine-Tuned-LM",
                "49.60",
                "72.67",
                "47.60",
                "71.33"
            ],
            [
                "CS-only-disc",
                "[BOLD] 75.60",
                "70.40",
                "70.80",
                "70.53"
            ],
            [
                "Fine-Tuned-disc",
                "70.80",
                "[BOLD] 74.40",
                "[BOLD] 75.33",
                "[BOLD] 75.87"
            ]
        ],
        "question": "Is it true that The CS-ONLY-DISCRIMINATIVE model is able to prioritize the gold sentence better than all other models, under both conditions?",
        "answer_label": "no"
    },
    {
        "id": "6532fb08-f821-4080-912e-391b0e279557",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
        "table_column_names": [
            "[EMPTY]",
            "Prec.",
            "Rec.",
            "F1"
        ],
        "table_content_values": [
            [
                "(A1) BiLSTM-CNN",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "(A2) Standard attention",
                "0.466",
                "0.638",
                "0.539"
            ],
            [
                "(A3) Window size ( [ITALIC] ws)=5",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "(A4) Window size ( [ITALIC] ws)=10",
                "0.510",
                "0.640",
                "0.568"
            ],
            [
                "(A5) Softmax",
                "0.490",
                "0.658",
                "0.562"
            ],
            [
                "(A6) Max-pool",
                "0.492",
                "0.600",
                "0.541"
            ]
        ],
        "question": "Is it true that Increasing the window size to 10 increases the F1 score marginally (A3\u2212A4)?",
        "answer_label": "no"
    },
    {
        "id": "ef19ef2f-f972-4231-b2ce-9604c2f0de42",
        "table_caption": "Aligning Vector-spaces with Noisy Supervised Lexicons Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
        "table_column_names": [
            "Method",
            "En\u2192It best",
            "En\u2192It avg",
            "En\u2192It iters",
            "En\u2192De best",
            "En\u2192De avg",
            "En\u2192De iters",
            "En\u2192Fi best",
            "En\u2192Fi avg",
            "En\u2192Fi iters",
            "En\u2192Es best",
            "En\u2192Es avg",
            "En\u2192Es iters"
        ],
        "table_content_values": [
            [
                "Artetxe et\u00a0al., 2018b",
                "[BOLD] 48.53",
                "48.13",
                "573",
                "48.47",
                "48.19",
                "773",
                "33.50",
                "32.63",
                "988",
                "37.60",
                "37.33",
                "808"
            ],
            [
                "Noise-aware Alignment",
                "[BOLD] 48.53",
                "[BOLD] 48.20",
                "471",
                "[BOLD] 49.67",
                "[BOLD] 48.89",
                "568",
                "[BOLD] 33.98",
                "[BOLD] 33.68",
                "502",
                "[BOLD] 38.40",
                "[BOLD] 37.79",
                "551"
            ]
        ],
        "question": "Is it true that In contrast, the noise-aware model requires more iterations to converge?",
        "answer_label": "no"
    },
    {
        "id": "689f8a9c-3097-4448-b010-e9413fcaeabc",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.",
        "table_column_names": [
            "Corpus",
            "Metric",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "Europarl",
                "TotalTerms:",
                "957",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "836",
                "1,000"
            ],
            [
                "Europarl",
                "TotalRoots:",
                "44",
                "1",
                "1",
                "1",
                "1",
                "43",
                "1"
            ],
            [
                "Europarl",
                "NumberRels:",
                "1,588",
                "1,025",
                "1,028",
                "1,185",
                "1,103",
                "1,184",
                "999"
            ],
            [
                "Europarl",
                "MaxDepth:",
                "21",
                "921",
                "901",
                "788",
                "835",
                "8",
                "15"
            ],
            [
                "Europarl",
                "MinDepth:",
                "1",
                "921",
                "901",
                "788",
                "835",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgDepth:",
                "11.82",
                "921",
                "901",
                "788",
                "835",
                "3.05",
                "8.46"
            ],
            [
                "Europarl",
                "DepthCohesion:",
                "1.78",
                "1",
                "1",
                "1",
                "1",
                "2.62",
                "1.77"
            ],
            [
                "Europarl",
                "MaxWidth:",
                "20",
                "2",
                "3",
                "4",
                "3",
                "88",
                "41"
            ],
            [
                "Europarl",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgWidth:",
                "1.99",
                "1.03",
                "1.03",
                "1.19",
                "1.10",
                "4.20",
                "2.38"
            ],
            [
                "TED Talks",
                "TotalTerms:",
                "476",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000"
            ],
            [
                "TED Talks",
                "TotalRoots:",
                "164",
                "2",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "NumberRels:",
                "521",
                "1,029",
                "1,331",
                "3,025",
                "3,438",
                "3,802",
                "1,009"
            ],
            [
                "TED Talks",
                "MaxDepth:",
                "16",
                "915",
                "658",
                "454",
                "395",
                "118",
                "12"
            ],
            [
                "TED Talks",
                "MinDepth:",
                "1",
                "913",
                "658",
                "454",
                "395",
                "110",
                "1"
            ],
            [
                "TED Talks",
                "AvgDepth:",
                "5.82",
                "914",
                "658",
                "454",
                "395",
                "112.24",
                "5.95"
            ],
            [
                "TED Talks",
                "DepthCohesion:",
                "2.75",
                "1",
                "1",
                "1",
                "1",
                "1.05",
                "2.02"
            ],
            [
                "TED Talks",
                "MaxWidth:",
                "25",
                "2",
                "77",
                "13",
                "12",
                "66",
                "98"
            ],
            [
                "TED Talks",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "AvgWidth:",
                "1.83",
                "1.03",
                "1.36",
                "3.03",
                "3.44",
                "6.64",
                "2.35"
            ]
        ],
        "question": "Is it true that [CONTINUE] For example, using relations generated by TF model using the Europarl corpus, we can understand the MaxDepth as having 789 terms with different values of term frequency, while having 211 that share the same value of term frequency with other terms?",
        "answer_label": "yes"
    },
    {
        "id": "f8dc8a50-467a-4ac0-8ec0-796d2151a87d",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that [CONTINUE] For both datasets, our approach substantially outperforms the baselines?",
        "answer_label": "yes"
    },
    {
        "id": "ae9d1040-1c3b-4d4b-8743-4b74fea43a96",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc\u2217: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.",
        "table_column_names": [
            "Model",
            "BLEU",
            "Acc\u2217"
        ],
        "table_content_values": [
            [
                "fu-1",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Multi-decoder",
                "7.6",
                "0.792"
            ],
            [
                "Style embed.",
                "15.4",
                "0.095"
            ],
            [
                "simple-transfer",
                "simple-transfer",
                "simple-transfer"
            ],
            [
                "Template",
                "18.0",
                "0.867"
            ],
            [
                "Delete/Retrieve",
                "12.6",
                "0.909"
            ],
            [
                "yang2018unsupervised",
                "yang2018unsupervised",
                "yang2018unsupervised"
            ],
            [
                "LM",
                "13.4",
                "0.854"
            ],
            [
                "LM + classifier",
                "[BOLD] 22.3",
                "0.900"
            ],
            [
                "Untransferred",
                "[BOLD] 31.4",
                "0.024"
            ]
        ],
        "question": "Is it true that We additionally find that supervised BLEU shows a trade-off with Acc: for a single model type, higher Acc generally corresponds to lower BLEU?",
        "answer_label": "yes"
    },
    {
        "id": "6c15ac43-fcb9-4598-a50f-607a89c8074f",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "Base ACC",
            "Base Time",
            "+LN ACC",
            "+LN Time",
            "+BERT ACC",
            "+BERT Time",
            "+LN+BERT ACC",
            "+LN+BERT Time"
        ],
        "table_content_values": [
            [
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "250K",
                "83.50",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-"
            ],
            [
                "This",
                "LSTM",
                "8.36M",
                "84.27",
                "0.262",
                "86.03",
                "0.432",
                "89.95",
                "0.544",
                "[BOLD] 90.49",
                "0.696"
            ],
            [
                "This",
                "GRU",
                "6.41M",
                "[BOLD] 85.71",
                "0.245",
                "[BOLD] 86.05",
                "0.419",
                "[BOLD] 90.29",
                "0.529",
                "90.10",
                "0.695"
            ],
            [
                "This",
                "ATR",
                "2.87M",
                "84.88",
                "0.210",
                "85.81",
                "0.307",
                "90.00",
                "0.494",
                "90.28",
                "0.580"
            ],
            [
                "Work",
                "SRU",
                "5.48M",
                "84.28",
                "0.258",
                "85.32",
                "0.283",
                "89.98",
                "0.543",
                "90.09",
                "0.555"
            ],
            [
                "[EMPTY]",
                "LRN",
                "4.25M",
                "84.88",
                "[BOLD] 0.209",
                "85.06",
                "[BOLD] 0.223",
                "89.98",
                "[BOLD] 0.488",
                "89.93",
                "[BOLD] 0.506"
            ]
        ],
        "question": "Is it true that Overall results show that ATR achieves the best performance and consumes the least training time?",
        "answer_label": "no"
    },
    {
        "id": "4534055e-da0f-4973-ac32-54f5b588cf48",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that GDPL achieves extremely high performance in the task success on account of the substantial improvement in inform F1 and match rate over the baselines?",
        "answer_label": "yes"
    },
    {
        "id": "32381fb3-5662-4d51-bf38-51a4d3b1023b",
        "table_caption": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set",
        "table_column_names": [
            "Method",
            "WER (%)",
            "DCE"
        ],
        "table_content_values": [
            [
                "No enhancement",
                "17.3",
                "0.828"
            ],
            [
                "Wiener filter",
                "19.5",
                "0.722"
            ],
            [
                "Minimizing DCE",
                "15.8",
                "[BOLD] 0.269"
            ],
            [
                "FSEGAN",
                "14.9",
                "0.291"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
                "15.6",
                "0.330"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
                "[BOLD] 14.4",
                "0.303"
            ],
            [
                "Clean speech",
                "5.7",
                "0.0"
            ]
        ],
        "question": "Is it true that [CONTINUE] In Librispeech + DEMAND, minimizing DCE (15.8%) and FSEGAN (14.9%) achieves a lower WER than acoustic supervision (15.6%) and multi-task learning (14.4%)?",
        "answer_label": "no"
    },
    {
        "id": "bfa7b13c-5cb0-4467-ae28-9a92c6efd558",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
        "table_column_names": [
            "[EMPTY]",
            "WN-N P",
            "WN-N R",
            "WN-N F",
            "WN-V P",
            "WN-V R",
            "WN-V F",
            "VN P",
            "VN R",
            "VN F"
        ],
        "table_content_values": [
            [
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2"
            ],
            [
                "type",
                ".700",
                ".654",
                ".676",
                ".535",
                ".474",
                ".503",
                ".327",
                ".309",
                ".318"
            ],
            [
                "x+POS",
                ".699",
                ".651",
                ".674",
                ".544",
                ".472",
                ".505",
                ".339",
                ".312",
                ".325"
            ],
            [
                "lemma",
                ".706",
                ".660",
                ".682",
                ".576",
                ".520",
                ".547",
                ".384",
                ".360",
                ".371"
            ],
            [
                "x+POS",
                "<bold>.710</bold>",
                "<bold>.662</bold>",
                "<bold>.685</bold>",
                "<bold>.589</bold>",
                "<bold>.529</bold>",
                "<bold>.557</bold>",
                "<bold>.410</bold>",
                "<bold>.389</bold>",
                "<bold>.399</bold>"
            ],
            [
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep"
            ],
            [
                "type",
                ".712",
                ".661",
                ".686",
                ".545",
                ".457",
                ".497",
                ".324",
                ".296",
                ".310"
            ],
            [
                "x+POS",
                ".715",
                ".659",
                ".686",
                ".560",
                ".464",
                ".508",
                ".349",
                ".320",
                ".334"
            ],
            [
                "lemma",
                "<bold>.725</bold>",
                "<bold>.668</bold>",
                "<bold>.696</bold>",
                ".591",
                ".512",
                ".548",
                ".408",
                ".371",
                ".388"
            ],
            [
                "x+POS",
                ".722",
                ".666",
                ".693",
                "<bold>.609</bold>",
                "<bold>.527</bold>",
                "<bold>.565</bold>",
                "<bold>.412</bold>",
                "<bold>.381</bold>",
                "<bold>.396</bold>"
            ]
        ],
        "question": "Is it true that For window-based w2 contexts POS disambiguation yields significantly better F scores on lemmatized targets for VN (p \u2264 .005) with borderline significance for WN-N and WN-V (p \u2248 .05)?",
        "answer_label": "yes"
    },
    {
        "id": "c8ffde58-4d95-40e3-aa21-fbdbe463b9dd",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.",
        "table_column_names": [
            "[EMPTY]",
            "Italian Original",
            "Italian Debiased",
            "Italian English",
            "Italian Reduction",
            "German Original",
            "German Debiased",
            "German English",
            "German Reduction"
        ],
        "table_content_values": [
            [
                "Same Gender",
                "0.442",
                "0.434",
                "0.424",
                "\u2013",
                "0.491",
                "0.478",
                "0.446",
                "\u2013"
            ],
            [
                "Different Gender",
                "0.385",
                "0.421",
                "0.415",
                "\u2013",
                "0.415",
                "0.435",
                "0.403",
                "\u2013"
            ],
            [
                "difference",
                "0.057",
                "0.013",
                "0.009",
                "[BOLD] 91.67%",
                "0.076",
                "0.043",
                "0.043",
                "[BOLD] 100%"
            ]
        ],
        "question": "Is it true that In Italian, we get an increase of 91.67% of the gap with respect to English?",
        "answer_label": "no"
    },
    {
        "id": "9846b931-84f9-407f-9a32-b37c96b7c9f1",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.",
        "table_column_names": [
            "Metric",
            "Method of validation",
            "Yelp",
            "Lit."
        ],
        "table_content_values": [
            [
                "Acc",
                "% of machine and human judgments that match",
                "94",
                "84"
            ],
            [
                "Sim",
                "Spearman\u2019s  [ITALIC] \u03c1 b/w Sim and human ratings of semantic preservation",
                "0.79",
                "0.75"
            ],
            [
                "PP",
                "Spearman\u2019s  [ITALIC] \u03c1 b/w negative PP and human ratings of fluency",
                "0.81",
                "0.67"
            ]
        ],
        "question": "Is it true that [CONTINUE] To validate Acc, human annotators were asked to judge the style of 100 transferred sentences [CONTINUE] We then compute the percentage of machine and human judgments that match?",
        "answer_label": "yes"
    },
    {
        "id": "e8f0310f-c946-4b95-88b3-d257d8ea56f7",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Type",
            "[BOLD] English-German #P",
            "[BOLD] English-German B",
            "[BOLD] English-German C",
            "[BOLD] English-Czech #P",
            "[BOLD] English-Czech B",
            "[BOLD] English-Czech C"
        ],
        "table_content_values": [
            [
                "BoW+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "12.2",
                "-",
                "-",
                "7.5",
                "-"
            ],
            [
                "CNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "13.7",
                "-",
                "-",
                "8.7",
                "-"
            ],
            [
                "BiRNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "16.1",
                "-",
                "-",
                "9.6",
                "-"
            ],
            [
                "PB-SMT (Beck et al.,  2018 )",
                "Single",
                "-",
                "12.8",
                "43.2",
                "-",
                "8.6",
                "36.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Single",
                "41.4M",
                "15.5",
                "40.8",
                "39.1M",
                "8.9",
                "33.8"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Single",
                "41.2M",
                "16.7",
                "42.4",
                "38.8M",
                "9.8",
                "33.3"
            ],
            [
                "DCGCN (ours)",
                "Single",
                "[BOLD]  29.7M",
                "[BOLD] 19.0",
                "[BOLD] 44.1",
                "[BOLD]  28.3M",
                "[BOLD] 12.1",
                "[BOLD] 37.1"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Ensemble",
                "207M",
                "19.0",
                "44.1",
                "195M",
                "11.3",
                "36.4"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Ensemble",
                "206M",
                "19.6",
                "45.1",
                "194M",
                "11.7",
                "35.9"
            ],
            [
                "DCGCN (ours)",
                "Ensemble",
                "[BOLD]  149M",
                "[BOLD] 20.5",
                "[BOLD] 45.8",
                "[BOLD]  142M",
                "[BOLD] 13.1",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that Additionally, the ensemble DCGCN models achieve 20.5 and 13.1 BLEU points on the En-De and En-Cs tasks, respectively?",
        "answer_label": "yes"
    },
    {
        "id": "a84bc7ed-7512-4465-8dc1-256e680d2065",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE VIII: Precision scores for the Semantic Analogy Test",
        "table_column_names": [
            "Questions Subset",
            "# of Questions Seen",
            "GloVe",
            "Word2Vec",
            "Proposed"
        ],
        "table_content_values": [
            [
                "All",
                "8783",
                "78.94",
                "81.03",
                "79.96"
            ],
            [
                "At least one",
                "1635",
                "67.58",
                "70.89",
                "67.89"
            ],
            [
                "concept word",
                "1635",
                "67.58",
                "70.89",
                "67.89"
            ],
            [
                "All concept words",
                "110",
                "77.27",
                "89.09",
                "83.64"
            ]
        ],
        "question": "Is it true that However, the greatest performance increase is seen for the last scenario, which underscores the extent to which the semantic features captured by embeddings can be improved with a reasonable selection of the lexical resource from which the concept wordgroups were derived?",
        "answer_label": "yes"
    },
    {
        "id": "401dfb4f-8750-4c42-bc8d-b0d5942f798f",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "Base ACC",
            "Base Time",
            "+LN ACC",
            "+LN Time",
            "+BERT ACC",
            "+BERT Time",
            "+LN+BERT ACC",
            "+LN+BERT Time"
        ],
        "table_content_values": [
            [
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "250K",
                "83.50",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-"
            ],
            [
                "This",
                "LSTM",
                "8.36M",
                "84.27",
                "0.262",
                "86.03",
                "0.432",
                "89.95",
                "0.544",
                "[BOLD] 90.49",
                "0.696"
            ],
            [
                "This",
                "GRU",
                "6.41M",
                "[BOLD] 85.71",
                "0.245",
                "[BOLD] 86.05",
                "0.419",
                "[BOLD] 90.29",
                "0.529",
                "90.10",
                "0.695"
            ],
            [
                "This",
                "ATR",
                "2.87M",
                "84.88",
                "0.210",
                "85.81",
                "0.307",
                "90.00",
                "0.494",
                "90.28",
                "0.580"
            ],
            [
                "Work",
                "SRU",
                "5.48M",
                "84.28",
                "0.258",
                "85.32",
                "0.283",
                "89.98",
                "0.543",
                "90.09",
                "0.555"
            ],
            [
                "[EMPTY]",
                "LRN",
                "4.25M",
                "84.88",
                "[BOLD] 0.209",
                "85.06",
                "[BOLD] 0.223",
                "89.98",
                "[BOLD] 0.488",
                "89.93",
                "[BOLD] 0.506"
            ]
        ],
        "question": "Is it true that Overall results show that LRN achieves competitive performance but consumes the least training time?",
        "answer_label": "yes"
    },
    {
        "id": "f7a0d205-c70c-45a4-ad95-ee004bb48b14",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that Interestingly, G2S-GGNN has better performance among our models?",
        "answer_label": "yes"
    },
    {
        "id": "c0e96242-c3ea-48c3-a932-693d83be5c5c",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that [CONTINUE] Surprisingly, GDPL even outperforms human in completing the task, and its average dialog turns are close to those of humans, though GDPL is inferior in terms of match rate?",
        "answer_label": "yes"
    },
    {
        "id": "922444a0-578f-4b72-a5f4-e457f6e26693",
        "table_caption": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] ROUGE-1  [BOLD] R (%)",
            "[BOLD] ROUGE-1  [BOLD] P (%)",
            "[BOLD] ROUGE-1  [BOLD] F (%)",
            "[BOLD] ROUGE-2  [BOLD] R (%)",
            "[BOLD] ROUGE-2  [BOLD] P (%)",
            "[BOLD] ROUGE-2  [BOLD] F (%)",
            "[BOLD] Sentence-Level  [BOLD] R (%)",
            "[BOLD] Sentence-Level  [BOLD] P (%)",
            "[BOLD] Sentence-Level  [BOLD] F (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] ILP",
                "24.5",
                "41.1",
                "29.3\u00b10.5",
                "7.9",
                "15.0",
                "9.9\u00b10.5",
                "13.6",
                "22.6",
                "15.6\u00b10.4"
            ],
            [
                "[BOLD] Sum-Basic",
                "28.4",
                "44.4",
                "33.1\u00b10.5",
                "8.5",
                "15.6",
                "10.4\u00b10.4",
                "14.7",
                "22.9",
                "16.7\u00b10.5"
            ],
            [
                "[BOLD] KL-Sum",
                "39.5",
                "34.6",
                "35.5\u00b10.5",
                "13.0",
                "12.7",
                "12.3\u00b10.5",
                "15.2",
                "21.1",
                "16.3\u00b10.5"
            ],
            [
                "[BOLD] LexRank",
                "42.1",
                "39.5",
                "38.7\u00b10.5",
                "14.7",
                "15.3",
                "14.2\u00b10.5",
                "14.3",
                "21.5",
                "16.0\u00b10.5"
            ],
            [
                "[BOLD] MEAD",
                "45.5",
                "36.5",
                "38.5\u00b1 0.5",
                "17.9",
                "14.9",
                "15.4\u00b10.5",
                "27.8",
                "29.2",
                "26.8\u00b10.5"
            ],
            [
                "[BOLD] SVM",
                "19.0",
                "48.8",
                "24.7\u00b10.8",
                "7.5",
                "21.1",
                "10.0\u00b10.5",
                "32.7",
                "34.3",
                "31.4\u00b10.4"
            ],
            [
                "[BOLD] LogReg",
                "26.9",
                "34.5",
                "28.7\u00b10.6",
                "6.4",
                "9.9",
                "7.3\u00b10.4",
                "12.2",
                "14.9",
                "12.7\u00b10.5"
            ],
            [
                "[BOLD] LogReg [ITALIC] r",
                "28.0",
                "34.8",
                "29.4\u00b10.6",
                "6.9",
                "10.4",
                "7.8\u00b10.4",
                "12.1",
                "14.5",
                "12.5\u00b10.5"
            ],
            [
                "[BOLD] HAN",
                "31.0",
                "42.8",
                "33.7\u00b10.7",
                "11.2",
                "17.8",
                "12.7\u00b10.5",
                "26.9",
                "34.1",
                "32.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT",
                "32.2",
                "42.4",
                "34.4\u00b10.7",
                "11.5",
                "17.5",
                "12.9\u00b10.5",
                "29.6",
                "35.8",
                "32.2\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU",
                "32.1",
                "42.1",
                "33.8\u00b10.7",
                "11.6",
                "17.6",
                "12.9\u00b10.5",
                "30.1",
                "35.6",
                "32.3\u00b10.5"
            ],
            [
                "[BOLD] HAN [ITALIC] r",
                "38.1",
                "40.5",
                "[BOLD] 37.8\u00b10.5",
                "14.0",
                "17.1",
                "[BOLD] 14.7\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.5",
                "16.8",
                "[BOLD] 14.4\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.6",
                "16.9",
                "[BOLD] 14.4\u00b10.5",
                "33.9",
                "33.8",
                "[BOLD] 33.8\u00b10.5"
            ]
        ],
        "question": "Is it true that HAN models outperform both LogReg and SVM using the current set of features?",
        "answer_label": "yes"
    },
    {
        "id": "11b3b856-eb9c-4249-a21b-9c084802ad70",
        "table_caption": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance Table 1: Absolute Pearson correlations with segment-level human judgments in 7 language pairs on WMT17 dataset.",
        "table_column_names": [
            "Setting",
            "Metrics",
            "<bold>Direct Assessment</bold> cs-en",
            "<bold>Direct Assessment</bold> de-en",
            "<bold>Direct Assessment</bold> fi-en",
            "<bold>Direct Assessment</bold> lv-en",
            "<bold>Direct Assessment</bold> ru-en",
            "<bold>Direct Assessment</bold> tr-en",
            "<bold>Direct Assessment</bold> zh-en",
            "<bold>Direct Assessment</bold> Average"
        ],
        "table_content_values": [
            [
                "Baselines",
                "METEOR++",
                "0.552",
                "0.538",
                "0.720",
                "0.563",
                "0.627",
                "0.626",
                "0.646",
                "0.610"
            ],
            [
                "Baselines",
                "RUSE(*)",
                "0.624",
                "0.644",
                "0.750",
                "0.697",
                "0.673",
                "0.716",
                "0.691",
                "0.685"
            ],
            [
                "Baselines",
                "BERTScore-F1",
                "0.670",
                "0.686",
                "0.820",
                "0.710",
                "0.729",
                "0.714",
                "0.704",
                "0.719"
            ],
            [
                "Sent-Mover",
                "Smd + W2V",
                "0.438",
                "0.505",
                "0.540",
                "0.442",
                "0.514",
                "0.456",
                "0.494",
                "0.484"
            ],
            [
                "Sent-Mover",
                "Smd + ELMO + PMeans",
                "0.569",
                "0.558",
                "0.732",
                "0.525",
                "0.581",
                "0.620",
                "0.584",
                "0.595"
            ],
            [
                "Sent-Mover",
                "Smd + BERT + PMeans",
                "0.607",
                "0.623",
                "0.770",
                "0.639",
                "0.667",
                "0.641",
                "0.619",
                "0.652"
            ],
            [
                "Sent-Mover",
                "Smd + BERT + MNLI + PMeans",
                "0.616",
                "0.643",
                "0.785",
                "0.660",
                "0.664",
                "0.668",
                "0.633",
                "0.667"
            ],
            [
                "Word-Mover",
                "Wmd-1 + W2V",
                "0.392",
                "0.463",
                "0.558",
                "0.463",
                "0.456",
                "0.485",
                "0.481",
                "0.471"
            ],
            [
                "Word-Mover",
                "Wmd-1 + ELMO + PMeans",
                "0.579",
                "0.588",
                "0.753",
                "0.559",
                "0.617",
                "0.679",
                "0.645",
                "0.631"
            ],
            [
                "Word-Mover",
                "Wmd-1 + BERT + PMeans",
                "0.662",
                "0.687",
                "0.823",
                "0.714",
                "0.735",
                "0.734",
                "0.719",
                "0.725"
            ],
            [
                "Word-Mover",
                "Wmd-1 + BERT + MNLI + PMeans",
                "0.670",
                "0.708",
                "<bold>0.835</bold>",
                "<bold>0.746</bold>",
                "<bold>0.738</bold>",
                "0.762",
                "<bold>0.744</bold>",
                "<bold>0.743</bold>"
            ],
            [
                "Word-Mover",
                "Wmd-2 + BERT + MNLI + PMeans",
                "<bold>0.679</bold>",
                "<bold>0.710</bold>",
                "0.832",
                "0.745",
                "0.736",
                "<bold>0.763</bold>",
                "0.740",
                "<bold>0.743</bold>"
            ]
        ],
        "question": "Is it true that Table 1: In all language pairs, the best correlation is not achieved by our word mover metrics that use a BERT pretrained on MNLI as the embedding generator and PMeans to aggregate the embeddings from different BERT layers, i.e., WMD-1/2+BERT+MNLI+PMeans?",
        "answer_label": "no"
    },
    {
        "id": "b8a62c5c-3087-4351-a5fd-9c2882dcdd08",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE VII: Precision scores for the Analogy Test",
        "table_column_names": [
            "Methods",
            "# dims",
            "Analg. (sem)",
            "Analg. (syn)",
            "Total"
        ],
        "table_content_values": [
            [
                "GloVe",
                "300",
                "78.94",
                "64.12",
                "70.99"
            ],
            [
                "Word2Vec",
                "300",
                "81.03",
                "66.11",
                "73.03"
            ],
            [
                "OIWE-IPG",
                "300",
                "19.99",
                "23.44",
                "21.84"
            ],
            [
                "SOV",
                "3000",
                "64.09",
                "46.26",
                "54.53"
            ],
            [
                "SPINE",
                "1000",
                "17.07",
                "8.68",
                "12.57"
            ],
            [
                "Word2Sense",
                "2250",
                "12.94",
                "19.44",
                "5.84"
            ],
            [
                "Proposed",
                "300",
                "79.96",
                "63.52",
                "71.15"
            ]
        ],
        "question": "Is it true that Our proposed method does not outperform GloVe in semantic analogy test set and in overall results, while GloVe performs slightly better in syntactic test set?",
        "answer_label": "no"
    },
    {
        "id": "6d7e29b8-084e-4ad5-8c41-e8975a7ddf6b",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.",
        "table_column_names": [
            "[BOLD] Decoder configuration",
            "[BOLD] es-en  [BOLD] Health",
            "[BOLD] es-en  [BOLD] Bio",
            "[BOLD] en-de  [BOLD] News",
            "[BOLD] en-de  [BOLD] TED",
            "[BOLD] en-de  [BOLD] IT"
        ],
        "table_content_values": [
            [
                "Oracle model",
                "35.9",
                "36.1",
                "37.8",
                "24.1",
                "39.6"
            ],
            [
                "Uniform",
                "33.1",
                "36.4",
                "21.9",
                "18.4",
                "38.9"
            ],
            [
                "Identity-BI",
                "35.0",
                "36.6",
                "32.7",
                "25.3",
                "42.6"
            ],
            [
                "BI",
                "35.9",
                "36.5",
                "38.0",
                "26.1",
                "[BOLD] 44.7"
            ],
            [
                "IS",
                "[BOLD] 36.0",
                "36.8",
                "37.5",
                "25.6",
                "43.3"
            ],
            [
                "BI + IS",
                "[BOLD] 36.0",
                "[BOLD] 36.9",
                "[BOLD] 38.4",
                "[BOLD] 26.4",
                "[BOLD] 44.7"
            ]
        ],
        "question": "Is it true that BI and IS both individually outperform the oracle for all domains, [CONTINUE] With adaptive decoding, we can assume that a uniform ensemble will always perform better than a single model for any potentially unknown domain?",
        "answer_label": "no"
    },
    {
        "id": "3d29f63f-a0e4-422c-8299-a1e9ebd041b4",
        "table_caption": "Improving Generalization by Incorporating Coverage in Natural Language Inference Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
        "table_column_names": [
            "[EMPTY]",
            "in-domain MultiNLI",
            "out-of-domain SNLI",
            "out-of-domain Glockner",
            "out-of-domain SICK"
        ],
        "table_content_values": [
            [
                "MQAN",
                "72.30",
                "60.91",
                "41.82",
                "53.95"
            ],
            [
                "+ coverage",
                "<bold>73.84</bold>",
                "<bold>65.38</bold>",
                "<bold>78.69</bold>",
                "<bold>54.55</bold>"
            ],
            [
                "ESIM (ELMO)",
                "80.04",
                "68.70",
                "60.21",
                "51.37"
            ],
            [
                "+ coverage",
                "<bold>80.38</bold>",
                "<bold>70.05</bold>",
                "<bold>67.47</bold>",
                "<bold>52.65</bold>"
            ]
        ],
        "question": "Is it true that The results show that coverage information does not improve the generalization of both examined models across various NLI datasets?",
        "answer_label": "no"
    },
    {
        "id": "bcd7aad2-e4b4-4978-ad9e-8aee69123d1c",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
        "table_column_names": [
            "[BOLD] Emoji alias",
            "[BOLD] N",
            "[BOLD] emoji #",
            "[BOLD] emoji %",
            "[BOLD] no-emoji #",
            "[BOLD] no-emoji %",
            "[BOLD] \u0394%"
        ],
        "table_content_values": [
            [
                "mask",
                "163",
                "154",
                "94.48",
                "134",
                "82.21",
                "- 12.27"
            ],
            [
                "two_hearts",
                "87",
                "81",
                "93.10",
                "77",
                "88.51",
                "- 4.59"
            ],
            [
                "heart_eyes",
                "122",
                "109",
                "89.34",
                "103",
                "84.43",
                "- 4.91"
            ],
            [
                "heart",
                "267",
                "237",
                "88.76",
                "235",
                "88.01",
                "- 0.75"
            ],
            [
                "rage",
                "92",
                "78",
                "84.78",
                "66",
                "71.74",
                "- 13.04"
            ],
            [
                "cry",
                "116",
                "97",
                "83.62",
                "83",
                "71.55",
                "- 12.07"
            ],
            [
                "sob",
                "490",
                "363",
                "74.08",
                "345",
                "70.41",
                "- 3.67"
            ],
            [
                "unamused",
                "167",
                "121",
                "72.46",
                "116",
                "69.46",
                "- 3.00"
            ],
            [
                "weary",
                "204",
                "140",
                "68.63",
                "139",
                "68.14",
                "- 0.49"
            ],
            [
                "joy",
                "978",
                "649",
                "66.36",
                "629",
                "64.31",
                "- 2.05"
            ],
            [
                "sweat_smile",
                "111",
                "73",
                "65.77",
                "75",
                "67.57",
                "1.80"
            ],
            [
                "confused",
                "77",
                "46",
                "59.74",
                "48",
                "62.34",
                "2.60"
            ]
        ],
        "question": "Is it true that [CONTINUE] Further, contrary to intuition, the sob emoji contributes less than cry, despite representing a stronger emotion?",
        "answer_label": "yes"
    },
    {
        "id": "b8dd43d6-532b-40e2-8758-5c8e8fd69a02",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Method",
            "SUBJ",
            "CR",
            "MR",
            "MPQA",
            "MRPC",
            "TREC",
            "SICK-E",
            "SST2",
            "SST5",
            "STS-B",
            "SICK-R"
        ],
        "table_content_values": [
            [
                "CBOW/784",
                "90.0",
                "[BOLD] 79.2",
                "[BOLD] 74.0",
                "87.1",
                "71.6",
                "85.6",
                "78.9",
                "78.5",
                "42.1",
                "61.0",
                "[BOLD] 78.1"
            ],
            [
                "CMOW/784",
                "87.5",
                "73.4",
                "70.6",
                "[BOLD] 87.3",
                "69.6",
                "[BOLD] 88.0",
                "77.2",
                "74.7",
                "37.9",
                "56.5",
                "76.2"
            ],
            [
                "Hybrid",
                "[BOLD] 90.2",
                "78.7",
                "73.7",
                "[BOLD] 87.3",
                "[BOLD] 72.7",
                "87.6",
                "[BOLD] 79.4",
                "[BOLD] 79.6",
                "[BOLD] 43.3",
                "[BOLD] 63.4",
                "77.8"
            ],
            [
                "cmp. CBOW",
                "+0.2%",
                "-0.6%",
                "-0.4%",
                "+0.2%",
                "+1.5%",
                "+2.3%",
                "+0.6%",
                "+1.4%",
                "+2.9%",
                "+3.9%",
                "-0.4%"
            ],
            [
                "cmp. CMOW",
                "+3.1%",
                "+7.2%",
                "+4.4%",
                "+0%",
                "+4.5%",
                "-0.5%",
                "+2.9%",
                "+6.7%",
                "+14.3",
                "+12.2%",
                "+2.1%"
            ]
        ],
        "question": "Is it true that However, CMOW generally outperforms CBOW embeddings?",
        "answer_label": "no"
    },
    {
        "id": "89a48433-b8ad-489b-a96c-267e8f760ad5",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Original",
                "TGen\u2212",
                "63.37",
                "7.7188",
                "41.99",
                "68.53",
                "1.9355",
                "00.06",
                "15.77",
                "00.11",
                "15.94"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen",
                "66.41",
                "8.5565",
                "45.07",
                "69.17",
                "2.2253",
                "00.14",
                "04.11",
                "00.03",
                "04.27"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen+",
                "67.06",
                "8.5871",
                "45.83",
                "69.73",
                "2.2681",
                "00.04",
                "01.75",
                "00.01",
                "01.80"
            ],
            [
                "Original",
                "[BOLD] Original",
                "SC-LSTM",
                "39.11",
                "5.6704",
                "36.83",
                "50.02",
                "0.6045",
                "02.79",
                "18.90",
                "09.79",
                "31.51"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen\u2212",
                "65.87",
                "8.6400",
                "44.20",
                "67.51",
                "2.1710",
                "00.20",
                "00.56",
                "00.21",
                "00.97"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen",
                "66.24",
                "8.6889",
                "44.66",
                "67.85",
                "2.2181",
                "00.10",
                "00.02",
                "00.00",
                "00.12"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen+",
                "65.97",
                "8.6630",
                "44.45",
                "67.59",
                "2.1855",
                "00.02",
                "00.00",
                "00.00",
                "00.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "SC-LSTM",
                "38.52",
                "5.7125",
                "37.45",
                "48.50",
                "0.4343",
                "03.85",
                "17.39",
                "08.12",
                "29.37"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen\u2212",
                "66.28",
                "8.5202",
                "43.96",
                "67.83",
                "2.1375",
                "00.14",
                "02.26",
                "00.22",
                "02.61"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen",
                "67.00",
                "8.6889",
                "44.97",
                "68.19",
                "2.2228",
                "00.06",
                "00.44",
                "00.03",
                "00.53"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen+",
                "66.74",
                "8.6649",
                "44.84",
                "67.95",
                "2.2018",
                "00.00",
                "00.21",
                "00.03",
                "00.24"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen\u2212",
                "64.40",
                "7.9692",
                "42.81",
                "68.87",
                "2.0563",
                "00.01",
                "13.08",
                "00.00",
                "13.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen",
                "66.23",
                "8.5578",
                "45.12",
                "68.87",
                "2.2548",
                "00.04",
                "03.04",
                "00.00",
                "03.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen+",
                "65.96",
                "8.5238",
                "45.49",
                "68.79",
                "2.2456",
                "00.00",
                "01.44",
                "00.00",
                "01.45"
            ]
        ],
        "question": "Is it true that The results in the table suggest that cleaning the missing slots did not provide more complex training examples?",
        "answer_label": "no"
    },
    {
        "id": "e2df8ba1-ef82-4c6d-9407-a8cd9c22a003",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 1: Benchmark performance, Spearman\u2019s \u03c1. SGNS results with * taken from [morphfit]. Best results per column (benchmark) annotated for our setup only.",
        "table_column_names": [
            "Context: w2",
            "Context: w2 SimLex",
            "Context: w2 SimLex",
            "Context: w2 SimLex",
            "Context: w2 SimLex",
            "Context: w2 SimVerb"
        ],
        "table_content_values": [
            [
                "target",
                "N",
                "V",
                "A",
                "all",
                "V"
            ],
            [
                "type",
                ".334",
                "<bold>.336</bold>",
                "<bold>.518</bold>",
                ".348",
                ".307"
            ],
            [
                "x + POS",
                ".342",
                ".323",
                ".513",
                ".350",
                ".279"
            ],
            [
                "lemma",
                "<bold>.362</bold>",
                ".333",
                ".497",
                "<bold>.351</bold>",
                ".400"
            ],
            [
                "x + POS",
                ".354",
                "<bold>.336</bold>",
                ".504",
                ".345",
                "<bold>.406</bold>"
            ],
            [
                "* type",
                "-",
                "-",
                "-",
                ".339",
                ".277"
            ],
            [
                "* type MFit-A",
                "-",
                "-",
                "-",
                ".385",
                "-"
            ],
            [
                "* type MFit-AR",
                "-",
                "-",
                "-",
                ".439",
                ".381"
            ],
            [
                "Context: dep-W",
                "Context: dep-W",
                "Context: dep-W",
                "Context: dep-W",
                "Context: dep-W",
                "Context: dep-W"
            ],
            [
                "type",
                ".366",
                ".365",
                ".489",
                ".362",
                ".314"
            ],
            [
                "x + POS",
                ".364",
                ".351",
                ".482",
                ".359",
                ".287"
            ],
            [
                "lemma",
                "<bold>.391</bold>",
                ".380",
                "<bold>.522</bold>",
                "<bold>.379</bold>",
                ".401"
            ],
            [
                "x + POS",
                ".384",
                "<bold>.388</bold>",
                ".480",
                ".366",
                "<bold>.431</bold>"
            ],
            [
                "* type",
                "-",
                "-",
                "-",
                ".376",
                ".313"
            ],
            [
                "* type MFit-AR",
                "-",
                "-",
                "-",
                ".434",
                ".418"
            ]
        ],
        "question": "Is it true that [CONTINUE] Lemmatized targets generally perform better, with the boost being more pronounced on SimVerb?",
        "answer_label": "yes"
    },
    {
        "id": "ed79ce87-cdaa-4117-94e3-e9003fdbdb66",
        "table_caption": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] ROUGE-1  [BOLD] R (%)",
            "[BOLD] ROUGE-1  [BOLD] P (%)",
            "[BOLD] ROUGE-1  [BOLD] F (%)",
            "[BOLD] ROUGE-2  [BOLD] R (%)",
            "[BOLD] ROUGE-2  [BOLD] P (%)",
            "[BOLD] ROUGE-2  [BOLD] F (%)",
            "[BOLD] Sentence-Level  [BOLD] R (%)",
            "[BOLD] Sentence-Level  [BOLD] P (%)",
            "[BOLD] Sentence-Level  [BOLD] F (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] ILP",
                "24.5",
                "41.1",
                "29.3\u00b10.5",
                "7.9",
                "15.0",
                "9.9\u00b10.5",
                "13.6",
                "22.6",
                "15.6\u00b10.4"
            ],
            [
                "[BOLD] Sum-Basic",
                "28.4",
                "44.4",
                "33.1\u00b10.5",
                "8.5",
                "15.6",
                "10.4\u00b10.4",
                "14.7",
                "22.9",
                "16.7\u00b10.5"
            ],
            [
                "[BOLD] KL-Sum",
                "39.5",
                "34.6",
                "35.5\u00b10.5",
                "13.0",
                "12.7",
                "12.3\u00b10.5",
                "15.2",
                "21.1",
                "16.3\u00b10.5"
            ],
            [
                "[BOLD] LexRank",
                "42.1",
                "39.5",
                "38.7\u00b10.5",
                "14.7",
                "15.3",
                "14.2\u00b10.5",
                "14.3",
                "21.5",
                "16.0\u00b10.5"
            ],
            [
                "[BOLD] MEAD",
                "45.5",
                "36.5",
                "38.5\u00b1 0.5",
                "17.9",
                "14.9",
                "15.4\u00b10.5",
                "27.8",
                "29.2",
                "26.8\u00b10.5"
            ],
            [
                "[BOLD] SVM",
                "19.0",
                "48.8",
                "24.7\u00b10.8",
                "7.5",
                "21.1",
                "10.0\u00b10.5",
                "32.7",
                "34.3",
                "31.4\u00b10.4"
            ],
            [
                "[BOLD] LogReg",
                "26.9",
                "34.5",
                "28.7\u00b10.6",
                "6.4",
                "9.9",
                "7.3\u00b10.4",
                "12.2",
                "14.9",
                "12.7\u00b10.5"
            ],
            [
                "[BOLD] LogReg [ITALIC] r",
                "28.0",
                "34.8",
                "29.4\u00b10.6",
                "6.9",
                "10.4",
                "7.8\u00b10.4",
                "12.1",
                "14.5",
                "12.5\u00b10.5"
            ],
            [
                "[BOLD] HAN",
                "31.0",
                "42.8",
                "33.7\u00b10.7",
                "11.2",
                "17.8",
                "12.7\u00b10.5",
                "26.9",
                "34.1",
                "32.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT",
                "32.2",
                "42.4",
                "34.4\u00b10.7",
                "11.5",
                "17.5",
                "12.9\u00b10.5",
                "29.6",
                "35.8",
                "32.2\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU",
                "32.1",
                "42.1",
                "33.8\u00b10.7",
                "11.6",
                "17.6",
                "12.9\u00b10.5",
                "30.1",
                "35.6",
                "32.3\u00b10.5"
            ],
            [
                "[BOLD] HAN [ITALIC] r",
                "38.1",
                "40.5",
                "[BOLD] 37.8\u00b10.5",
                "14.0",
                "17.1",
                "[BOLD] 14.7\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.5",
                "16.8",
                "[BOLD] 14.4\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.6",
                "16.9",
                "[BOLD] 14.4\u00b10.5",
                "33.9",
                "33.8",
                "[BOLD] 33.8\u00b10.5"
            ]
        ],
        "question": "Is it true that When redundancy removal was applied to LogReg, it produces significant improvement?",
        "answer_label": "no"
    },
    {
        "id": "4e7fe7a0-0c7a-4e48-b4e9-0aaa1d9a2c28",
        "table_caption": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. \u201cRaw\u201d indicates the usage of original prosodic features (Section 3.2), while \u201cinnovations\u201d indicate the usage of innovation features (Section 3.3).",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Model",
            "[BOLD] dev mean",
            "[BOLD] dev best",
            "[BOLD] test mean",
            "[BOLD] test best",
            "[ITALIC] \u03b1"
        ],
        "table_content_values": [
            [
                "single",
                "text",
                "86.54",
                "86.80",
                "86.47",
                "86.96",
                "\u2013"
            ],
            [
                "single",
                "raw",
                "35.00",
                "37.33",
                "35.78",
                "37.70",
                "\u2013"
            ],
            [
                "single",
                "innovations",
                "80.86",
                "81.51",
                "80.28",
                "82.15",
                "\u2013"
            ],
            [
                "early",
                "text + raw",
                "86.46",
                "86.65",
                "86.24",
                "86.53",
                "\u2013"
            ],
            [
                "early",
                "text + innovations",
                "86.53",
                "86.77",
                "86.54",
                "87.00",
                "\u2013"
            ],
            [
                "early",
                "text + raw + innovations",
                "86.35",
                "86.69",
                "86.55",
                "86.44",
                "\u2013"
            ],
            [
                "late",
                "text + raw",
                "86.71",
                "87.05",
                "86.35",
                "86.71",
                "0.2"
            ],
            [
                "late",
                "text + innovations",
                "[BOLD] 86.98",
                "[BOLD] 87.48",
                "[BOLD] 86.68",
                "[BOLD] 87.02",
                "0.5"
            ],
            [
                "late",
                "text + raw + innovations",
                "86.95",
                "87.30",
                "86.60",
                "86.87",
                "0.5"
            ]
        ],
        "question": "Is it true that We found that innovations are not helpful in both early and late fusion frameworks, and late fusion does not perform better on average?",
        "answer_label": "no"
    },
    {
        "id": "8871d76b-c023-4dac-9ec0-866526d5cbed",
        "table_caption": "Evaluation of Greek Word Embeddings Table 3: Summary for 3CosAdd and top-1 nearest vectors.",
        "table_column_names": [
            "Category Semantic",
            "Category no oov words",
            "gr_def 58.42%",
            "gr_neg10 59.33%",
            "cc.el.300  [BOLD] 68.80%",
            "wiki.el 27.20%",
            "gr_cbow_def 31.76%",
            "gr_d300_nosub 60.79%",
            "gr_w2v_sg_n5 52.70%"
        ],
        "table_content_values": [
            [
                "[EMPTY]",
                "with oov words",
                "52.97%",
                "55.33%",
                "[BOLD] 64.34%",
                "25.73%",
                "28.80%",
                "55.11%",
                "47.82%"
            ],
            [
                "Syntactic",
                "no oov words",
                "65.73%",
                "61.02%",
                "[BOLD] 69.35%",
                "40.90%",
                "64.02%",
                "53.69%",
                "52.60%"
            ],
            [
                "[EMPTY]",
                "with oov words",
                "[BOLD] 53.95%",
                "48.69%",
                "49.43%",
                "28.42%",
                "52.54%",
                "44.06%",
                "43.13%"
            ],
            [
                "Overall",
                "no oov words",
                "63.02%",
                "59.96%",
                "[BOLD] 68.97%",
                "36.45%",
                "52.04%",
                "56.30%",
                "52.66%"
            ],
            [
                "[EMPTY]",
                "with oov words",
                "53.60%",
                "51.00%",
                "[BOLD] 54.60%",
                "27.50%",
                "44.30%",
                "47.90%",
                "44.80%"
            ]
        ],
        "question": "Is it true that Considering the two aggregated categories of syntactic and semantic word analogies respectively and both 3CosAdd and 3CosMul metrics, model cc.el.300 has outperformed all the other models apart from the case of the Syntactic category when we included the out-of-vocabulary (oov) terms [CONTINUE] where the model gr def had the best performance?",
        "answer_label": "yes"
    },
    {
        "id": "cdad25c1-2680-41e3-b279-9383fc241c09",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
        "table_column_names": [
            "[EMPTY]",
            "Prec.",
            "Rec.",
            "F1"
        ],
        "table_content_values": [
            [
                "(A1) BiLSTM-CNN",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "(A2) Standard attention",
                "0.466",
                "0.638",
                "0.539"
            ],
            [
                "(A3) Window size ( [ITALIC] ws)=5",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "(A4) Window size ( [ITALIC] ws)=10",
                "0.510",
                "0.640",
                "0.568"
            ],
            [
                "(A5) Softmax",
                "0.490",
                "0.658",
                "0.562"
            ],
            [
                "(A6) Max-pool",
                "0.492",
                "0.600",
                "0.541"
            ]
        ],
        "question": "Is it true that Replacing the attention normalizing function with softmax operation increases the F1 score marginally (A3\u2212A5)?",
        "answer_label": "no"
    },
    {
        "id": "3bdf91f0-992d-410b-9583-2f70056ed281",
        "table_caption": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.",
        "table_column_names": [
            "[EMPTY]",
            "dev perp \u2193",
            "dev acc \u2191",
            "dev wer \u2193",
            "test perp \u2193",
            "test acc \u2191",
            "test wer \u2193"
        ],
        "table_content_values": [
            [
                "Spanish-only-LM",
                "329.68",
                "26.6",
                "30.47",
                "322.26",
                "25.1",
                "29.62"
            ],
            [
                "English-only-LM",
                "320.92",
                "29.3",
                "32.02",
                "314.04",
                "30.3",
                "32.51"
            ],
            [
                "All:CS-last-LM",
                "76.64",
                "47.8",
                "14.56",
                "76.97",
                "49.2",
                "14.13"
            ],
            [
                "All:Shuffled-LM",
                "68.00",
                "51.8",
                "13.64",
                "68.72",
                "51.4",
                "13.89"
            ],
            [
                "CS-only-LM",
                "43.20",
                "60.7",
                "12.60",
                "43.42",
                "57.9",
                "12.18"
            ],
            [
                "CS-only+vocab-LM",
                "45.61",
                "61.0",
                "12.56",
                "45.79",
                "58.8",
                "12.49"
            ],
            [
                "Fine-Tuned-LM",
                "39.76",
                "66.9",
                "10.71",
                "40.11",
                "65.4",
                "10.17"
            ],
            [
                "CS-only-disc",
                "\u2013",
                "72.0",
                "6.35",
                "\u2013",
                "70.5",
                "6.70"
            ],
            [
                "Fine-Tuned-disc",
                "\u2013",
                "[BOLD] 74.2",
                "[BOLD] 5.85",
                "\u2013",
                "[BOLD] 75.5",
                "[BOLD] 5.59"
            ]
        ],
        "question": "Is it true that For each model we report both perplexity and accuracy (except for discriminative training, where perplexity is not valid), where each of them is reported according to the best performing model on that measure (on the dev set)?",
        "answer_label": "yes"
    },
    {
        "id": "aa179cf6-fac1-48d2-8bcf-e5ad45f72d25",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 3: ARI and Silhouette coefficient scores.",
        "table_column_names": [
            "Methods",
            "Seanad Abolition ARI",
            "Seanad Abolition  [ITALIC] Sil",
            "Video Games ARI",
            "Video Games  [ITALIC] Sil",
            "Pornography ARI",
            "Pornography  [ITALIC] Sil"
        ],
        "table_content_values": [
            [
                "TF-IDF",
                "0.23",
                "0.02",
                "-0.01",
                "0.01",
                "-0.02",
                "0.01"
            ],
            [
                "WMD",
                "0.09",
                "0.01",
                "0.01",
                "0.01",
                "-0.02",
                "0.01"
            ],
            [
                "Sent2vec",
                "-0.01",
                "-0.01",
                "0.11",
                "0.06",
                "0.01",
                "0.02"
            ],
            [
                "Doc2vec",
                "-0.01",
                "-0.03",
                "-0.01",
                "0.01",
                "0.02",
                "-0.01"
            ],
            [
                "BERT",
                "0.03",
                "-0.04",
                "0.08",
                "0.05",
                "-0.01",
                "0.03"
            ],
            [
                "OD-parse",
                "0.01",
                "-0.04",
                "-0.01",
                "0.02",
                "0.07",
                "0.05"
            ],
            [
                "OD",
                "[BOLD] 0.54",
                "[BOLD] 0.31",
                "[BOLD] 0.56",
                "[BOLD] 0.42",
                "[BOLD] 0.41",
                "[BOLD] 0.41"
            ]
        ],
        "question": "Is it true that [CONTINUE] A notable exception is the \"Seanad Abolition\" dataset, where TF-IDF performs relatively better than WMD, Sent2vec and Doc2vec?",
        "answer_label": "yes"
    },
    {
        "id": "2b367697-ccca-45ce-a3ac-ef1ce5323dac",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Acc",
            "[BOLD] F1",
            "[BOLD] AUC"
        ],
        "table_content_values": [
            [
                "Most Frequent Class",
                "64.2",
                "39.1",
                "0.500"
            ],
            [
                "LR-All Features \u2013 Original Data",
                "80.5",
                "78.0",
                "0.873"
            ],
            [
                "Dist. Supervision + Pooling",
                "77.2",
                "75.7",
                "0.853"
            ],
            [
                "Dist. Supervision + EasyAdapt",
                "[BOLD] 81.2",
                "[BOLD] 79.0",
                "[BOLD] 0.885"
            ]
        ],
        "question": "Is it true that Results presented in Table 7 show that the domain adaptation approach does not significantly boost F1 (t-test, p>0.5) and ROC AUC (0.012)?",
        "answer_label": "no"
    },
    {
        "id": "058d2a40-7061-4dfa-bb41-bf00efd55f2c",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.",
        "table_column_names": [
            "[EMPTY]",
            "Italian Original",
            "Italian Debiased",
            "Italian English",
            "Italian Reduction",
            "German Original",
            "German Debiased",
            "German English",
            "German Reduction"
        ],
        "table_content_values": [
            [
                "Same Gender",
                "0.442",
                "0.434",
                "0.424",
                "\u2013",
                "0.491",
                "0.478",
                "0.446",
                "\u2013"
            ],
            [
                "Different Gender",
                "0.385",
                "0.421",
                "0.415",
                "\u2013",
                "0.415",
                "0.435",
                "0.403",
                "\u2013"
            ],
            [
                "difference",
                "0.057",
                "0.013",
                "0.009",
                "[BOLD] 91.67%",
                "0.076",
                "0.043",
                "0.043",
                "[BOLD] 100%"
            ]
        ],
        "question": "Is it true that In German, we get a reduction of 100%?",
        "answer_label": "yes"
    },
    {
        "id": "ca96fdb8-2600-46e2-b129-d6c81562945f",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "-{4} dense block",
                "24.8",
                "54.9"
            ],
            [
                "-{3, 4} dense blocks",
                "23.8",
                "54.1"
            ],
            [
                "-{2, 3, 4} dense blocks",
                "23.2",
                "53.1"
            ]
        ],
        "question": "Is it true that Although these four models have the same number of layers, dense connections allow the model to achieve much better performance?",
        "answer_label": "yes"
    },
    {
        "id": "87b2347d-2e19-4a25-bb66-a742f0e8ebfd",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
        "table_column_names": [
            "Model",
            "Training data",
            "Overall",
            "Easy",
            "Hard"
        ],
        "table_content_values": [
            [
                "BERT-large",
                "B-COPA",
                "70.5 (\u00b1 2.5)",
                "72.6 (\u00b1 2.3)",
                "[BOLD] 69.1 (\u00b1 2.7)"
            ],
            [
                "BERT-large",
                "B-COPA (50%)",
                "69.9 (\u00b1 1.9)",
                "71.2 (\u00b1 1.3)",
                "69.0 (\u00b1 3.5)"
            ],
            [
                "BERT-large",
                "COPA",
                "[BOLD] 71.7 (\u00b1 0.5)",
                "[BOLD] 80.5 (\u00b1 0.4)",
                "66.3 (\u00b1 0.8)"
            ],
            [
                "RoBERTa-large",
                "B-COPA",
                "[BOLD] 76.7 (\u00b1 0.8)",
                "73.3 (\u00b1 1.5)",
                "[BOLD] 78.8 (\u00b1 2.0)"
            ],
            [
                "RoBERTa-large",
                "B-COPA (50%)",
                "72.4 (\u00b1 2.0)",
                "72.1 (\u00b1 1.7)",
                "72.6 (\u00b1 2.1)"
            ],
            [
                "RoBERTa-large",
                "COPA",
                "76.4 (\u00b1 0.7)",
                "[BOLD] 79.6 (\u00b1 1.0)",
                "74.4 (\u00b1 1.1)"
            ],
            [
                "BERT-base-NSP",
                "None",
                "[BOLD] 66.4",
                "66.2",
                "[BOLD] 66.7"
            ],
            [
                "BERT-large-NSP",
                "None",
                "65.0",
                "[BOLD] 66.9",
                "62.1"
            ]
        ],
        "question": "Is it true that The relatively low accuracies of BERT-large, RoBERTa-large and BERT-*-NSP show that these pretrained models are not well-equipped to perform this task \"out-of-the-box\"?",
        "answer_label": "no"
    },
    {
        "id": "6943a94d-d91a-4fb2-965f-97aa1fa957ce",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that Longer sentences pose additional challenges to the models?",
        "answer_label": "yes"
    },
    {
        "id": "e8e28650-51c4-4fbe-b946-b7966b1625a2",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold\u2019s folding technique, and TensorFlow\u2019s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Training",
            "Throughput (instances/s) Training",
            "Throughput (instances/s) Training"
        ],
        "table_content_values": [
            [
                "Batch size",
                "Iter",
                "Recur",
                "Fold",
                "Iter",
                "Recur",
                "Fold"
            ],
            [
                "1",
                "19.2",
                "81.4",
                "16.5",
                "2.5",
                "4.8",
                "9.0"
            ],
            [
                "10",
                "49.3",
                "217.9",
                "52.2",
                "4.0",
                "4.2",
                "37.5"
            ],
            [
                "25",
                "72.1",
                "269.9",
                "61.6",
                "5.5",
                "3.6",
                "54.7"
            ]
        ],
        "question": "Is it true that [CONTINUE] As a result, the folding technique performs better than the recursive approach for the training task?",
        "answer_label": "yes"
    },
    {
        "id": "377ecb73-e548-4831-8bbb-d0c2edda5227",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that The largest gain is by 4% on the CoordInv task?",
        "answer_label": "no"
    },
    {
        "id": "755c48ed-ff95-42cf-9e30-670ae9546e4d",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that This is expected as encoding a bigger graph (containing more information) is harder than encoding smaller graphs?",
        "answer_label": "yes"
    },
    {
        "id": "1cd4c25a-774f-4e75-a511-1dead6a68155",
        "table_caption": "Sparse and Structured Visual Attention Table 2: Human evaluation results on MSCOCO.",
        "table_column_names": [
            "[EMPTY]",
            "caption",
            "attention relevance"
        ],
        "table_content_values": [
            [
                "softmax",
                "3.50",
                "3.38"
            ],
            [
                "sparsemax",
                "3.71",
                "3.89"
            ],
            [
                "TVmax",
                "[BOLD] 3.87",
                "[BOLD] 4.10"
            ]
        ],
        "question": "Is it true that The inferior score on attention relevance shows that TVMAX is worse at selecting the relevant features and its output is less interpretable?",
        "answer_label": "no"
    },
    {
        "id": "cd3bfb66-26db-4ea5-bbc7-33e9dd881d74",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.",
        "table_column_names": [
            "[ITALIC] Block",
            "[ITALIC] n",
            "[ITALIC] m",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "1",
                "1",
                "1",
                "17.6",
                "48.3"
            ],
            [
                "1",
                "1",
                "2",
                "19.2",
                "50.3"
            ],
            [
                "1",
                "2",
                "1",
                "18.4",
                "49.1"
            ],
            [
                "1",
                "1",
                "3",
                "19.6",
                "49.4"
            ],
            [
                "1",
                "3",
                "1",
                "20.0",
                "50.5"
            ],
            [
                "1",
                "3",
                "3",
                "21.4",
                "51.0"
            ],
            [
                "1",
                "3",
                "6",
                "21.8",
                "51.7"
            ],
            [
                "1",
                "6",
                "3",
                "21.7",
                "51.5"
            ],
            [
                "1",
                "6",
                "6",
                "22.0",
                "52.1"
            ],
            [
                "2",
                "3",
                "6",
                "[BOLD] 23.5",
                "53.3"
            ],
            [
                "2",
                "6",
                "3",
                "23.3",
                "[BOLD] 53.4"
            ],
            [
                "2",
                "6",
                "6",
                "22.0",
                "52.1"
            ]
        ],
        "question": "Is it true that We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give significantly different results for both 1 DCGCN block and 2 DCGCN blocks?",
        "answer_label": "no"
    },
    {
        "id": "fd328d46-3d4e-4a19-9b01-87e794fed8b9",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
        "table_column_names": [
            "Model",
            "Method",
            "Training Data",
            "Overall",
            "Easy",
            "Hard",
            "p-value (%)"
        ],
        "table_content_values": [
            [
                "goodwin-etal-2012-utdhlt",
                "PMI",
                "unsupervised",
                "61.8",
                "64.7",
                "60.0",
                "19.8"
            ],
            [
                "gordon_commonsense_2011-1",
                "PMI",
                "unsupervised",
                "65.4",
                "65.8",
                "65.2",
                "83.5"
            ],
            [
                "sasaki-etal-2017-handling",
                "PMI",
                "unsupervised",
                "71.4",
                "75.3",
                "69.0",
                "4.8\u2217"
            ],
            [
                "Word frequency",
                "wordfreq",
                "COPA",
                "53.5",
                "57.4",
                "51.3",
                "9.8"
            ],
            [
                "BERT-large-FT",
                "LM, NSP",
                "COPA",
                "76.5 (\u00b1 2.7)",
                "83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)",
                "0.0\u2217"
            ],
            [
                "RoBERTa-large-FT",
                "LM",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)",
                "0.0\u2217"
            ]
        ],
        "question": "Is it true that (2017).8 Overall both BERT (76.5%) and RoBERTa (87.7%) do not outperform the best previous model (71.4%) on Hard instances without superficial cues?",
        "answer_label": "no"
    },
    {
        "id": "1ea08ac9-44c5-4494-894b-728c4468c01b",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 1: Classifier performance",
        "table_column_names": [
            "Dataset",
            "Class",
            "Precision",
            "Recall",
            "F1"
        ],
        "table_content_values": [
            [
                "[ITALIC] W. & H.",
                "Racism",
                "0.73",
                "0.79",
                "0.76"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.69",
                "0.73",
                "0.71"
            ],
            [
                "[EMPTY]",
                "Neither",
                "0.88",
                "0.85",
                "0.86"
            ],
            [
                "[ITALIC] W.",
                "Racism",
                "0.56",
                "0.77",
                "0.65"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.62",
                "0.73",
                "0.67"
            ],
            [
                "[EMPTY]",
                "R. & S.",
                "0.56",
                "0.62",
                "0.59"
            ],
            [
                "[EMPTY]",
                "Neither",
                "0.95",
                "0.92",
                "0.94"
            ],
            [
                "[ITALIC] D. et al.",
                "Hate",
                "0.32",
                "0.53",
                "0.4"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.96",
                "0.88",
                "0.92"
            ],
            [
                "[EMPTY]",
                "Neither",
                "0.81",
                "0.95",
                "0.87"
            ],
            [
                "[ITALIC] G. et al.",
                "Harass.",
                "0.41",
                "0.19",
                "0.26"
            ],
            [
                "[EMPTY]",
                "Non.",
                "0.75",
                "0.9",
                "0.82"
            ],
            [
                "[ITALIC] F. et al.",
                "Hate",
                "0.33",
                "0.42",
                "0.37"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.87",
                "0.88",
                "0.88"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.5",
                "0.7",
                "0.58"
            ],
            [
                "[EMPTY]",
                "Neither",
                "0.88",
                "0.77",
                "0.82"
            ]
        ],
        "question": "Is it true that In particular, we see that hate speech and harassment are relatively easy to detect?",
        "answer_label": "no"
    },
    {
        "id": "207bf674-dc1a-4979-8432-1342a80d538f",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 4: Manual evaluation results (%) using models from Table\u00a02 (i.e., with roughly fixed Acc). > means \u201cbetter than\u201d. \u0394Sim=Sim(A)\u2212Sim(B), and \u0394PP=PP(A)\u2212PP(B) (note that lower PP generally means better fluency). Each row uses at least 120 sentence pairs. A cell is bold if it represents a model win of at least 10%.",
        "table_column_names": [
            "Dataset",
            "Models A",
            "Models B",
            "Transfer quality A>B",
            "Transfer quality B>A",
            "Transfer quality Tie",
            "Semantic preservation A>B",
            "Semantic preservation B>A",
            "Semantic preservation Tie",
            "Semantic preservation \u0394Sim",
            "Fluency A>B",
            "Fluency B>A",
            "Fluency Tie",
            "Fluency \u0394PP"
        ],
        "table_content_values": [
            [
                "[EMPTY]",
                "M0",
                "M2",
                "9.0",
                "6.0",
                "85.1",
                "1.5",
                "[BOLD] 25.4",
                "73.1",
                "-0.05",
                "10.4",
                "[BOLD] 23.9",
                "65.7",
                "0.9"
            ],
            [
                "Yelp",
                "M0",
                "M7",
                "9.6",
                "14.7",
                "75.8",
                "2.5",
                "[BOLD] 54.5",
                "42.9",
                "-0.09",
                "4.6",
                "[BOLD] 39.4",
                "56.1",
                "8.3"
            ],
            [
                "Yelp",
                "M6",
                "M7",
                "13.7",
                "11.6",
                "74.7",
                "16.0",
                "16.7",
                "67.4",
                "0.01",
                "10.3",
                "20.0",
                "69.7",
                "14.3"
            ],
            [
                "[EMPTY]",
                "M2",
                "M7",
                "5.8",
                "9.3",
                "84.9",
                "8.1",
                "[BOLD] 25.6",
                "66.3",
                "-0.04",
                "14.0",
                "[BOLD] 26.7",
                "59.3",
                "7.4"
            ],
            [
                "Literature",
                "M2",
                "M6",
                "4.2",
                "6.7",
                "89.2",
                "16.7",
                "20.8",
                "62.5",
                "0.01",
                "[BOLD] 40.8",
                "13.3",
                "45.8",
                "-13.3"
            ],
            [
                "Literature",
                "M6",
                "M7",
                "15.8",
                "13.3",
                "70.8",
                "[BOLD] 25.0",
                "9.2",
                "65.8",
                "0.03",
                "14.2",
                "20.8",
                "65.0",
                "14.2"
            ]
        ],
        "question": "Is it true that For example, on Yelp, large differences in human judgments of semantic preservation (M2>M0, M7>M0, M7>M2) also show the largest differences in Sim, while M6 and M7 have very similar human judgments but significantly different Sim scores?",
        "answer_label": "no"
    },
    {
        "id": "4d0e6a67-dd03-4a72-9b81-f8e50e9faae3",
        "table_caption": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. \u201cRaw\u201d indicates the usage of original prosodic features (Section 3.2), while \u201cinnovations\u201d indicate the usage of innovation features (Section 3.3).",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Model",
            "[BOLD] dev mean",
            "[BOLD] dev best",
            "[BOLD] test mean",
            "[BOLD] test best",
            "[ITALIC] \u03b1"
        ],
        "table_content_values": [
            [
                "single",
                "text",
                "86.54",
                "86.80",
                "86.47",
                "86.96",
                "\u2013"
            ],
            [
                "single",
                "raw",
                "35.00",
                "37.33",
                "35.78",
                "37.70",
                "\u2013"
            ],
            [
                "single",
                "innovations",
                "80.86",
                "81.51",
                "80.28",
                "82.15",
                "\u2013"
            ],
            [
                "early",
                "text + raw",
                "86.46",
                "86.65",
                "86.24",
                "86.53",
                "\u2013"
            ],
            [
                "early",
                "text + innovations",
                "86.53",
                "86.77",
                "86.54",
                "87.00",
                "\u2013"
            ],
            [
                "early",
                "text + raw + innovations",
                "86.35",
                "86.69",
                "86.55",
                "86.44",
                "\u2013"
            ],
            [
                "late",
                "text + raw",
                "86.71",
                "87.05",
                "86.35",
                "86.71",
                "0.2"
            ],
            [
                "late",
                "text + innovations",
                "[BOLD] 86.98",
                "[BOLD] 87.48",
                "[BOLD] 86.68",
                "[BOLD] 87.02",
                "0.5"
            ],
            [
                "late",
                "text + raw + innovations",
                "86.95",
                "87.30",
                "86.60",
                "86.87",
                "0.5"
            ]
        ],
        "question": "Is it true that The interpolation weight \u03b1 for the late fusion experiments is high when innovations are used, which further indicates that innovation features are useful in overall prediction?",
        "answer_label": "yes"
    },
    {
        "id": "82060521-bd71-4f0d-90fd-6b0e9de930b3",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that The performances of all models increase as the diameters of the graphs increase?",
        "answer_label": "no"
    },
    {
        "id": "7d1503a6-0b59-4b98-885a-a90ffb1da624",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
        "table_column_names": [
            "Model",
            "#Params",
            "BLEU",
            "Train",
            "Decode"
        ],
        "table_content_values": [
            [
                "GNMT",
                "-",
                "24.61",
                "-",
                "-"
            ],
            [
                "GRU",
                "206M",
                "26.28",
                "2.67",
                "45.35"
            ],
            [
                "ATR",
                "122M",
                "25.70",
                "1.33",
                "[BOLD] 34.40"
            ],
            [
                "SRU",
                "170M",
                "25.91",
                "1.34",
                "42.84"
            ],
            [
                "LRN",
                "143M",
                "26.26",
                "[BOLD] 0.99",
                "36.50"
            ],
            [
                "oLRN",
                "164M",
                "[BOLD] 26.73",
                "1.15",
                "40.19"
            ]
        ],
        "question": "Is it true that [CONTINUE] however, GRU yields the best BLEU score of 26.28, outperforming oLRN (+0.45 BLEU)?",
        "answer_label": "no"
    },
    {
        "id": "92d4db6a-df9b-45a3-bb55-a309229fec18",
        "table_caption": "On the difficulty of a distributional semantics of spoken language Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.",
        "table_column_names": [
            "[EMPTY]",
            "Recall@10 (%)",
            "Median rank",
            "RSAimage"
        ],
        "table_content_values": [
            [
                "VGS",
                "27",
                "6",
                "0.4"
            ],
            [
                "SegMatch",
                "[BOLD] 10",
                "[BOLD] 37",
                "[BOLD] 0.5"
            ],
            [
                "Audio2vec-U",
                "5",
                "105",
                "0.0"
            ],
            [
                "Audio2vec-C",
                "2",
                "647",
                "0.0"
            ],
            [
                "Mean MFCC",
                "1",
                "1,414",
                "0.0"
            ],
            [
                "Chance",
                "0",
                "3,955",
                "0.0"
            ]
        ],
        "question": "Is it true that It does not come close to VGS on paraphrase retrieval, but it does correlate with the visual modality even better?",
        "answer_label": "yes"
    },
    {
        "id": "dfce52f0-6525-4195-a26e-a69faf5d99eb",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "36.85",
                "5.3782",
                "35.14",
                "55.01",
                "1.6016",
                "00.34",
                "09.81",
                "00.15",
                "10.31"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen",
                "39.23",
                "6.0217",
                "36.97",
                "55.52",
                "1.7623",
                "00.40",
                "03.59",
                "00.07",
                "04.05"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen+",
                "40.25",
                "6.1448",
                "37.50",
                "56.19",
                "1.8181",
                "00.21",
                "01.99",
                "00.05",
                "02.24"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.88",
                "3.9310",
                "32.11",
                "39.90",
                "0.5036",
                "07.73",
                "17.76",
                "09.52",
                "35.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.19",
                "6.0543",
                "37.38",
                "55.88",
                "1.8104",
                "00.17",
                "01.31",
                "00.25",
                "01.72"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen",
                "40.73",
                "6.1711",
                "37.76",
                "56.09",
                "1.8518",
                "00.07",
                "00.72",
                "00.08",
                "00.87"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen+",
                "40.51",
                "6.1226",
                "37.61",
                "55.98",
                "1.8286",
                "00.02",
                "00.63",
                "00.06",
                "00.70"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.66",
                "3.9511",
                "32.93",
                "39.29",
                "0.3855",
                "07.89",
                "15.60",
                "08.44",
                "31.94"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.48",
                "6.0269",
                "37.26",
                "56.19",
                "1.7999",
                "00.43",
                "02.84",
                "00.26",
                "03.52"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen",
                "41.57",
                "6.2830",
                "37.99",
                "56.36",
                "1.8849",
                "00.37",
                "01.40",
                "00.09",
                "01.86"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen+",
                "41.56",
                "6.2700",
                "37.94",
                "56.38",
                "1.8827",
                "00.21",
                "01.04",
                "00.07",
                "01.31"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "35.99",
                "5.0734",
                "34.74",
                "54.79",
                "1.5259",
                "00.02",
                "11.58",
                "00.02",
                "11.62"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen",
                "40.07",
                "6.1243",
                "37.45",
                "55.81",
                "1.8026",
                "00.05",
                "03.23",
                "00.01",
                "03.29"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen+",
                "40.80",
                "6.2197",
                "37.86",
                "56.13",
                "1.8422",
                "00.01",
                "01.87",
                "00.01",
                "01.88"
            ]
        ],
        "question": "Is it true that [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe no main effect on SER from cleaning the missed slots, with only slight reductions in insertions and deletions?",
        "answer_label": "no"
    },
    {
        "id": "ac0a6d46-3c6c-4e6f-8342-d436e43c9c8f",
        "table_caption": "One-to-X analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts Table 3: Average diachronic performance",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Algorithm",
            "[BOLD] Precision",
            "[BOLD] Recall",
            "[BOLD] F1"
        ],
        "table_content_values": [
            [
                "Giga",
                "Baseline",
                "0.19",
                "0.51",
                "0.28"
            ],
            [
                "Giga",
                "Threshold",
                "0.46",
                "0.41",
                "[BOLD] 0.41"
            ],
            [
                "NOW",
                "Baseline",
                "0.26",
                "0.53",
                "0.34"
            ],
            [
                "NOW",
                "Threshold",
                "0.42",
                "0.41",
                "[BOLD] 0.41"
            ]
        ],
        "question": "Is it true that For both Gigaword and NOW datasets (and the corresponding embeddings), using the cosinebased threshold decreases recall and increases precision (differences are statistically significant with t-test, p < 0.05)?",
        "answer_label": "yes"
    },
    {
        "id": "ad0e53cd-7e67-4eb5-bb8d-33949d1f8e6a",
        "table_caption": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task Table 5: Textual similarity scores (asymmetric, Multi30k).",
        "table_column_names": [
            "[EMPTY]",
            "EN \u2192 DE R@1",
            "EN \u2192 DE R@5",
            "EN \u2192 DE R@10",
            "DE \u2192 EN R@1",
            "DE \u2192 EN R@5",
            "DE \u2192 EN R@10"
        ],
        "table_content_values": [
            [
                "FME",
                "51.4",
                "76.4",
                "84.5",
                "46.9",
                "71.2",
                "79.1"
            ],
            [
                "AME",
                "[BOLD] 51.7",
                "[BOLD] 76.7",
                "[BOLD] 85.1",
                "[BOLD] 49.1",
                "[BOLD] 72.6",
                "[BOLD] 80.5"
            ]
        ],
        "question": "Is it true that AME outperforms the FME model, confirming the importance of word embeddings adaptation?",
        "answer_label": "yes"
    },
    {
        "id": "5323b8d2-37fa-4b1b-a9d9-05db8157d527",
        "table_caption": "Building a Production Model for Retrieval-Based Chatbots Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Parameters",
            "[BOLD] Validation AUC@0.05",
            "[BOLD] Test AUC@0.05"
        ],
        "table_content_values": [
            [
                "Base",
                "8.0M",
                "[BOLD] 0.871",
                "0.816"
            ],
            [
                "4L SRU \u2192 2L LSTM",
                "7.3M",
                "0.864",
                "[BOLD] 0.829"
            ],
            [
                "4L SRU \u2192 2L SRU",
                "7.8M",
                "0.856",
                "[BOLD] 0.829"
            ],
            [
                "Flat \u2192 hierarchical",
                "12.4M",
                "0.825",
                "0.559"
            ],
            [
                "Cross entropy \u2192 hinge loss",
                "8.0M",
                "0.765",
                "0.693"
            ],
            [
                "6.6M \u2192 1M examples",
                "8.0M",
                "0.835",
                "0.694"
            ],
            [
                "6.6M \u2192 100K examples",
                "8.0M",
                "0.565",
                "0.417"
            ],
            [
                "200 \u2192 100 negatives",
                "8.0M",
                "0.864",
                "0.647"
            ],
            [
                "200 \u2192 10 negatives",
                "8.0M",
                "0.720",
                "0.412"
            ]
        ],
        "question": "Is it true that [CONTINUE] We observed no advantage to using a hierachical encoder, [CONTINUE] Finally, we see that a 2 layer LSTM performs similarly to either a 4 layer or a 2 layer SRU with a comparable number of parameters?",
        "answer_label": "yes"
    },
    {
        "id": "67184fb9-20ca-445e-8366-7d03160cce3a",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that The single DCGCN model achieves a BLEU score of 30.4 and a CHRF++ score of 59.6, outperforming the ensemble approach based on combining five DCGCN models initialized with different random seeds?",
        "answer_label": "no"
    },
    {
        "id": "2a87f7a2-dfb7-48c8-944d-aab7eb874e0e",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE IX: Accuracies (%) for Sentiment Classification Task",
        "table_column_names": [
            "GloVe",
            "Word2Vec",
            "OIWE-IPG",
            "SOV",
            "SPINE",
            "Word2Sense",
            "Proposed"
        ],
        "table_content_values": [
            [
                "77.34",
                "77.91",
                "74.27",
                "78.43",
                "74.13",
                "81.21",
                "78.26"
            ]
        ],
        "question": "Is it true that The proposed method does not outperform the original embeddings and performs worse than the SOV?",
        "answer_label": "no"
    },
    {
        "id": "292204b8-c7d0-4b68-8a35-f392930d4194",
        "table_caption": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 4: Results of Self-Play Evaluation.",
        "table_column_names": [
            "System",
            "TGPC Succ. (%)",
            "TGPC #Turns",
            "CWC Succ. (%)",
            "CWC #Turns"
        ],
        "table_content_values": [
            [
                "Retrieval\u00a0",
                "7.16",
                "4.17",
                "0",
                "-"
            ],
            [
                "Retrieval-Stgy\u00a0",
                "47.80",
                "6.7",
                "44.6",
                "7.42"
            ],
            [
                "PMI\u00a0",
                "35.36",
                "6.38",
                "47.4",
                "5.29"
            ],
            [
                "Neural\u00a0",
                "54.76",
                "4.73",
                "47.6",
                "5.16"
            ],
            [
                "Kernel\u00a0",
                "62.56",
                "4.65",
                "53.2",
                "4.08"
            ],
            [
                "DKRN (ours)",
                "[BOLD] 89.0",
                "5.02",
                "[BOLD] 84.4",
                "4.20"
            ]
        ],
        "question": "Is it true that This superior confirms the effectiveness of our approach?",
        "answer_label": "yes"
    },
    {
        "id": "026892e8-1d2c-412c-8881-af5a2b4400b2",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
        "table_column_names": [
            "VS.",
            "Efficiency W",
            "Efficiency D",
            "Efficiency L",
            "Quality W",
            "Quality D",
            "Quality L",
            "Success W",
            "Success D",
            "Success L"
        ],
        "table_content_values": [
            [
                "ACER",
                "55",
                "25",
                "20",
                "44",
                "32",
                "24",
                "52",
                "30",
                "18"
            ],
            [
                "PPO",
                "74",
                "13",
                "13",
                "56",
                "26",
                "18",
                "59",
                "31",
                "10"
            ],
            [
                "ALDM",
                "69",
                "19",
                "12",
                "49",
                "25",
                "26",
                "61",
                "24",
                "15"
            ]
        ],
        "question": "Is it true that Among all the baselines, GDPL does not obtain the most preference against PPO?",
        "answer_label": "no"
    },
    {
        "id": "69b2e0e5-215d-4de9-840b-6752ca98311a",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Type",
            "[BOLD] English-German #P",
            "[BOLD] English-German B",
            "[BOLD] English-German C",
            "[BOLD] English-Czech #P",
            "[BOLD] English-Czech B",
            "[BOLD] English-Czech C"
        ],
        "table_content_values": [
            [
                "BoW+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "12.2",
                "-",
                "-",
                "7.5",
                "-"
            ],
            [
                "CNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "13.7",
                "-",
                "-",
                "8.7",
                "-"
            ],
            [
                "BiRNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "16.1",
                "-",
                "-",
                "9.6",
                "-"
            ],
            [
                "PB-SMT (Beck et al.,  2018 )",
                "Single",
                "-",
                "12.8",
                "43.2",
                "-",
                "8.6",
                "36.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Single",
                "41.4M",
                "15.5",
                "40.8",
                "39.1M",
                "8.9",
                "33.8"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Single",
                "41.2M",
                "16.7",
                "42.4",
                "38.8M",
                "9.8",
                "33.3"
            ],
            [
                "DCGCN (ours)",
                "Single",
                "[BOLD]  29.7M",
                "[BOLD] 19.0",
                "[BOLD] 44.1",
                "[BOLD]  28.3M",
                "[BOLD] 12.1",
                "[BOLD] 37.1"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Ensemble",
                "207M",
                "19.0",
                "44.1",
                "195M",
                "11.3",
                "36.4"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Ensemble",
                "206M",
                "19.6",
                "45.1",
                "194M",
                "11.7",
                "35.9"
            ],
            [
                "DCGCN (ours)",
                "Ensemble",
                "[BOLD]  149M",
                "[BOLD] 20.5",
                "[BOLD] 45.8",
                "[BOLD]  142M",
                "[BOLD] 13.1",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that The DCGCN models do not achieve the highest BLEU points on the En-De and En-Cs tasks, respectively?",
        "answer_label": "no"
    },
    {
        "id": "95306f5e-c196-47cd-99a0-f54bb70d41e7",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that Longer sentences do not pose additional challenges to the models?",
        "answer_label": "no"
    },
    {
        "id": "47460ee9-1544-4470-82f5-665181faad47",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
        "table_column_names": [
            "<bold>Model</bold>",
            "REF \u21d2 GEN <bold>ENT</bold>",
            "REF \u21d2 GEN <bold>CON</bold>",
            "REF \u21d2 GEN <bold>NEU</bold>"
        ],
        "table_content_values": [
            [
                "S2S",
                "38.45",
                "11.17",
                "50.38"
            ],
            [
                "G2S-GIN",
                "49.78",
                "9.80",
                "40.42"
            ],
            [
                "G2S-GAT",
                "49.48",
                "8.09",
                "42.43"
            ],
            [
                "G2S-GGNN",
                "51.32",
                "8.82",
                "39.86"
            ],
            [
                "[EMPTY]",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF"
            ],
            [
                "<bold>Model</bold>",
                "<bold>ENT</bold>",
                "<bold>CON</bold>",
                "<bold>NEU</bold>"
            ],
            [
                "S2S",
                "73.79",
                "12.75",
                "13.46"
            ],
            [
                "G2S-GIN",
                "76.27",
                "10.65",
                "13.08"
            ],
            [
                "G2S-GAT",
                "77.54",
                "8.54",
                "13.92"
            ],
            [
                "G2S-GGNN",
                "77.64",
                "9.64",
                "12.72"
            ]
        ],
        "question": "Is it true that This suggests that our models are not capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences?",
        "answer_label": "no"
    },
    {
        "id": "d93f4cc1-346e-4d0f-a0c7-c1cd87778fb1",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that Adding either the global node or the linear combination improves the baseline models with only dense connections?",
        "answer_label": "yes"
    },
    {
        "id": "5cd2b818-6cbd-43ff-a379-23d4544992da",
        "table_caption": "Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
        "table_column_names": [
            "[BOLD] ResNet-34",
            "[BOLD] Eval set %",
            "[BOLD] #param"
        ],
        "table_content_values": [
            [
                "Baseline (No SA)Anderson et al. ( 2018 )",
                "55.00",
                "0M"
            ],
            [
                "SA (S: 1,2,3 - B: 1)",
                "55.11",
                "} 0.107M"
            ],
            [
                "SA (S: 1,2,3 - B: 2)",
                "55.17",
                "} 0.107M"
            ],
            [
                "[BOLD] SA (S: 1,2,3 - B: 3)",
                "[BOLD] 55.27",
                "} 0.107M"
            ]
        ],
        "question": "Is it true that We showed that it is not possible to improve the feature extraction procedure for the VQA task by adding self-attention modules in the different ResNet blocks?",
        "answer_label": "no"
    },
    {
        "id": "7bb70ac8-2e77-430a-9c0c-c47235dad046",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold\u2019s folding technique, and TensorFlow\u2019s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Training",
            "Throughput (instances/s) Training",
            "Throughput (instances/s) Training"
        ],
        "table_content_values": [
            [
                "Batch size",
                "Iter",
                "Recur",
                "Fold",
                "Iter",
                "Recur",
                "Fold"
            ],
            [
                "1",
                "19.2",
                "81.4",
                "16.5",
                "2.5",
                "4.8",
                "9.0"
            ],
            [
                "10",
                "49.3",
                "217.9",
                "52.2",
                "4.0",
                "4.2",
                "37.5"
            ],
            [
                "25",
                "72.1",
                "269.9",
                "61.6",
                "5.5",
                "3.6",
                "54.7"
            ]
        ],
        "question": "Is it true that The amount of resources is insufficient for executing forward computations, and therefore our framework does not outperform the folding technique for the inference task with up to 4.93x faster throughput?",
        "answer_label": "no"
    },
    {
        "id": "5a52f554-0dbd-442d-af13-23015337b5f0",
        "table_caption": "Building a Production Model for Retrieval-Based Chatbots Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Parameters",
            "[BOLD] Validation AUC@0.05",
            "[BOLD] Test AUC@0.05"
        ],
        "table_content_values": [
            [
                "Base",
                "8.0M",
                "[BOLD] 0.871",
                "0.816"
            ],
            [
                "4L SRU \u2192 2L LSTM",
                "7.3M",
                "0.864",
                "[BOLD] 0.829"
            ],
            [
                "4L SRU \u2192 2L SRU",
                "7.8M",
                "0.856",
                "[BOLD] 0.829"
            ],
            [
                "Flat \u2192 hierarchical",
                "12.4M",
                "0.825",
                "0.559"
            ],
            [
                "Cross entropy \u2192 hinge loss",
                "8.0M",
                "0.765",
                "0.693"
            ],
            [
                "6.6M \u2192 1M examples",
                "8.0M",
                "0.835",
                "0.694"
            ],
            [
                "6.6M \u2192 100K examples",
                "8.0M",
                "0.565",
                "0.417"
            ],
            [
                "200 \u2192 100 negatives",
                "8.0M",
                "0.864",
                "0.647"
            ],
            [
                "200 \u2192 10 negatives",
                "8.0M",
                "0.720",
                "0.412"
            ]
        ],
        "question": "Is it true that We observed an advantage to using a hierachical encoder, [CONTINUE] Finally, we see that a 2 layer LSTM performs worse than either a 4 layer or a 2 layer SRU with a comparable number of parameters?",
        "answer_label": "no"
    },
    {
        "id": "ef8adc9d-e855-47d4-b3d1-f5099f9892f7",
        "table_caption": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment Table 3: Effect of removing sense priors and context sensitivity (attention) from the model.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] PPA Acc."
        ],
        "table_content_values": [
            [
                "full",
                "89.7"
            ],
            [
                "- sense priors",
                "88.4"
            ],
            [
                "- attention",
                "87.5"
            ]
        ],
        "question": "Is it true that The second row in Table 3 shows the test accuracy of a system trained without sense priors [CONTINUE] and the third row shows the effect of making the token representations context-insensitive by giving a similar attention score to all related concepts, essentially making them type level representations, but still grounded in WordNet?",
        "answer_label": "yes"
    },
    {
        "id": "58fa3a44-8ffc-4329-bddb-79ec2c2b9b21",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Present",
            "[BOLD] Not Present"
        ],
        "table_content_values": [
            [
                "Emoji",
                "4805 (76.6%)",
                "23952 (68.0%)"
            ],
            [
                "Hashtags",
                "2122 (70.5%)",
                "26635 (69.4%)"
            ]
        ],
        "question": "Is it true that [CONTINUE] Hashtags also have a [CONTINUE] positive effect on classification performance, however it is less significant?",
        "answer_label": "yes"
    },
    {
        "id": "284e66c0-e63c-4e04-99a7-e91ee70cdd14",
        "table_caption": "Deriving Machine Attention from Human Rationales Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.",
        "table_column_names": [
            "Source",
            "Target",
            "Svm",
            "Ra-Svm\u2021",
            "Ra-Cnn\u2021",
            "Trans\u2020",
            "Ra-Trans\u2021\u2020",
            "Ours\u2021\u2020",
            "Oracle\u2020"
        ],
        "table_content_values": [
            [
                "Beer aroma+palate",
                "Beer look",
                "74.41",
                "74.83",
                "74.94",
                "72.75",
                "76.41",
                "[BOLD] 79.53",
                "80.29"
            ],
            [
                "Beer look+palate",
                "Beer aroma",
                "68.57",
                "69.23",
                "67.55",
                "69.92",
                "76.45",
                "[BOLD] 77.94",
                "78.11"
            ],
            [
                "Beer look+aroma",
                "Beer palate",
                "63.88",
                "67.82",
                "65.72",
                "74.66",
                "73.40",
                "[BOLD] 75.24",
                "75.50"
            ]
        ],
        "question": "Is it true that Our model (OURS) obtains substantial gains in accuracy over the baselines across all three target aspects?",
        "answer_label": "yes"
    },
    {
        "id": "9c92823f-0db0-4455-b09c-2636c5e90d5d",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that With the coverage mechanism, the result drops by 1.7/2.4 points for B/C scores?",
        "answer_label": "no"
    },
    {
        "id": "a6442b25-639a-4e9f-acc1-2af93942e266",
        "table_caption": "Two Causal Principles for Improving Visual Dialog Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table\u00a01. Note that only applying P2 is implemented by the implementations in Section\u00a05 with the history shortcut.",
        "table_column_names": [
            "Model",
            "LF\u00a0",
            "HCIAE\u00a0",
            "CoAtt\u00a0",
            "RvA\u00a0"
        ],
        "table_content_values": [
            [
                "baseline",
                "57.21",
                "56.98",
                "56.46",
                "56.74"
            ],
            [
                "+P1",
                "61.88",
                "60.12",
                "60.27",
                "61.02"
            ],
            [
                "+P2",
                "72.65",
                "71.50",
                "71.41",
                "71.44"
            ],
            [
                "+P1+P2",
                "[BOLD] 73.63",
                "71.99",
                "71.87",
                "72.88"
            ]
        ],
        "question": "Is it true that Note that the effectiveness of P1 and P2 are not necessarily additive, as combining P1 and P2 does not always perform the best?",
        "answer_label": "no"
    },
    {
        "id": "5b31abdf-f132-46a7-8da5-490adbe8d469",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 3: Performance comparison of our model with different values of m on the two datasets.",
        "table_column_names": [
            "[ITALIC] m",
            "NYT10 Prec.",
            "NYT10 Rec.",
            "NYT10 F1",
            "NYT11 Prec.",
            "NYT11 Rec.",
            "NYT11 F1"
        ],
        "table_content_values": [
            [
                "1",
                "0.541",
                "0.595",
                "[BOLD] 0.566",
                "0.495",
                "0.621",
                "0.551"
            ],
            [
                "2",
                "0.521",
                "0.597",
                "0.556",
                "0.482",
                "0.656",
                "0.555"
            ],
            [
                "3",
                "0.490",
                "0.617",
                "0.547",
                "0.509",
                "0.633",
                "0.564"
            ],
            [
                "4",
                "0.449",
                "0.623",
                "0.522",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "5",
                "0.467",
                "0.609",
                "0.529",
                "0.488",
                "0.677",
                "0.567"
            ]
        ],
        "question": "Is it true that We observe that for the NYT10 dataset, m = 4 gives the highest F1 score?",
        "answer_label": "no"
    },
    {
        "id": "2096086d-1f21-4a72-992f-724d69319e5d",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
        "table_column_names": [
            "[BOLD] Training data",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] Disfl"
        ],
        "table_content_values": [
            [
                "Original",
                "0",
                "22",
                "0",
                "14"
            ],
            [
                "Cleaned added",
                "0",
                "23",
                "0",
                "14"
            ],
            [
                "Cleaned missing",
                "0",
                "1",
                "0",
                "2"
            ],
            [
                "Cleaned",
                "0",
                "0",
                "0",
                "5"
            ]
        ],
        "question": "Is it true that The systems trained on the original data or with cleaned added slots clearly perform worse in terms of both semantic accuracy and fluency?",
        "answer_label": "yes"
    },
    {
        "id": "750beeba-88c2-479e-a030-0576133256a4",
        "table_caption": "One-to-X analogical reasoning on word embeddings: a case for diachronic armed conflict prediction from news texts Table 3: Average diachronic performance",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Algorithm",
            "[BOLD] Precision",
            "[BOLD] Recall",
            "[BOLD] F1"
        ],
        "table_content_values": [
            [
                "Giga",
                "Baseline",
                "0.19",
                "0.51",
                "0.28"
            ],
            [
                "Giga",
                "Threshold",
                "0.46",
                "0.41",
                "[BOLD] 0.41"
            ],
            [
                "NOW",
                "Baseline",
                "0.26",
                "0.53",
                "0.34"
            ],
            [
                "NOW",
                "Threshold",
                "0.42",
                "0.41",
                "[BOLD] 0.41"
            ]
        ],
        "question": "Is it true that For both Gigaword and NOW datasets (and the corresponding embeddings), using the cosinebased threshold increases recall and decreases precision (differences are statistically significant with t-test, p < 0.05)?",
        "answer_label": "no"
    },
    {
        "id": "e02742aa-05d4-4e91-ab12-6f4aaa5409ee",
        "table_caption": "Two Causal Principles for Improving Visual Dialog Table 1: Performance (NDCG%) comparison for the experiments of applying our principles on the validation set of VisDial v1.0. LF is the enhanced version as we mentioned. QT, S and D denote question type, answer score sampling, and hidden dictionary learning, respectively. R0, R1, R2, R3 denote regressive loss, weighted softmax loss, binary sigmoid loss ,and generalized ranking loss, respectively.",
        "table_column_names": [
            "Model",
            "baseline",
            "QT",
            "S  [ITALIC] R0",
            "S  [ITALIC] R1",
            "S  [ITALIC] R2",
            "S  [ITALIC] R3",
            "D"
        ],
        "table_content_values": [
            [
                "LF\u00a0",
                "57.21",
                "58.97",
                "67.82",
                "71.27",
                "72.04",
                "72.36",
                "72.65"
            ],
            [
                "LF +P1",
                "61.88",
                "62.87",
                "69.47",
                "72.16",
                "72.85",
                "73.42",
                "[BOLD] 73.63"
            ]
        ],
        "question": "Is it true that Overall, all of the implementations can improve the performances of base models?",
        "answer_label": "yes"
    },
    {
        "id": "7dad5701-2235-4fc8-b66b-306812347530",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that In terms of relative numbers, the hybrid model improves upon CBOW in all probing tasks except WC?",
        "answer_label": "no"
    },
    {
        "id": "38bfa8f1-7948-49d5-bc60-08c75e385df9",
        "table_caption": "Two Causal Principles for Improving Visual Dialog Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table\u00a01. Note that only applying P2 is implemented by the implementations in Section\u00a05 with the history shortcut.",
        "table_column_names": [
            "Model",
            "LF\u00a0",
            "HCIAE\u00a0",
            "CoAtt\u00a0",
            "RvA\u00a0"
        ],
        "table_content_values": [
            [
                "baseline",
                "57.21",
                "56.98",
                "56.46",
                "56.74"
            ],
            [
                "+P1",
                "61.88",
                "60.12",
                "60.27",
                "61.02"
            ],
            [
                "+P2",
                "72.65",
                "71.50",
                "71.41",
                "71.44"
            ],
            [
                "+P1+P2",
                "[BOLD] 73.63",
                "71.99",
                "71.87",
                "72.88"
            ]
        ],
        "question": "Is it true that Note that the effectiveness of P1 and P2 are additive, which means combining P1 and P2 performs the best?",
        "answer_label": "yes"
    },
    {
        "id": "47e27211-3006-49e1-a5d6-34a1993698c1",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that Despite achieving high performance in the task success, GDPL does not show substantial improvement in inform F1 and match rate over the baselines?",
        "answer_label": "no"
    },
    {
        "id": "6077db2c-6ffe-4629-9f6c-42197e0ad297",
        "table_caption": "Aligning Vector-spaces with Noisy Supervised Lexicons Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
        "table_column_names": [
            "Method",
            "En\u2192It best",
            "En\u2192It avg",
            "En\u2192It iters",
            "En\u2192De best",
            "En\u2192De avg",
            "En\u2192De iters",
            "En\u2192Fi best",
            "En\u2192Fi avg",
            "En\u2192Fi iters",
            "En\u2192Es best",
            "En\u2192Es avg",
            "En\u2192Es iters"
        ],
        "table_content_values": [
            [
                "Artetxe et\u00a0al., 2018b",
                "[BOLD] 48.53",
                "48.13",
                "573",
                "48.47",
                "48.19",
                "773",
                "33.50",
                "32.63",
                "988",
                "37.60",
                "37.33",
                "808"
            ],
            [
                "Noise-aware Alignment",
                "[BOLD] 48.53",
                "[BOLD] 48.20",
                "471",
                "[BOLD] 49.67",
                "[BOLD] 48.89",
                "568",
                "[BOLD] 33.98",
                "[BOLD] 33.68",
                "502",
                "[BOLD] 38.40",
                "[BOLD] 37.79",
                "551"
            ]
        ],
        "question": "Is it true that In most setups our best case is not better than the former best case?",
        "answer_label": "no"
    },
    {
        "id": "024b4b9e-0169-4451-a529-066860ae0555",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that [CONTINUE] As we can observe, it seems that clustering semantically related terms will increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment) as expected?",
        "answer_label": "yes"
    },
    {
        "id": "0abaf60d-6117-4b20-8493-f3678aadd259",
        "table_caption": "Sparse and Structured Visual Attention Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
        "table_column_names": [
            "[EMPTY]",
            "MSCOCO spice",
            "MSCOCO cider",
            "MSCOCO rouge [ITALIC] L",
            "MSCOCO bleu4",
            "MSCOCO meteor",
            "MSCOCO rep\u2193",
            "Flickr30k spice",
            "Flickr30k cider",
            "Flickr30k rouge [ITALIC] L",
            "Flickr30k bleu4",
            "Flickr30k meteor",
            "Flickr30k rep\u2193"
        ],
        "table_content_values": [
            [
                "softmax",
                "18.4",
                "0.967",
                "52.9",
                "29.9",
                "24.9",
                "3.76",
                "13.5",
                "0.443",
                "44.2",
                "19.9",
                "19.1",
                "6.09"
            ],
            [
                "sparsemax",
                "[BOLD] 18.9",
                "[BOLD] 0.990",
                "[BOLD] 53.5",
                "[BOLD] 31.5",
                "[BOLD] 25.3",
                "3.69",
                "[BOLD] 13.7",
                "[BOLD] 0.444",
                "[BOLD] 44.3",
                "[BOLD] 20.7",
                "[BOLD] 19.3",
                "5.84"
            ],
            [
                "TVmax",
                "18.5",
                "0.974",
                "53.1",
                "29.9",
                "25.1",
                "[BOLD] 3.17",
                "13.3",
                "0.438",
                "44.2",
                "20.5",
                "19.0",
                "[BOLD] 3.97"
            ]
        ],
        "question": "Is it true that Selective attention mechanisms like sparsemax and especially TVMAX do not reduce repetition, as measured by the REP metric reported in Table 1?",
        "answer_label": "no"
    },
    {
        "id": "df4b4d38-b005-4456-b84e-1780b5fcc389",
        "table_caption": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation Table 1: Overall performance comparing to the state-of-the-art methods with golden-standard entities.",
        "table_column_names": [
            "[BOLD] Method",
            "[BOLD] Trigger  [BOLD] Identification (%)",
            "[BOLD] Trigger  [BOLD] Identification (%)",
            "[BOLD] Trigger  [BOLD] Identification (%)",
            "[BOLD] Trigger  [BOLD] Classification (%)",
            "[BOLD] Trigger  [BOLD] Classification (%)",
            "[BOLD] Trigger  [BOLD] Classification (%)",
            "[BOLD] Argument  [BOLD] Identification (%)",
            "[BOLD] Argument  [BOLD] Identification (%)",
            "[BOLD] Argument  [BOLD] Identification (%)",
            "[BOLD] Argument  [BOLD] Role (%)",
            "[BOLD] Argument  [BOLD] Role (%)",
            "[BOLD] Argument  [BOLD] Role (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] Method",
                "[ITALIC] P",
                "[ITALIC] R",
                "[ITALIC] F1",
                "[ITALIC] P",
                "[ITALIC] R",
                "[ITALIC] F1",
                "[ITALIC] P",
                "[ITALIC] R",
                "[ITALIC] F1",
                "[ITALIC] P",
                "[ITALIC] R",
                "[ITALIC] F1"
            ],
            [
                "Cross-Event",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "68.7",
                "68.9",
                "68.8",
                "50.9",
                "49.7",
                "50.3",
                "45.1",
                "44.1",
                "44.6"
            ],
            [
                "JointBeam",
                "76.9",
                "65.0",
                "70.4",
                "73.7",
                "62.3",
                "67.5",
                "69.8",
                "47.9",
                "56.8",
                "64.7",
                "44.4",
                "52.7"
            ],
            [
                "DMCNN",
                "[BOLD] 80.4",
                "67.7",
                "73.5",
                "75.6",
                "63.6",
                "69.1",
                "68.8",
                "51.9",
                "59.1",
                "62.2",
                "46.9",
                "53.5"
            ],
            [
                "PSL",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "75.3",
                "64.4",
                "69.4",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "JRNN",
                "68.5",
                "[BOLD] 75.7",
                "71.9",
                "66.0",
                "[BOLD] 73.0",
                "69.3",
                "61.4",
                "64.2",
                "62.8",
                "54.2",
                "56.7",
                "55.4"
            ],
            [
                "dbRNN",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "74.1",
                "69.8",
                "71.9",
                "71.3",
                "64.5",
                "67.7",
                "66.2",
                "52.8",
                "58.7"
            ],
            [
                "[BOLD] JMEE",
                "80.2",
                "72.1",
                "[BOLD] 75.9",
                "[BOLD] 76.3",
                "71.3",
                "[BOLD] 73.7",
                "[BOLD] 71.4",
                "[BOLD] 65.6",
                "[BOLD] 68.4",
                "[BOLD] 66.8",
                "[BOLD] 54.9",
                "[BOLD] 60.3"
            ]
        ],
        "question": "Is it true that From the table, we can see that our JMEE framework does not achieve the best F1 scores for both trigger classification and argument-related subtasks among all the compared methods?",
        "answer_label": "no"
    },
    {
        "id": "c38e08e0-2ef9-4fdc-bf65-6218e5f62a85",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Original",
                "TGen\u2212",
                "63.37",
                "7.7188",
                "41.99",
                "68.53",
                "1.9355",
                "00.06",
                "15.77",
                "00.11",
                "15.94"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen",
                "66.41",
                "8.5565",
                "45.07",
                "69.17",
                "2.2253",
                "00.14",
                "04.11",
                "00.03",
                "04.27"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen+",
                "67.06",
                "8.5871",
                "45.83",
                "69.73",
                "2.2681",
                "00.04",
                "01.75",
                "00.01",
                "01.80"
            ],
            [
                "Original",
                "[BOLD] Original",
                "SC-LSTM",
                "39.11",
                "5.6704",
                "36.83",
                "50.02",
                "0.6045",
                "02.79",
                "18.90",
                "09.79",
                "31.51"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen\u2212",
                "65.87",
                "8.6400",
                "44.20",
                "67.51",
                "2.1710",
                "00.20",
                "00.56",
                "00.21",
                "00.97"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen",
                "66.24",
                "8.6889",
                "44.66",
                "67.85",
                "2.2181",
                "00.10",
                "00.02",
                "00.00",
                "00.12"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen+",
                "65.97",
                "8.6630",
                "44.45",
                "67.59",
                "2.1855",
                "00.02",
                "00.00",
                "00.00",
                "00.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "SC-LSTM",
                "38.52",
                "5.7125",
                "37.45",
                "48.50",
                "0.4343",
                "03.85",
                "17.39",
                "08.12",
                "29.37"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen\u2212",
                "66.28",
                "8.5202",
                "43.96",
                "67.83",
                "2.1375",
                "00.14",
                "02.26",
                "00.22",
                "02.61"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen",
                "67.00",
                "8.6889",
                "44.97",
                "68.19",
                "2.2228",
                "00.06",
                "00.44",
                "00.03",
                "00.53"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen+",
                "66.74",
                "8.6649",
                "44.84",
                "67.95",
                "2.2018",
                "00.00",
                "00.21",
                "00.03",
                "00.24"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen\u2212",
                "64.40",
                "7.9692",
                "42.81",
                "68.87",
                "2.0563",
                "00.01",
                "13.08",
                "00.00",
                "13.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen",
                "66.23",
                "8.5578",
                "45.12",
                "68.87",
                "2.2548",
                "00.04",
                "03.04",
                "00.00",
                "03.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen+",
                "65.96",
                "8.5238",
                "45.49",
                "68.79",
                "2.2456",
                "00.00",
                "01.44",
                "00.00",
                "01.45"
            ]
        ],
        "question": "Is it true that The results in Table 2 (top half) for the original setup confirm that the ranking mechanism for TGen is effective for both WOMs and SER, whereas the SC-LSTM seems to have trouble scaling to the E2E dataset?",
        "answer_label": "yes"
    },
    {
        "id": "db48fd7d-eac7-402b-986a-1295738e6236",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 6: Test BLEU for 2-model es-en and 3-model en-de model ensembling for models adapted with EWC, compared to oracle model last trained on each domain, chosen if test domain is known. BI+IS outperforms uniform ensembling and in some cases outperforms the oracle.",
        "table_column_names": [
            "[BOLD] Decoder configuration",
            "[BOLD] es-en  [BOLD] Health",
            "[BOLD] es-en  [BOLD] Bio",
            "[BOLD] en-de  [BOLD] News",
            "[BOLD] en-de  [BOLD] TED",
            "[BOLD] en-de  [BOLD] IT"
        ],
        "table_content_values": [
            [
                "Oracle model",
                "35.9",
                "37.8",
                "37.8",
                "27.0",
                "57.0"
            ],
            [
                "Uniform",
                "36.0",
                "36.4",
                "[BOLD] 38.9",
                "26.0",
                "43.5"
            ],
            [
                "BI + IS",
                "[BOLD] 36.2",
                "[BOLD] 38.0",
                "38.7",
                "[BOLD] 26.1",
                "[BOLD] 56.4"
            ]
        ],
        "question": "Is it true that [CONTINUE] EWC models do not perform as well as uniform ensembling, as evidenced by the fact that in some cases, uniform ensembling outperforms the oracle?",
        "answer_label": "no"
    },
    {
        "id": "ac84d56e-4583-4f9c-ae2a-0731f62551ba",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
        "table_column_names": [
            "[BOLD] Method",
            "[BOLD] R-1",
            "[BOLD] R-2",
            "[BOLD] R-SU"
        ],
        "table_content_values": [
            [
                "First-1",
                "26.83",
                "7.25",
                "6.46"
            ],
            [
                "First-2",
                "35.99",
                "10.17",
                "12.06"
            ],
            [
                "First-3",
                "39.41",
                "11.77",
                "14.51"
            ],
            [
                "LexRank Erkan and Radev ( 2004 )",
                "38.27",
                "12.70",
                "13.20"
            ],
            [
                "TextRank Mihalcea and Tarau ( 2004 )",
                "38.44",
                "13.10",
                "13.50"
            ],
            [
                "MMR Carbonell and Goldstein ( 1998 )",
                "38.77",
                "11.98",
                "12.91"
            ],
            [
                "PG-Original Lebanoff et\u00a0al. ( 2018 )",
                "41.85",
                "12.91",
                "16.46"
            ],
            [
                "PG-MMR Lebanoff et\u00a0al. ( 2018 )",
                "40.55",
                "12.36",
                "15.87"
            ],
            [
                "PG-BRNN Gehrmann et\u00a0al. ( 2018 )",
                "42.80",
                "14.19",
                "16.75"
            ],
            [
                "CopyTransformer Gehrmann et\u00a0al. ( 2018 )",
                "[BOLD] 43.57",
                "14.03",
                "17.37"
            ],
            [
                "Hi-MAP (Our Model)",
                "43.47",
                "[BOLD] 14.89",
                "[BOLD] 17.41"
            ]
        ],
        "question": "Is it true that Our model outperforms PG-MMR when trained and tested on the Multi-News dataset?",
        "answer_label": "yes"
    },
    {
        "id": "bb3c6b73-b0df-4c55-a45f-44744409c0cf",
        "table_caption": "Zero-Shot Grounding of Objects from Natural Language Queries Table 3: Category-wise performance with the default split of Flickr30k Entities.",
        "table_column_names": [
            "Method",
            "Overall",
            "people",
            "clothing",
            "bodyparts",
            "animals",
            "vehicles",
            "instruments",
            "scene",
            "other"
        ],
        "table_content_values": [
            [
                "QRC - VGG(det)",
                "60.21",
                "75.08",
                "55.9",
                "20.27",
                "73.36",
                "68.95",
                "45.68",
                "65.27",
                "38.8"
            ],
            [
                "CITE - VGG(det)",
                "61.89",
                "[BOLD] 75.95",
                "58.50",
                "30.78",
                "[BOLD] 77.03",
                "[BOLD] 79.25",
                "48.15",
                "58.78",
                "43.24"
            ],
            [
                "ZSGNet - VGG (cls)",
                "60.12",
                "72.52",
                "60.57",
                "38.51",
                "63.61",
                "64.47",
                "49.59",
                "64.66",
                "41.09"
            ],
            [
                "ZSGNet - Res50 (cls)",
                "[BOLD] 63.39",
                "73.87",
                "[BOLD] 66.18",
                "[BOLD] 45.27",
                "73.79",
                "71.38",
                "[BOLD] 58.54",
                "[BOLD] 66.49",
                "[BOLD] 45.53"
            ]
        ],
        "question": "Is it true that However, on the classes like \"clothing\" and \"bodyparts\" our model ZSGNet shows much better performance?",
        "answer_label": "yes"
    },
    {
        "id": "6eaf2d50-5277-4860-8a5f-c43beb58c9e3",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that The coverage mechanism is not effective in our models?",
        "answer_label": "no"
    },
    {
        "id": "54e7aefb-6033-47d9-9435-ec4661f93470",
        "table_caption": "Low-supervision urgency detection and transfer in short crisis messages TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal",
        "table_column_names": [
            "System",
            "Accuracy",
            "Precision",
            "Recall",
            "F-Measure"
        ],
        "table_content_values": [
            [
                "Local",
                "63.97%",
                "64.27%",
                "64.50%",
                "63.93%"
            ],
            [
                "Manual",
                "64.25%",
                "[BOLD] 70.84%\u2217\u2217",
                "48.50%",
                "57.11%"
            ],
            [
                "Wiki",
                "67.25%",
                "66.51%",
                "69.50%",
                "67.76%"
            ],
            [
                "Local-Manual",
                "65.75%",
                "67.96%",
                "59.50%",
                "62.96%"
            ],
            [
                "Wiki-Local",
                "67.40%",
                "65.54%",
                "68.50%",
                "66.80%"
            ],
            [
                "Wiki-Manual",
                "67.75%",
                "70.38%",
                "63.00%",
                "65.79%"
            ],
            [
                "[ITALIC] Our Approach",
                "[BOLD] 69.25%\u2217\u2217\u2217",
                "68.76%",
                "[BOLD] 70.50%\u2217\u2217",
                "[BOLD] 69.44%\u2217\u2217\u2217"
            ]
        ],
        "question": "Is it true that Similarly, manual features reduce recall, but help the system to improve accuracy and precision (sometimes considerably)?",
        "answer_label": "yes"
    },
    {
        "id": "549bac5e-c1c3-4601-9908-b900f7c78abd",
        "table_caption": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.",
        "table_column_names": [
            "Model",
            "Diversity",
            "App",
            "Good%",
            "OK%",
            "Invalid%"
        ],
        "table_content_values": [
            [
                "DAMD",
                "3.12",
                "2.50",
                "56.5%",
                "[BOLD] 37.4%",
                "6.1%"
            ],
            [
                "DAMD (+)",
                "[BOLD] 3.65",
                "[BOLD] 2.53",
                "[BOLD] 63.0%",
                "27.1%",
                "9.9%"
            ],
            [
                "HDSA (+)",
                "2.14",
                "2.47",
                "57.5%",
                "32.5%",
                "[BOLD] 10.0%"
            ]
        ],
        "question": "Is it true that [CONTINUE] However, the slightly increased invalid response percentage [CONTINUE] We also observe our DAMD model outperforms HDSA in both diversity and appropriateness scores?",
        "answer_label": "yes"
    },
    {
        "id": "8f6c2db9-abdd-4e42-9ad8-3898a329957d",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
        "table_column_names": [
            "[EMPTY]",
            "Difference Function",
            "Seanad Abolition",
            "Video Games",
            "Pornography"
        ],
        "table_content_values": [
            [
                "OD-parse",
                "Absolute",
                "0.01",
                "-0.01",
                "0.07"
            ],
            [
                "OD-parse",
                "JS div.",
                "0.01",
                "-0.01",
                "-0.01"
            ],
            [
                "OD-parse",
                "EMD",
                "0.07",
                "0.01",
                "-0.01"
            ],
            [
                "OD",
                "Absolute",
                "[BOLD] 0.54",
                "[BOLD] 0.56",
                "[BOLD] 0.41"
            ],
            [
                "OD",
                "JS div.",
                "0.07",
                "-0.01",
                "-0.02"
            ],
            [
                "OD",
                "EMD",
                "0.26",
                "-0.01",
                "0.01"
            ],
            [
                "OD (no polarity shifters)",
                "Absolute",
                "0.23",
                "0.08",
                "0.04"
            ],
            [
                "OD (no polarity shifters)",
                "JS div.",
                "0.09",
                "-0.01",
                "-0.02"
            ],
            [
                "OD (no polarity shifters)",
                "EMD",
                "0.10",
                "0.01",
                "-0.01"
            ]
        ],
        "question": "Is it true that On the three datasets, OD achieves an average weighted F1 score of 0.54, 0.56 and 0.41 respectively compared to the scores of 0.01, -0.01 and 0.07 by OD-parse?",
        "answer_label": "yes"
    },
    {
        "id": "f248d065-435c-4785-bd05-398870db94b1",
        "table_caption": "Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
        "table_column_names": [
            "[BOLD] ResNet-34",
            "[BOLD] Eval set %",
            "[BOLD] #param"
        ],
        "table_content_values": [
            [
                "Baseline (No SA)Anderson et al. ( 2018 )",
                "55.00",
                "0M"
            ],
            [
                "SA (S: 1,2,3 - B: 1)",
                "55.11",
                "} 0.107M"
            ],
            [
                "SA (S: 1,2,3 - B: 2)",
                "55.17",
                "} 0.107M"
            ],
            [
                "[BOLD] SA (S: 1,2,3 - B: 3)",
                "[BOLD] 55.27",
                "} 0.107M"
            ]
        ],
        "question": "Is it true that [CONTINUE] We notice small improvements relative to the baseline showing that self-attention alone does improve the VQA task?",
        "answer_label": "yes"
    },
    {
        "id": "e0cadaaf-cbe4-40d0-82a3-380e6977dab4",
        "table_caption": "Syntactic Dependency Representations in Neural Relation Classification Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
        "table_column_names": [
            "[BOLD] Representation",
            "[BOLD] Hyper parameters Filter size",
            "[BOLD] Hyper parameters Num. Feature maps",
            "[BOLD] Hyper parameters Activation func.",
            "[BOLD] Hyper parameters L2 Reg.",
            "[BOLD] Hyper parameters Learning rate",
            "[BOLD] Hyper parameters Dropout Prob.",
            "[BOLD] F1.(avg. in 5-fold) with default values",
            "[BOLD] F1.(avg. in 5-fold) with optimal values"
        ],
        "table_content_values": [
            [
                "CoNLL08",
                "4-5",
                "1000",
                "Softplus",
                "1.15e+01",
                "1.13e-03",
                "1",
                "73.34",
                "74.49"
            ],
            [
                "SB",
                "4-5",
                "806",
                "Sigmoid",
                "8.13e-02",
                "1.79e-03",
                "0.87",
                "72.83",
                "[BOLD] 75.05"
            ],
            [
                "UD v1.3",
                "5",
                "716",
                "Softplus",
                "1.66e+00",
                "9.63E-04",
                "1",
                "68.93",
                "69.57"
            ]
        ],
        "question": "Is it true that We observe that the results for the UD representation are comparable to the two others?",
        "answer_label": "no"
    },
    {
        "id": "cc099e12-c222-4508-992e-c442f61982cf",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 1: Benchmark performance, Spearman\u2019s \u03c1. SGNS results with * taken from [morphfit]. Best results per column (benchmark) annotated for our setup only.",
        "table_column_names": [
            "Context: w2",
            "Context: w2 SimLex",
            "Context: w2 SimLex",
            "Context: w2 SimLex",
            "Context: w2 SimLex",
            "Context: w2 SimVerb"
        ],
        "table_content_values": [
            [
                "target",
                "N",
                "V",
                "A",
                "all",
                "V"
            ],
            [
                "type",
                ".334",
                "<bold>.336</bold>",
                "<bold>.518</bold>",
                ".348",
                ".307"
            ],
            [
                "x + POS",
                ".342",
                ".323",
                ".513",
                ".350",
                ".279"
            ],
            [
                "lemma",
                "<bold>.362</bold>",
                ".333",
                ".497",
                "<bold>.351</bold>",
                ".400"
            ],
            [
                "x + POS",
                ".354",
                "<bold>.336</bold>",
                ".504",
                ".345",
                "<bold>.406</bold>"
            ],
            [
                "* type",
                "-",
                "-",
                "-",
                ".339",
                ".277"
            ],
            [
                "* type MFit-A",
                "-",
                "-",
                "-",
                ".385",
                "-"
            ],
            [
                "* type MFit-AR",
                "-",
                "-",
                "-",
                ".439",
                ".381"
            ],
            [
                "Context: dep-W",
                "Context: dep-W",
                "Context: dep-W",
                "Context: dep-W",
                "Context: dep-W",
                "Context: dep-W"
            ],
            [
                "type",
                ".366",
                ".365",
                ".489",
                ".362",
                ".314"
            ],
            [
                "x + POS",
                ".364",
                ".351",
                ".482",
                ".359",
                ".287"
            ],
            [
                "lemma",
                "<bold>.391</bold>",
                ".380",
                "<bold>.522</bold>",
                "<bold>.379</bold>",
                ".401"
            ],
            [
                "x + POS",
                ".384",
                "<bold>.388</bold>",
                ".480",
                ".366",
                "<bold>.431</bold>"
            ],
            [
                "* type",
                "-",
                "-",
                "-",
                ".376",
                ".313"
            ],
            [
                "* type MFit-AR",
                "-",
                "-",
                "-",
                ".434",
                ".418"
            ]
        ],
        "question": "Is it true that Lemmatized targets generally do not perform better, with the boost being more pronounced on SimVerb?",
        "answer_label": "no"
    },
    {
        "id": "af376ecf-fa75-4847-bbcc-3b39da24aa06",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 3: Scores on unsupervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Method",
            "STS12",
            "STS13",
            "STS14",
            "STS15",
            "STS16"
        ],
        "table_content_values": [
            [
                "CBOW",
                "43.5",
                "[BOLD] 50.0",
                "[BOLD] 57.7",
                "[BOLD] 63.2",
                "61.0"
            ],
            [
                "CMOW",
                "39.2",
                "31.9",
                "38.7",
                "49.7",
                "52.2"
            ],
            [
                "Hybrid",
                "[BOLD] 49.6",
                "46.0",
                "55.1",
                "62.4",
                "[BOLD] 62.1"
            ],
            [
                "cmp. CBOW",
                "+14.6%",
                "-8%",
                "-4.5%",
                "-1.5%",
                "+1.8%"
            ],
            [
                "cmp. CMOW",
                "+26.5%",
                "+44.2%",
                "+42.4",
                "+25.6%",
                "+19.0%"
            ]
        ],
        "question": "Is it true that The hybrid model is not able to repair this deficit, increasing the difference to 8%?",
        "answer_label": "no"
    },
    {
        "id": "f21cc3b7-2533-4f2f-a259-ac3cd13583d2",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Original",
                "TGen\u2212",
                "63.37",
                "7.7188",
                "41.99",
                "68.53",
                "1.9355",
                "00.06",
                "15.77",
                "00.11",
                "15.94"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen",
                "66.41",
                "8.5565",
                "45.07",
                "69.17",
                "2.2253",
                "00.14",
                "04.11",
                "00.03",
                "04.27"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen+",
                "67.06",
                "8.5871",
                "45.83",
                "69.73",
                "2.2681",
                "00.04",
                "01.75",
                "00.01",
                "01.80"
            ],
            [
                "Original",
                "[BOLD] Original",
                "SC-LSTM",
                "39.11",
                "5.6704",
                "36.83",
                "50.02",
                "0.6045",
                "02.79",
                "18.90",
                "09.79",
                "31.51"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen\u2212",
                "65.87",
                "8.6400",
                "44.20",
                "67.51",
                "2.1710",
                "00.20",
                "00.56",
                "00.21",
                "00.97"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen",
                "66.24",
                "8.6889",
                "44.66",
                "67.85",
                "2.2181",
                "00.10",
                "00.02",
                "00.00",
                "00.12"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen+",
                "65.97",
                "8.6630",
                "44.45",
                "67.59",
                "2.1855",
                "00.02",
                "00.00",
                "00.00",
                "00.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "SC-LSTM",
                "38.52",
                "5.7125",
                "37.45",
                "48.50",
                "0.4343",
                "03.85",
                "17.39",
                "08.12",
                "29.37"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen\u2212",
                "66.28",
                "8.5202",
                "43.96",
                "67.83",
                "2.1375",
                "00.14",
                "02.26",
                "00.22",
                "02.61"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen",
                "67.00",
                "8.6889",
                "44.97",
                "68.19",
                "2.2228",
                "00.06",
                "00.44",
                "00.03",
                "00.53"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen+",
                "66.74",
                "8.6649",
                "44.84",
                "67.95",
                "2.2018",
                "00.00",
                "00.21",
                "00.03",
                "00.24"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen\u2212",
                "64.40",
                "7.9692",
                "42.81",
                "68.87",
                "2.0563",
                "00.01",
                "13.08",
                "00.00",
                "13.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen",
                "66.23",
                "8.5578",
                "45.12",
                "68.87",
                "2.2548",
                "00.04",
                "03.04",
                "00.00",
                "03.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen+",
                "65.96",
                "8.5238",
                "45.49",
                "68.79",
                "2.2456",
                "00.00",
                "01.44",
                "00.00",
                "01.45"
            ]
        ],
        "question": "Is it true that However, the main improvement of SER comes from training on cleaned data with up to 94% error reduction without the ranker and 97% with.11 just cleaning the training data has a much less dramatic effect than using a semantic control mechanism, such as the reranker (4.27% vs. 0.97% SER)?",
        "answer_label": "no"
    },
    {
        "id": "3a75d020-da89-4447-ab2b-ae91ec897986",
        "table_caption": "Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
        "table_column_names": [
            "[BOLD] ResNet-34",
            "[BOLD] Eval set %",
            "[BOLD] #param"
        ],
        "table_content_values": [
            [
                "SA (S: 3 - M: 1)",
                "55.25",
                "} 0.082M"
            ],
            [
                "[BOLD] SA (S: 3 - B: 3)",
                "[BOLD] 55.42",
                "} 0.082M"
            ],
            [
                "SA (S: 3 - B: 4)",
                "55.33",
                "} 0.082M"
            ],
            [
                "SA (S: 3 - B: 6)",
                "55.31",
                "} 0.082M"
            ],
            [
                "SA (S: 3 - B: 1,3,5)",
                "55.45",
                "} 0.245M"
            ],
            [
                "[BOLD] SA (S: 3 - B: 2,4,6)",
                "[BOLD] 55.56",
                "} 0.245M"
            ]
        ],
        "question": "Is it true that Though the improvement is slim, it is encouraging to continue researching into visual modulation?",
        "answer_label": "yes"
    },
    {
        "id": "ad70b00f-de99-4e50-88ae-fb0d5320778d",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that Consequently, with an 8% i is substantially more linguistically informed than CBOW?",
        "answer_label": "yes"
    },
    {
        "id": "82ec1cdd-1c49-4aaa-9764-e45b072fb22d",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
        "table_column_names": [
            "[EMPTY]",
            "Difference Function",
            "Seanad Abolition",
            "Video Games",
            "Pornography"
        ],
        "table_content_values": [
            [
                "OD-parse",
                "Absolute",
                "0.01",
                "-0.01",
                "0.07"
            ],
            [
                "OD-parse",
                "JS div.",
                "0.01",
                "-0.01",
                "-0.01"
            ],
            [
                "OD-parse",
                "EMD",
                "0.07",
                "0.01",
                "-0.01"
            ],
            [
                "OD",
                "Absolute",
                "[BOLD] 0.54",
                "[BOLD] 0.56",
                "[BOLD] 0.41"
            ],
            [
                "OD",
                "JS div.",
                "0.07",
                "-0.01",
                "-0.02"
            ],
            [
                "OD",
                "EMD",
                "0.26",
                "-0.01",
                "0.01"
            ],
            [
                "OD (no polarity shifters)",
                "Absolute",
                "0.23",
                "0.08",
                "0.04"
            ],
            [
                "OD (no polarity shifters)",
                "JS div.",
                "0.09",
                "-0.01",
                "-0.02"
            ],
            [
                "OD (no polarity shifters)",
                "EMD",
                "0.10",
                "0.01",
                "-0.01"
            ]
        ],
        "question": "Is it true that [CONTINUE] Sentiment polarity shifters have a low impact on clustering performance of opinion distance: We find that not utilizing the sentiment polarity shifters, especially in case of datasets \"Video games\" and \"Pornography\" does not significantly hurt the Opinion Representation phase, and thereby does not lead to incorrect computation of opinion distance?",
        "answer_label": "no"
    },
    {
        "id": "435103bb-73be-4283-91b1-b429a3b988b7",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
        "table_column_names": [
            "# of Heads",
            "Accuracy",
            "Val. Loss",
            "Effect"
        ],
        "table_content_values": [
            [
                "1",
                "89.44%",
                "0.2811",
                "-6.84%"
            ],
            [
                "2",
                "91.20%",
                "0.2692",
                "-5.08%"
            ],
            [
                "4",
                "93.85%",
                "0.2481",
                "-2.43%"
            ],
            [
                "8",
                "96.02%",
                "0.2257",
                "-0.26%"
            ],
            [
                "10",
                "96.28%",
                "0.2197",
                "[EMPTY]"
            ],
            [
                "16",
                "96.32%",
                "0.2190",
                "+0.04"
            ]
        ],
        "question": "Is it true that Using only one attention head, thereby attending to only one context position at once, degrades the performance to less than the performance of 10 heads using the standard finetuning scheme?",
        "answer_label": "yes"
    },
    {
        "id": "e3b3b4b9-da35-4fa1-b2ca-bd86c3df2677",
        "table_caption": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.",
        "table_column_names": [
            "Feature",
            "LR P",
            "LR R",
            "LR F1",
            "SVM P",
            "SVM R",
            "SVM F1",
            "ANN P",
            "ANN R",
            "ANN F1"
        ],
        "table_content_values": [
            [
                "+BoW",
                "0.93",
                "0.91",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+BoC (Wiki-PubMed-PMC)",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.91",
                "0.91",
                "[BOLD] 0.91"
            ],
            [
                "+BoC (GloVe)",
                "0.93",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+ASM",
                "0.90",
                "0.85",
                "0.88",
                "0.90",
                "0.86",
                "0.88",
                "0.89",
                "0.89",
                "0.89"
            ],
            [
                "+Sentence Embeddings(SEs)",
                "0.89",
                "0.89",
                "0.89",
                "0.90",
                "0.86",
                "0.88",
                "0.88",
                "0.88",
                "0.88"
            ],
            [
                "+BoC(Wiki-PubMed-PMC)+SEs",
                "0.92",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ]
        ],
        "question": "Is it true that [CONTINUE] Wikipedia-PubMed-PMC embeddings (Moen and Ananiadou, 2013) does not outperform GloVe (Mikolov et al., 2013a) in the extraction of most relation types (Table 1) [CONTINUE] the combination feature of BoC and sentence embeddings does not outperform sentence embeddings alone, and does not exceed the upper boundary of BoC feature, in which again demonstrating the lack of competitiveness of BoC feature?",
        "answer_label": "no"
    },
    {
        "id": "3de653b7-7800-41e0-8431-4f7ea3574f5d",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1192",
                "0.0083",
                "0.0137",
                "0.0150",
                "0.0150",
                "0.0445",
                "0.0326"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1022",
                "0.0069",
                "0.0060",
                "0.0092",
                "0.0090",
                "0.0356",
                "0.0162"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5710",
                "0.1948",
                "0.3855",
                "0.5474",
                "0.4485",
                "[BOLD] 0.8052",
                "0.4058"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "[BOLD] 0.6304",
                "0.1870",
                "0.3250",
                "0.5312",
                "0.4576",
                "0.6064",
                "0.3698"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0037",
                "0.3278",
                "0.5941",
                "0.6486",
                "[BOLD] 0.6490",
                "0.0017",
                "0.0003"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0002",
                "0.1486",
                "0.4332",
                "[BOLD] 0.6467",
                "0.6332",
                "0.0967",
                "0.0003"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0002",
                "0.1562",
                "0.5157",
                "[BOLD] 0.7255",
                "0.5932",
                "0.0032",
                "0.0001"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "2.10-5",
                "0.0507",
                "0.4492",
                "[BOLD] 0.7000",
                "0.5887",
                "0.1390",
                "0.0002"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0073",
                "0.0162",
                "0.0268",
                "[BOLD] 0.0293",
                "[BOLD] 0.0293",
                "0.0033",
                "0.0006"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0004",
                "0.0132",
                "0.0118",
                "0.0181",
                "0.0179",
                "[BOLD] 0.0520",
                "0.0005"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0005",
                "0.1733",
                "0.4412",
                "[BOLD] 0.6240",
                "0.5109",
                "0.0064",
                "0.0002"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "4.10-5",
                "0.0798",
                "0.3771",
                "[BOLD] 0.6040",
                "0.5149",
                "0.2261",
                "0.0004"
            ]
        ],
        "question": "Is it true that When increasing the number of terms to 10,000, the DocSub models using Europarl corpora performed better than when using TED Talks corpora?",
        "answer_label": "yes"
    },
    {
        "id": "7e8e2c4f-92ea-4c80-a27c-84cae2005a80",
        "table_caption": "On the difficulty of a distributional semantics of spoken language Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.",
        "table_column_names": [
            "[EMPTY]",
            "Recall@10 (%)",
            "Median rank",
            "RSAimage"
        ],
        "table_content_values": [
            [
                "VGS",
                "27",
                "6",
                "0.4"
            ],
            [
                "SegMatch",
                "[BOLD] 10",
                "[BOLD] 37",
                "[BOLD] 0.5"
            ],
            [
                "Audio2vec-U",
                "5",
                "105",
                "0.0"
            ],
            [
                "Audio2vec-C",
                "2",
                "647",
                "0.0"
            ],
            [
                "Mean MFCC",
                "1",
                "1,414",
                "0.0"
            ],
            [
                "Chance",
                "0",
                "3,955",
                "0.0"
            ]
        ],
        "question": "Is it true that SegMatch works slightly better than Audio2vec according to both criteria?",
        "answer_label": "no"
    },
    {
        "id": "85a0c565-04f4-4ff4-9b4f-9f8c0e1be995",
        "table_caption": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents Table 1: The scores of our three submitted runs for similarity threshold 50%.",
        "table_column_names": [
            "Run ID",
            "Official score",
            "Score with correction"
        ],
        "table_content_values": [
            [
                "ep_1",
                "60.29",
                "66.76"
            ],
            [
                "ep_2",
                "[BOLD] 60.90",
                "[BOLD] 67.35"
            ],
            [
                "ep_3",
                "60.61",
                "67.07"
            ]
        ],
        "question": "Is it true that The system's official score was 60.9% (micro-F1) [CONTINUE] af [CONTINUE] However, re-scoring our second submission after replacing these 10 files with the ones from our first submission resulted in a lower score of 67.07%?",
        "answer_label": "no"
    },
    {
        "id": "37504e87-a51e-4d90-bc49-27f562c1784e",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
        "table_column_names": [
            "<bold>Model</bold>",
            "REF \u21d2 GEN <bold>ENT</bold>",
            "REF \u21d2 GEN <bold>CON</bold>",
            "REF \u21d2 GEN <bold>NEU</bold>"
        ],
        "table_content_values": [
            [
                "S2S",
                "38.45",
                "11.17",
                "50.38"
            ],
            [
                "G2S-GIN",
                "49.78",
                "9.80",
                "40.42"
            ],
            [
                "G2S-GAT",
                "49.48",
                "8.09",
                "42.43"
            ],
            [
                "G2S-GGNN",
                "51.32",
                "8.82",
                "39.86"
            ],
            [
                "[EMPTY]",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF"
            ],
            [
                "<bold>Model</bold>",
                "<bold>ENT</bold>",
                "<bold>CON</bold>",
                "<bold>NEU</bold>"
            ],
            [
                "S2S",
                "73.79",
                "12.75",
                "13.46"
            ],
            [
                "G2S-GIN",
                "76.27",
                "10.65",
                "13.08"
            ],
            [
                "G2S-GAT",
                "77.54",
                "8.54",
                "13.92"
            ],
            [
                "G2S-GGNN",
                "77.64",
                "9.64",
                "12.72"
            ]
        ],
        "question": "Is it true that All G2S models have [CONTINUE] higher entailment compared to S2S?",
        "answer_label": "yes"
    },
    {
        "id": "6ff4558a-e94b-4bf7-88fc-ab7a7f88c1fa",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
        "table_column_names": [
            "[BOLD] Method",
            "[BOLD] R-1",
            "[BOLD] R-2",
            "[BOLD] R-SU"
        ],
        "table_content_values": [
            [
                "First-1",
                "26.83",
                "7.25",
                "6.46"
            ],
            [
                "First-2",
                "35.99",
                "10.17",
                "12.06"
            ],
            [
                "First-3",
                "39.41",
                "11.77",
                "14.51"
            ],
            [
                "LexRank Erkan and Radev ( 2004 )",
                "38.27",
                "12.70",
                "13.20"
            ],
            [
                "TextRank Mihalcea and Tarau ( 2004 )",
                "38.44",
                "13.10",
                "13.50"
            ],
            [
                "MMR Carbonell and Goldstein ( 1998 )",
                "38.77",
                "11.98",
                "12.91"
            ],
            [
                "PG-Original Lebanoff et\u00a0al. ( 2018 )",
                "41.85",
                "12.91",
                "16.46"
            ],
            [
                "PG-MMR Lebanoff et\u00a0al. ( 2018 )",
                "40.55",
                "12.36",
                "15.87"
            ],
            [
                "PG-BRNN Gehrmann et\u00a0al. ( 2018 )",
                "42.80",
                "14.19",
                "16.75"
            ],
            [
                "CopyTransformer Gehrmann et\u00a0al. ( 2018 )",
                "[BOLD] 43.57",
                "14.03",
                "17.37"
            ],
            [
                "Hi-MAP (Our Model)",
                "43.47",
                "[BOLD] 14.89",
                "[BOLD] 17.41"
            ]
        ],
        "question": "Is it true that Our model does not outperform PG-MMR when trained and tested on the Multi-News dataset?",
        "answer_label": "no"
    },
    {
        "id": "6200a720-4ce2-40ec-a91f-ad7f8ddfd37c",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that Compared to CMOW, the hybrid model shows significant differences?",
        "answer_label": "no"
    },
    {
        "id": "b3d0a10d-235d-4eb2-bede-f573f3a44812",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.",
        "table_column_names": [
            "[BOLD] Language pair",
            "[BOLD] Model type",
            "[BOLD] Oracle model",
            "[BOLD] Decoder configuration  [BOLD] Uniform",
            "[BOLD] Decoder configuration  [BOLD] BI + IS"
        ],
        "table_content_values": [
            [
                "es-en",
                "Unadapted",
                "36.4",
                "34.7",
                "36.6"
            ],
            [
                "es-en",
                "No-reg",
                "36.6",
                "34.8",
                "-"
            ],
            [
                "es-en",
                "EWC",
                "37.0",
                "36.3",
                "[BOLD] 37.2"
            ],
            [
                "en-de",
                "Unadapted",
                "36.4",
                "26.8",
                "38.8"
            ],
            [
                "en-de",
                "No-reg",
                "41.7",
                "31.8",
                "-"
            ],
            [
                "en-de",
                "EWC",
                "42.1",
                "38.6",
                "[BOLD] 42.0"
            ]
        ],
        "question": "Is it true that BI+IS decoding with single-domain trained models does not achieve gains over both the naive uniform approach and over oracle single-domain models?",
        "answer_label": "no"
    },
    {
        "id": "a3e27840-583c-4bc7-a60e-1acc2790dea2",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.",
        "table_column_names": [
            "[BOLD] Decoder configuration",
            "[BOLD] es-en  [BOLD] Health",
            "[BOLD] es-en  [BOLD] Bio",
            "[BOLD] en-de  [BOLD] News",
            "[BOLD] en-de  [BOLD] TED",
            "[BOLD] en-de  [BOLD] IT"
        ],
        "table_content_values": [
            [
                "Oracle model",
                "35.9",
                "36.1",
                "37.8",
                "24.1",
                "39.6"
            ],
            [
                "Uniform",
                "33.1",
                "36.4",
                "21.9",
                "18.4",
                "38.9"
            ],
            [
                "Identity-BI",
                "35.0",
                "36.6",
                "32.7",
                "25.3",
                "42.6"
            ],
            [
                "BI",
                "35.9",
                "36.5",
                "38.0",
                "26.1",
                "[BOLD] 44.7"
            ],
            [
                "IS",
                "[BOLD] 36.0",
                "36.8",
                "37.5",
                "25.6",
                "43.3"
            ],
            [
                "BI + IS",
                "[BOLD] 36.0",
                "[BOLD] 36.9",
                "[BOLD] 38.4",
                "[BOLD] 26.4",
                "[BOLD] 44.7"
            ]
        ],
        "question": "Is it true that Table 5 shows that uniform ensembling outperforms all oracle models except es-en Bio, especially on general domains?",
        "answer_label": "no"
    },
    {
        "id": "ec1c8929-3671-4e7f-8738-bf6ed393975e",
        "table_caption": "Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
        "table_column_names": [
            "Dataset",
            "Metric",
            "Illinois",
            "IlliCons",
            "rahman2012resolving",
            "KnowFeat",
            "KnowCons",
            "KnowComb"
        ],
        "table_content_values": [
            [
                "[ITALIC] Winograd",
                "Precision",
                "51.48",
                "53.26",
                "73.05",
                "71.81",
                "74.93",
                "[BOLD] 76.41"
            ],
            [
                "[ITALIC] WinoCoref",
                "AntePre",
                "68.37",
                "74.32",
                "\u2014\u2013",
                "88.48",
                "88.95",
                "[BOLD] 89.32"
            ]
        ],
        "question": "Is it true that The results show that it is better to add knowledge as features when the knowledge quality is high than compile them into constraints?",
        "answer_label": "no"
    },
    {
        "id": "5879912f-a133-4644-b984-422b306d3d34",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 7: Comparisons of different DCGCN models under almost the same parameter budget.",
        "table_column_names": [
            "[BOLD] Model",
            "D",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN(1)",
                "300",
                "10.9M",
                "20.9",
                "52.0"
            ],
            [
                "DCGCN(2)",
                "180",
                "10.9M",
                "[BOLD] 22.2",
                "[BOLD] 52.3"
            ],
            [
                "DCGCN(2)",
                "240",
                "11.3M",
                "22.8",
                "52.8"
            ],
            [
                "DCGCN(4)",
                "180",
                "11.4M",
                "[BOLD] 23.4",
                "[BOLD] 53.4"
            ],
            [
                "DCGCN(1)",
                "420",
                "12.6M",
                "22.2",
                "52.4"
            ],
            [
                "DCGCN(2)",
                "300",
                "12.5M",
                "23.8",
                "53.8"
            ],
            [
                "DCGCN(3)",
                "240",
                "12.3M",
                "[BOLD] 23.9",
                "[BOLD] 54.1"
            ],
            [
                "DCGCN(2)",
                "360",
                "14.0M",
                "24.2",
                "[BOLD] 54.4"
            ],
            [
                "DCGCN(3)",
                "300",
                "14.0M",
                "[BOLD] 24.4",
                "54.2"
            ],
            [
                "DCGCN(2)",
                "420",
                "15.6M",
                "24.1",
                "53.7"
            ],
            [
                "DCGCN(4)",
                "300",
                "15.6M",
                "[BOLD] 24.6",
                "[BOLD] 54.8"
            ],
            [
                "DCGCN(3)",
                "420",
                "18.6M",
                "24.5",
                "54.6"
            ],
            [
                "DCGCN(4)",
                "360",
                "18.4M",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that In general, we found when the parameter budget is the same, deeper DCGCN models can obtain better results than the shallower ones?",
        "answer_label": "yes"
    },
    {
        "id": "08c3b235-a0d2-41d2-a7f1-046b2f03f269",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 3: Literature results with various systems and automatic metrics at a nearly-fixed Acc, with best scores in boldface. We use M0 to denote shen-1.",
        "table_column_names": [
            "[EMPTY]",
            "Acc",
            "Sim",
            "PP",
            "GM"
        ],
        "table_content_values": [
            [
                "M0: shen-1",
                "0.694",
                "0.728",
                "[BOLD] 22.3",
                "8.81"
            ],
            [
                "M1: M0 [ITALIC] +para",
                "0.702",
                "0.747",
                "23.6",
                "11.7"
            ],
            [
                "M2: M0 [ITALIC] +cyc",
                "0.692",
                "0.781",
                "49.9",
                "[BOLD] 12.8"
            ],
            [
                "M3: M0 [ITALIC] +cyc+lang",
                "0.698",
                "0.754",
                "39.2",
                "12.0"
            ],
            [
                "M4: M0 [ITALIC] +cyc+para",
                "0.702",
                "0.757",
                "33.9",
                "[BOLD] 12.8"
            ],
            [
                "M5: M0 [ITALIC] +cyc+para+lang",
                "0.688",
                "0.753",
                "28.6",
                "11.8"
            ],
            [
                "M6: M0 [ITALIC] +cyc+2d",
                "0.704",
                "[BOLD] 0.794",
                "63.2",
                "[BOLD] 12.8"
            ],
            [
                "M7: M6+ [ITALIC] para+lang",
                "0.706",
                "0.768",
                "49.0",
                "[BOLD] 12.8"
            ]
        ],
        "question": "Is it true that [CONTINUE] When comparing between M2 and M3, between M4 and M5, and between M6 and M7, we find that the addition of the language modeling loss increases PP, sometimes at a slight cost of semantic preservation?",
        "answer_label": "no"
    },
    {
        "id": "dfc949b8-d660-4336-b9bd-4cd308f74cd4",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.",
        "table_column_names": [
            "[BOLD] Decoder configuration",
            "[BOLD] es-en  [BOLD] Health",
            "[BOLD] es-en  [BOLD] Bio",
            "[BOLD] en-de  [BOLD] News",
            "[BOLD] en-de  [BOLD] TED",
            "[BOLD] en-de  [BOLD] IT"
        ],
        "table_content_values": [
            [
                "Oracle model",
                "35.9",
                "36.1",
                "37.8",
                "24.1",
                "39.6"
            ],
            [
                "Uniform",
                "33.1",
                "36.4",
                "21.9",
                "18.4",
                "38.9"
            ],
            [
                "Identity-BI",
                "35.0",
                "36.6",
                "32.7",
                "25.3",
                "42.6"
            ],
            [
                "BI",
                "35.9",
                "36.5",
                "38.0",
                "26.1",
                "[BOLD] 44.7"
            ],
            [
                "IS",
                "[BOLD] 36.0",
                "36.8",
                "37.5",
                "25.6",
                "43.3"
            ],
            [
                "BI + IS",
                "[BOLD] 36.0",
                "[BOLD] 36.9",
                "[BOLD] 38.4",
                "[BOLD] 26.4",
                "[BOLD] 44.7"
            ]
        ],
        "question": "Is it true that BI and IS both individually outperform the oracle for all but IS-News, [CONTINUE] With adaptive decoding, we do not need to assume whether a uniform ensemble or a single model might perform better for some potentially unknown domain?",
        "answer_label": "yes"
    },
    {
        "id": "2861b4ef-ab1a-4f84-b5ff-37cfcd01f477",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that Despite the models having fewer examples of bigger graphs to learn from, this does not lead to worse performance when handling graphs with higher diameters?",
        "answer_label": "no"
    },
    {
        "id": "952269e3-91ba-422b-9157-7a84243d785f",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that Analyzing Table 3, we can observe that all values of precision using the English corpora have higher scores when compared with the Portuguese corpora?",
        "answer_label": "no"
    },
    {
        "id": "b9d6270e-87be-44f9-982a-a037c7cfd0bd",
        "table_caption": "Syntactic Dependency Representations in Neural Relation Classification Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
        "table_column_names": [
            "[BOLD] Representation",
            "[BOLD] Hyper parameters Filter size",
            "[BOLD] Hyper parameters Num. Feature maps",
            "[BOLD] Hyper parameters Activation func.",
            "[BOLD] Hyper parameters L2 Reg.",
            "[BOLD] Hyper parameters Learning rate",
            "[BOLD] Hyper parameters Dropout Prob.",
            "[BOLD] F1.(avg. in 5-fold) with default values",
            "[BOLD] F1.(avg. in 5-fold) with optimal values"
        ],
        "table_content_values": [
            [
                "CoNLL08",
                "4-5",
                "1000",
                "Softplus",
                "1.15e+01",
                "1.13e-03",
                "1",
                "73.34",
                "74.49"
            ],
            [
                "SB",
                "4-5",
                "806",
                "Sigmoid",
                "8.13e-02",
                "1.79e-03",
                "0.87",
                "72.83",
                "[BOLD] 75.05"
            ],
            [
                "UD v1.3",
                "5",
                "716",
                "Softplus",
                "1.66e+00",
                "9.63E-04",
                "1",
                "68.93",
                "69.57"
            ]
        ],
        "question": "Is it true that The results furthermore show that the sdps based on the Stanford Basic (SB) representation do not provide the best performance, followed by the CoNLL08 representation?",
        "answer_label": "no"
    },
    {
        "id": "48134843-e7f0-4987-98fd-e830f58e1b3a",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
        "table_column_names": [
            "[BOLD] Emoji alias",
            "[BOLD] N",
            "[BOLD] emoji #",
            "[BOLD] emoji %",
            "[BOLD] no-emoji #",
            "[BOLD] no-emoji %",
            "[BOLD] \u0394%"
        ],
        "table_content_values": [
            [
                "mask",
                "163",
                "154",
                "94.48",
                "134",
                "82.21",
                "- 12.27"
            ],
            [
                "two_hearts",
                "87",
                "81",
                "93.10",
                "77",
                "88.51",
                "- 4.59"
            ],
            [
                "heart_eyes",
                "122",
                "109",
                "89.34",
                "103",
                "84.43",
                "- 4.91"
            ],
            [
                "heart",
                "267",
                "237",
                "88.76",
                "235",
                "88.01",
                "- 0.75"
            ],
            [
                "rage",
                "92",
                "78",
                "84.78",
                "66",
                "71.74",
                "- 13.04"
            ],
            [
                "cry",
                "116",
                "97",
                "83.62",
                "83",
                "71.55",
                "- 12.07"
            ],
            [
                "sob",
                "490",
                "363",
                "74.08",
                "345",
                "70.41",
                "- 3.67"
            ],
            [
                "unamused",
                "167",
                "121",
                "72.46",
                "116",
                "69.46",
                "- 3.00"
            ],
            [
                "weary",
                "204",
                "140",
                "68.63",
                "139",
                "68.14",
                "- 0.49"
            ],
            [
                "joy",
                "978",
                "649",
                "66.36",
                "629",
                "64.31",
                "- 2.05"
            ],
            [
                "sweat_smile",
                "111",
                "73",
                "65.77",
                "75",
                "67.57",
                "1.80"
            ],
            [
                "confused",
                "77",
                "46",
                "59.74",
                "48",
                "62.34",
                "2.60"
            ]
        ],
        "question": "Is it true that [CONTINUE] When removing sweat smile and confused accuracy increased,?",
        "answer_label": "yes"
    },
    {
        "id": "84aa63b6-4dad-4cc3-885a-d5d43259dae9",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 2: Experiment 1",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.001",
                "0.003",
                "-20.818",
                "***",
                "0.505"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.083",
                "0.048",
                "101.636",
                "***",
                "1.724"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.001",
                "0.001",
                "0.035",
                "[EMPTY]",
                "1.001"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.023",
                "0.012",
                "64.418",
                "***",
                "1.993"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.002",
                "0.001",
                "4.047",
                "***",
                "1.120"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.049",
                "0.019",
                "120.986",
                "***",
                "2.573"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.173",
                "0.065",
                "243.285",
                "***",
                "2.653"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.032",
                "0.023",
                "39.483",
                "***",
                "1.396"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.111",
                "0.061",
                "122.707",
                "***",
                "1.812"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.178",
                "0.080",
                "211.319",
                "***",
                "2.239"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.028",
                "0.015",
                "63.131",
                "***",
                "1.854"
            ]
        ],
        "question": "Is it true that For Waseem (2016) we see that there is a significant difference in the estimated rates at which tweets are classified as racist across groups, with higher rates for the white group?",
        "answer_label": "no"
    },
    {
        "id": "7b320190-e9bb-4603-8a20-b4a8f2c7fdbc",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] External",
            "B"
        ],
        "table_content_values": [
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "-",
                "22.0"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "-",
                "23.3"
            ],
            [
                "GCNSEQ (Damonte and Cohen,  2019 )",
                "-",
                "24.4"
            ],
            [
                "DCGCN(single)",
                "-",
                "25.9"
            ],
            [
                "DCGCN(ensemble)",
                "-",
                "[BOLD] 28.2"
            ],
            [
                "TSP (Song et al.,  2016 )",
                "ALL",
                "22.4"
            ],
            [
                "PBMT (Pourdamghani et al.,  2016 )",
                "ALL",
                "26.9"
            ],
            [
                "Tree2Str (Flanigan et al.,  2016 )",
                "ALL",
                "23.0"
            ],
            [
                "SNRG (Song et al.,  2017 )",
                "ALL",
                "25.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "0.2M",
                "27.4"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "0.2M",
                "28.2"
            ],
            [
                "DCGCN(single)",
                "0.1M",
                "29.0"
            ],
            [
                "DCGCN(single)",
                "0.2M",
                "[BOLD] 31.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "2M",
                "32.3"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "2M",
                "33.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "20M",
                "33.8"
            ],
            [
                "DCGCN(single)",
                "0.3M",
                "33.2"
            ],
            [
                "DCGCN(ensemble)",
                "0.3M",
                "[BOLD] 35.3"
            ]
        ],
        "question": "Is it true that DCGCN model is not able to achieve a competitive BLEU points (33.2) by using 0.3M external data, while GraphLSTM achieves a higher score of 33.6 by using 2M data and Seq2SeqK achieves an even higher score of 33.8 by using 20M data?",
        "answer_label": "no"
    },
    {
        "id": "423aa89a-9bca-4bf2-a8aa-78154555b63b",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
        "table_column_names": [
            "Cue",
            "App.",
            "Prod.",
            "Cov."
        ],
        "table_content_values": [
            [
                "in",
                "47",
                "55.3",
                "9.40"
            ],
            [
                "was",
                "55",
                "61.8",
                "11.0"
            ],
            [
                "to",
                "82",
                "40.2",
                "16.4"
            ],
            [
                "the",
                "85",
                "38.8",
                "17.0"
            ],
            [
                "a",
                "106",
                "57.5",
                "21.2"
            ]
        ],
        "question": "Is it true that For example, a is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 21.2% of COPA training instances?",
        "answer_label": "yes"
    },
    {
        "id": "deefe413-93b4-46a5-915c-e5ff31c48ab8",
        "table_caption": "Keyphrase Generation for Scientific Articles using GANs Table 2: \u03b1-nDCG@5 metrics",
        "table_column_names": [
            "Model",
            "Inspec",
            "Krapivin",
            "NUS",
            "KP20k"
        ],
        "table_content_values": [
            [
                "Catseq",
                "0.87803",
                "0.781",
                "0.82118",
                "0.804"
            ],
            [
                "Catseq-RL",
                "0.8602",
                "[BOLD] 0.786",
                "0.83",
                "0.809"
            ],
            [
                "GAN",
                "[BOLD] 0.891",
                "0.771",
                "[BOLD] 0.853",
                "[BOLD] 0.85"
            ]
        ],
        "question": "Is it true that Our model does not obtain the best performance on three out of the four datasets?",
        "answer_label": "no"
    },
    {
        "id": "6c9eeac3-a181-46e5-9b58-03aece5eb01a",
        "table_caption": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 5: Results of the Human Rating on CWC.",
        "table_column_names": [
            "System",
            "Succ. (%)",
            "Smoothness"
        ],
        "table_content_values": [
            [
                "Retrieval-Stgy\u00a0",
                "54.0",
                "2.48"
            ],
            [
                "PMI\u00a0",
                "46.0",
                "2.56"
            ],
            [
                "Neural\u00a0",
                "36.0",
                "2.50"
            ],
            [
                "Kernel\u00a0",
                "58.0",
                "2.48"
            ],
            [
                "DKRN (ours)",
                "[BOLD] 88.0",
                "[BOLD] 3.22"
            ]
        ],
        "question": "Is it true that All other agents outperform our DKRN agent with a large margin?",
        "answer_label": "no"
    },
    {
        "id": "e0687e8f-e6c6-43ea-854d-2257d5ec9015",
        "table_caption": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents Table 1: The scores of our three submitted runs for similarity threshold 50%.",
        "table_column_names": [
            "Run ID",
            "Official score",
            "Score with correction"
        ],
        "table_content_values": [
            [
                "ep_1",
                "60.29",
                "66.76"
            ],
            [
                "ep_2",
                "[BOLD] 60.90",
                "[BOLD] 67.35"
            ],
            [
                "ep_3",
                "60.61",
                "67.07"
            ]
        ],
        "question": "Is it true that The system's official score was 60.9% (micro-F1) [CONTINUE] af [CONTINUE] Therefore, we report both the official score (from our second submission) and the result of re-scoring our second submission after replacing these 10 files with the ones from our first submission?",
        "answer_label": "yes"
    },
    {
        "id": "8e604b9f-d3fa-4a47-bd09-06cd2caf18c1",
        "table_caption": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task Table 2: Image-caption ranking results for German (Multi30k)",
        "table_column_names": [
            "[EMPTY]",
            "Image to Text R@1",
            "Image to Text R@5",
            "Image to Text R@10",
            "Image to Text Mr",
            "Text to Image R@1",
            "Text to Image R@5",
            "Text to Image R@10",
            "Text to Image Mr",
            "Alignment"
        ],
        "table_content_values": [
            [
                "[BOLD] symmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Parallel\u00a0gella:17",
                "28.2",
                "57.7",
                "71.3",
                "4",
                "20.9",
                "46.9",
                "59.3",
                "6",
                "-"
            ],
            [
                "Mono",
                "34.2",
                "67.5",
                "79.6",
                "3",
                "26.5",
                "54.7",
                "66.2",
                "4",
                "-"
            ],
            [
                "FME",
                "36.8",
                "69.4",
                "80.8",
                "2",
                "26.6",
                "56.2",
                "68.5",
                "4",
                "76.81%"
            ],
            [
                "AME",
                "[BOLD] 39.6",
                "[BOLD] 72.7",
                "[BOLD] 82.7",
                "[BOLD] 2",
                "[BOLD] 28.9",
                "[BOLD] 58.0",
                "[BOLD] 68.7",
                "[BOLD] 4",
                "66.91%"
            ],
            [
                "[BOLD] asymmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Pivot\u00a0gella:17",
                "28.2",
                "61.9",
                "73.4",
                "3",
                "22.5",
                "49.3",
                "61.7",
                "6",
                "-"
            ],
            [
                "Parallel\u00a0gella:17",
                "30.2",
                "60.4",
                "72.8",
                "3",
                "21.8",
                "50.5",
                "62.3",
                "5",
                "-"
            ],
            [
                "Mono",
                "[BOLD] 42.0",
                "72.5",
                "83.0",
                "2",
                "29.6",
                "58.4",
                "69.6",
                "4",
                "-"
            ],
            [
                "FME",
                "40.5",
                "73.3",
                "83.4",
                "2",
                "29.6",
                "59.2",
                "[BOLD] 72.1",
                "3",
                "76.81%"
            ],
            [
                "AME",
                "40.5",
                "[BOLD] 74.3",
                "[BOLD] 83.4",
                "[BOLD] 2",
                "[BOLD] 31.0",
                "[BOLD] 60.5",
                "70.6",
                "[BOLD] 3",
                "73.10%"
            ]
        ],
        "question": "Is it true that For German descriptions, The results are 11.05% better on average compared to (Gella et al., 2017) in symmetric mode?",
        "answer_label": "yes"
    },
    {
        "id": "bd3a799a-c562-4476-ad82-23a43d147582",
        "table_caption": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations Table 8: Sentiment classification evaluation, using different classifiers on the test set.",
        "table_column_names": [
            "Classifier",
            "Positive Sentiment Precision",
            "Positive Sentiment Recall",
            "Positive Sentiment Fscore"
        ],
        "table_content_values": [
            [
                "SVM-w/o neg.",
                "0.57",
                "0.72",
                "0.64"
            ],
            [
                "SVM-Punct. neg.",
                "0.58",
                "0.70",
                "0.63"
            ],
            [
                "SVM-our-neg.",
                "0.58",
                "0.73",
                "0.65"
            ],
            [
                "CNN",
                "0.63",
                "0.83",
                "0.72"
            ],
            [
                "CNN-LSTM",
                "0.71",
                "0.72",
                "0.72"
            ],
            [
                "CNN-LSTM-Our-neg-Ant",
                "[BOLD] 0.78",
                "[BOLD] 0.77",
                "[BOLD] 0.78"
            ],
            [
                "[EMPTY]",
                "Negative Sentiment",
                "Negative Sentiment",
                "Negative Sentiment"
            ],
            [
                "[EMPTY]",
                "Precision",
                "Recall",
                "Fscore"
            ],
            [
                "SVM-w/o neg.",
                "0.78",
                "0.86",
                "0.82"
            ],
            [
                "SVM-Punct. neg.",
                "0.78",
                "0.87",
                "0.83"
            ],
            [
                "SVM-Our neg.",
                "0.80",
                "0.87",
                "0.83"
            ],
            [
                "CNN",
                "0.88",
                "0.72",
                "0.79"
            ],
            [
                "CNN-LSTM.",
                "0.83",
                "0.83",
                "0.83"
            ],
            [
                "CNN-LSTM-our-neg-Ant",
                "[BOLD] 0.87",
                "[BOLD] 0.87",
                "[BOLD] 0.87"
            ],
            [
                "[EMPTY]",
                "Train",
                "[EMPTY]",
                "Test"
            ],
            [
                "Positive tweets",
                "5121",
                "[EMPTY]",
                "1320"
            ],
            [
                "Negative tweets",
                "9094",
                "[EMPTY]",
                "2244"
            ]
        ],
        "question": "Is it true that The proposed CNN-LSTMOur-neg-Ant does not improve upon the simple CNNLSTM-w/o neg?",
        "answer_label": "no"
    },
    {
        "id": "22787daa-80c1-4204-aced-20a547c65df4",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
        "table_column_names": [
            "Type",
            "Inform Mean",
            "Inform Num",
            "Match Mean",
            "Match Num",
            "Success Mean",
            "Success Num"
        ],
        "table_content_values": [
            [
                "Full",
                "8.413",
                "903",
                "10.59",
                "450",
                "11.18",
                "865"
            ],
            [
                "Other",
                "-99.95",
                "76",
                "-48.15",
                "99",
                "-71.62",
                "135"
            ]
        ],
        "question": "Is it true that [CONTINUE] It can be observed that the learned reward function has good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise?",
        "answer_label": "yes"
    },
    {
        "id": "d7e9fa5f-41af-46a2-b8e9-34424334ea6f",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Feature",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Feature",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams"
            ],
            [
                "not",
                ".154",
                "[URL]",
                ".150"
            ],
            [
                "my",
                ".131",
                "!",
                ".082"
            ],
            [
                "working",
                ".124",
                "he",
                ".069"
            ],
            [
                "still",
                ".123",
                "thank",
                ".067"
            ],
            [
                "on",
                ".119",
                ",",
                ".064"
            ],
            [
                "can\u2019t",
                ".113",
                "love",
                ".064"
            ],
            [
                "service",
                ".112",
                "lol",
                ".061"
            ],
            [
                "customer",
                ".109",
                "you",
                ".060"
            ],
            [
                "why",
                ".108",
                "great",
                ".058"
            ],
            [
                "website",
                ".107",
                "win",
                ".058"
            ],
            [
                "no",
                ".104",
                "\u2019",
                ".058"
            ],
            [
                "?",
                ".098",
                "she",
                ".054"
            ],
            [
                "fix",
                ".093",
                ":",
                ".053"
            ],
            [
                "won\u2019t",
                ".092",
                "that",
                ".053"
            ],
            [
                "been",
                ".090",
                "more",
                ".052"
            ],
            [
                "issue",
                ".089",
                "it",
                ".052"
            ],
            [
                "days",
                ".088",
                "would",
                ".051"
            ],
            [
                "error",
                ".087",
                "him",
                ".047"
            ],
            [
                "is",
                ".084",
                "life",
                ".046"
            ],
            [
                "charged",
                ".083",
                "good",
                ".046"
            ],
            [
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)"
            ],
            [
                "VBN",
                ".141",
                "UH",
                ".104"
            ],
            [
                "$",
                ".118",
                "NNP",
                ".098"
            ],
            [
                "VBZ",
                ".114",
                "PRP",
                ".076"
            ],
            [
                "NN_VBZ",
                ".114",
                "HT",
                ".076"
            ],
            [
                "PRP$",
                ".107",
                "PRP_.",
                ".076"
            ],
            [
                "PRP$_NN",
                ".105",
                "PRP_RB",
                ".067"
            ],
            [
                "VBG",
                ".093",
                "NNP_NNP",
                ".062"
            ],
            [
                "CD",
                ".092",
                "VBP_PRP",
                ".054"
            ],
            [
                "WRB_VBZ",
                ".084",
                "JJ",
                ".053"
            ],
            [
                "VBZ_VBN",
                ".084",
                "DT_JJ",
                ".051"
            ]
        ],
        "question": "Is it true that On the other hand, the presence of terms that show positive sentiment or emotions (good, great, win, POSEMO, AFFECT, ASSENT) are among the least distinctive features for a tweet not being labeled as a complaint?",
        "answer_label": "no"
    },
    {
        "id": "8b7d76ba-3bdd-45c0-9afe-72d3024ef13d",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).",
        "table_column_names": [
            "Model",
            "#Params",
            "Base",
            "+Elmo"
        ],
        "table_content_values": [
            [
                "rnet*",
                "-",
                "71.1/79.5",
                "-/-"
            ],
            [
                "LSTM",
                "2.67M",
                "[BOLD] 70.46/78.98",
                "75.17/82.79"
            ],
            [
                "GRU",
                "2.31M",
                "70.41/ [BOLD] 79.15",
                "75.81/83.12"
            ],
            [
                "ATR",
                "1.59M",
                "69.73/78.70",
                "75.06/82.76"
            ],
            [
                "SRU",
                "2.44M",
                "69.27/78.41",
                "74.56/82.50"
            ],
            [
                "LRN",
                "2.14M",
                "70.11/78.83",
                "[BOLD] 76.14/ [BOLD] 83.83"
            ]
        ],
        "question": "Is it true that Table 4 shows that LRN has the highest EM/F1 score?",
        "answer_label": "no"
    },
    {
        "id": "ac541d7d-74e8-4def-b326-4c2e2c8fc6bd",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Type",
            "[BOLD] English-German #P",
            "[BOLD] English-German B",
            "[BOLD] English-German C",
            "[BOLD] English-Czech #P",
            "[BOLD] English-Czech B",
            "[BOLD] English-Czech C"
        ],
        "table_content_values": [
            [
                "BoW+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "12.2",
                "-",
                "-",
                "7.5",
                "-"
            ],
            [
                "CNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "13.7",
                "-",
                "-",
                "8.7",
                "-"
            ],
            [
                "BiRNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "16.1",
                "-",
                "-",
                "9.6",
                "-"
            ],
            [
                "PB-SMT (Beck et al.,  2018 )",
                "Single",
                "-",
                "12.8",
                "43.2",
                "-",
                "8.6",
                "36.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Single",
                "41.4M",
                "15.5",
                "40.8",
                "39.1M",
                "8.9",
                "33.8"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Single",
                "41.2M",
                "16.7",
                "42.4",
                "38.8M",
                "9.8",
                "33.3"
            ],
            [
                "DCGCN (ours)",
                "Single",
                "[BOLD]  29.7M",
                "[BOLD] 19.0",
                "[BOLD] 44.1",
                "[BOLD]  28.3M",
                "[BOLD] 12.1",
                "[BOLD] 37.1"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Ensemble",
                "207M",
                "19.0",
                "44.1",
                "195M",
                "11.3",
                "36.4"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Ensemble",
                "206M",
                "19.6",
                "45.1",
                "194M",
                "11.7",
                "35.9"
            ],
            [
                "DCGCN (ours)",
                "Ensemble",
                "[BOLD]  149M",
                "[BOLD] 20.5",
                "[BOLD] 45.8",
                "[BOLD]  142M",
                "[BOLD] 13.1",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that Our single model DCGCN(single) achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively, significantly outperforming all the single models?",
        "answer_label": "yes"
    },
    {
        "id": "bab35b27-fc3c-4a34-802c-79f633a9de4f",
        "table_caption": "Building a Production Model for Retrieval-Based Chatbots Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Parameters",
            "[BOLD] Validation AUC@0.05",
            "[BOLD] Test AUC@0.05"
        ],
        "table_content_values": [
            [
                "Base",
                "8.0M",
                "[BOLD] 0.871",
                "0.816"
            ],
            [
                "4L SRU \u2192 2L LSTM",
                "7.3M",
                "0.864",
                "[BOLD] 0.829"
            ],
            [
                "4L SRU \u2192 2L SRU",
                "7.8M",
                "0.856",
                "[BOLD] 0.829"
            ],
            [
                "Flat \u2192 hierarchical",
                "12.4M",
                "0.825",
                "0.559"
            ],
            [
                "Cross entropy \u2192 hinge loss",
                "8.0M",
                "0.765",
                "0.693"
            ],
            [
                "6.6M \u2192 1M examples",
                "8.0M",
                "0.835",
                "0.694"
            ],
            [
                "6.6M \u2192 100K examples",
                "8.0M",
                "0.565",
                "0.417"
            ],
            [
                "200 \u2192 100 negatives",
                "8.0M",
                "0.864",
                "0.647"
            ],
            [
                "200 \u2192 10 negatives",
                "8.0M",
                "0.720",
                "0.412"
            ]
        ],
        "question": "Is it true that The model performs significantly worse when trained with hinge loss instead of cross-entropy loss, indicating the importance of the loss function?",
        "answer_label": "yes"
    },
    {
        "id": "582f8cd2-36bc-478b-a34b-95e07733d714",
        "table_caption": "Deriving Machine Attention from Human Rationales Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.",
        "table_column_names": [
            "Source",
            "Target",
            "Svm",
            "Ra-Svm\u2021",
            "Ra-Cnn\u2021",
            "Trans\u2020",
            "Ra-Trans\u2021\u2020",
            "Ours\u2021\u2020",
            "Oracle\u2020"
        ],
        "table_content_values": [
            [
                "Beer aroma+palate",
                "Beer look",
                "74.41",
                "74.83",
                "74.94",
                "72.75",
                "76.41",
                "[BOLD] 79.53",
                "80.29"
            ],
            [
                "Beer look+palate",
                "Beer aroma",
                "68.57",
                "69.23",
                "67.55",
                "69.92",
                "76.45",
                "[BOLD] 77.94",
                "78.11"
            ],
            [
                "Beer look+aroma",
                "Beer palate",
                "63.88",
                "67.82",
                "65.72",
                "74.66",
                "73.40",
                "[BOLD] 75.24",
                "75.50"
            ]
        ],
        "question": "Is it true that We observe that the transfer baseline that directly uses rationale as augmented supervision (RA-TRANS) outperforms ORACLE in all aspects?",
        "answer_label": "no"
    },
    {
        "id": "7a24d9e2-8e4f-4318-83fc-7fdad95942a0",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] External",
            "B"
        ],
        "table_content_values": [
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "-",
                "22.0"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "-",
                "23.3"
            ],
            [
                "GCNSEQ (Damonte and Cohen,  2019 )",
                "-",
                "24.4"
            ],
            [
                "DCGCN(single)",
                "-",
                "25.9"
            ],
            [
                "DCGCN(ensemble)",
                "-",
                "[BOLD] 28.2"
            ],
            [
                "TSP (Song et al.,  2016 )",
                "ALL",
                "22.4"
            ],
            [
                "PBMT (Pourdamghani et al.,  2016 )",
                "ALL",
                "26.9"
            ],
            [
                "Tree2Str (Flanigan et al.,  2016 )",
                "ALL",
                "23.0"
            ],
            [
                "SNRG (Song et al.,  2017 )",
                "ALL",
                "25.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "0.2M",
                "27.4"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "0.2M",
                "28.2"
            ],
            [
                "DCGCN(single)",
                "0.1M",
                "29.0"
            ],
            [
                "DCGCN(single)",
                "0.2M",
                "[BOLD] 31.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "2M",
                "32.3"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "2M",
                "33.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "20M",
                "33.8"
            ],
            [
                "DCGCN(single)",
                "0.3M",
                "33.2"
            ],
            [
                "DCGCN(ensemble)",
                "0.3M",
                "[BOLD] 35.3"
            ]
        ],
        "question": "Is it true that When using the same amount of 0.2M data, the performance of DCGCN is 4.2 and 3.4 BLEU points higher than Seq2SeqK and GraphLSTM?",
        "answer_label": "yes"
    },
    {
        "id": "51729d04-4ea1-4472-8452-f5cb29bddfdc",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 7: Scores for initialization strategies on probing tasks.",
        "table_column_names": [
            "Initialization",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "N(0,0.1)",
                "29.7",
                "71.5",
                "82.0",
                "78.5",
                "60.1",
                "80.5",
                "76.3",
                "74.7",
                "[BOLD] 51.3",
                "52.5"
            ],
            [
                "Glorot",
                "31.3",
                "[BOLD] 72.3",
                "81.8",
                "78.7",
                "59.4",
                "81.3",
                "76.6",
                "[BOLD] 74.6",
                "50.4",
                "57.0"
            ],
            [
                "Our paper",
                "[BOLD] 35.1",
                "70.8",
                "[BOLD] 82.0",
                "[BOLD] 80.2",
                "[BOLD] 61.8",
                "[BOLD] 82.8",
                "[BOLD] 79.7",
                "74.2",
                "50.7",
                "[BOLD] 72.9"
            ]
        ],
        "question": "Is it true that While Glorot achieves slightly better results on BShift and TopConst, CMOW's ability to memorize word content is improved by a wide [CONTINUE] margin by our initialization strategy?",
        "answer_label": "yes"
    },
    {
        "id": "ba7d16da-1cc2-4350-aac4-6bc6e31ada78",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
        "table_column_names": [
            "[BOLD] Model",
            "R",
            "MUC P",
            "[ITALIC] F1",
            "R",
            "B3 P",
            "[ITALIC] F1",
            "R",
            "CEAF- [ITALIC] e P",
            "[ITALIC] F1",
            "CoNLL  [ITALIC] F1"
        ],
        "table_content_values": [
            [
                "[BOLD] Baselines",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Cluster+Lemma",
                "76.5",
                "79.9",
                "78.1",
                "71.7",
                "85",
                "77.8",
                "75.5",
                "71.7",
                "73.6",
                "76.5"
            ],
            [
                "CV Cybulska and Vossen ( 2015a )",
                "71",
                "75",
                "73",
                "71",
                "78",
                "74",
                "-",
                "-",
                "64",
                "73"
            ],
            [
                "KCP Kenyon-Dean et\u00a0al. ( 2018 )",
                "67",
                "71",
                "69",
                "71",
                "67",
                "69",
                "71",
                "67",
                "69",
                "69"
            ],
            [
                "Cluster+KCP",
                "68.4",
                "79.3",
                "73.4",
                "67.2",
                "87.2",
                "75.9",
                "77.4",
                "66.4",
                "71.5",
                "73.6"
            ],
            [
                "[BOLD] Model Variants",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Disjoint",
                "75.5",
                "83.6",
                "79.4",
                "75.4",
                "86",
                "80.4",
                "80.3",
                "71.9",
                "75.9",
                "78.5"
            ],
            [
                "Joint",
                "77.6",
                "84.5",
                "80.9",
                "76.1",
                "85.1",
                "80.3",
                "81",
                "73.8",
                "77.3",
                "[BOLD] 79.5"
            ]
        ],
        "question": "Is it true that The results of CLUSTER+KCP indicate that pre-clustering of documents to topics is not beneficial, performing substantially worse than our joint model?",
        "answer_label": "no"
    },
    {
        "id": "4df10533-fe4d-4bca-a7ae-e944285f1f9a",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 3: Performance comparison of our model with different values of m on the two datasets.",
        "table_column_names": [
            "[ITALIC] m",
            "NYT10 Prec.",
            "NYT10 Rec.",
            "NYT10 F1",
            "NYT11 Prec.",
            "NYT11 Rec.",
            "NYT11 F1"
        ],
        "table_content_values": [
            [
                "1",
                "0.541",
                "0.595",
                "[BOLD] 0.566",
                "0.495",
                "0.621",
                "0.551"
            ],
            [
                "2",
                "0.521",
                "0.597",
                "0.556",
                "0.482",
                "0.656",
                "0.555"
            ],
            [
                "3",
                "0.490",
                "0.617",
                "0.547",
                "0.509",
                "0.633",
                "0.564"
            ],
            [
                "4",
                "0.449",
                "0.623",
                "0.522",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "5",
                "0.467",
                "0.609",
                "0.529",
                "0.488",
                "0.677",
                "0.567"
            ]
        ],
        "question": "Is it true that These experiments show that the number of factors giving the best performance may vary depending on the underlying data distribution?",
        "answer_label": "yes"
    },
    {
        "id": "08e785fd-5276-4bfa-89cb-743853a254f3",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
        "table_column_names": [
            "Model",
            "#Params",
            "BLEU",
            "Train",
            "Decode"
        ],
        "table_content_values": [
            [
                "GNMT",
                "-",
                "24.61",
                "-",
                "-"
            ],
            [
                "GRU",
                "206M",
                "26.28",
                "2.67",
                "45.35"
            ],
            [
                "ATR",
                "122M",
                "25.70",
                "1.33",
                "[BOLD] 34.40"
            ],
            [
                "SRU",
                "170M",
                "25.91",
                "1.34",
                "42.84"
            ],
            [
                "LRN",
                "143M",
                "26.26",
                "[BOLD] 0.99",
                "36.50"
            ],
            [
                "oLRN",
                "164M",
                "[BOLD] 26.73",
                "1.15",
                "40.19"
            ]
        ],
        "question": "Is it true that The results in Table 3 show that translation quality of LRN is significantly worse than that of GRU (-0.57 BLEU)?",
        "answer_label": "no"
    },
    {
        "id": "42cd498e-f308-4f89-ad6b-90f76be30594",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. \u201cEn\u201d column is an English autoencoder. BLEU scores are given for reference.",
        "table_column_names": [
            "[ITALIC] k",
            "Ar",
            "Es",
            "Fr",
            "Ru",
            "Zh",
            "En"
        ],
        "table_content_values": [
            [
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy"
            ],
            [
                "0",
                "88.0",
                "87.9",
                "87.9",
                "87.8",
                "87.7",
                "87.4"
            ],
            [
                "1",
                "92.4",
                "91.9",
                "92.1",
                "92.1",
                "91.5",
                "89.4"
            ],
            [
                "2",
                "91.9",
                "91.8",
                "91.8",
                "91.8",
                "91.3",
                "88.3"
            ],
            [
                "3",
                "92.0",
                "92.3",
                "92.1",
                "91.6",
                "91.2",
                "87.9"
            ],
            [
                "4",
                "92.1",
                "92.4",
                "92.5",
                "92.0",
                "90.5",
                "86.9"
            ],
            [
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy"
            ],
            [
                "0",
                "81.9",
                "81.9",
                "81.8",
                "81.8",
                "81.8",
                "81.2"
            ],
            [
                "1",
                "87.9",
                "87.7",
                "87.8",
                "87.9",
                "87.7",
                "84.5"
            ],
            [
                "2",
                "87.4",
                "87.5",
                "87.4",
                "87.3",
                "87.2",
                "83.2"
            ],
            [
                "3",
                "87.8",
                "87.9",
                "87.9",
                "87.3",
                "87.3",
                "82.9"
            ],
            [
                "4",
                "88.3",
                "88.6",
                "88.4",
                "88.1",
                "87.7",
                "82.1"
            ],
            [
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU"
            ],
            [
                "[EMPTY]",
                "32.7",
                "49.1",
                "38.5",
                "34.2",
                "32.1",
                "96.6"
            ]
        ],
        "question": "Is it true that Comparing layers 1 through 4, we see that in 3/5 target languages (Ar, Ru, Zh), POS tagging accuracy peaks at layer 4 and does not improve at lower layers, with some drops at layers 1 and 2?",
        "answer_label": "no"
    },
    {
        "id": "127db6c3-81b7-450c-8784-9b686fdb189b",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 8: Fraction of elements in the output that are not present in the input (ADDED) and the fraction of elements in the input graph that are missing in the generated sentence (MISS), for the test set of LDC2017T10. The token lemmas are used in the comparison. GOLD refers to the reference sentences.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>ADDED</bold>",
            "<bold>MISS</bold>"
        ],
        "table_content_values": [
            [
                "S2S",
                "47.34",
                "37.14"
            ],
            [
                "G2S-GIN",
                "48.67",
                "33.64"
            ],
            [
                "G2S-GAT",
                "48.24",
                "33.73"
            ],
            [
                "G2S-GGNN",
                "48.66",
                "34.06"
            ],
            [
                "GOLD",
                "50.77",
                "28.35"
            ],
            [
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ]
        ],
        "question": "Is it true that As shown in Table 8, G2S approaches outperform the S2S baseline?",
        "answer_label": "yes"
    },
    {
        "id": "966334f3-986a-4d6b-8dcd-efda97f994b6",
        "table_caption": "Sparse and Structured Visual Attention Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
        "table_column_names": [
            "[EMPTY]",
            "MSCOCO spice",
            "MSCOCO cider",
            "MSCOCO rouge [ITALIC] L",
            "MSCOCO bleu4",
            "MSCOCO meteor",
            "MSCOCO rep\u2193",
            "Flickr30k spice",
            "Flickr30k cider",
            "Flickr30k rouge [ITALIC] L",
            "Flickr30k bleu4",
            "Flickr30k meteor",
            "Flickr30k rep\u2193"
        ],
        "table_content_values": [
            [
                "softmax",
                "18.4",
                "0.967",
                "52.9",
                "29.9",
                "24.9",
                "3.76",
                "13.5",
                "0.443",
                "44.2",
                "19.9",
                "19.1",
                "6.09"
            ],
            [
                "sparsemax",
                "[BOLD] 18.9",
                "[BOLD] 0.990",
                "[BOLD] 53.5",
                "[BOLD] 31.5",
                "[BOLD] 25.3",
                "3.69",
                "[BOLD] 13.7",
                "[BOLD] 0.444",
                "[BOLD] 44.3",
                "[BOLD] 20.7",
                "[BOLD] 19.3",
                "5.84"
            ],
            [
                "TVmax",
                "18.5",
                "0.974",
                "53.1",
                "29.9",
                "25.1",
                "[BOLD] 3.17",
                "13.3",
                "0.438",
                "44.2",
                "20.5",
                "19.0",
                "[BOLD] 3.97"
            ]
        ],
        "question": "Is it true that [CONTINUE] Selective attention mechanisms like sparsemax and especially TVMAX reduce repetition, as measured by the REP metric reported in Table 1?",
        "answer_label": "yes"
    },
    {
        "id": "845e45a0-fc97-4bc4-afbd-da01039c86c7",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that For example, on AMR17, the single DCGCN model is 1 BLEU point higher than the ensemble model of Seq2SeqB?",
        "answer_label": "yes"
    },
    {
        "id": "3a961802-e664-47df-a85d-d017f7b3250f",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that On the same dataset, we have competitive results to Damonte and Cohen (2019)?",
        "answer_label": "yes"
    },
    {
        "id": "8a0a5630-4ceb-4984-86ac-cfebf28d6f81",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that Adding either the global node or the linear combination does not improve the baseline models with only dense connections?",
        "answer_label": "no"
    },
    {
        "id": "0caf7d9c-4732-4d3f-9f66-d8b7b5105251",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Feature",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Feature",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams"
            ],
            [
                "not",
                ".154",
                "[URL]",
                ".150"
            ],
            [
                "my",
                ".131",
                "!",
                ".082"
            ],
            [
                "working",
                ".124",
                "he",
                ".069"
            ],
            [
                "still",
                ".123",
                "thank",
                ".067"
            ],
            [
                "on",
                ".119",
                ",",
                ".064"
            ],
            [
                "can\u2019t",
                ".113",
                "love",
                ".064"
            ],
            [
                "service",
                ".112",
                "lol",
                ".061"
            ],
            [
                "customer",
                ".109",
                "you",
                ".060"
            ],
            [
                "why",
                ".108",
                "great",
                ".058"
            ],
            [
                "website",
                ".107",
                "win",
                ".058"
            ],
            [
                "no",
                ".104",
                "\u2019",
                ".058"
            ],
            [
                "?",
                ".098",
                "she",
                ".054"
            ],
            [
                "fix",
                ".093",
                ":",
                ".053"
            ],
            [
                "won\u2019t",
                ".092",
                "that",
                ".053"
            ],
            [
                "been",
                ".090",
                "more",
                ".052"
            ],
            [
                "issue",
                ".089",
                "it",
                ".052"
            ],
            [
                "days",
                ".088",
                "would",
                ".051"
            ],
            [
                "error",
                ".087",
                "him",
                ".047"
            ],
            [
                "is",
                ".084",
                "life",
                ".046"
            ],
            [
                "charged",
                ".083",
                "good",
                ".046"
            ],
            [
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)"
            ],
            [
                "VBN",
                ".141",
                "UH",
                ".104"
            ],
            [
                "$",
                ".118",
                "NNP",
                ".098"
            ],
            [
                "VBZ",
                ".114",
                "PRP",
                ".076"
            ],
            [
                "NN_VBZ",
                ".114",
                "HT",
                ".076"
            ],
            [
                "PRP$",
                ".107",
                "PRP_.",
                ".076"
            ],
            [
                "PRP$_NN",
                ".105",
                "PRP_RB",
                ".067"
            ],
            [
                "VBG",
                ".093",
                "NNP_NNP",
                ".062"
            ],
            [
                "CD",
                ".092",
                "VBP_PRP",
                ".054"
            ],
            [
                "WRB_VBZ",
                ".084",
                "JJ",
                ".053"
            ],
            [
                "VBZ_VBN",
                ".084",
                "DT_JJ",
                ".051"
            ]
        ],
        "question": "Is it true that [CONTINUE] In addition, other words and clusters expressing positive states such as gratitude (thank, great, love) or laughter (lol) are also distinctive for tweets that are not complaints?",
        "answer_label": "yes"
    },
    {
        "id": "53df7857-7812-405d-909a-cebde5395c17",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 9: Performance of models trained with tweets from one domain and tested on other domains. All results are reported in ROC AUC. The All line displays results on training on all categories except the category in testing.",
        "table_column_names": [
            "[BOLD] Test",
            "F&B",
            "A",
            "R",
            "Ca",
            "Se",
            "So",
            "T",
            "E",
            "O"
        ],
        "table_content_values": [
            [
                "[BOLD] Train",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Food & Bev.",
                "\u2013",
                "58.1",
                "52.5",
                "66.4",
                "59.7",
                "58.9",
                "54.1",
                "61.4",
                "53.7"
            ],
            [
                "Apparel",
                "63.9",
                "\u2013",
                "74.4",
                "65.1",
                "70.8",
                "71.2",
                "68.5",
                "76.9",
                "85.6"
            ],
            [
                "Retail",
                "58.8",
                "74.4",
                "\u2013",
                "70.1",
                "72.6",
                "69.9",
                "68.7",
                "69.6",
                "82.7"
            ],
            [
                "Cars",
                "68.7",
                "61.1",
                "65.1",
                "\u2013",
                "58.8",
                "67.",
                "59.3",
                "62.9",
                "68.2"
            ],
            [
                "Services",
                "65.",
                "74.2",
                "75.8",
                "74.",
                "\u2013",
                "68.8",
                "74.2",
                "77.9",
                "77.9"
            ],
            [
                "Software",
                "62.",
                "74.2",
                "68.",
                "67.9",
                "72.8",
                "\u2013",
                "72.8",
                "72.1",
                "80.6"
            ],
            [
                "Transport",
                "59.3",
                "71.7",
                "72.4",
                "67.",
                "74.6",
                "75.",
                "\u2013",
                "72.6",
                "81.7"
            ],
            [
                "Electronics",
                "61.6",
                "75.2",
                "71.",
                "68.",
                "75.",
                "69.9",
                "68.2",
                "\u2013",
                "78.7"
            ],
            [
                "Other",
                "56.1",
                "71.3",
                "72.4",
                "70.2",
                "73.5",
                "67.2",
                "68.5",
                "71.",
                "\u2013"
            ],
            [
                "All",
                "70.3",
                "77.7",
                "79.5",
                "82.0",
                "79.6",
                "80.1",
                "76.8",
                "81.7",
                "88.2"
            ]
        ],
        "question": "Is it true that We observe that predictive performance is relatively consistent across all domains with two exceptions ('Food & Beverage' consistently shows lower performance, while 'Other' achieves higher performance) when using all the data available from the other domains?",
        "answer_label": "yes"
    },
    {
        "id": "6b921c63-e9aa-422c-9036-ad53b45fc2fb",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 9: Performance of models trained with tweets from one domain and tested on other domains. All results are reported in ROC AUC. The All line displays results on training on all categories except the category in testing.",
        "table_column_names": [
            "[BOLD] Test",
            "F&B",
            "A",
            "R",
            "Ca",
            "Se",
            "So",
            "T",
            "E",
            "O"
        ],
        "table_content_values": [
            [
                "[BOLD] Train",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Food & Bev.",
                "\u2013",
                "58.1",
                "52.5",
                "66.4",
                "59.7",
                "58.9",
                "54.1",
                "61.4",
                "53.7"
            ],
            [
                "Apparel",
                "63.9",
                "\u2013",
                "74.4",
                "65.1",
                "70.8",
                "71.2",
                "68.5",
                "76.9",
                "85.6"
            ],
            [
                "Retail",
                "58.8",
                "74.4",
                "\u2013",
                "70.1",
                "72.6",
                "69.9",
                "68.7",
                "69.6",
                "82.7"
            ],
            [
                "Cars",
                "68.7",
                "61.1",
                "65.1",
                "\u2013",
                "58.8",
                "67.",
                "59.3",
                "62.9",
                "68.2"
            ],
            [
                "Services",
                "65.",
                "74.2",
                "75.8",
                "74.",
                "\u2013",
                "68.8",
                "74.2",
                "77.9",
                "77.9"
            ],
            [
                "Software",
                "62.",
                "74.2",
                "68.",
                "67.9",
                "72.8",
                "\u2013",
                "72.8",
                "72.1",
                "80.6"
            ],
            [
                "Transport",
                "59.3",
                "71.7",
                "72.4",
                "67.",
                "74.6",
                "75.",
                "\u2013",
                "72.6",
                "81.7"
            ],
            [
                "Electronics",
                "61.6",
                "75.2",
                "71.",
                "68.",
                "75.",
                "69.9",
                "68.2",
                "\u2013",
                "78.7"
            ],
            [
                "Other",
                "56.1",
                "71.3",
                "72.4",
                "70.2",
                "73.5",
                "67.2",
                "68.5",
                "71.",
                "\u2013"
            ],
            [
                "All",
                "70.3",
                "77.7",
                "79.5",
                "82.0",
                "79.6",
                "80.1",
                "76.8",
                "81.7",
                "88.2"
            ]
        ],
        "question": "Is it true that We observe that predictive performance is not consistent across all domains, with 'Food & Beverage' consistently showing lower performance and 'Other' achieving higher performance when using all the data available from the other domains?",
        "answer_label": "no"
    },
    {
        "id": "851a3937-519e-4d91-8e18-bb809245164e",
        "table_caption": "Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
        "table_column_names": [
            "Dataset",
            "Metric",
            "Illinois",
            "IlliCons",
            "rahman2012resolving",
            "KnowFeat",
            "KnowCons",
            "KnowComb"
        ],
        "table_content_values": [
            [
                "[ITALIC] Winograd",
                "Precision",
                "51.48",
                "53.26",
                "73.05",
                "71.81",
                "74.93",
                "[BOLD] 76.41"
            ],
            [
                "[ITALIC] WinoCoref",
                "AntePre",
                "68.37",
                "74.32",
                "\u2014\u2013",
                "88.48",
                "88.95",
                "[BOLD] 89.32"
            ]
        ],
        "question": "Is it true that On the WinoCoref dataset, it improves by 15%?",
        "answer_label": "yes"
    },
    {
        "id": "b8b4f008-0499-4c41-98ce-62faa07f200c",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 2: Ablation study results.",
        "table_column_names": [
            "[BOLD] Variation",
            "[BOLD] Accuracy (%)",
            "[BOLD] \u0394%"
        ],
        "table_content_values": [
            [
                "Submitted",
                "[BOLD] 69.23",
                "-"
            ],
            [
                "No emoji",
                "68.36",
                "- 0.87"
            ],
            [
                "No ELMo",
                "65.52",
                "- 3.71"
            ],
            [
                "Concat Pooling",
                "68.47",
                "- 0.76"
            ],
            [
                "LSTM hidden=4096",
                "69.10",
                "- 0.13"
            ],
            [
                "LSTM hidden=1024",
                "68.93",
                "- 0.30"
            ],
            [
                "LSTM hidden=512",
                "68.43",
                "- 0.80"
            ],
            [
                "POS emb dim=100",
                "68.99",
                "- 0.24"
            ],
            [
                "POS emb dim=75",
                "68.61",
                "- 0.62"
            ],
            [
                "POS emb dim=50",
                "69.33",
                "+ 0.10"
            ],
            [
                "POS emb dim=25",
                "69.21",
                "- 0.02"
            ],
            [
                "SGD optim lr=1",
                "64.33",
                "- 4.90"
            ],
            [
                "SGD optim lr=0.1",
                "66.11",
                "- 3.12"
            ],
            [
                "SGD optim lr=0.01",
                "60.72",
                "- 8.51"
            ],
            [
                "SGD optim lr=0.001",
                "30.49",
                "- 38.74"
            ]
        ],
        "question": "Is it true that [CONTINUE] Using a greater BiLSTM hidden size did not help the model, [CONTINUE] We found that using 50-dimensional part-ofspeech embeddings slightly improved results, [CONTINUE] Regarding optimization strategies, we also tried using SGD with different learning rates and a stepwise learning rate schedule as described by Conneau et al?",
        "answer_label": "yes"
    },
    {
        "id": "4e3c4acc-419f-40ad-99d3-b60a432f2e43",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
        "table_column_names": [
            "Cue",
            "App.",
            "Prod.",
            "Cov."
        ],
        "table_content_values": [
            [
                "in",
                "47",
                "55.3",
                "9.40"
            ],
            [
                "was",
                "55",
                "61.8",
                "11.0"
            ],
            [
                "to",
                "82",
                "40.2",
                "16.4"
            ],
            [
                "the",
                "85",
                "38.8",
                "17.0"
            ],
            [
                "a",
                "106",
                "57.5",
                "21.2"
            ]
        ],
        "question": "Is it true that Its productivity of 57.5% expresses that it appears in incorrect alternatives 7.5% more often than expected by random chance?",
        "answer_label": "no"
    },
    {
        "id": "43802ed8-5f82-4f5a-b44a-fa832e72a6c7",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 6: Results on SimLex-999 and WordSim-353, in Italian and German, before and after debiasing.",
        "table_column_names": [
            "[EMPTY]",
            "Italian Orig",
            "Italian Debias",
            "German Orig",
            "German Debias"
        ],
        "table_content_values": [
            [
                "SimLex",
                "0.280",
                "[BOLD] 0.288",
                "0.343",
                "[BOLD] 0.356"
            ],
            [
                "WordSim",
                "0.548",
                "[BOLD] 0.577",
                "0.547",
                "[BOLD] 0.553"
            ]
        ],
        "question": "Is it true that In both cases, the original embeddings perform better than the new ones?",
        "answer_label": "no"
    },
    {
        "id": "9a2c00b9-5f14-4637-8bc2-cac94c5b48ee",
        "table_caption": "Argument Generation with Retrieval, Planning, and Realization Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
        "table_column_names": [
            "[EMPTY]",
            "[ITALIC] w/ System Retrieval  [BOLD] B-2",
            "[ITALIC] w/ System Retrieval  [BOLD] B-4",
            "[ITALIC] w/ System Retrieval  [BOLD] R-2",
            "[ITALIC] w/ System Retrieval  [BOLD] MTR",
            "[ITALIC] w/ System Retrieval  [BOLD] #Word",
            "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
        ],
        "table_content_values": [
            [
                "Human",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22"
            ],
            [
                "Retrieval",
                "7.55",
                "1.11",
                "8.64",
                "14.38",
                "123",
                "23",
                "10.97",
                "3.05",
                "23.49",
                "20.08",
                "140",
                "21"
            ],
            [
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Seq2seq",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15"
            ],
            [
                "Seq2seqAug",
                "8.26",
                "2.24",
                "13.79",
                "15.75",
                "78",
                "14",
                "10.98",
                "4.41",
                "22.97",
                "19.62",
                "71",
                "14"
            ],
            [
                "[ITALIC] w/o psg",
                "7.94",
                "2.28",
                "10.13",
                "15.71",
                "75",
                "12",
                "9.89",
                "3.34",
                "14.20",
                "18.40",
                "66",
                "12"
            ],
            [
                "H&W\u00a0Hua and Wang ( 2018 )",
                "3.64",
                "0.92",
                "8.83",
                "11.78",
                "51",
                "12",
                "8.51",
                "2.86",
                "18.89",
                "17.18",
                "58",
                "12"
            ],
            [
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "CANDELA",
                "12.02\u2217",
                "[BOLD] 2.99\u2217",
                "[BOLD] 14.93\u2217",
                "[BOLD] 16.92\u2217",
                "119",
                "22",
                "15.80\u2217",
                "[BOLD] 5.00\u2217",
                "[BOLD] 23.75",
                "[BOLD] 20.18",
                "116",
                "22"
            ],
            [
                "[ITALIC] w/o psg",
                "[BOLD] 12.33\u2217",
                "2.86\u2217",
                "14.53\u2217",
                "16.60\u2217",
                "123",
                "23",
                "[BOLD] 16.33\u2217",
                "4.98\u2217",
                "23.65",
                "19.94",
                "123",
                "23"
            ]
        ],
        "question": "Is it true that Furthermore, our model generates longer sentences whose lengths are comparable with human arguments, both with about 22 words per sentence?",
        "answer_label": "yes"
    },
    {
        "id": "ff1ab06e-56c2-42af-bae0-f25b765a45dc",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.",
        "table_column_names": [
            "Uni",
            "POS",
            "0 87.9",
            "1 92.0",
            "2 91.7",
            "3 91.8",
            "4 91.9"
        ],
        "table_content_values": [
            [
                "Uni",
                "SEM",
                "81.8",
                "87.8",
                "87.4",
                "87.6",
                "88.2"
            ],
            [
                "Bi",
                "POS",
                "87.9",
                "93.3",
                "92.9",
                "93.2",
                "92.8"
            ],
            [
                "Bi",
                "SEM",
                "81.9",
                "91.3",
                "90.8",
                "91.9",
                "91.9"
            ],
            [
                "Res",
                "POS",
                "87.9",
                "92.5",
                "91.9",
                "92.0",
                "92.4"
            ],
            [
                "Res",
                "SEM",
                "81.9",
                "88.2",
                "87.5",
                "87.6",
                "88.5"
            ]
        ],
        "question": "Is it true that Some of our bidirectional models obtain 92-93% accuracy?",
        "answer_label": "yes"
    },
    {
        "id": "1515785d-efa6-40d2-af51-95a8c13ee95c",
        "table_caption": "Low-supervision urgency detection and transfer in short crisis messages TABLE II: Details on datasets used for experiments.",
        "table_column_names": [
            "Dataset",
            "Unlabeled / Labeled Messages",
            "Urgent / Non-urgent Messages",
            "Unique Tokens",
            "Avg. Tokens / Message",
            "Time Range"
        ],
        "table_content_values": [
            [
                "Nepal",
                "6,063/400",
                "201/199",
                "1,641",
                "14",
                "04/05/2015-05/06/2015"
            ],
            [
                "Macedonia",
                "0/205",
                "92/113",
                "129",
                "18",
                "09/18/2018-09/21/2018"
            ],
            [
                "Kerala",
                "92,046/400",
                "125/275",
                "19,393",
                "15",
                "08/17/2018-08/22/2018"
            ]
        ],
        "question": "Is it true that Table II shows that Nepal and Macedonia are roughly balanced, while Kerala is imbalanced?",
        "answer_label": "no"
    },
    {
        "id": "4cd6be97-c5bd-44fa-a79d-e54753f6893d",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "Base ACC",
            "Base Time",
            "+LN ACC",
            "+LN Time",
            "+BERT ACC",
            "+BERT Time",
            "+LN+BERT ACC",
            "+LN+BERT Time"
        ],
        "table_content_values": [
            [
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "250K",
                "83.50",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-"
            ],
            [
                "This",
                "LSTM",
                "8.36M",
                "84.27",
                "0.262",
                "86.03",
                "0.432",
                "89.95",
                "0.544",
                "[BOLD] 90.49",
                "0.696"
            ],
            [
                "This",
                "GRU",
                "6.41M",
                "[BOLD] 85.71",
                "0.245",
                "[BOLD] 86.05",
                "0.419",
                "[BOLD] 90.29",
                "0.529",
                "90.10",
                "0.695"
            ],
            [
                "This",
                "ATR",
                "2.87M",
                "84.88",
                "0.210",
                "85.81",
                "0.307",
                "90.00",
                "0.494",
                "90.28",
                "0.580"
            ],
            [
                "Work",
                "SRU",
                "5.48M",
                "84.28",
                "0.258",
                "85.32",
                "0.283",
                "89.98",
                "0.543",
                "90.09",
                "0.555"
            ],
            [
                "[EMPTY]",
                "LRN",
                "4.25M",
                "84.88",
                "[BOLD] 0.209",
                "85.06",
                "[BOLD] 0.223",
                "89.98",
                "[BOLD] 0.488",
                "89.93",
                "[BOLD] 0.506"
            ]
        ],
        "question": "Is it true that Although LSTM and GRU outperform LRN by 0.3\u223c0.9 in terms of accuracy, these recurrent units sacrifice running efficiency (about 7%\u223c48%) depending on whether LN and BERT are applied?",
        "answer_label": "yes"
    },
    {
        "id": "862ee0dc-0f85-4a85-a1c3-b15e962ba324",
        "table_caption": "Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers Table 1: Impact of linguistic features on deep-coref models on the CoNLL development set.",
        "table_column_names": [
            "[EMPTY]",
            "MUC",
            "<italic>B</italic>3",
            "CEAF<italic>e</italic>",
            "CoNLL",
            "LEA"
        ],
        "table_content_values": [
            [
                "ranking",
                "74.31",
                "64.23",
                "59.73",
                "66.09",
                "60.47"
            ],
            [
                "+linguistic",
                "74.35",
                "63.96",
                "60.19",
                "66.17",
                "60.20"
            ],
            [
                "top-pairs",
                "73.95",
                "63.98",
                "59.52",
                "65.82",
                "60.07"
            ],
            [
                "+linguistic",
                "74.32",
                "64.45",
                "60.19",
                "66.32",
                "60.62"
            ]
        ],
        "question": "Is it true that [CONTINUE] However, it does not improve significantly over \"ranking\"?",
        "answer_label": "yes"
    },
    {
        "id": "fd89ce86-c88f-4273-882a-21c474839874",
        "table_caption": "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns Table 7: Performance of our baselines on the development set in the gold-two-mention task (access to the two candidate name spans). Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.",
        "table_column_names": [
            "[EMPTY]",
            "M",
            "F",
            "B",
            "O"
        ],
        "table_content_values": [
            [
                "Random",
                "47.5",
                "50.5",
                "[ITALIC] 1.06",
                "49.0"
            ],
            [
                "Token Distance",
                "50.6",
                "47.5",
                "[ITALIC] 0.94",
                "49.1"
            ],
            [
                "Topical Entity",
                "50.2",
                "47.3",
                "[ITALIC] 0.94",
                "48.8"
            ],
            [
                "Syntactic Distance",
                "66.7",
                "66.7",
                "[ITALIC]  [BOLD] 1.00",
                "66.7"
            ],
            [
                "Parallelism",
                "[BOLD] 69.3",
                "[BOLD] 69.2",
                "[ITALIC]  [BOLD] 1.00",
                "[BOLD] 69.2"
            ],
            [
                "Parallelism+URL",
                "[BOLD] 74.2",
                "[BOLD] 71.6",
                "[ITALIC]  [BOLD] 0.96",
                "[BOLD] 72.9"
            ],
            [
                "Transformer-Single",
                "59.6",
                "56.6",
                "[ITALIC] 0.95",
                "58.1"
            ],
            [
                "Transformer-Multi",
                "62.9",
                "61.7",
                "[ITALIC] 0.98",
                "62.3"
            ]
        ],
        "question": "Is it true that RANDOM is the best performing baseline here, and other baselines are far from gender-parity?",
        "answer_label": "no"
    },
    {
        "id": "039561ae-0905-4fd8-85e4-f75db58ff616",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 2: Experiment 1",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.001",
                "0.003",
                "-20.818",
                "***",
                "0.505"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.083",
                "0.048",
                "101.636",
                "***",
                "1.724"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.001",
                "0.001",
                "0.035",
                "[EMPTY]",
                "1.001"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.023",
                "0.012",
                "64.418",
                "***",
                "1.993"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.002",
                "0.001",
                "4.047",
                "***",
                "1.120"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.049",
                "0.019",
                "120.986",
                "***",
                "2.573"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.173",
                "0.065",
                "243.285",
                "***",
                "2.653"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.032",
                "0.023",
                "39.483",
                "***",
                "1.396"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.111",
                "0.061",
                "122.707",
                "***",
                "1.812"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.178",
                "0.080",
                "211.319",
                "***",
                "2.239"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.028",
                "0.015",
                "63.131",
                "***",
                "1.854"
            ]
        ],
        "question": "Is it true that Tweets in the black-aligned corpus are classified as containing sexism almost twice as frequently and 1.1 times as frequently classified as containing racism and sexism compared to those in the white-aligned corpus, but the difference is not statistically significant?",
        "answer_label": "no"
    },
    {
        "id": "20733675-9d80-4d43-9f7d-92d3d2a434bf",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
        "table_column_names": [
            "[BOLD] Model",
            "R",
            "MUC P",
            "[ITALIC] F1",
            "R",
            "B3 P",
            "[ITALIC] F1",
            "R",
            "CEAF- [ITALIC] e P",
            "[ITALIC] F1",
            "CoNLL  [ITALIC] F1"
        ],
        "table_content_values": [
            [
                "[BOLD] Baselines",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Cluster+Lemma",
                "76.5",
                "79.9",
                "78.1",
                "71.7",
                "85",
                "77.8",
                "75.5",
                "71.7",
                "73.6",
                "76.5"
            ],
            [
                "CV Cybulska and Vossen ( 2015a )",
                "71",
                "75",
                "73",
                "71",
                "78",
                "74",
                "-",
                "-",
                "64",
                "73"
            ],
            [
                "KCP Kenyon-Dean et\u00a0al. ( 2018 )",
                "67",
                "71",
                "69",
                "71",
                "67",
                "69",
                "71",
                "67",
                "69",
                "69"
            ],
            [
                "Cluster+KCP",
                "68.4",
                "79.3",
                "73.4",
                "67.2",
                "87.2",
                "75.9",
                "77.4",
                "66.4",
                "71.5",
                "73.6"
            ],
            [
                "[BOLD] Model Variants",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Disjoint",
                "75.5",
                "83.6",
                "79.4",
                "75.4",
                "86",
                "80.4",
                "80.3",
                "71.9",
                "75.9",
                "78.5"
            ],
            [
                "Joint",
                "77.6",
                "84.5",
                "80.9",
                "76.1",
                "85.1",
                "80.3",
                "81",
                "73.8",
                "77.3",
                "[BOLD] 79.5"
            ]
        ],
        "question": "Is it true that [CONTINUE] The results of CLUSTER+KCP again indicate that pre-clustering of documents to topics is beneficial, improving upon the KCP performance by 4.6 points, though still performing substantially worse than our joint model?",
        "answer_label": "yes"
    },
    {
        "id": "2b0d604e-1c16-41b1-9485-1f37db56aebb",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section\u00a03).",
        "table_column_names": [
            "[BOLD] Dataset",
            "[BOLD] Part",
            "[BOLD] MRs",
            "[BOLD] Refs",
            "[BOLD] SER(%)"
        ],
        "table_content_values": [
            [
                "Original",
                "Train",
                "4,862",
                "42,061",
                "17.69"
            ],
            [
                "Original",
                "Dev",
                "547",
                "4,672",
                "11.42"
            ],
            [
                "Original",
                "Test",
                "630",
                "4,693",
                "11.49"
            ],
            [
                "[0.5pt/2pt] Cleaned",
                "Train",
                "8,362",
                "33,525",
                "(0.00)"
            ],
            [
                "[0.5pt/2pt] Cleaned",
                "Dev",
                "1,132",
                "4,299",
                "(0.00)"
            ],
            [
                "[0.5pt/2pt] Cleaned",
                "Test",
                "1,358",
                "4,693",
                "(0.00)"
            ]
        ],
        "question": "Is it true that This means that the cleaned dataset is less complex overall, with more references per MR and fewer diverse MRs?",
        "answer_label": "no"
    },
    {
        "id": "1722ed89-381a-467b-a50f-39d73e119b85",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that For both datasets, our approach does not substantially outperform the baselines?",
        "answer_label": "no"
    },
    {
        "id": "5142bc85-da69-4450-bd65-28cd5ce2831e",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.",
        "table_column_names": [
            "[EMPTY]",
            "Italian Original",
            "Italian Debiased",
            "Italian English",
            "Italian Reduction",
            "German Original",
            "German Debiased",
            "German English",
            "German Reduction"
        ],
        "table_content_values": [
            [
                "Same Gender",
                "0.442",
                "0.434",
                "0.424",
                "\u2013",
                "0.491",
                "0.478",
                "0.446",
                "\u2013"
            ],
            [
                "Different Gender",
                "0.385",
                "0.421",
                "0.415",
                "\u2013",
                "0.415",
                "0.435",
                "0.403",
                "\u2013"
            ],
            [
                "difference",
                "0.057",
                "0.013",
                "0.009",
                "[BOLD] 91.67%",
                "0.076",
                "0.043",
                "0.043",
                "[BOLD] 100%"
            ]
        ],
        "question": "Is it true that [CONTINUE] As expected, in both languages, the difference between the average of the two sets with the debiased embeddings is much lower?",
        "answer_label": "yes"
    },
    {
        "id": "78540156-ca72-40ea-bf46-e0aae6172d16",
        "table_caption": "Sparse and Structured Visual Attention Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
        "table_column_names": [
            "[EMPTY]",
            "Att. to image",
            "Att. to bounding boxes",
            "Test-Dev Yes/No",
            "Test-Dev Number",
            "Test-Dev Other",
            "Test-Dev Overall",
            "Test-Standard Yes/No",
            "Test-Standard Number",
            "Test-Standard Other",
            "Test-Standard Overall"
        ],
        "table_content_values": [
            [
                "softmax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "42.65",
                "55.74",
                "65.52",
                "83.55",
                "42.68",
                "56.01",
                "65.97"
            ],
            [
                "sparsemax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "43.19",
                "55.79",
                "65.60",
                "83.33",
                "42.99",
                "56.06",
                "65.94"
            ],
            [
                "soft-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.13",
                "43.53",
                "56.01",
                "65.76",
                "83.63",
                "43.24",
                "56.10",
                "66.11"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.10",
                "43.30",
                "56.14",
                "65.79",
                "83.66",
                "43.18",
                "56.21",
                "66.17"
            ],
            [
                "softmax",
                "[EMPTY]",
                "\u2713",
                "85.14",
                "49.59",
                "58.72",
                "68.57",
                "85.56",
                "49.54",
                "59.11",
                "69.04"
            ],
            [
                "sparsemax",
                "[EMPTY]",
                "\u2713",
                "[BOLD] 85.40",
                "[BOLD] 50.87",
                "58.67",
                "68.79",
                "[BOLD] 85.80",
                "50.18",
                "59.08",
                "69.19"
            ],
            [
                "softmax",
                "\u2713",
                "\u2713",
                "85.33",
                "50.49",
                "58.88",
                "68.82",
                "85.58",
                "50.42",
                "59.18",
                "69.17"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "\u2713",
                "85.35",
                "50.52",
                "[BOLD] 59.15",
                "[BOLD] 68.96",
                "85.72",
                "[BOLD] 50.66",
                "[BOLD] 59.22",
                "[BOLD] 69.28"
            ]
        ],
        "question": "Is it true that As can be seen in the results presented in Table 3 the models using TVMAX in the output attention layer outperform the models using softmax and sparsemax?",
        "answer_label": "yes"
    },
    {
        "id": "9064b2de-b304-408b-9aa9-81c8f1be3e65",
        "table_caption": "The MeMAD Submission to the WMT18 Multimodal Translation Task Table 3: Using automatically translated image captions and domain labels (BLEU% scores). A is short for Amun, T for Transformer.",
        "table_column_names": [
            "[EMPTY]",
            "en-fr",
            "flickr16",
            "flickr17",
            "mscoco17"
        ],
        "table_content_values": [
            [
                "A",
                "subs1M [ITALIC]  [ITALIC] H+MS-COCO",
                "66.3",
                "60.5",
                "52.1"
            ],
            [
                "A",
                "+domain-tuned",
                "66.8",
                "60.6",
                "52.0"
            ],
            [
                "A",
                "+labels",
                "[BOLD] 67.2",
                "60.4",
                "51.7"
            ],
            [
                "T",
                "subs1M [ITALIC]  [ITALIC] LM+MS-COCO",
                "66.9",
                "60.3",
                "[BOLD] 52.8"
            ],
            [
                "T",
                "+labels",
                "[BOLD] 67.2",
                "[BOLD] 60.9",
                "52.7"
            ],
            [
                "[EMPTY]",
                "en-de",
                "flickr16",
                "flickr17",
                "mscoco17"
            ],
            [
                "A",
                "subs1M [ITALIC]  [ITALIC] H+MS-COCO",
                "43.1",
                "39.0",
                "35.1"
            ],
            [
                "A",
                "+domain-tuned",
                "43.9",
                "39.4",
                "35.8"
            ],
            [
                "A",
                "+labels",
                "43.2",
                "39.3",
                "34.3"
            ],
            [
                "T",
                "subs1M [ITALIC]  [ITALIC] LM+MS-COCO",
                "[BOLD] 44.4",
                "39.4",
                "35.0"
            ],
            [
                "T",
                "+labels",
                "44.1",
                "[BOLD] 39.8",
                "[BOLD] 36.5"
            ]
        ],
        "question": "Is it true that For Marian amun, the effect of adding domain labels is significant as we can see in Table 3?",
        "answer_label": "no"
    },
    {
        "id": "d8b4c1e7-8cf5-4916-87af-b646b8ef4b6b",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Present",
            "[BOLD] Not Present"
        ],
        "table_content_values": [
            [
                "Emoji",
                "4805 (76.6%)",
                "23952 (68.0%)"
            ],
            [
                "Hashtags",
                "2122 (70.5%)",
                "26635 (69.4%)"
            ]
        ],
        "question": "Is it true that Tweets containing emoji seem to be harder for the model to classify than those without?",
        "answer_label": "no"
    },
    {
        "id": "55e35248-14b5-4372-91ab-184449934829",
        "table_caption": "Syntactic Dependency Representations in Neural Relation Classification Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
        "table_column_names": [
            "[BOLD] Representation",
            "[BOLD] Hyper parameters Filter size",
            "[BOLD] Hyper parameters Num. Feature maps",
            "[BOLD] Hyper parameters Activation func.",
            "[BOLD] Hyper parameters L2 Reg.",
            "[BOLD] Hyper parameters Learning rate",
            "[BOLD] Hyper parameters Dropout Prob.",
            "[BOLD] F1.(avg. in 5-fold) with default values",
            "[BOLD] F1.(avg. in 5-fold) with optimal values"
        ],
        "table_content_values": [
            [
                "CoNLL08",
                "4-5",
                "1000",
                "Softplus",
                "1.15e+01",
                "1.13e-03",
                "1",
                "73.34",
                "74.49"
            ],
            [
                "SB",
                "4-5",
                "806",
                "Sigmoid",
                "8.13e-02",
                "1.79e-03",
                "0.87",
                "72.83",
                "[BOLD] 75.05"
            ],
            [
                "UD v1.3",
                "5",
                "716",
                "Softplus",
                "1.66e+00",
                "9.63E-04",
                "1",
                "68.93",
                "69.57"
            ]
        ],
        "question": "Is it true that We observe that the results for the UD representation are quite a bit lower than the two others?",
        "answer_label": "yes"
    },
    {
        "id": "adadfacf-0744-4462-951f-d4678d921ee0",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 6: Test BLEU for 2-model es-en and 3-model en-de model ensembling for models adapted with EWC, compared to oracle model last trained on each domain, chosen if test domain is known. BI+IS outperforms uniform ensembling and in some cases outperforms the oracle.",
        "table_column_names": [
            "[BOLD] Decoder configuration",
            "[BOLD] es-en  [BOLD] Health",
            "[BOLD] es-en  [BOLD] Bio",
            "[BOLD] en-de  [BOLD] News",
            "[BOLD] en-de  [BOLD] TED",
            "[BOLD] en-de  [BOLD] IT"
        ],
        "table_content_values": [
            [
                "Oracle model",
                "35.9",
                "37.8",
                "37.8",
                "27.0",
                "57.0"
            ],
            [
                "Uniform",
                "36.0",
                "36.4",
                "[BOLD] 38.9",
                "26.0",
                "43.5"
            ],
            [
                "BI + IS",
                "[BOLD] 36.2",
                "[BOLD] 38.0",
                "38.7",
                "[BOLD] 26.1",
                "[BOLD] 56.4"
            ]
        ],
        "question": "Is it true that [CONTINUE] EWC models perform well over multiple domains, so the improvement over uniform ensembling is less striking than for unadapted models?",
        "answer_label": "yes"
    },
    {
        "id": "711764b0-0eb4-4498-8325-afce0b6667b5",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 4: Lexicon member coverage (%)",
        "table_column_names": [
            "target",
            "VN",
            "WN-V",
            "WN-N"
        ],
        "table_content_values": [
            [
                "type",
                "81",
                "66",
                "47"
            ],
            [
                "x+POS",
                "54",
                "39",
                "43"
            ],
            [
                "lemma",
                "88",
                "76",
                "53"
            ],
            [
                "x+POS",
                "79",
                "63",
                "50"
            ],
            [
                "shared",
                "54",
                "39",
                "41"
            ]
        ],
        "question": "Is it true that WN-N shows high coverage containing many high-frequency members?",
        "answer_label": "no"
    },
    {
        "id": "84408bed-7687-4049-9b6b-35bed42eda8f",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 4: Experiment 2, t= \u201cb*tch\u201d",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.010",
                "0.010",
                "-0.632",
                "[EMPTY]",
                "0.978"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.963",
                "0.944",
                "20.064",
                "***",
                "1.020"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.011",
                "0.011",
                "-1.254",
                "[EMPTY]",
                "0.955"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.349",
                "0.290",
                "28.803",
                "***",
                "1.203"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.012",
                "0.012",
                "-0.162",
                "[EMPTY]",
                "0.995"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.017",
                "0.015",
                "4.698",
                "***",
                "1.152"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.988",
                "0.991",
                "-6.289",
                "***",
                "0.997"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.099",
                "0.091",
                "6.273",
                "***",
                "1.091"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.074",
                "0.027",
                "46.054",
                "***",
                "2.728"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.925",
                "0.968",
                "-41.396",
                "***",
                "0.956"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.010",
                "0.010",
                "0.000",
                "[EMPTY]",
                "1.000"
            ]
        ],
        "question": "Is it true that In both cases the classifiers trained upon their data are still more likely to flag white-aligned tweets as sexism?",
        "answer_label": "no"
    },
    {
        "id": "3a6e389b-f580-4a8a-a2be-ac2fe511e577",
        "table_caption": "Sparse and Structured Visual Attention Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
        "table_column_names": [
            "[EMPTY]",
            "Att. to image",
            "Att. to bounding boxes",
            "Test-Dev Yes/No",
            "Test-Dev Number",
            "Test-Dev Other",
            "Test-Dev Overall",
            "Test-Standard Yes/No",
            "Test-Standard Number",
            "Test-Standard Other",
            "Test-Standard Overall"
        ],
        "table_content_values": [
            [
                "softmax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "42.65",
                "55.74",
                "65.52",
                "83.55",
                "42.68",
                "56.01",
                "65.97"
            ],
            [
                "sparsemax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "43.19",
                "55.79",
                "65.60",
                "83.33",
                "42.99",
                "56.06",
                "65.94"
            ],
            [
                "soft-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.13",
                "43.53",
                "56.01",
                "65.76",
                "83.63",
                "43.24",
                "56.10",
                "66.11"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.10",
                "43.30",
                "56.14",
                "65.79",
                "83.66",
                "43.18",
                "56.21",
                "66.17"
            ],
            [
                "softmax",
                "[EMPTY]",
                "\u2713",
                "85.14",
                "49.59",
                "58.72",
                "68.57",
                "85.56",
                "49.54",
                "59.11",
                "69.04"
            ],
            [
                "sparsemax",
                "[EMPTY]",
                "\u2713",
                "[BOLD] 85.40",
                "[BOLD] 50.87",
                "58.67",
                "68.79",
                "[BOLD] 85.80",
                "50.18",
                "59.08",
                "69.19"
            ],
            [
                "softmax",
                "\u2713",
                "\u2713",
                "85.33",
                "50.49",
                "58.88",
                "68.82",
                "85.58",
                "50.42",
                "59.18",
                "69.17"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "\u2713",
                "85.35",
                "50.52",
                "[BOLD] 59.15",
                "[BOLD] 68.96",
                "85.72",
                "[BOLD] 50.66",
                "[BOLD] 59.22",
                "[BOLD] 69.28"
            ]
        ],
        "question": "Is it true that However, the model using TVMAX in the final attention layer does not necessarily achieve the highest accuracy, showing that features obtained using the TVMAX transformation are not necessarily a better complement to bounding box features?",
        "answer_label": "no"
    },
    {
        "id": "ac292ba0-cc9c-4235-8e92-4901c60e2903",
        "table_caption": "Deriving Machine Attention from Human Rationales Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.",
        "table_column_names": [
            "Source",
            "Target",
            "Svm",
            "Ra-Svm\u2021",
            "Ra-Cnn\u2021",
            "Trans\u2020",
            "Ra-Trans\u2021\u2020",
            "Ours\u2021\u2020",
            "Oracle\u2020"
        ],
        "table_content_values": [
            [
                "Beer aroma+palate",
                "Beer look",
                "74.41",
                "74.83",
                "74.94",
                "72.75",
                "76.41",
                "[BOLD] 79.53",
                "80.29"
            ],
            [
                "Beer look+palate",
                "Beer aroma",
                "68.57",
                "69.23",
                "67.55",
                "69.92",
                "76.45",
                "[BOLD] 77.94",
                "78.11"
            ],
            [
                "Beer look+aroma",
                "Beer palate",
                "63.88",
                "67.82",
                "65.72",
                "74.66",
                "73.40",
                "[BOLD] 75.24",
                "75.50"
            ]
        ],
        "question": "Is it true that It closely matches the performance of ORACLE with only 0.40% absolute difference?",
        "answer_label": "yes"
    },
    {
        "id": "6e133404-6df1-467e-b5af-f0a895d24779",
        "table_caption": "Towards Scalable and Reliable Capsule Networksfor Challenging NLP Applications Table 2: Comparisons of our NLP-Cap approach and baselines on two text classification benchmarks, where \u2019-\u2019 denotes methods that failed to scale due to memory issues.",
        "table_column_names": [
            "<bold>Datasets</bold>",
            "<bold>Metrics</bold>",
            "<bold>FastXML</bold>",
            "<bold>PD-Sparse</bold>",
            "<bold>FastText</bold>",
            "<bold>Bow-CNN</bold>",
            "<bold>CNN-Kim</bold>",
            "<bold>XML-CNN</bold>",
            "<bold>Cap-Zhao</bold>",
            "<bold>NLP-Cap</bold>",
            "<bold>Impv</bold>"
        ],
        "table_content_values": [
            [
                "RCV1",
                "PREC@1",
                "94.62",
                "95.16",
                "95.40",
                "96.40",
                "93.54",
                "96.86",
                "96.63",
                "<bold>97.05</bold>",
                "+0.20%"
            ],
            [
                "RCV1",
                "PREC@3",
                "78.40",
                "79.46",
                "79.96",
                "81.17",
                "76.15",
                "81.11",
                "81.02",
                "<bold>81.27</bold>",
                "+0.20%"
            ],
            [
                "RCV1",
                "PREC@5",
                "54.82",
                "55.61",
                "55.64",
                "<bold>56.74</bold>",
                "52.94",
                "56.07",
                "56.12",
                "56.33",
                "-0.72%"
            ],
            [
                "[EMPTY]",
                "NDCG@1",
                "94.62",
                "95.16",
                "95.40",
                "96.40",
                "93.54",
                "96.88",
                "96.63",
                "<bold>97.05</bold>",
                "+0.20%"
            ],
            [
                "[EMPTY]",
                "NDCG@3",
                "89.21",
                "90.29",
                "90.95",
                "92.04",
                "87.26",
                "92.22",
                "92.31",
                "<bold>92.47</bold>",
                "+0.17%"
            ],
            [
                "[EMPTY]",
                "NDCG@5",
                "90.27",
                "91.29",
                "91.68",
                "92.89",
                "88.20",
                "92.63",
                "92.75",
                "<bold>93.11</bold>",
                "+0.52%"
            ],
            [
                "EUR-Lex",
                "PREC@1",
                "68.12",
                "72.10",
                "71.51",
                "64.99",
                "68.35",
                "75.65",
                "-",
                "<bold>80.20</bold>",
                "+6.01%"
            ],
            [
                "EUR-Lex",
                "PREC@3",
                "57.93",
                "57.74",
                "60.37",
                "51.68",
                "54.45",
                "61.81",
                "-",
                "<bold>65.48</bold>",
                "+5.93%"
            ],
            [
                "EUR-Lex",
                "PREC@5",
                "48.97",
                "47.48",
                "50.41",
                "42.32",
                "44.07",
                "50.90",
                "-",
                "<bold>52.83</bold>",
                "+3.79%"
            ],
            [
                "[EMPTY]",
                "NDCG@1",
                "68.12",
                "72.10",
                "71.51",
                "64.99",
                "68.35",
                "75.65",
                "-",
                "<bold>80.20</bold>",
                "+6.01%"
            ],
            [
                "[EMPTY]",
                "NDCG@3",
                "60.66",
                "61.33",
                "63.32",
                "55.03",
                "59.81",
                "66.71",
                "-",
                "<bold>71.11</bold>",
                "+6.59%"
            ],
            [
                "[EMPTY]",
                "NDCG@5",
                "56.42",
                "55.93",
                "58.56",
                "49.92",
                "57.99",
                "64.45",
                "-",
                "<bold>68.80</bold>",
                "+6.75%"
            ]
        ],
        "question": "Is it true that In Table 2, we can see a noticeable margin brought by our capsule-based approach over the strong baselines on EUR-Lex, and competitive results on RCV1?",
        "answer_label": "yes"
    },
    {
        "id": "f4879cd6-63b6-4f55-9bca-036a3a0a0b90",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that Surprisingly, S2S has a better performance than G2S-GGNN and G2S-GAT when handling graphs that contain low degree nodes?",
        "answer_label": "no"
    },
    {
        "id": "b5999ce8-28d9-47e9-a847-e7d72dcfde52",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
        "table_column_names": [
            "Model",
            "#Params",
            "BLEU",
            "Train",
            "Decode"
        ],
        "table_content_values": [
            [
                "GNMT",
                "-",
                "24.61",
                "-",
                "-"
            ],
            [
                "GRU",
                "206M",
                "26.28",
                "2.67",
                "45.35"
            ],
            [
                "ATR",
                "122M",
                "25.70",
                "1.33",
                "[BOLD] 34.40"
            ],
            [
                "SRU",
                "170M",
                "25.91",
                "1.34",
                "42.84"
            ],
            [
                "LRN",
                "143M",
                "26.26",
                "[BOLD] 0.99",
                "36.50"
            ],
            [
                "oLRN",
                "164M",
                "[BOLD] 26.73",
                "1.15",
                "40.19"
            ]
        ],
        "question": "Is it true that Still, both LRN and oLRN translate sentences faster than SRU (+15%/+6%)?",
        "answer_label": "yes"
    },
    {
        "id": "c1eb6469-562e-4b25-9c8e-0d11fb645a96",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that Compared to CMOW, the hybrid model shows rather small differences?",
        "answer_label": "yes"
    },
    {
        "id": "d69416ad-80be-4be2-bc76-6806f2a74b90",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.",
        "table_column_names": [
            "Model",
            "Encoder",
            "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1",
            "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
            "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
            "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
            "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1",
            "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
            "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
            "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
        ],
        "table_content_values": [
            [
                "MLP",
                "CNN-RNN",
                ".311",
                ".340",
                ".486",
                ".532",
                ".318",
                ".335",
                ".481",
                ".524"
            ],
            [
                "MLP",
                "PMeans-RNN",
                ".313",
                ".331",
                ".489",
                ".536",
                ".354",
                ".375",
                ".502",
                ".556"
            ],
            [
                "MLP",
                "BERT",
                "[BOLD] .487",
                "[BOLD] .526",
                "[BOLD] .544",
                "[BOLD] .597",
                "[BOLD] .505",
                "[BOLD] .531",
                "[BOLD] .556",
                "[BOLD] .608"
            ],
            [
                "SimRed",
                "CNN",
                ".340",
                ".392",
                ".470",
                ".515",
                ".396",
                ".443",
                ".499",
                ".549"
            ],
            [
                "SimRed",
                "PMeans",
                ".354",
                ".393",
                ".493",
                ".541",
                ".370",
                ".374",
                ".507",
                ".551"
            ],
            [
                "SimRed",
                "BERT",
                ".266",
                ".296",
                ".458",
                ".495",
                ".325",
                ".338",
                ".485",
                ".533"
            ],
            [
                "Peyrard and Gurevych ( 2018 )",
                "Peyrard and Gurevych ( 2018 )",
                ".177",
                ".189",
                ".271",
                ".306",
                ".175",
                ".186",
                ".268",
                ".174"
            ]
        ],
        "question": "Is it true that MLP with BERT as encoder does not have the best overall performance?",
        "answer_label": "no"
    },
    {
        "id": "dbbed763-8147-4853-9749-d8a28167584b",
        "table_caption": "Distant Learning for Entity Linking with Automatic Noise Detection Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
        "table_column_names": [
            "System",
            "All P",
            "All R",
            "All F1",
            "In  [ITALIC] E+ P",
            "In  [ITALIC] E+ R",
            "In  [ITALIC] E+ F1"
        ],
        "table_content_values": [
            [
                "Name matching",
                "15.03",
                "15.03",
                "15.03",
                "29.13",
                "29.13",
                "29.13"
            ],
            [
                "MIL (model 1)",
                "35.87",
                "35.87",
                "35.87 \u00b10.72",
                "69.38",
                "69.38",
                "69.38 \u00b11.29"
            ],
            [
                "MIL-ND (model 2)",
                "37.42",
                "[BOLD] 37.42",
                "37.42 \u00b10.35",
                "72.50",
                "[BOLD] 72.50",
                "[BOLD] 72.50 \u00b10.68"
            ],
            [
                "[ITALIC] \u03c4MIL-ND (model 2)",
                "[BOLD] 38.91",
                "36.73",
                "[BOLD] 37.78 \u00b10.26",
                "[BOLD] 73.19",
                "71.15",
                "72.16 \u00b10.48"
            ],
            [
                "Supervised learning",
                "42.90",
                "42.90",
                "42.90 \u00b10.59",
                "83.12",
                "83.12",
                "83.12 \u00b11.15"
            ]
        ],
        "question": "Is it true that MIL-ND does not significantly outperform MIL: the 95% confidence intervals for them overlap?",
        "answer_label": "no"
    },
    {
        "id": "75819ab9-8d94-432d-bb30-590df24c67b7",
        "table_caption": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment Table 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] Full UAS",
            "[BOLD] PPA Acc."
        ],
        "table_content_values": [
            [
                "RBG",
                "94.17",
                "88.51"
            ],
            [
                "RBG + HPCD (full)",
                "94.19",
                "89.59"
            ],
            [
                "RBG + LSTM-PP",
                "94.14",
                "86.35"
            ],
            [
                "RBG + OntoLSTM-PP",
                "94.30",
                "90.11"
            ],
            [
                "RBG + Oracle PP",
                "94.60",
                "98.97"
            ]
        ],
        "question": "Is it true that However, when gold PP attachment are used, we note only a small improvement of 10.46 points in PP attachment accuracies (between the PPA accuracy for RBG and RBG + Oracle PP), which suggests that adding PP predictions as features is not an effective approach?",
        "answer_label": "no"
    },
    {
        "id": "a86eec51-fb50-4409-9bde-27aa6dd58d44",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.",
        "table_column_names": [
            "Model",
            "Val. Accuracy",
            "Loss",
            "Val. Loss",
            "Pretraining Time",
            "Finetuning Time"
        ],
        "table_content_values": [
            [
                "Siamese Networks",
                "77.42%",
                "0.5601",
                "0.5329",
                "[EMPTY]",
                "4m per epoch"
            ],
            [
                "BERT",
                "87.47%",
                "0.4655",
                "0.4419",
                "66 hours",
                "2m per epoch"
            ],
            [
                "GPT-2",
                "90.99%",
                "0.2172",
                "0.1826",
                "78 hours",
                "4m per epoch"
            ],
            [
                "ULMFiT",
                "91.59%",
                "0.3750",
                "0.1972",
                "11 hours",
                "2m per epoch"
            ],
            [
                "ULMFiT (no LM Finetuning)",
                "78.11%",
                "0.5512",
                "0.5409",
                "11 hours",
                "2m per epoch"
            ],
            [
                "BERT + Multitasking",
                "91.20%",
                "0.3155",
                "0.3023",
                "66 hours",
                "4m per epoch"
            ],
            [
                "GPT-2 + Multitasking",
                "96.28%",
                "0.2609",
                "0.2197",
                "78 hours",
                "5m per epoch"
            ]
        ],
        "question": "Is it true that BERT achieved a final accuracy of 87.47%, lower than ULMFiT's full performance?",
        "answer_label": "no"
    },
    {
        "id": "9b0e1193-f48e-4334-b899-f5e92f4df3da",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 7: Cross-lingual embedding alignment in Italian and in German, before and after debiasing.",
        "table_column_names": [
            "[EMPTY]",
            "Italian \u2192 En",
            "Italian En \u2192",
            "German \u2192 En",
            "German En \u2192"
        ],
        "table_content_values": [
            [
                "Orig",
                "58.73",
                "59.68",
                "47.58",
                "50.48"
            ],
            [
                "Debias",
                "[BOLD] 60.03",
                "[BOLD] 60.96",
                "[BOLD] 47.89",
                "[BOLD] 51.76"
            ]
        ],
        "question": "Is it true that The results reported in Table 7 show that precision on BDI indeed increases as a result of the reduced effect of grammatical gender on the embeddings for German and Italian, i.e?",
        "answer_label": "yes"
    },
    {
        "id": "6a0f3f3f-8f25-43cb-938d-3808d00199ac",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
        "table_column_names": [
            "[EMPTY]",
            "Prec.",
            "Rec.",
            "F1"
        ],
        "table_content_values": [
            [
                "(A1) BiLSTM-CNN",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "(A2) Standard attention",
                "0.466",
                "0.638",
                "0.539"
            ],
            [
                "(A3) Window size ( [ITALIC] ws)=5",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "(A4) Window size ( [ITALIC] ws)=10",
                "0.510",
                "0.640",
                "0.568"
            ],
            [
                "(A5) Softmax",
                "0.490",
                "0.658",
                "0.562"
            ],
            [
                "(A6) Max-pool",
                "0.492",
                "0.600",
                "0.541"
            ]
        ],
        "question": "Is it true that Replacing the attention normalizing function with softmax operation also reduces the F1 score marginally (A3\u2212A5)?",
        "answer_label": "yes"
    },
    {
        "id": "6f84236f-e476-4ea2-9bba-f83a1b157df1",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Feature",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Feature",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams"
            ],
            [
                "not",
                ".154",
                "[URL]",
                ".150"
            ],
            [
                "my",
                ".131",
                "!",
                ".082"
            ],
            [
                "working",
                ".124",
                "he",
                ".069"
            ],
            [
                "still",
                ".123",
                "thank",
                ".067"
            ],
            [
                "on",
                ".119",
                ",",
                ".064"
            ],
            [
                "can\u2019t",
                ".113",
                "love",
                ".064"
            ],
            [
                "service",
                ".112",
                "lol",
                ".061"
            ],
            [
                "customer",
                ".109",
                "you",
                ".060"
            ],
            [
                "why",
                ".108",
                "great",
                ".058"
            ],
            [
                "website",
                ".107",
                "win",
                ".058"
            ],
            [
                "no",
                ".104",
                "\u2019",
                ".058"
            ],
            [
                "?",
                ".098",
                "she",
                ".054"
            ],
            [
                "fix",
                ".093",
                ":",
                ".053"
            ],
            [
                "won\u2019t",
                ".092",
                "that",
                ".053"
            ],
            [
                "been",
                ".090",
                "more",
                ".052"
            ],
            [
                "issue",
                ".089",
                "it",
                ".052"
            ],
            [
                "days",
                ".088",
                "would",
                ".051"
            ],
            [
                "error",
                ".087",
                "him",
                ".047"
            ],
            [
                "is",
                ".084",
                "life",
                ".046"
            ],
            [
                "charged",
                ".083",
                "good",
                ".046"
            ],
            [
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)"
            ],
            [
                "VBN",
                ".141",
                "UH",
                ".104"
            ],
            [
                "$",
                ".118",
                "NNP",
                ".098"
            ],
            [
                "VBZ",
                ".114",
                "PRP",
                ".076"
            ],
            [
                "NN_VBZ",
                ".114",
                "HT",
                ".076"
            ],
            [
                "PRP$",
                ".107",
                "PRP_.",
                ".076"
            ],
            [
                "PRP$_NN",
                ".105",
                "PRP_RB",
                ".067"
            ],
            [
                "VBG",
                ".093",
                "NNP_NNP",
                ".062"
            ],
            [
                "CD",
                ".092",
                "VBP_PRP",
                ".054"
            ],
            [
                "WRB_VBZ",
                ".084",
                "JJ",
                ".053"
            ],
            [
                "VBZ_VBN",
                ".084",
                "DT_JJ",
                ".051"
            ]
        ],
        "question": "Is it true that [CONTINUE] A distinctive part-of-speech pattern common in complaints is possessive pronouns followed by nouns (PRP$ NN) which refer to items of services possessed by the complainer (e.g., my account, my order)?",
        "answer_label": "yes"
    },
    {
        "id": "e58cd347-4775-4698-9703-16d155e90bc7",
        "table_caption": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] ROUGE-1  [BOLD] R (%)",
            "[BOLD] ROUGE-1  [BOLD] P (%)",
            "[BOLD] ROUGE-1  [BOLD] F (%)",
            "[BOLD] ROUGE-2  [BOLD] R (%)",
            "[BOLD] ROUGE-2  [BOLD] P (%)",
            "[BOLD] ROUGE-2  [BOLD] F (%)",
            "[BOLD] Sentence-Level  [BOLD] R (%)",
            "[BOLD] Sentence-Level  [BOLD] P (%)",
            "[BOLD] Sentence-Level  [BOLD] F (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] ILP",
                "24.5",
                "41.1",
                "29.3\u00b10.5",
                "7.9",
                "15.0",
                "9.9\u00b10.5",
                "13.6",
                "22.6",
                "15.6\u00b10.4"
            ],
            [
                "[BOLD] Sum-Basic",
                "28.4",
                "44.4",
                "33.1\u00b10.5",
                "8.5",
                "15.6",
                "10.4\u00b10.4",
                "14.7",
                "22.9",
                "16.7\u00b10.5"
            ],
            [
                "[BOLD] KL-Sum",
                "39.5",
                "34.6",
                "35.5\u00b10.5",
                "13.0",
                "12.7",
                "12.3\u00b10.5",
                "15.2",
                "21.1",
                "16.3\u00b10.5"
            ],
            [
                "[BOLD] LexRank",
                "42.1",
                "39.5",
                "38.7\u00b10.5",
                "14.7",
                "15.3",
                "14.2\u00b10.5",
                "14.3",
                "21.5",
                "16.0\u00b10.5"
            ],
            [
                "[BOLD] MEAD",
                "45.5",
                "36.5",
                "38.5\u00b1 0.5",
                "17.9",
                "14.9",
                "15.4\u00b10.5",
                "27.8",
                "29.2",
                "26.8\u00b10.5"
            ],
            [
                "[BOLD] SVM",
                "19.0",
                "48.8",
                "24.7\u00b10.8",
                "7.5",
                "21.1",
                "10.0\u00b10.5",
                "32.7",
                "34.3",
                "31.4\u00b10.4"
            ],
            [
                "[BOLD] LogReg",
                "26.9",
                "34.5",
                "28.7\u00b10.6",
                "6.4",
                "9.9",
                "7.3\u00b10.4",
                "12.2",
                "14.9",
                "12.7\u00b10.5"
            ],
            [
                "[BOLD] LogReg [ITALIC] r",
                "28.0",
                "34.8",
                "29.4\u00b10.6",
                "6.9",
                "10.4",
                "7.8\u00b10.4",
                "12.1",
                "14.5",
                "12.5\u00b10.5"
            ],
            [
                "[BOLD] HAN",
                "31.0",
                "42.8",
                "33.7\u00b10.7",
                "11.2",
                "17.8",
                "12.7\u00b10.5",
                "26.9",
                "34.1",
                "32.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT",
                "32.2",
                "42.4",
                "34.4\u00b10.7",
                "11.5",
                "17.5",
                "12.9\u00b10.5",
                "29.6",
                "35.8",
                "32.2\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU",
                "32.1",
                "42.1",
                "33.8\u00b10.7",
                "11.6",
                "17.6",
                "12.9\u00b10.5",
                "30.1",
                "35.6",
                "32.3\u00b10.5"
            ],
            [
                "[BOLD] HAN [ITALIC] r",
                "38.1",
                "40.5",
                "[BOLD] 37.8\u00b10.5",
                "14.0",
                "17.1",
                "[BOLD] 14.7\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.5",
                "16.8",
                "[BOLD] 14.4\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.6",
                "16.9",
                "[BOLD] 14.4\u00b10.5",
                "33.9",
                "33.8",
                "[BOLD] 33.8\u00b10.5"
            ]
        ],
        "question": "Is it true that The HAN models do not outperform MEAD in terms of sentence prediction?",
        "answer_label": "no"
    },
    {
        "id": "718a9e4f-712c-4c8b-bb79-52a817600a8e",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that Interestingly, G2S-GIN has better performance among our models?",
        "answer_label": "no"
    },
    {
        "id": "f5a1f7ce-a335-4908-a0e1-baea0d34c863",
        "table_caption": "Distant Learning for Entity Linking with Automatic Noise Detection Table 3: % errors on the development set for different named entity types under two settings. (Smaller is better.)",
        "table_column_names": [
            "System",
            "All LOC",
            "All ORG",
            "All PER",
            "All MISC",
            "In  [ITALIC] E+ LOC",
            "In  [ITALIC] E+ ORG",
            "In  [ITALIC] E+ PER",
            "In  [ITALIC] E+ MISC"
        ],
        "table_content_values": [
            [
                "Name matching",
                "96.26",
                "89.48",
                "57.38",
                "96.60",
                "92.32",
                "76.87",
                "47.40",
                "76.29"
            ],
            [
                "MIL",
                "57.09",
                "[BOLD] 76.30",
                "41.35",
                "93.35",
                "11.90",
                "[BOLD] 47.90",
                "27.60",
                "53.61"
            ],
            [
                "MIL-ND",
                "57.15",
                "77.15",
                "35.95",
                "92.47",
                "12.02",
                "49.77",
                "20.94",
                "47.42"
            ],
            [
                "[ITALIC] \u03c4MIL-ND",
                "[BOLD] 55.15",
                "76.56",
                "[BOLD] 34.03",
                "[BOLD] 92.15",
                "[BOLD] 11.14",
                "51.18",
                "[BOLD] 20.59",
                "[BOLD] 40.00"
            ],
            [
                "Supervised learning",
                "55.58",
                "61.32",
                "24.98",
                "89.96",
                "8.80",
                "14.95",
                "7.40",
                "29.90"
            ]
        ],
        "question": "Is it true that [CONTINUE] For LOC, it turns out that candidate selection is not a bottleneck: when candidate selection was flawless, the models made only about 55% errors, down from about 96%?",
        "answer_label": "no"
    },
    {
        "id": "ebd1548f-ff05-41b1-917c-9a5e04da6635",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that TF has the best values of recall and f-measure for all corpora but the English version of TED Talks which has in DF the best value of recall and in DocSub the best value of f-measure?",
        "answer_label": "yes"
    },
    {
        "id": "5ecc4b82-ccbe-480e-af2d-c5e567617179",
        "table_caption": "Semantic Neural Machine Translation using AMR Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.",
        "table_column_names": [
            "AMR Anno.",
            "BLEU"
        ],
        "table_content_values": [
            [
                "Automatic",
                "16.8"
            ],
            [
                "Gold",
                "[BOLD] *17.5*"
            ]
        ],
        "question": "Is it true that Table 4 shows the BLEU scores of our Dual2seq model taking gold or automatic AMRs as inputs?",
        "answer_label": "yes"
    },
    {
        "id": "8dac475d-fd22-4945-a778-12d64244ffb8",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. \u2020 denotes the previous best state-of-the-art model.",
        "table_column_names": [
            "Model",
            "NYT10 Prec.",
            "NYT10 Rec.",
            "NYT10 F1",
            "NYT11 Prec.",
            "NYT11 Rec.",
            "NYT11 F1"
        ],
        "table_content_values": [
            [
                "CNN zeng2014relation",
                "0.413",
                "0.591",
                "0.486",
                "0.444",
                "0.625",
                "0.519"
            ],
            [
                "PCNN zeng2015distant",
                "0.380",
                "[BOLD] 0.642",
                "0.477",
                "0.446",
                "0.679",
                "0.538\u2020"
            ],
            [
                "EA huang2016attention",
                "0.443",
                "0.638",
                "0.523\u2020",
                "0.419",
                "0.677",
                "0.517"
            ],
            [
                "BGWA jat2018attention",
                "0.364",
                "0.632",
                "0.462",
                "0.417",
                "[BOLD] 0.692",
                "0.521"
            ],
            [
                "BiLSTM-CNN",
                "0.490",
                "0.507",
                "0.498",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "Our model",
                "[BOLD] 0.541",
                "0.595",
                "[BOLD] 0.566*",
                "[BOLD] 0.507",
                "0.652",
                "[BOLD] 0.571*"
            ]
        ],
        "question": "Is it true that Our model does not improve the precision scores on both datasets with good recall scores?",
        "answer_label": "no"
    },
    {
        "id": "4c9a4b70-f8f6-4aac-b271-616dbbee6ad4",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Acc",
            "[BOLD] F1",
            "[BOLD] AUC"
        ],
        "table_content_values": [
            [
                "Most Frequent Class",
                "64.2",
                "39.1",
                "0.500"
            ],
            [
                "Logistic Regression",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Sentiment \u2013 MPQA",
                "64.2",
                "39.1",
                "0.499"
            ],
            [
                "Sentiment \u2013 NRC",
                "63.9",
                "42.2",
                "0.599"
            ],
            [
                "Sentiment \u2013 V&B",
                "68.9",
                "60.0",
                "0.696"
            ],
            [
                "Sentiment \u2013 VADER",
                "66.0",
                "54.2",
                "0.654"
            ],
            [
                "Sentiment \u2013 Stanford",
                "68.0",
                "55.6",
                "0.696"
            ],
            [
                "Complaint Specific (all)",
                "65.7",
                "55.2",
                "0.634"
            ],
            [
                "Request",
                "64.2",
                "39.1",
                "0.583"
            ],
            [
                "Intensifiers",
                "64.5",
                "47.3",
                "0.639"
            ],
            [
                "Downgraders",
                "65.4",
                "49.8",
                "0.615"
            ],
            [
                "Temporal References",
                "64.2",
                "43.7",
                "0.535"
            ],
            [
                "Pronoun Types",
                "64.1",
                "39.1",
                "0.545"
            ],
            [
                "POS Bigrams",
                "72.2",
                "66.8",
                "0.756"
            ],
            [
                "LIWC",
                "71.6",
                "65.8",
                "0.784"
            ],
            [
                "Word2Vec Clusters",
                "67.7",
                "58.3",
                "0.738"
            ],
            [
                "Bag-of-Words",
                "79.8",
                "77.5",
                "0.866"
            ],
            [
                "All Features",
                "[BOLD] 80.5",
                "[BOLD] 78.0",
                "[BOLD] 0.873"
            ],
            [
                "Neural Networks",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "MLP",
                "78.3",
                "76.2",
                "0.845"
            ],
            [
                "LSTM",
                "80.2",
                "77.0",
                "0.864"
            ]
        ],
        "question": "Is it true that The topical features such as the LIWC dictionaries (which combine syntactic and semantic information) and Word2Vec topics perform in the same range as the part of speech tags?",
        "answer_label": "yes"
    },
    {
        "id": "ed36c8eb-dda1-48b8-8fa3-7a9456fdfd05",
        "table_caption": "Zero-Shot Grounding of Objects from Natural Language Queries Table 6: Ablation study: BM=Base Model, softmax means we classify only one candidate box as foreground, BCE = Binary Cross Entropy means we classify each candidate box as the foreground or background, FL = Focal Loss, Img-Resize: use images of dimension 600\u00d7600",
        "table_column_names": [
            "Model",
            "Accuracy on RefClef"
        ],
        "table_content_values": [
            [
                "BM + Softmax",
                "48.54"
            ],
            [
                "BM + BCE",
                "55.20"
            ],
            [
                "BM + FL",
                "57.13"
            ],
            [
                "BM + FL + Img-Resize",
                "[BOLD] 61.75"
            ]
        ],
        "question": "Is it true that [CONTINUE] However, the highest accuracy was achieved by using Binary Cross Entropy, with a score of 55.20?",
        "answer_label": "no"
    },
    {
        "id": "6e2893ee-586e-4dd1-8a74-1fd4f036362e",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
        "table_column_names": [
            "Model",
            "Answerability Macro P/R/F",
            "# Answerable",
            "Answer Prec.",
            "Derivation Prec. RG-L (P/R/F)",
            "Derivation Prec. BL-4"
        ],
        "table_content_values": [
            [
                "Shortest Path",
                "54.8/55.5/53.2",
                "976",
                "3.6",
                "56.7/38.5/41.5",
                "31.3"
            ],
            [
                "PRKGC",
                "52.6/51.5/50.7",
                "1,021",
                "45.2",
                "40.7/60.7/44.7",
                "30.9"
            ],
            [
                "PRKGC+NS",
                "53.6/54.1/52.1",
                "980",
                "45.4",
                "42.2/61.6/46.1",
                "33.4"
            ]
        ],
        "question": "Is it true that The use of annotated NLDs as supervision does not improve the generalization ability of question answering?",
        "answer_label": "no"
    },
    {
        "id": "3ebe7506-55ba-4319-86df-794421f4d65f",
        "table_caption": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] ROUGE-1  [BOLD] R (%)",
            "[BOLD] ROUGE-1  [BOLD] P (%)",
            "[BOLD] ROUGE-1  [BOLD] F (%)",
            "[BOLD] ROUGE-2  [BOLD] R (%)",
            "[BOLD] ROUGE-2  [BOLD] P (%)",
            "[BOLD] ROUGE-2  [BOLD] F (%)",
            "[BOLD] Sentence-Level  [BOLD] R (%)",
            "[BOLD] Sentence-Level  [BOLD] P (%)",
            "[BOLD] Sentence-Level  [BOLD] F (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] ILP",
                "24.5",
                "41.1",
                "29.3\u00b10.5",
                "7.9",
                "15.0",
                "9.9\u00b10.5",
                "13.6",
                "22.6",
                "15.6\u00b10.4"
            ],
            [
                "[BOLD] Sum-Basic",
                "28.4",
                "44.4",
                "33.1\u00b10.5",
                "8.5",
                "15.6",
                "10.4\u00b10.4",
                "14.7",
                "22.9",
                "16.7\u00b10.5"
            ],
            [
                "[BOLD] KL-Sum",
                "39.5",
                "34.6",
                "35.5\u00b10.5",
                "13.0",
                "12.7",
                "12.3\u00b10.5",
                "15.2",
                "21.1",
                "16.3\u00b10.5"
            ],
            [
                "[BOLD] LexRank",
                "42.1",
                "39.5",
                "38.7\u00b10.5",
                "14.7",
                "15.3",
                "14.2\u00b10.5",
                "14.3",
                "21.5",
                "16.0\u00b10.5"
            ],
            [
                "[BOLD] MEAD",
                "45.5",
                "36.5",
                "38.5\u00b1 0.5",
                "17.9",
                "14.9",
                "15.4\u00b10.5",
                "27.8",
                "29.2",
                "26.8\u00b10.5"
            ],
            [
                "[BOLD] SVM",
                "19.0",
                "48.8",
                "24.7\u00b10.8",
                "7.5",
                "21.1",
                "10.0\u00b10.5",
                "32.7",
                "34.3",
                "31.4\u00b10.4"
            ],
            [
                "[BOLD] LogReg",
                "26.9",
                "34.5",
                "28.7\u00b10.6",
                "6.4",
                "9.9",
                "7.3\u00b10.4",
                "12.2",
                "14.9",
                "12.7\u00b10.5"
            ],
            [
                "[BOLD] LogReg [ITALIC] r",
                "28.0",
                "34.8",
                "29.4\u00b10.6",
                "6.9",
                "10.4",
                "7.8\u00b10.4",
                "12.1",
                "14.5",
                "12.5\u00b10.5"
            ],
            [
                "[BOLD] HAN",
                "31.0",
                "42.8",
                "33.7\u00b10.7",
                "11.2",
                "17.8",
                "12.7\u00b10.5",
                "26.9",
                "34.1",
                "32.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT",
                "32.2",
                "42.4",
                "34.4\u00b10.7",
                "11.5",
                "17.5",
                "12.9\u00b10.5",
                "29.6",
                "35.8",
                "32.2\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU",
                "32.1",
                "42.1",
                "33.8\u00b10.7",
                "11.6",
                "17.6",
                "12.9\u00b10.5",
                "30.1",
                "35.6",
                "32.3\u00b10.5"
            ],
            [
                "[BOLD] HAN [ITALIC] r",
                "38.1",
                "40.5",
                "[BOLD] 37.8\u00b10.5",
                "14.0",
                "17.1",
                "[BOLD] 14.7\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.5",
                "16.8",
                "[BOLD] 14.4\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.6",
                "16.9",
                "[BOLD] 14.4\u00b10.5",
                "33.9",
                "33.8",
                "[BOLD] 33.8\u00b10.5"
            ]
        ],
        "question": "Is it true that [CONTINUE] Pretraining the HAN models yields significantly better results than those without?",
        "answer_label": "no"
    },
    {
        "id": "8dd76692-4ea1-4658-9ce7-d242e640238e",
        "table_caption": "Sparse and Structured Visual Attention Table 2: Human evaluation results on MSCOCO.",
        "table_column_names": [
            "[EMPTY]",
            "caption",
            "attention relevance"
        ],
        "table_content_values": [
            [
                "softmax",
                "3.50",
                "3.38"
            ],
            [
                "sparsemax",
                "3.71",
                "3.89"
            ],
            [
                "TVmax",
                "[BOLD] 3.87",
                "[BOLD] 4.10"
            ]
        ],
        "question": "Is it true that Despite performing slightly worse than sparsemax under automatic metrics, TVMAX does not outperform sparsemax and softmax in the caption human evaluation and the attention relevance human evaluation, reported in Table 2?",
        "answer_label": "no"
    },
    {
        "id": "083d7f85-28ec-4a2f-87e7-0c54bd378f36",
        "table_caption": "Suggestion Mining from Online Reviews using ULMFiT Table 3: Performance of different models on the provided train and test dataset for Sub Task A.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] F1 (train)",
            "[BOLD] F1 (test)"
        ],
        "table_content_values": [
            [
                "[BOLD] Multinomial Naive Bayes (using Count Vectorizer)",
                "0.641",
                "0.517"
            ],
            [
                "[BOLD] Logistic Regression (using Count Vectorizer)",
                "0.679",
                "0.572"
            ],
            [
                "[BOLD] SVM (Linear Kernel) (using TfIdf Vectorizer)",
                "0.695",
                "0.576"
            ],
            [
                "[BOLD] LSTM (128 LSTM Units)",
                "0.731",
                "0.591"
            ],
            [
                "[BOLD] Provided Baseline",
                "0.720",
                "0.267"
            ],
            [
                "[BOLD] ULMFit*",
                "0.861",
                "0.701"
            ]
        ],
        "question": "Is it true that [CONTINUE] The ULMFiT model achieved the best results with a F1-score of 0.861 on the training dataset and a F1-score of 0.701 on the test dataset?",
        "answer_label": "yes"
    },
    {
        "id": "7b4f6a72-1867-4e14-b9ff-5414a76d5834",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 2: Applicability (App.), Productivity (Prod.) and Coverage (Cov.) of the various words in the alternatives of the COPA dev set.",
        "table_column_names": [
            "Cue",
            "App.",
            "Prod.",
            "Cov."
        ],
        "table_content_values": [
            [
                "in",
                "47",
                "55.3",
                "9.40"
            ],
            [
                "was",
                "55",
                "61.8",
                "11.0"
            ],
            [
                "to",
                "82",
                "40.2",
                "16.4"
            ],
            [
                "the",
                "85",
                "38.8",
                "17.0"
            ],
            [
                "a",
                "106",
                "57.5",
                "21.2"
            ]
        ],
        "question": "Is it true that For example, the is the token with the highest coverage and appears in either a correct alternative or wrong alternative in 17.0% of COPA training instances?",
        "answer_label": "no"
    },
    {
        "id": "167f52a1-a645-4d80-8cfc-577a4d19e5d3",
        "table_caption": "Solving Hard Coreference Problems Table 7: Performance results on Winograd and WinoCoref datasets. All our three systems are trained on WinoCoref, and we evaluate the predictions on both datasets. Our systems improve over the baselines by over than 20% on Winograd and over 15% on WinoCoref.",
        "table_column_names": [
            "Dataset",
            "Metric",
            "Illinois",
            "IlliCons",
            "rahman2012resolving",
            "KnowFeat",
            "KnowCons",
            "KnowComb"
        ],
        "table_content_values": [
            [
                "[ITALIC] Winograd",
                "Precision",
                "51.48",
                "53.26",
                "73.05",
                "71.81",
                "74.93",
                "[BOLD] 76.41"
            ],
            [
                "[ITALIC] WinoCoref",
                "AntePre",
                "68.37",
                "74.32",
                "\u2014\u2013",
                "88.48",
                "88.95",
                "[BOLD] 89.32"
            ]
        ],
        "question": "Is it true that The best performing system is not KnowComb?",
        "answer_label": "no"
    },
    {
        "id": "a62ed321-045e-4c34-9771-274b95b428c9",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "36.85",
                "5.3782",
                "35.14",
                "55.01",
                "1.6016",
                "00.34",
                "09.81",
                "00.15",
                "10.31"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen",
                "39.23",
                "6.0217",
                "36.97",
                "55.52",
                "1.7623",
                "00.40",
                "03.59",
                "00.07",
                "04.05"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen+",
                "40.25",
                "6.1448",
                "37.50",
                "56.19",
                "1.8181",
                "00.21",
                "01.99",
                "00.05",
                "02.24"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.88",
                "3.9310",
                "32.11",
                "39.90",
                "0.5036",
                "07.73",
                "17.76",
                "09.52",
                "35.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.19",
                "6.0543",
                "37.38",
                "55.88",
                "1.8104",
                "00.17",
                "01.31",
                "00.25",
                "01.72"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen",
                "40.73",
                "6.1711",
                "37.76",
                "56.09",
                "1.8518",
                "00.07",
                "00.72",
                "00.08",
                "00.87"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen+",
                "40.51",
                "6.1226",
                "37.61",
                "55.98",
                "1.8286",
                "00.02",
                "00.63",
                "00.06",
                "00.70"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.66",
                "3.9511",
                "32.93",
                "39.29",
                "0.3855",
                "07.89",
                "15.60",
                "08.44",
                "31.94"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.48",
                "6.0269",
                "37.26",
                "56.19",
                "1.7999",
                "00.43",
                "02.84",
                "00.26",
                "03.52"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen",
                "41.57",
                "6.2830",
                "37.99",
                "56.36",
                "1.8849",
                "00.37",
                "01.40",
                "00.09",
                "01.86"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen+",
                "41.56",
                "6.2700",
                "37.94",
                "56.38",
                "1.8827",
                "00.21",
                "01.04",
                "00.07",
                "01.31"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "35.99",
                "5.0734",
                "34.74",
                "54.79",
                "1.5259",
                "00.02",
                "11.58",
                "00.02",
                "11.62"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen",
                "40.07",
                "6.1243",
                "37.45",
                "55.81",
                "1.8026",
                "00.05",
                "03.23",
                "00.01",
                "03.29"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen+",
                "40.80",
                "6.2197",
                "37.86",
                "56.13",
                "1.8422",
                "00.01",
                "01.87",
                "00.01",
                "01.88"
            ]
        ],
        "question": "Is it true that The results for testing on cleaned data (Table 3, top half) do not confirm the positive impact of cleaned training data and also show that the cleaned test data is not more challenging (cf?",
        "answer_label": "no"
    },
    {
        "id": "388ba17a-36df-4217-ac48-845683103ee5",
        "table_caption": "Language Independent Sequence Labelling for Opinion Target Extraction Table 6: ABSA SemEval 2016: Comparison of multilingual results in terms of F1 scores.",
        "table_column_names": [
            "Language",
            "System",
            "F1"
        ],
        "table_content_values": [
            [
                "es",
                "GTI",
                "68.51"
            ],
            [
                "es",
                "L +  [BOLD] CW600 + W2VW300",
                "[BOLD] 69.92"
            ],
            [
                "es",
                "Baseline",
                "51.91"
            ],
            [
                "fr",
                "IIT-T",
                "66.67"
            ],
            [
                "fr",
                "L +  [BOLD] CW100",
                "[BOLD] 69.50"
            ],
            [
                "fr",
                "Baseline",
                "45.45"
            ],
            [
                "nl",
                "IIT-T",
                "56.99"
            ],
            [
                "nl",
                "L +  [BOLD] W2VW400",
                "[BOLD] 66.39"
            ],
            [
                "nl",
                "Baseline",
                "50.64"
            ],
            [
                "ru",
                "Danii.",
                "33.47"
            ],
            [
                "ru",
                "L +  [BOLD] CW500",
                "[BOLD] 65.53"
            ],
            [
                "ru",
                "Baseline",
                "49.31"
            ],
            [
                "tr",
                "L +  [BOLD] BW",
                "[BOLD] 60.22"
            ],
            [
                "tr",
                "Baseline",
                "41.86"
            ]
        ],
        "question": "Is it true that Table 6 shows that our system outperforms the best previous approaches across the five languages?",
        "answer_label": "yes"
    },
    {
        "id": "a120da2d-bd9e-4499-bde4-f927dc66e638",
        "table_caption": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 3: Ablation study of capsule net and word-level attention on Wikidata dataset.",
        "table_column_names": [
            "Recall",
            "0.1",
            "0.2",
            "0.3",
            "AUC"
        ],
        "table_content_values": [
            [
                "-Word-ATT",
                "0.648",
                "0.515",
                "0.395",
                "0.389"
            ],
            [
                "-Capsule",
                "0.635",
                "0.507",
                "0.413",
                "0.386"
            ],
            [
                "Our Model",
                "0.650",
                "0.519",
                "0.422",
                "0.405"
            ]
        ],
        "question": "Is it true that According to the table, the drop of precision demonstrates that the capsule net is more useful than the word-level attention?",
        "answer_label": "no"
    },
    {
        "id": "53788df3-ebe9-4242-bcca-f4aae9867517",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 5: Human sentence-level validation of metrics; 100 examples for each dataset for validating Acc; 150 each for Sim and PP; see text for validation of GM.",
        "table_column_names": [
            "Metric",
            "Method of validation",
            "Yelp",
            "Lit."
        ],
        "table_content_values": [
            [
                "Acc",
                "% of machine and human judgments that match",
                "94",
                "84"
            ],
            [
                "Sim",
                "Spearman\u2019s  [ITALIC] \u03c1 b/w Sim and human ratings of semantic preservation",
                "0.79",
                "0.75"
            ],
            [
                "PP",
                "Spearman\u2019s  [ITALIC] \u03c1 b/w negative PP and human ratings of fluency",
                "0.81",
                "0.67"
            ]
        ],
        "question": "Is it true that [CONTINUE] We validate Sim and PP by computing sentence-level Spearman's \u03c1 between the metric and human judgments [CONTINUE] From Table 5, all validations show weak correlations on the Yelp dataset and poor correlations on Literature?",
        "answer_label": "no"
    },
    {
        "id": "be8b847f-1195-49a2-85e7-98fd7b3de34a",
        "table_caption": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.",
        "table_column_names": [
            "[EMPTY]",
            "dev perp \u2193",
            "dev acc \u2191",
            "dev wer \u2193",
            "test perp \u2193",
            "test acc \u2191",
            "test wer \u2193"
        ],
        "table_content_values": [
            [
                "Spanish-only-LM",
                "329.68",
                "26.6",
                "30.47",
                "322.26",
                "25.1",
                "29.62"
            ],
            [
                "English-only-LM",
                "320.92",
                "29.3",
                "32.02",
                "314.04",
                "30.3",
                "32.51"
            ],
            [
                "All:CS-last-LM",
                "76.64",
                "47.8",
                "14.56",
                "76.97",
                "49.2",
                "14.13"
            ],
            [
                "All:Shuffled-LM",
                "68.00",
                "51.8",
                "13.64",
                "68.72",
                "51.4",
                "13.89"
            ],
            [
                "CS-only-LM",
                "43.20",
                "60.7",
                "12.60",
                "43.42",
                "57.9",
                "12.18"
            ],
            [
                "CS-only+vocab-LM",
                "45.61",
                "61.0",
                "12.56",
                "45.79",
                "58.8",
                "12.49"
            ],
            [
                "Fine-Tuned-LM",
                "39.76",
                "66.9",
                "10.71",
                "40.11",
                "65.4",
                "10.17"
            ],
            [
                "CS-only-disc",
                "\u2013",
                "72.0",
                "6.35",
                "\u2013",
                "70.5",
                "6.70"
            ],
            [
                "Fine-Tuned-disc",
                "\u2013",
                "[BOLD] 74.2",
                "[BOLD] 5.85",
                "\u2013",
                "[BOLD] 75.5",
                "[BOLD] 5.59"
            ]
        ],
        "question": "Is it true that We gain further improvement by adding monolingual data and get an accuracy of 74.2%, which is only 0.3 points higher than the best language model?",
        "answer_label": "no"
    },
    {
        "id": "295db104-a00f-4932-b5c0-740e1efa09b9",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 4: Lexicon member coverage (%)",
        "table_column_names": [
            "target",
            "VN",
            "WN-V",
            "WN-N"
        ],
        "table_content_values": [
            [
                "type",
                "81",
                "66",
                "47"
            ],
            [
                "x+POS",
                "54",
                "39",
                "43"
            ],
            [
                "lemma",
                "88",
                "76",
                "53"
            ],
            [
                "x+POS",
                "79",
                "63",
                "50"
            ],
            [
                "shared",
                "54",
                "39",
                "41"
            ]
        ],
        "question": "Is it true that POS-disambiguation, in turn, fragments the vocabulary and consistently reduces the coverage with the effect being less pronounced for lemmatized targets?",
        "answer_label": "yes"
    },
    {
        "id": "73b4f514-f040-4c25-bced-8300abf76759",
        "table_caption": "Argument Generation with Retrieval, Planning, and Realization Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
        "table_column_names": [
            "[EMPTY]",
            "[ITALIC] w/ System Retrieval  [BOLD] B-2",
            "[ITALIC] w/ System Retrieval  [BOLD] B-4",
            "[ITALIC] w/ System Retrieval  [BOLD] R-2",
            "[ITALIC] w/ System Retrieval  [BOLD] MTR",
            "[ITALIC] w/ System Retrieval  [BOLD] #Word",
            "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
        ],
        "table_content_values": [
            [
                "Human",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22"
            ],
            [
                "Retrieval",
                "7.55",
                "1.11",
                "8.64",
                "14.38",
                "123",
                "23",
                "10.97",
                "3.05",
                "23.49",
                "20.08",
                "140",
                "21"
            ],
            [
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Seq2seq",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15"
            ],
            [
                "Seq2seqAug",
                "8.26",
                "2.24",
                "13.79",
                "15.75",
                "78",
                "14",
                "10.98",
                "4.41",
                "22.97",
                "19.62",
                "71",
                "14"
            ],
            [
                "[ITALIC] w/o psg",
                "7.94",
                "2.28",
                "10.13",
                "15.71",
                "75",
                "12",
                "9.89",
                "3.34",
                "14.20",
                "18.40",
                "66",
                "12"
            ],
            [
                "H&W\u00a0Hua and Wang ( 2018 )",
                "3.64",
                "0.92",
                "8.83",
                "11.78",
                "51",
                "12",
                "8.51",
                "2.86",
                "18.89",
                "17.18",
                "58",
                "12"
            ],
            [
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "CANDELA",
                "12.02\u2217",
                "[BOLD] 2.99\u2217",
                "[BOLD] 14.93\u2217",
                "[BOLD] 16.92\u2217",
                "119",
                "22",
                "15.80\u2217",
                "[BOLD] 5.00\u2217",
                "[BOLD] 23.75",
                "[BOLD] 20.18",
                "116",
                "22"
            ],
            [
                "[ITALIC] w/o psg",
                "[BOLD] 12.33\u2217",
                "2.86\u2217",
                "14.53\u2217",
                "16.60\u2217",
                "123",
                "23",
                "[BOLD] 16.33\u2217",
                "4.98\u2217",
                "23.65",
                "19.94",
                "123",
                "23"
            ]
        ],
        "question": "Is it true that [CONTINUE] Under system setup, our model CANDELA does not statistically significantly outperform all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < [CONTINUE] .0005)?",
        "answer_label": "no"
    },
    {
        "id": "7228065d-6c7a-44a1-86e6-7d8408d8557d",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
        "table_column_names": [
            "Topic Name",
            "Size",
            "TF-IDF ARI",
            "WMD ARI",
            "Sent2vec ARI",
            "Doc2vec ARI",
            "BERT ARI",
            "[ITALIC] OD-w2v ARI",
            "[ITALIC] OD-d2v ARI",
            "TF-IDF  [ITALIC] Sil.",
            "WMD  [ITALIC] Sil.",
            "Sent2vec  [ITALIC] Sil.",
            "Doc2vec  [ITALIC] Sil.",
            "BERT  [ITALIC] Sil.",
            "[ITALIC] OD-w2v  [ITALIC] Sil.",
            "[ITALIC] OD-d2v  [ITALIC] Sil."
        ],
        "table_content_values": [
            [
                "Affirmative Action",
                "81",
                "-0.07",
                "-0.02",
                "0.03",
                "-0.01",
                "-0.02",
                "[BOLD] 0.14",
                "[ITALIC] 0.02",
                "0.01",
                "0.01",
                "-0.01",
                "-0.02",
                "-0.04",
                "[BOLD] 0.06",
                "[ITALIC] 0.01"
            ],
            [
                "Atheism",
                "116",
                "[BOLD] 0.19",
                "0.07",
                "0.00",
                "0.03",
                "-0.01",
                "0.11",
                "[ITALIC] 0.16",
                "0.02",
                "0.01",
                "0.02",
                "0.01",
                "0.01",
                "[ITALIC] 0.05",
                "[BOLD] 0.07"
            ],
            [
                "Austerity Measures",
                "20",
                "[ITALIC] 0.04",
                "[ITALIC] 0.04",
                "-0.01",
                "-0.05",
                "0.04",
                "[BOLD] 0.21",
                "-0.01",
                "0.06",
                "0.07",
                "0.05",
                "-0.03",
                "0.10",
                "[BOLD] 0.19",
                "0.1"
            ],
            [
                "Democratization",
                "76",
                "0.02",
                "-0.01",
                "0.00",
                "[ITALIC] 0.09",
                "-0.01",
                "[BOLD] 0.11",
                "0.07",
                "0.01",
                "0.01",
                "0.02",
                "0.02",
                "0.03",
                "[BOLD] 0.16",
                "[ITALIC] 0.11"
            ],
            [
                "Education Voucher Scheme",
                "30",
                "[BOLD] 0.25",
                "0.12",
                "0.08",
                "-0.02",
                "0.04",
                "0.13",
                "[ITALIC] 0.19",
                "0.01",
                "0.01",
                "0.01",
                "-0.01",
                "0.02",
                "[ITALIC] 0.38",
                "[BOLD] 0.40"
            ],
            [
                "Gambling",
                "60",
                "-0.06",
                "-0.01",
                "-0.02",
                "0.04",
                "0.09",
                "[ITALIC] 0.35",
                "[BOLD] 0.39",
                "0.01",
                "0.02",
                "0.03",
                "0.01",
                "0.09",
                "[BOLD] 0.30",
                "[ITALIC] 0.22"
            ],
            [
                "Housing",
                "30",
                "0.01",
                "-0.01",
                "-0.01",
                "-0.02",
                "0.08",
                "[BOLD] 0.27",
                "0.01",
                "0.02",
                "0.03",
                "0.03",
                "0.01",
                "0.11",
                "[BOLD] 0.13",
                "[ITALIC] 0.13"
            ],
            [
                "Hydroelectric Dams",
                "110",
                "[BOLD] 0.47",
                "[ITALIC] 0.45",
                "[ITALIC] 0.45",
                "-0.01",
                "0.38",
                "0.35",
                "0.14",
                "0.04",
                "0.08",
                "0.12",
                "0.01",
                "0.19",
                "[BOLD] 0.26",
                "[ITALIC] 0.09"
            ],
            [
                "Intellectual Property",
                "66",
                "0.01",
                "0.01",
                "0.00",
                "0.03",
                "0.03",
                "[ITALIC] 0.05",
                "[BOLD] 0.14",
                "0.01",
                "[ITALIC] 0.04",
                "0.03",
                "0.01",
                "0.03",
                "[ITALIC] 0.04",
                "[BOLD] 0.12"
            ],
            [
                "Keystone pipeline",
                "18",
                "0.01",
                "0.01",
                "0.00",
                "-0.13",
                "[BOLD] 0.07",
                "-0.01",
                "[BOLD] 0.07",
                "-0.01",
                "-0.03",
                "-0.03",
                "-0.07",
                "0.03",
                "[BOLD] 0.05",
                "[ITALIC] 0.02"
            ],
            [
                "Monarchy",
                "61",
                "-0.04",
                "0.01",
                "0.00",
                "0.03",
                "-0.02",
                "[BOLD] 0.15",
                "[BOLD] 0.15",
                "0.01",
                "0.02",
                "0.02",
                "0.01",
                "0.01",
                "[BOLD] 0.11",
                "[ITALIC] 0.09"
            ],
            [
                "National Service",
                "33",
                "0.14",
                "-0.03",
                "-0.01",
                "0.02",
                "0.01",
                "[ITALIC] 0.31",
                "[BOLD] 0.39",
                "0.02",
                "0.04",
                "0.02",
                "0.01",
                "0.02",
                "[BOLD] 0.25",
                "[BOLD] 0.25"
            ],
            [
                "One-child policy China",
                "67",
                "-0.05",
                "0.01",
                "[BOLD] 0.11",
                "-0.02",
                "0.02",
                "[BOLD] 0.11",
                "0.01",
                "0.01",
                "0.02",
                "[ITALIC] 0.04",
                "-0.01",
                "0.03",
                "[BOLD] 0.07",
                "-0.02"
            ],
            [
                "Open-source Software",
                "48",
                "-0.02",
                "-0.01",
                "[ITALIC] 0.05",
                "0.01",
                "0.12",
                "[BOLD] 0.09",
                "-0.02",
                "0.01",
                "-0.01",
                "0.00",
                "-0.02",
                "0.03",
                "[BOLD] 0.18",
                "0.01"
            ],
            [
                "Pornography",
                "52",
                "-0.02",
                "0.01",
                "0.01",
                "-0.02",
                "-0.01",
                "[BOLD] 0.41",
                "[BOLD] 0.41",
                "0.01",
                "0.01",
                "0.02",
                "-0.01",
                "0.03",
                "[BOLD] 0.47",
                "[ITALIC] 0.41"
            ],
            [
                "Seanad Abolition",
                "25",
                "0.23",
                "0.09",
                "-0.01",
                "-0.01",
                "0.03",
                "[ITALIC] 0.32",
                "[BOLD] 0.54",
                "0.02",
                "0.01",
                "-0.01",
                "-0.03",
                "-0.04",
                "[ITALIC] 0.15",
                "[BOLD] 0.31"
            ],
            [
                "Trades Unions",
                "19",
                "[ITALIC] 0.44",
                "[ITALIC] 0.44",
                "[BOLD] 0.60",
                "-0.05",
                "0.44",
                "[ITALIC] 0.44",
                "0.29",
                "0.1",
                "0.17",
                "0.21",
                "0.01",
                "0.26",
                "[BOLD] 0.48",
                "[ITALIC] 0.32"
            ],
            [
                "Video Games",
                "72",
                "-0.01",
                "0.01",
                "0.12",
                "0.01",
                "0.08",
                "[ITALIC] 0.40",
                "[BOLD] 0.56",
                "0.01",
                "0.01",
                "0.06",
                "0.01",
                "0.05",
                "[ITALIC] 0.32",
                "[BOLD] 0.42"
            ],
            [
                "Average",
                "54.67",
                "0.09",
                "0.07",
                "0.08",
                "0.01",
                "0.08",
                "[BOLD] 0.22",
                "[ITALIC] 0.20",
                "0.02",
                "0.03",
                "0.04",
                "-0.01",
                "0.05",
                "[BOLD] 0.20",
                "[ITALIC] 0.17"
            ]
        ],
        "question": "Is it true that Opinion distance methods do not generally outperform the competition on both ARI and Silhouette coefficient?",
        "answer_label": "no"
    },
    {
        "id": "6708b66f-e81e-4a90-bc90-eb9ba3b61e0b",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 6: Results on Yelp sentiment transfer, where BLEU is between 1000 transferred sentences and human references, and Acc is restricted to the same 1000 sentences. Our best models (right table) achieve higher BLEU than prior work at similar levels of Acc, but untransferred sentences achieve the highest BLEU. Acc\u2217: the definition of Acc varies by row because of different classifiers in use. Other results from simple-transfer are not included as they are worse.",
        "table_column_names": [
            "Model",
            "BLEU",
            "Acc\u2217"
        ],
        "table_content_values": [
            [
                "fu-1",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Multi-decoder",
                "7.6",
                "0.792"
            ],
            [
                "Style embed.",
                "15.4",
                "0.095"
            ],
            [
                "simple-transfer",
                "simple-transfer",
                "simple-transfer"
            ],
            [
                "Template",
                "18.0",
                "0.867"
            ],
            [
                "Delete/Retrieve",
                "12.6",
                "0.909"
            ],
            [
                "yang2018unsupervised",
                "yang2018unsupervised",
                "yang2018unsupervised"
            ],
            [
                "LM",
                "13.4",
                "0.854"
            ],
            [
                "LM + classifier",
                "[BOLD] 22.3",
                "0.900"
            ],
            [
                "Untransferred",
                "[BOLD] 31.4",
                "0.024"
            ]
        ],
        "question": "Is it true that However, at similar levels of Acc, our models have higher BLEU scores than prior work?",
        "answer_label": "yes"
    },
    {
        "id": "0773f240-5761-43fd-a7d3-d55af3879cfd",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 7: Comparisons of different DCGCN models under almost the same parameter budget.",
        "table_column_names": [
            "[BOLD] Model",
            "D",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN(1)",
                "300",
                "10.9M",
                "20.9",
                "52.0"
            ],
            [
                "DCGCN(2)",
                "180",
                "10.9M",
                "[BOLD] 22.2",
                "[BOLD] 52.3"
            ],
            [
                "DCGCN(2)",
                "240",
                "11.3M",
                "22.8",
                "52.8"
            ],
            [
                "DCGCN(4)",
                "180",
                "11.4M",
                "[BOLD] 23.4",
                "[BOLD] 53.4"
            ],
            [
                "DCGCN(1)",
                "420",
                "12.6M",
                "22.2",
                "52.4"
            ],
            [
                "DCGCN(2)",
                "300",
                "12.5M",
                "23.8",
                "53.8"
            ],
            [
                "DCGCN(3)",
                "240",
                "12.3M",
                "[BOLD] 23.9",
                "[BOLD] 54.1"
            ],
            [
                "DCGCN(2)",
                "360",
                "14.0M",
                "24.2",
                "[BOLD] 54.4"
            ],
            [
                "DCGCN(3)",
                "300",
                "14.0M",
                "[BOLD] 24.4",
                "54.2"
            ],
            [
                "DCGCN(2)",
                "420",
                "15.6M",
                "24.1",
                "53.7"
            ],
            [
                "DCGCN(4)",
                "300",
                "15.6M",
                "[BOLD] 24.6",
                "[BOLD] 54.8"
            ],
            [
                "DCGCN(3)",
                "420",
                "18.6M",
                "24.5",
                "54.6"
            ],
            [
                "DCGCN(4)",
                "360",
                "18.4M",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN2 obtains 22.2 BLEU points, which is higher than DCGCN1 (20.9)?",
        "answer_label": "yes"
    },
    {
        "id": "1c8b2caa-9827-4d03-9c99-7d4d00a815da",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that In conclusion, these results above can show the robustness and effectiveness of our DCGCN models?",
        "answer_label": "yes"
    },
    {
        "id": "a4769518-b932-490d-990d-4465bce4aa8f",
        "table_caption": "On the difficulty of a distributional semantics of spoken language Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.",
        "table_column_names": [
            "[EMPTY]",
            "Recall@10 (%)",
            "Median rank",
            "RSAimage"
        ],
        "table_content_values": [
            [
                "VGS",
                "27",
                "6",
                "0.4"
            ],
            [
                "SegMatch",
                "[BOLD] 10",
                "[BOLD] 37",
                "[BOLD] 0.5"
            ],
            [
                "Audio2vec-U",
                "5",
                "105",
                "0.0"
            ],
            [
                "Audio2vec-C",
                "2",
                "647",
                "0.0"
            ],
            [
                "Mean MFCC",
                "1",
                "1,414",
                "0.0"
            ],
            [
                "Chance",
                "0",
                "3,955",
                "0.0"
            ]
        ],
        "question": "Is it true that SegMatch works much better than Audio2vec according to both criteria?",
        "answer_label": "yes"
    },
    {
        "id": "ae185a9a-f330-4515-8005-8ecce9a6d6df",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that In contrast, our DCGCN models cannot be trained using a large number of layers?",
        "answer_label": "no"
    },
    {
        "id": "f80d1fe5-769f-49f8-ba8e-2014665930eb",
        "table_caption": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.",
        "table_column_names": [
            "Feature",
            "LR P",
            "LR R",
            "LR F1",
            "SVM P",
            "SVM R",
            "SVM F1",
            "ANN P",
            "ANN R",
            "ANN F1"
        ],
        "table_content_values": [
            [
                "+BoW",
                "0.93",
                "0.91",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+BoC (Wiki-PubMed-PMC)",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.91",
                "0.91",
                "[BOLD] 0.91"
            ],
            [
                "+BoC (GloVe)",
                "0.93",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+ASM",
                "0.90",
                "0.85",
                "0.88",
                "0.90",
                "0.86",
                "0.88",
                "0.89",
                "0.89",
                "0.89"
            ],
            [
                "+Sentence Embeddings(SEs)",
                "0.89",
                "0.89",
                "0.89",
                "0.90",
                "0.86",
                "0.88",
                "0.88",
                "0.88",
                "0.88"
            ],
            [
                "+BoC(Wiki-PubMed-PMC)+SEs",
                "0.92",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ]
        ],
        "question": "Is it true that [CONTINUE] Logistic Regression outperforms other classifiers in extracting most relations?",
        "answer_label": "no"
    },
    {
        "id": "f2f7734e-5b8b-4a24-bae8-acd7f6758765",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
        "table_column_names": [
            "<bold>Model</bold>",
            "R",
            "MUC P",
            "<italic>F</italic>1",
            "R",
            "B3 P",
            "<italic>F</italic>1",
            "R",
            "CEAF-<italic>e</italic> P",
            "<italic>F</italic>1",
            "CoNLL <italic>F</italic>1"
        ],
        "table_content_values": [
            [
                "<bold>Baselines</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Cluster+Lemma",
                "76.5",
                "79.9",
                "78.1",
                "71.7",
                "85",
                "77.8",
                "75.5",
                "71.7",
                "73.6",
                "76.5"
            ],
            [
                "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
                "71",
                "75",
                "73",
                "71",
                "78",
                "74",
                "-",
                "-",
                "64",
                "73"
            ],
            [
                "KCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>)",
                "67",
                "71",
                "69",
                "71",
                "67",
                "69",
                "71",
                "67",
                "69",
                "69"
            ],
            [
                "Cluster+KCP",
                "68.4",
                "79.3",
                "73.4",
                "67.2",
                "87.2",
                "75.9",
                "77.4",
                "66.4",
                "71.5",
                "73.6"
            ],
            [
                "<bold>Model Variants</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Disjoint",
                "75.5",
                "83.6",
                "79.4",
                "75.4",
                "86",
                "80.4",
                "80.3",
                "71.9",
                "75.9",
                "78.5"
            ],
            [
                "Joint",
                "77.6",
                "84.5",
                "80.9",
                "76.1",
                "85.1",
                "80.3",
                "81",
                "73.8",
                "77.3",
                "<bold>79.5</bold>"
            ]
        ],
        "question": "Is it true that Our joint model outperforms all the base [CONTINUE] The results reconfirm that the lemma baseline, when combined with effective topic clustering, is a strong baseline for CD event coreference resolution on the ECB+ corpus (Upadhyay et al., 2016)?",
        "answer_label": "yes"
    },
    {
        "id": "62eed765-15d0-4c11-9d2b-d12a4be5f764",
        "table_caption": "Sparse and Structured Visual Attention Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
        "table_column_names": [
            "[EMPTY]",
            "Att. to image",
            "Att. to bounding boxes",
            "Test-Dev Yes/No",
            "Test-Dev Number",
            "Test-Dev Other",
            "Test-Dev Overall",
            "Test-Standard Yes/No",
            "Test-Standard Number",
            "Test-Standard Other",
            "Test-Standard Overall"
        ],
        "table_content_values": [
            [
                "softmax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "42.65",
                "55.74",
                "65.52",
                "83.55",
                "42.68",
                "56.01",
                "65.97"
            ],
            [
                "sparsemax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "43.19",
                "55.79",
                "65.60",
                "83.33",
                "42.99",
                "56.06",
                "65.94"
            ],
            [
                "soft-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.13",
                "43.53",
                "56.01",
                "65.76",
                "83.63",
                "43.24",
                "56.10",
                "66.11"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.10",
                "43.30",
                "56.14",
                "65.79",
                "83.66",
                "43.18",
                "56.21",
                "66.17"
            ],
            [
                "softmax",
                "[EMPTY]",
                "\u2713",
                "85.14",
                "49.59",
                "58.72",
                "68.57",
                "85.56",
                "49.54",
                "59.11",
                "69.04"
            ],
            [
                "sparsemax",
                "[EMPTY]",
                "\u2713",
                "[BOLD] 85.40",
                "[BOLD] 50.87",
                "58.67",
                "68.79",
                "[BOLD] 85.80",
                "50.18",
                "59.08",
                "69.19"
            ],
            [
                "softmax",
                "\u2713",
                "\u2713",
                "85.33",
                "50.49",
                "58.88",
                "68.82",
                "85.58",
                "50.42",
                "59.18",
                "69.17"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "\u2713",
                "85.35",
                "50.52",
                "[BOLD] 59.15",
                "[BOLD] 68.96",
                "85.72",
                "[BOLD] 50.66",
                "[BOLD] 59.22",
                "[BOLD] 69.28"
            ]
        ],
        "question": "Is it true that Moreover, the model using TVMAX in the final attention layer achieves the highest accuracy, showing that features obtained using the TVMAX transformation are a better complement to bounding box features?",
        "answer_label": "yes"
    },
    {
        "id": "3f58abfe-7545-46fa-85bb-fc60fb38b406",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Acc",
            "[BOLD] F1",
            "[BOLD] AUC"
        ],
        "table_content_values": [
            [
                "Most Frequent Class",
                "64.2",
                "39.1",
                "0.500"
            ],
            [
                "Logistic Regression",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Sentiment \u2013 MPQA",
                "64.2",
                "39.1",
                "0.499"
            ],
            [
                "Sentiment \u2013 NRC",
                "63.9",
                "42.2",
                "0.599"
            ],
            [
                "Sentiment \u2013 V&B",
                "68.9",
                "60.0",
                "0.696"
            ],
            [
                "Sentiment \u2013 VADER",
                "66.0",
                "54.2",
                "0.654"
            ],
            [
                "Sentiment \u2013 Stanford",
                "68.0",
                "55.6",
                "0.696"
            ],
            [
                "Complaint Specific (all)",
                "65.7",
                "55.2",
                "0.634"
            ],
            [
                "Request",
                "64.2",
                "39.1",
                "0.583"
            ],
            [
                "Intensifiers",
                "64.5",
                "47.3",
                "0.639"
            ],
            [
                "Downgraders",
                "65.4",
                "49.8",
                "0.615"
            ],
            [
                "Temporal References",
                "64.2",
                "43.7",
                "0.535"
            ],
            [
                "Pronoun Types",
                "64.1",
                "39.1",
                "0.545"
            ],
            [
                "POS Bigrams",
                "72.2",
                "66.8",
                "0.756"
            ],
            [
                "LIWC",
                "71.6",
                "65.8",
                "0.784"
            ],
            [
                "Word2Vec Clusters",
                "67.7",
                "58.3",
                "0.738"
            ],
            [
                "Bag-of-Words",
                "79.8",
                "77.5",
                "0.866"
            ],
            [
                "All Features",
                "[BOLD] 80.5",
                "[BOLD] 78.0",
                "[BOLD] 0.873"
            ],
            [
                "Neural Networks",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "MLP",
                "78.3",
                "76.2",
                "0.845"
            ],
            [
                "LSTM",
                "80.2",
                "77.0",
                "0.864"
            ]
        ],
        "question": "Is it true that Syntactic part-ofspeech features do not obtain higher performance than any sentiment or complaint feature group, showing the syntactic patterns discussed in the previous section do not hold high predictive accuracy for the task?",
        "answer_label": "no"
    },
    {
        "id": "ec6aee2f-4336-4c2a-8da2-10020466e7fc",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
        "table_column_names": [
            "Topic Name",
            "Size",
            "TF-IDF ARI",
            "WMD ARI",
            "Sent2vec ARI",
            "Doc2vec ARI",
            "BERT ARI",
            "[ITALIC] OD-w2v ARI",
            "[ITALIC] OD-d2v ARI",
            "TF-IDF  [ITALIC] Sil.",
            "WMD  [ITALIC] Sil.",
            "Sent2vec  [ITALIC] Sil.",
            "Doc2vec  [ITALIC] Sil.",
            "BERT  [ITALIC] Sil.",
            "[ITALIC] OD-w2v  [ITALIC] Sil.",
            "[ITALIC] OD-d2v  [ITALIC] Sil."
        ],
        "table_content_values": [
            [
                "Affirmative Action",
                "81",
                "-0.07",
                "-0.02",
                "0.03",
                "-0.01",
                "-0.02",
                "[BOLD] 0.14",
                "[ITALIC] 0.02",
                "0.01",
                "0.01",
                "-0.01",
                "-0.02",
                "-0.04",
                "[BOLD] 0.06",
                "[ITALIC] 0.01"
            ],
            [
                "Atheism",
                "116",
                "[BOLD] 0.19",
                "0.07",
                "0.00",
                "0.03",
                "-0.01",
                "0.11",
                "[ITALIC] 0.16",
                "0.02",
                "0.01",
                "0.02",
                "0.01",
                "0.01",
                "[ITALIC] 0.05",
                "[BOLD] 0.07"
            ],
            [
                "Austerity Measures",
                "20",
                "[ITALIC] 0.04",
                "[ITALIC] 0.04",
                "-0.01",
                "-0.05",
                "0.04",
                "[BOLD] 0.21",
                "-0.01",
                "0.06",
                "0.07",
                "0.05",
                "-0.03",
                "0.10",
                "[BOLD] 0.19",
                "0.1"
            ],
            [
                "Democratization",
                "76",
                "0.02",
                "-0.01",
                "0.00",
                "[ITALIC] 0.09",
                "-0.01",
                "[BOLD] 0.11",
                "0.07",
                "0.01",
                "0.01",
                "0.02",
                "0.02",
                "0.03",
                "[BOLD] 0.16",
                "[ITALIC] 0.11"
            ],
            [
                "Education Voucher Scheme",
                "30",
                "[BOLD] 0.25",
                "0.12",
                "0.08",
                "-0.02",
                "0.04",
                "0.13",
                "[ITALIC] 0.19",
                "0.01",
                "0.01",
                "0.01",
                "-0.01",
                "0.02",
                "[ITALIC] 0.38",
                "[BOLD] 0.40"
            ],
            [
                "Gambling",
                "60",
                "-0.06",
                "-0.01",
                "-0.02",
                "0.04",
                "0.09",
                "[ITALIC] 0.35",
                "[BOLD] 0.39",
                "0.01",
                "0.02",
                "0.03",
                "0.01",
                "0.09",
                "[BOLD] 0.30",
                "[ITALIC] 0.22"
            ],
            [
                "Housing",
                "30",
                "0.01",
                "-0.01",
                "-0.01",
                "-0.02",
                "0.08",
                "[BOLD] 0.27",
                "0.01",
                "0.02",
                "0.03",
                "0.03",
                "0.01",
                "0.11",
                "[BOLD] 0.13",
                "[ITALIC] 0.13"
            ],
            [
                "Hydroelectric Dams",
                "110",
                "[BOLD] 0.47",
                "[ITALIC] 0.45",
                "[ITALIC] 0.45",
                "-0.01",
                "0.38",
                "0.35",
                "0.14",
                "0.04",
                "0.08",
                "0.12",
                "0.01",
                "0.19",
                "[BOLD] 0.26",
                "[ITALIC] 0.09"
            ],
            [
                "Intellectual Property",
                "66",
                "0.01",
                "0.01",
                "0.00",
                "0.03",
                "0.03",
                "[ITALIC] 0.05",
                "[BOLD] 0.14",
                "0.01",
                "[ITALIC] 0.04",
                "0.03",
                "0.01",
                "0.03",
                "[ITALIC] 0.04",
                "[BOLD] 0.12"
            ],
            [
                "Keystone pipeline",
                "18",
                "0.01",
                "0.01",
                "0.00",
                "-0.13",
                "[BOLD] 0.07",
                "-0.01",
                "[BOLD] 0.07",
                "-0.01",
                "-0.03",
                "-0.03",
                "-0.07",
                "0.03",
                "[BOLD] 0.05",
                "[ITALIC] 0.02"
            ],
            [
                "Monarchy",
                "61",
                "-0.04",
                "0.01",
                "0.00",
                "0.03",
                "-0.02",
                "[BOLD] 0.15",
                "[BOLD] 0.15",
                "0.01",
                "0.02",
                "0.02",
                "0.01",
                "0.01",
                "[BOLD] 0.11",
                "[ITALIC] 0.09"
            ],
            [
                "National Service",
                "33",
                "0.14",
                "-0.03",
                "-0.01",
                "0.02",
                "0.01",
                "[ITALIC] 0.31",
                "[BOLD] 0.39",
                "0.02",
                "0.04",
                "0.02",
                "0.01",
                "0.02",
                "[BOLD] 0.25",
                "[BOLD] 0.25"
            ],
            [
                "One-child policy China",
                "67",
                "-0.05",
                "0.01",
                "[BOLD] 0.11",
                "-0.02",
                "0.02",
                "[BOLD] 0.11",
                "0.01",
                "0.01",
                "0.02",
                "[ITALIC] 0.04",
                "-0.01",
                "0.03",
                "[BOLD] 0.07",
                "-0.02"
            ],
            [
                "Open-source Software",
                "48",
                "-0.02",
                "-0.01",
                "[ITALIC] 0.05",
                "0.01",
                "0.12",
                "[BOLD] 0.09",
                "-0.02",
                "0.01",
                "-0.01",
                "0.00",
                "-0.02",
                "0.03",
                "[BOLD] 0.18",
                "0.01"
            ],
            [
                "Pornography",
                "52",
                "-0.02",
                "0.01",
                "0.01",
                "-0.02",
                "-0.01",
                "[BOLD] 0.41",
                "[BOLD] 0.41",
                "0.01",
                "0.01",
                "0.02",
                "-0.01",
                "0.03",
                "[BOLD] 0.47",
                "[ITALIC] 0.41"
            ],
            [
                "Seanad Abolition",
                "25",
                "0.23",
                "0.09",
                "-0.01",
                "-0.01",
                "0.03",
                "[ITALIC] 0.32",
                "[BOLD] 0.54",
                "0.02",
                "0.01",
                "-0.01",
                "-0.03",
                "-0.04",
                "[ITALIC] 0.15",
                "[BOLD] 0.31"
            ],
            [
                "Trades Unions",
                "19",
                "[ITALIC] 0.44",
                "[ITALIC] 0.44",
                "[BOLD] 0.60",
                "-0.05",
                "0.44",
                "[ITALIC] 0.44",
                "0.29",
                "0.1",
                "0.17",
                "0.21",
                "0.01",
                "0.26",
                "[BOLD] 0.48",
                "[ITALIC] 0.32"
            ],
            [
                "Video Games",
                "72",
                "-0.01",
                "0.01",
                "0.12",
                "0.01",
                "0.08",
                "[ITALIC] 0.40",
                "[BOLD] 0.56",
                "0.01",
                "0.01",
                "0.06",
                "0.01",
                "0.05",
                "[ITALIC] 0.32",
                "[BOLD] 0.42"
            ],
            [
                "Average",
                "54.67",
                "0.09",
                "0.07",
                "0.08",
                "0.01",
                "0.08",
                "[BOLD] 0.22",
                "[ITALIC] 0.20",
                "0.02",
                "0.03",
                "0.04",
                "-0.01",
                "0.05",
                "[BOLD] 0.20",
                "[ITALIC] 0.17"
            ]
        ],
        "question": "Is it true that [CONTINUE] In the exceptional case of \"Hydroelectric Dams\" dataset, the opinion distance OD performs particularly well compared to TF-IDF?",
        "answer_label": "no"
    },
    {
        "id": "b1c90f7c-a41c-4043-8869-314be2024b72",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
        "table_column_names": [
            "# of Heads",
            "Accuracy",
            "Val. Loss",
            "Effect"
        ],
        "table_content_values": [
            [
                "1",
                "89.44%",
                "0.2811",
                "-6.84%"
            ],
            [
                "2",
                "91.20%",
                "0.2692",
                "-5.08%"
            ],
            [
                "4",
                "93.85%",
                "0.2481",
                "-2.43%"
            ],
            [
                "8",
                "96.02%",
                "0.2257",
                "-0.26%"
            ],
            [
                "10",
                "96.28%",
                "0.2197",
                "[EMPTY]"
            ],
            [
                "16",
                "96.32%",
                "0.2190",
                "+0.04"
            ]
        ],
        "question": "Is it true that Using only one attention head, thereby attending to only one context position at once, does not degrade the performance to less than the performance of 10 heads using the standard finetuning scheme?",
        "answer_label": "no"
    },
    {
        "id": "c0b78cbb-c152-43d1-9c65-f0c6632ad296",
        "table_caption": "Predicting Discourse Structure using Distant Supervision from Sentiment Table 3: Discourse structure prediction results; tested on RST-DTtest and Instr-DTtest. Subscripts in inter-domain evaluation sub-table indicate the training set. Best performance in the category is bold. Consistently best model for inter-domain discourse structure prediction is underlined",
        "table_column_names": [
            "Approach",
            "RST-DTtest",
            "Instr-DTtest"
        ],
        "table_content_values": [
            [
                "Right Branching",
                "54.64",
                "58.47"
            ],
            [
                "Left Branching",
                "53.73",
                "48.15"
            ],
            [
                "Hier. Right Branch.",
                "[BOLD] 70.82",
                "[BOLD] 67.86"
            ],
            [
                "Hier. Left Branch.",
                "70.58",
                "63.49"
            ],
            [
                "[BOLD] Intra-Domain Evaluation",
                "[BOLD] Intra-Domain Evaluation",
                "[BOLD] Intra-Domain Evaluation"
            ],
            [
                "HILDAHernault et al. ( 2010 )",
                "83.00",
                "\u2014"
            ],
            [
                "DPLPJi and Eisenstein ( 2014 )",
                "82.08",
                "\u2014"
            ],
            [
                "CODRAJoty et al. ( 2015 )",
                "83.84",
                "[BOLD] 82.88"
            ],
            [
                "Two-StageWang et al. ( 2017 )",
                "[BOLD] 86.00",
                "77.28"
            ],
            [
                "[BOLD] Inter-Domain Evaluation",
                "[BOLD] Inter-Domain Evaluation",
                "[BOLD] Inter-Domain Evaluation"
            ],
            [
                "Two-StageRST-DT",
                "\u00d7",
                "73.65"
            ],
            [
                "Two-StageInstr-DT",
                "74.48",
                "\u00d7"
            ],
            [
                "Two-StageOurs(avg)",
                "76.42",
                "[BOLD] 74.22"
            ],
            [
                "Two-StageOurs(max)",
                "[BOLD] 77.24",
                "73.12"
            ],
            [
                "Human Morey et al. ( 2017 )",
                "88.30",
                "\u2014"
            ]
        ],
        "question": "Is it true that The first set of results in Table 3 shows that the completely right/left branching baselines dominate the hierarchical right/left branching ones?",
        "answer_label": "no"
    },
    {
        "id": "d6ea265e-a56d-4792-928f-4db6f95be5ef",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.",
        "table_column_names": [
            "[EMPTY]",
            "Italian Original",
            "Italian Debiased",
            "Italian English",
            "Italian Reduction",
            "German Original",
            "German Debiased",
            "German English",
            "German Reduction"
        ],
        "table_content_values": [
            [
                "Same Gender",
                "0.442",
                "0.434",
                "0.424",
                "\u2013",
                "0.491",
                "0.478",
                "0.446",
                "\u2013"
            ],
            [
                "Different Gender",
                "0.385",
                "0.421",
                "0.415",
                "\u2013",
                "0.415",
                "0.435",
                "0.403",
                "\u2013"
            ],
            [
                "difference",
                "0.057",
                "0.013",
                "0.009",
                "[BOLD] 91.67%",
                "0.076",
                "0.043",
                "0.043",
                "[BOLD] 100%"
            ]
        ],
        "question": "Is it true that In Italian, we get a reduction of 91.67% of the gap with respect to English?",
        "answer_label": "yes"
    },
    {
        "id": "1f5cde7c-845c-4204-beed-21e7759d023c",
        "table_caption": "Building a Production Model for Retrieval-Based Chatbots Table 5: Recall@k for random, frequency, and clustering whitelists of different sizes. The \u201c+\u201d indicates that the true response is added to the whitelist.",
        "table_column_names": [
            "[BOLD] Whitelist",
            "[BOLD] R@1",
            "[BOLD] R@3",
            "[BOLD] R@5",
            "[BOLD] R@10",
            "[BOLD] BLEU"
        ],
        "table_content_values": [
            [
                "Random 10K+",
                "0.252",
                "0.400",
                "0.472",
                "0.560",
                "37.71"
            ],
            [
                "Frequency 10K+",
                "0.257",
                "0.389",
                "0.455",
                "0.544",
                "41.34"
            ],
            [
                "Clustering 10K+",
                "0.230",
                "0.376",
                "0.447",
                "0.541",
                "37.59"
            ],
            [
                "Random 1K+",
                "0.496",
                "0.663",
                "0.728",
                "0.805",
                "59.28"
            ],
            [
                "Frequency 1K+",
                "0.513",
                "0.666",
                "0.726",
                "0.794",
                "67.05"
            ],
            [
                "Clustering 1K+",
                "0.481",
                "0.667",
                "0.745",
                "0.835",
                "61.88"
            ],
            [
                "Frequency 10K",
                "0.136",
                "0.261",
                "0.327",
                "0.420",
                "30.46"
            ],
            [
                "Clustering 10K",
                "0.164",
                "0.292",
                "0.360",
                "0.457",
                "31.47"
            ],
            [
                "Frequency 1K",
                "0.273",
                "0.465",
                "0.550",
                "0.658",
                "47.13"
            ],
            [
                "Clustering 1K",
                "0.331",
                "0.542",
                "0.650",
                "0.782",
                "49.26"
            ]
        ],
        "question": "Is it true that The results in Table 5 show that the three types of whitelists perform comparably to each other when the true response is added?",
        "answer_label": "yes"
    },
    {
        "id": "027fddad-0ece-4a91-a14f-dff863674aa2",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 3: Performance comparison of our model with different values of m on the two datasets.",
        "table_column_names": [
            "[ITALIC] m",
            "NYT10 Prec.",
            "NYT10 Rec.",
            "NYT10 F1",
            "NYT11 Prec.",
            "NYT11 Rec.",
            "NYT11 F1"
        ],
        "table_content_values": [
            [
                "1",
                "0.541",
                "0.595",
                "[BOLD] 0.566",
                "0.495",
                "0.621",
                "0.551"
            ],
            [
                "2",
                "0.521",
                "0.597",
                "0.556",
                "0.482",
                "0.656",
                "0.555"
            ],
            [
                "3",
                "0.490",
                "0.617",
                "0.547",
                "0.509",
                "0.633",
                "0.564"
            ],
            [
                "4",
                "0.449",
                "0.623",
                "0.522",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "5",
                "0.467",
                "0.609",
                "0.529",
                "0.488",
                "0.677",
                "0.567"
            ]
        ],
        "question": "Is it true that On the NYT11 dataset, m = 5 gives the best performance?",
        "answer_label": "no"
    },
    {
        "id": "d66c90ac-981c-4740-bc02-d771a854990f",
        "table_caption": "Building a Production Model for Retrieval-Based Chatbots Table 3: AUC and AUC@p of our model on the propriety help desk dataset.",
        "table_column_names": [
            "[BOLD] Metric",
            "[BOLD] Validation",
            "[BOLD] Test"
        ],
        "table_content_values": [
            [
                "AUC",
                "0.991",
                "0.977"
            ],
            [
                "AUC@0.1",
                "0.925",
                "0.885"
            ],
            [
                "AUC@0.05",
                "0.871",
                "0.816"
            ],
            [
                "AUC@0.01",
                "0.677",
                "0.630"
            ]
        ],
        "question": "Is it true that The high AUC indicates that our model can easily distinguish between the true response and negative responses?",
        "answer_label": "yes"
    },
    {
        "id": "151436ea-346d-43d1-bbfc-c82ce0fec3c7",
        "table_caption": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
        "table_column_names": [
            "ID LSTM-800",
            "5-fold CV 70.56",
            "\u0394 0.66",
            "Single model 67.54",
            "\u0394 0.78",
            "Ensemble 67.65",
            "\u0394 0.30"
        ],
        "table_content_values": [
            [
                "LSTM-400",
                "70.50",
                "0.60",
                "[BOLD] 67.59",
                "0.83",
                "[BOLD] 68.00",
                "0.65"
            ],
            [
                "IN-TITLE",
                "70.11",
                "0.21",
                "[EMPTY]",
                "[EMPTY]",
                "67.52",
                "0.17"
            ],
            [
                "[BOLD] SUBMISSION",
                "69.90",
                "\u2013",
                "66.76",
                "\u2013",
                "67.35",
                "\u2013"
            ],
            [
                "NO-HIGHWAY",
                "69.72",
                "\u22120.18",
                "66.42",
                "\u22120.34",
                "66.64",
                "\u22120.71"
            ],
            [
                "NO-OVERLAPS",
                "69.46",
                "\u22120.44",
                "65.07",
                "\u22121.69",
                "66.47",
                "\u22120.88"
            ],
            [
                "LSTM-400-DROPOUT",
                "69.45",
                "\u22120.45",
                "65.53",
                "\u22121.23",
                "67.28",
                "\u22120.07"
            ],
            [
                "NO-TRANSLATIONS",
                "69.42",
                "\u22120.48",
                "65.92",
                "\u22120.84",
                "67.23",
                "\u22120.12"
            ],
            [
                "NO-ELMO-FINETUNING",
                "67.71",
                "\u22122.19",
                "65.16",
                "\u22121.60",
                "65.42",
                "\u22121.93"
            ]
        ],
        "question": "Is it true that [CONTINUE] Also, our data augmentation technique (NO-TRANSLATIONS) seem to have far smaller impact on the final score then we expected?",
        "answer_label": "yes"
    },
    {
        "id": "b4c82e00-9bdb-4181-afaa-cdb6340d36f6",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Balanced",
            "Throughput (instances/s) Moderate",
            "Throughput (instances/s) Linear"
        ],
        "table_content_values": [
            [
                "1",
                "46.7",
                "27.3",
                "7.6"
            ],
            [
                "10",
                "125.2",
                "78.2",
                "22.7"
            ],
            [
                "25",
                "129.7",
                "83.1",
                "45.4"
            ]
        ],
        "question": "Is it true that For all batch sizes, the training throughput on the linear dataset is the highest, while the throughput on the balanced dataset is the lowest?",
        "answer_label": "no"
    },
    {
        "id": "3490236e-fba6-4622-8f84-7a5db25b3965",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.",
        "table_column_names": [
            "Corpus",
            "Metric",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "Europarl",
                "TotalTerms:",
                "957",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "836",
                "1,000"
            ],
            [
                "Europarl",
                "TotalRoots:",
                "44",
                "1",
                "1",
                "1",
                "1",
                "43",
                "1"
            ],
            [
                "Europarl",
                "NumberRels:",
                "1,588",
                "1,025",
                "1,028",
                "1,185",
                "1,103",
                "1,184",
                "999"
            ],
            [
                "Europarl",
                "MaxDepth:",
                "21",
                "921",
                "901",
                "788",
                "835",
                "8",
                "15"
            ],
            [
                "Europarl",
                "MinDepth:",
                "1",
                "921",
                "901",
                "788",
                "835",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgDepth:",
                "11.82",
                "921",
                "901",
                "788",
                "835",
                "3.05",
                "8.46"
            ],
            [
                "Europarl",
                "DepthCohesion:",
                "1.78",
                "1",
                "1",
                "1",
                "1",
                "2.62",
                "1.77"
            ],
            [
                "Europarl",
                "MaxWidth:",
                "20",
                "2",
                "3",
                "4",
                "3",
                "88",
                "41"
            ],
            [
                "Europarl",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgWidth:",
                "1.99",
                "1.03",
                "1.03",
                "1.19",
                "1.10",
                "4.20",
                "2.38"
            ],
            [
                "TED Talks",
                "TotalTerms:",
                "476",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000"
            ],
            [
                "TED Talks",
                "TotalRoots:",
                "164",
                "2",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "NumberRels:",
                "521",
                "1,029",
                "1,331",
                "3,025",
                "3,438",
                "3,802",
                "1,009"
            ],
            [
                "TED Talks",
                "MaxDepth:",
                "16",
                "915",
                "658",
                "454",
                "395",
                "118",
                "12"
            ],
            [
                "TED Talks",
                "MinDepth:",
                "1",
                "913",
                "658",
                "454",
                "395",
                "110",
                "1"
            ],
            [
                "TED Talks",
                "AvgDepth:",
                "5.82",
                "914",
                "658",
                "454",
                "395",
                "112.24",
                "5.95"
            ],
            [
                "TED Talks",
                "DepthCohesion:",
                "2.75",
                "1",
                "1",
                "1",
                "1",
                "1.05",
                "2.02"
            ],
            [
                "TED Talks",
                "MaxWidth:",
                "25",
                "2",
                "77",
                "13",
                "12",
                "66",
                "98"
            ],
            [
                "TED Talks",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "AvgWidth:",
                "1.83",
                "1.03",
                "1.36",
                "3.03",
                "3.44",
                "6.64",
                "2.35"
            ]
        ],
        "question": "Is it true that The Patt model was able to generate relations for all terms in the Europarl and TED Talks corpora, as evidenced by the metrics in Table 6?",
        "answer_label": "no"
    },
    {
        "id": "53148ebf-8660-4f07-a9c8-b0f61608cced",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
        "table_column_names": [
            "Model",
            "Method",
            "Training Data",
            "Overall",
            "Easy",
            "Hard",
            "p-value (%)"
        ],
        "table_content_values": [
            [
                "goodwin-etal-2012-utdhlt",
                "PMI",
                "unsupervised",
                "61.8",
                "64.7",
                "60.0",
                "19.8"
            ],
            [
                "gordon_commonsense_2011-1",
                "PMI",
                "unsupervised",
                "65.4",
                "65.8",
                "65.2",
                "83.5"
            ],
            [
                "sasaki-etal-2017-handling",
                "PMI",
                "unsupervised",
                "71.4",
                "75.3",
                "69.0",
                "4.8\u2217"
            ],
            [
                "Word frequency",
                "wordfreq",
                "COPA",
                "53.5",
                "57.4",
                "51.3",
                "9.8"
            ],
            [
                "BERT-large-FT",
                "LM, NSP",
                "COPA",
                "76.5 (\u00b1 2.7)",
                "83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)",
                "0.0\u2217"
            ],
            [
                "RoBERTa-large-FT",
                "LM",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)",
                "0.0\u2217"
            ]
        ],
        "question": "Is it true that We then compare BERT and RoBERTa with previous models on the Easy and Hard subsets.7 As Table 4 shows, previous models perform similarly on both subsets, with the exception of Sasaki et al?",
        "answer_label": "yes"
    },
    {
        "id": "53124f16-be1d-471c-b61f-74a9c8bb30cf",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 5: Scores for different training objectives on the supervised downstream tasks.",
        "table_column_names": [
            "Method",
            "SUBJ",
            "CR",
            "MR",
            "MPQA",
            "MRPC",
            "TREC",
            "SICK-E",
            "SST2",
            "SST5",
            "STS-B",
            "SICK-R"
        ],
        "table_content_values": [
            [
                "CMOW-C",
                "85.9",
                "72.1",
                "69.4",
                "87.0",
                "[BOLD] 71.9",
                "85.4",
                "74.2",
                "73.8",
                "37.6",
                "54.6",
                "71.3"
            ],
            [
                "CMOW-R",
                "[BOLD] 87.5",
                "[BOLD] 73.4",
                "[BOLD] 70.6",
                "[BOLD] 87.3",
                "69.6",
                "[BOLD] 88.0",
                "[BOLD] 77.2",
                "[BOLD] 74.7",
                "[BOLD] 37.9",
                "[BOLD] 56.5",
                "[BOLD] 76.2"
            ],
            [
                "CBOW-C",
                "[BOLD] 90.0",
                "[BOLD] 79.3",
                "[BOLD] 74.6",
                "[BOLD] 87.5",
                "[BOLD] 72.9",
                "85.0",
                "[BOLD] 80.0",
                "78.4",
                "41.0",
                "60.5",
                "[BOLD] 79.2"
            ],
            [
                "CBOW-R",
                "[BOLD] 90.0",
                "79.2",
                "74.0",
                "87.1",
                "71.6",
                "[BOLD] 85.6",
                "78.9",
                "[BOLD] 78.5",
                "[BOLD] 42.1",
                "[BOLD] 61.0",
                "78.1"
            ]
        ],
        "question": "Is it true that Consequently, CMOW-R does not outperform CMOW-C on 10 out of 11 supervised downstream tasks. On average over all downstream tasks, the relative improvement is not 20.8%?",
        "answer_label": "no"
    },
    {
        "id": "2d32e95f-0000-4e62-933b-42e10261b687",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 4: Lexicon member coverage (%)",
        "table_column_names": [
            "target",
            "VN",
            "WN-V",
            "WN-N"
        ],
        "table_content_values": [
            [
                "type",
                "81",
                "66",
                "47"
            ],
            [
                "x+POS",
                "54",
                "39",
                "43"
            ],
            [
                "lemma",
                "88",
                "76",
                "53"
            ],
            [
                "x+POS",
                "79",
                "63",
                "50"
            ],
            [
                "shared",
                "54",
                "39",
                "41"
            ]
        ],
        "question": "Is it true that WN-N shows low coverage containing many low-frequency members?",
        "answer_label": "yes"
    },
    {
        "id": "1c0742fa-63b7-44ed-b226-bfa550dabe1c",
        "table_caption": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set",
        "table_column_names": [
            "Method",
            "WER (%)",
            "DCE"
        ],
        "table_content_values": [
            [
                "No enhancement",
                "38.4",
                "0.958"
            ],
            [
                "Wiener filter",
                "41.0",
                "0.775"
            ],
            [
                "Minimizing DCE",
                "31.1",
                "[BOLD] 0.392"
            ],
            [
                "FSEGAN",
                "29.1",
                "0.421"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
                "27.7",
                "0.476"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
                "[BOLD] 26.1",
                "0.462"
            ],
            [
                "Clean speech",
                "9.3",
                "0.0"
            ]
        ],
        "question": "Is it true that acoustic supervision (27.7%) and multi-task learning (26.1%) show lower WER than minimizing DCE (31.1%) and FSEGAN (29.1%))?",
        "answer_label": "yes"
    },
    {
        "id": "5655d55f-686a-4173-ae58-87964c81a390",
        "table_caption": "Neural End-to-End Learning for Computational Argumentation Mining Table 4: C-F1 (100%) in % for the two indicated systems; essay vs.\u00a0paragraph level. Note that the mean performances are lower than the majority performances over the runs given in Table 2.",
        "table_column_names": [
            "[EMPTY]",
            "STagBLCC",
            "LSTM-Parser"
        ],
        "table_content_values": [
            [
                "Essay",
                "60.62\u00b13.54",
                "9.40\u00b113.57"
            ],
            [
                "Paragraph",
                "64.74\u00b11.97",
                "56.24\u00b12.87"
            ]
        ],
        "question": "Is it true that The difference is particularly striking on the essay level where the parsers often completely fail to learn, that is, their performance scores are close to 0%?",
        "answer_label": "yes"
    },
    {
        "id": "201a8927-705e-4154-837e-2527d42e84a1",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE VII: Precision scores for the Analogy Test",
        "table_column_names": [
            "Methods",
            "# dims",
            "Analg. (sem)",
            "Analg. (syn)",
            "Total"
        ],
        "table_content_values": [
            [
                "GloVe",
                "300",
                "78.94",
                "64.12",
                "70.99"
            ],
            [
                "Word2Vec",
                "300",
                "81.03",
                "66.11",
                "73.03"
            ],
            [
                "OIWE-IPG",
                "300",
                "19.99",
                "23.44",
                "21.84"
            ],
            [
                "SOV",
                "3000",
                "64.09",
                "46.26",
                "54.53"
            ],
            [
                "SPINE",
                "1000",
                "17.07",
                "8.68",
                "12.57"
            ],
            [
                "Word2Sense",
                "2250",
                "12.94",
                "19.44",
                "5.84"
            ],
            [
                "Proposed",
                "300",
                "79.96",
                "63.52",
                "71.15"
            ]
        ],
        "question": "Is it true that However, our proposed method does not outperform the original GloVe embeddings?",
        "answer_label": "no"
    },
    {
        "id": "7fa6751d-e5c9-44c7-9775-48f316b11f2b",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that [CONTINUE] Regarding the probing tasks, we observe that CMOW embeddings better encode the linguistic prop [CONTINUE] erties of sentences than CBOW?",
        "answer_label": "yes"
    },
    {
        "id": "c92525f9-3875-4445-bc8e-5e3cac3e8e9e",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1038",
                "0.0170",
                "0.0490",
                "0.0641",
                "0.0641",
                "0.0613",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1282",
                "0.0291",
                "0.0410",
                "0.0270",
                "0.0270",
                "0.1154",
                "0.0661"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.6185",
                "0.3744",
                "0.4144",
                "0.4394",
                "0.4394",
                "[BOLD] 0.7553",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.6308",
                "0.4124",
                "0.4404",
                "0.4515",
                "0.4945",
                "[BOLD] 0.8609",
                "0.5295"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "[BOLD] 0.0021",
                "0.0004",
                "0.0011",
                "0.0014",
                "0.0014",
                "0.0013",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0011",
                "0.0008",
                "0.0011",
                "0.0008",
                "0.0008",
                "[BOLD] 0.0030",
                "0.0018"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0012",
                "0.0008",
                "0.0009",
                "0.0010",
                "0.0010",
                "[BOLD] 0.0016",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0003",
                "0.0009",
                "0.0009",
                "0.0010",
                "0.0010",
                "[BOLD] 0.0017",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "[BOLD] 0.0041",
                "0.0007",
                "0.0021",
                "0.0027",
                "0.0027",
                "0.0026",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0022",
                "0.0016",
                "0.0022",
                "0.0015",
                "0.0015",
                "[BOLD] 0.0058",
                "0.0036"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0024",
                "0.0016",
                "0.0018",
                "0.0019",
                "0.0019",
                "[BOLD] 0.0031",
                "0.0023"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0005",
                "0.0018",
                "0.0018",
                "0.0020",
                "0.0021",
                "[BOLD] 0.0034",
                "0.0022"
            ]
        ],
        "question": "Is it true that On the other hand, choosing the best hypernym did not work very well for DocSub which obtained the lowest precision for the Portuguese corpora?",
        "answer_label": "no"
    },
    {
        "id": "d5553a4a-b710-4865-adb7-3ae9adb2f279",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 4: Experiment 2, t= \u201cb*tch\u201d",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.010",
                "0.010",
                "-0.632",
                "[EMPTY]",
                "0.978"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.963",
                "0.944",
                "20.064",
                "***",
                "1.020"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.011",
                "0.011",
                "-1.254",
                "[EMPTY]",
                "0.955"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.349",
                "0.290",
                "28.803",
                "***",
                "1.203"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.012",
                "0.012",
                "-0.162",
                "[EMPTY]",
                "0.995"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.017",
                "0.015",
                "4.698",
                "***",
                "1.152"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.988",
                "0.991",
                "-6.289",
                "***",
                "0.997"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.099",
                "0.091",
                "6.273",
                "***",
                "1.091"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.074",
                "0.027",
                "46.054",
                "***",
                "2.728"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.925",
                "0.968",
                "-41.396",
                "***",
                "0.956"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.010",
                "0.010",
                "0.000",
                "[EMPTY]",
                "1.000"
            ]
        ],
        "question": "Is it true that In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism?",
        "answer_label": "yes"
    },
    {
        "id": "54b24d05-5f41-461c-a0d5-53d4d9bb2b16",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that As we can observe, it seems that clustering semantically related terms does not necessarily increase the precision (at least for the top 1,000 terms in the English corpora used in this experiment) as expected?",
        "answer_label": "no"
    },
    {
        "id": "368b72b0-fb17-4ab7-b573-ba6f59ddc2a4",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
        "table_column_names": [
            "<bold>Model</bold>",
            "REF \u21d2 GEN <bold>ENT</bold>",
            "REF \u21d2 GEN <bold>CON</bold>",
            "REF \u21d2 GEN <bold>NEU</bold>"
        ],
        "table_content_values": [
            [
                "S2S",
                "38.45",
                "11.17",
                "50.38"
            ],
            [
                "G2S-GIN",
                "49.78",
                "9.80",
                "40.42"
            ],
            [
                "G2S-GAT",
                "49.48",
                "8.09",
                "42.43"
            ],
            [
                "G2S-GGNN",
                "51.32",
                "8.82",
                "39.86"
            ],
            [
                "[EMPTY]",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF"
            ],
            [
                "<bold>Model</bold>",
                "<bold>ENT</bold>",
                "<bold>CON</bold>",
                "<bold>NEU</bold>"
            ],
            [
                "S2S",
                "73.79",
                "12.75",
                "13.46"
            ],
            [
                "G2S-GIN",
                "76.27",
                "10.65",
                "13.08"
            ],
            [
                "G2S-GAT",
                "77.54",
                "8.54",
                "13.92"
            ],
            [
                "G2S-GGNN",
                "77.64",
                "9.64",
                "12.72"
            ]
        ],
        "question": "Is it true that G2S models also generate sentences that contradict the reference sentences less?",
        "answer_label": "yes"
    },
    {
        "id": "3b8e8d3d-432a-4be3-9875-accd92112337",
        "table_caption": "Semantic Neural Machine Translation using AMR Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.",
        "table_column_names": [
            "System",
            "NC-v11 BLEU",
            "NC-v11 TER\u2193",
            "NC-v11 Meteor",
            "Full BLEU",
            "Full TER\u2193",
            "Full Meteor"
        ],
        "table_content_values": [
            [
                "OpenNMT-tf",
                "15.1",
                "0.6902",
                "0.3040",
                "24.3",
                "0.5567",
                "0.4225"
            ],
            [
                "Transformer-tf",
                "17.1",
                "0.6647",
                "0.3578",
                "25.1",
                "0.5537",
                "0.4344"
            ],
            [
                "Seq2seq",
                "16.0",
                "0.6695",
                "0.3379",
                "23.7",
                "0.5590",
                "0.4258"
            ],
            [
                "Dual2seq-LinAMR",
                "17.3",
                "0.6530",
                "0.3612",
                "24.0",
                "0.5643",
                "0.4246"
            ],
            [
                "Duel2seq-SRL",
                "17.2",
                "0.6591",
                "0.3644",
                "23.8",
                "0.5626",
                "0.4223"
            ],
            [
                "Dual2seq-Dep",
                "17.8",
                "0.6516",
                "0.3673",
                "25.0",
                "0.5538",
                "0.4328"
            ],
            [
                "Dual2seq",
                "[BOLD] *19.2*",
                "[BOLD] 0.6305",
                "[BOLD] 0.3840",
                "[BOLD] *25.5*",
                "[BOLD] 0.5480",
                "[BOLD] 0.4376"
            ]
        ],
        "question": "Is it true that Dual2seq is consistently better than the other systems under all three metrics, [CONTINUE] Dual2seq is better than both OpenNMT-tf and Transformer-tf ?",
        "answer_label": "yes"
    },
    {
        "id": "5854ce90-8b2b-4494-8ab0-d273e8e50cf7",
        "table_caption": "Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
        "table_column_names": [
            "[BOLD] ResNet-34",
            "[BOLD] Eval set %",
            "[BOLD] #param"
        ],
        "table_content_values": [
            [
                "Baseline (No SA)Anderson et al. ( 2018 )",
                "55.00",
                "0M"
            ],
            [
                "SA (S: 1,2,3 - B: 1)",
                "55.11",
                "} 0.107M"
            ],
            [
                "SA (S: 1,2,3 - B: 2)",
                "55.17",
                "} 0.107M"
            ],
            [
                "[BOLD] SA (S: 1,2,3 - B: 3)",
                "[BOLD] 55.27",
                "} 0.107M"
            ]
        ],
        "question": "Is it true that We empirically found that self-attention was not the most efficient in the 3rd stage?",
        "answer_label": "no"
    },
    {
        "id": "fa3f471e-4365-44ac-bbe9-91cb1d44dee1",
        "table_caption": "What do Deep Networks Like to Read? Table 1: Example sentences of the different classifiers compared to the original on SST-2. We report further examples in the Appendix. <u> use for <UNK>.",
        "table_column_names": [
            "Orig",
            "<u> turns in a <u> screenplay that <u> at the edges ; it \u2019s so clever you want to hate it ."
        ],
        "table_content_values": [
            [
                "DAN",
                "<u> turns in a <u> screenplay screenplay screenplay of <u> edges edges edges shapes so clever easy want hate hate hate hate hate hate hate hate hate hate"
            ],
            [
                "CNN",
                "she turns on a on ( ( in in the the the edges \u2019s so clever \u201c want to hate it \u201d"
            ],
            [
                "RNN",
                "<u> turns in a <u> screenplay was <u> <u> <u> edges edges edges curves <u> clever clever you want hate hate it ."
            ]
        ],
        "question": "Is it true that In contrast, DAN does not always mask out punctuation and determiners using words indicative of the class label, as evidenced by the example sentence in the table?",
        "answer_label": "no"
    },
    {
        "id": "759631a8-3754-40cb-8f1e-d00481c7037b",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
        "table_column_names": [
            "[BOLD] Model",
            "R",
            "MUC P",
            "[ITALIC] F1",
            "R",
            "B3 P",
            "[ITALIC] F1",
            "R",
            "CEAF- [ITALIC] e P",
            "[ITALIC] F1",
            "CoNLL  [ITALIC] F1"
        ],
        "table_content_values": [
            [
                "Cluster+Lemma",
                "71.3",
                "83",
                "76.7",
                "53.4",
                "84.9",
                "65.6",
                "70.1",
                "52.5",
                "60",
                "67.4"
            ],
            [
                "Disjoint",
                "76.7",
                "80.8",
                "78.7",
                "63.2",
                "78.2",
                "69.9",
                "65.3",
                "58.3",
                "61.6",
                "70"
            ],
            [
                "Joint",
                "78.6",
                "80.9",
                "79.7",
                "65.5",
                "76.4",
                "70.5",
                "65.4",
                "61.3",
                "63.3",
                "[BOLD] 71.2"
            ]
        ],
        "question": "Is it true that Our joint model does not improve upon the strong lemma baseline by 3.8 points in CoNLL F1 score?",
        "answer_label": "no"
    },
    {
        "id": "9b3caf85-932d-41fa-83f8-e5b72f7ffc93",
        "table_caption": "Semantic Neural Machine Translation using AMR Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.",
        "table_column_names": [
            "System",
            "NC-v11 BLEU",
            "NC-v11 TER\u2193",
            "NC-v11 Meteor",
            "Full BLEU",
            "Full TER\u2193",
            "Full Meteor"
        ],
        "table_content_values": [
            [
                "OpenNMT-tf",
                "15.1",
                "0.6902",
                "0.3040",
                "24.3",
                "0.5567",
                "0.4225"
            ],
            [
                "Transformer-tf",
                "17.1",
                "0.6647",
                "0.3578",
                "25.1",
                "0.5537",
                "0.4344"
            ],
            [
                "Seq2seq",
                "16.0",
                "0.6695",
                "0.3379",
                "23.7",
                "0.5590",
                "0.4258"
            ],
            [
                "Dual2seq-LinAMR",
                "17.3",
                "0.6530",
                "0.3612",
                "24.0",
                "0.5643",
                "0.4246"
            ],
            [
                "Duel2seq-SRL",
                "17.2",
                "0.6591",
                "0.3644",
                "23.8",
                "0.5626",
                "0.4223"
            ],
            [
                "Dual2seq-Dep",
                "17.8",
                "0.6516",
                "0.3673",
                "25.0",
                "0.5538",
                "0.4328"
            ],
            [
                "Dual2seq",
                "[BOLD] *19.2*",
                "[BOLD] 0.6305",
                "[BOLD] 0.3840",
                "[BOLD] *25.5*",
                "[BOLD] 0.5480",
                "[BOLD] 0.4376"
            ]
        ],
        "question": "Is it true that [CONTINUE] When trained on the NC-v11 subset, the gap between Seq2seq and Dual2seq under Meteor (around 5 points) is greater than that under BLEU (around 3 points)?",
        "answer_label": "yes"
    },
    {
        "id": "41e926f4-dc54-48d5-a985-eb56c45a2131",
        "table_caption": "Improving Generalization by Incorporating Coverage in Natural Language Inference Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
        "table_column_names": [
            "[EMPTY]",
            "in-domain MultiNLI",
            "out-of-domain SNLI",
            "out-of-domain Glockner",
            "out-of-domain SICK"
        ],
        "table_content_values": [
            [
                "MQAN",
                "72.30",
                "60.91",
                "41.82",
                "53.95"
            ],
            [
                "+ coverage",
                "<bold>73.84</bold>",
                "<bold>65.38</bold>",
                "<bold>78.69</bold>",
                "<bold>54.55</bold>"
            ],
            [
                "ESIM (ELMO)",
                "80.04",
                "68.70",
                "60.21",
                "51.37"
            ],
            [
                "+ coverage",
                "<bold>80.38</bold>",
                "<bold>70.05</bold>",
                "<bold>67.47</bold>",
                "<bold>52.65</bold>"
            ]
        ],
        "question": "Is it true that The resulting cross-dataset improvements on the SNLI and Glockner datasets are not larger than those on the SICK dataset?",
        "answer_label": "no"
    },
    {
        "id": "0a65243e-547d-4f38-abe3-ed90345ed44d",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 4: Test BLEU for en-de adaptive training, with sequential adaptation to a third task. EWC-tuned models give the best performance on each domain.",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Training scheme",
            "[BOLD] News",
            "[BOLD] TED",
            "[BOLD] IT"
        ],
        "table_content_values": [
            [
                "1",
                "News",
                "37.8",
                "25.3",
                "35.3"
            ],
            [
                "2",
                "TED",
                "23.7",
                "24.1",
                "14.4"
            ],
            [
                "3",
                "IT",
                "1.6",
                "1.8",
                "39.6"
            ],
            [
                "4",
                "News and TED",
                "38.2",
                "25.5",
                "35.4"
            ],
            [
                "5",
                "1 then TED, No-reg",
                "30.6",
                "[BOLD] 27.0",
                "22.1"
            ],
            [
                "6",
                "1 then TED, L2",
                "37.9",
                "26.7",
                "31.8"
            ],
            [
                "7",
                "1 then TED, EWC",
                "[BOLD] 38.3",
                "[BOLD] 27.0",
                "33.1"
            ],
            [
                "8",
                "5 then IT, No-reg",
                "8.0",
                "6.9",
                "56.3"
            ],
            [
                "9",
                "6 then IT, L2",
                "32.3",
                "22.6",
                "56.9"
            ],
            [
                "10",
                "7 then IT, EWC",
                "35.8",
                "24.6",
                "[BOLD] 57.0"
            ]
        ],
        "question": "Is it true that In the en-de News/TED task (Table 4), all fine-tuning schemes give similar improvements on TED?",
        "answer_label": "yes"
    },
    {
        "id": "ec93a8d7-8518-41cb-be5e-f605bd53b660",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that G2S-GAT has a better performance in handling graphs with node out-degrees higher than 9?",
        "answer_label": "no"
    },
    {
        "id": "535d1def-2141-41d5-8a69-da4175cacf77",
        "table_caption": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE II: WERs (%) and DCE of different speech enhancement methods on CHiME4-simulated test set",
        "table_column_names": [
            "Method",
            "WER (%)",
            "DCE"
        ],
        "table_content_values": [
            [
                "No enhancement",
                "38.4",
                "0.958"
            ],
            [
                "Wiener filter",
                "41.0",
                "0.775"
            ],
            [
                "Minimizing DCE",
                "31.1",
                "[BOLD] 0.392"
            ],
            [
                "FSEGAN",
                "29.1",
                "0.421"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
                "27.7",
                "0.476"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
                "[BOLD] 26.1",
                "0.462"
            ],
            [
                "Clean speech",
                "9.3",
                "0.0"
            ]
        ],
        "question": "Is it true that The Wiener filtering method shows lower DCE, but higher WER than no enhancement?",
        "answer_label": "yes"
    },
    {
        "id": "c4fe7068-9584-4aac-900d-e743f0919833",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 5: We compare the quality of variants of Opinion Distance measures on opinion clustering task with ARI.",
        "table_column_names": [
            "[EMPTY]",
            "Difference Function",
            "Seanad Abolition",
            "Video Games",
            "Pornography"
        ],
        "table_content_values": [
            [
                "OD-parse",
                "Absolute",
                "0.01",
                "-0.01",
                "0.07"
            ],
            [
                "OD-parse",
                "JS div.",
                "0.01",
                "-0.01",
                "-0.01"
            ],
            [
                "OD-parse",
                "EMD",
                "0.07",
                "0.01",
                "-0.01"
            ],
            [
                "OD",
                "Absolute",
                "[BOLD] 0.54",
                "[BOLD] 0.56",
                "[BOLD] 0.41"
            ],
            [
                "OD",
                "JS div.",
                "0.07",
                "-0.01",
                "-0.02"
            ],
            [
                "OD",
                "EMD",
                "0.26",
                "-0.01",
                "0.01"
            ],
            [
                "OD (no polarity shifters)",
                "Absolute",
                "0.23",
                "0.08",
                "0.04"
            ],
            [
                "OD (no polarity shifters)",
                "JS div.",
                "0.09",
                "-0.01",
                "-0.02"
            ],
            [
                "OD (no polarity shifters)",
                "EMD",
                "0.10",
                "0.01",
                "-0.01"
            ]
        ],
        "question": "Is it true that This is evident from the insignificant drop in ARI score from OD to OD (no polarity shifters) since the only change in those variants is of sentiment polarity shifters?",
        "answer_label": "no"
    },
    {
        "id": "c2032a31-8e78-411f-aa54-87bf791b98b3",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Acc",
            "[BOLD] F1",
            "[BOLD] AUC"
        ],
        "table_content_values": [
            [
                "Most Frequent Class",
                "64.2",
                "39.1",
                "0.500"
            ],
            [
                "LR-All Features \u2013 Original Data",
                "80.5",
                "78.0",
                "0.873"
            ],
            [
                "Dist. Supervision + Pooling",
                "77.2",
                "75.7",
                "0.853"
            ],
            [
                "Dist. Supervision + EasyAdapt",
                "[BOLD] 81.2",
                "[BOLD] 79.0",
                "[BOLD] 0.885"
            ]
        ],
        "question": "Is it true that Results presented in Table 7 show that the domain adaptation approach further boosts F1 by 1 point to 79 (t-test, p<0.5) and ROC AUC by 0.012?",
        "answer_label": "yes"
    },
    {
        "id": "291119c8-4554-4704-a71f-715bfb6ea711",
        "table_caption": "Aligning Vector-spaces with Noisy Supervised Lexicons Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
        "table_column_names": [
            "Method",
            "En\u2192It best",
            "En\u2192It avg",
            "En\u2192It iters",
            "En\u2192De best",
            "En\u2192De avg",
            "En\u2192De iters",
            "En\u2192Fi best",
            "En\u2192Fi avg",
            "En\u2192Fi iters",
            "En\u2192Es best",
            "En\u2192Es avg",
            "En\u2192Es iters"
        ],
        "table_content_values": [
            [
                "Artetxe et\u00a0al., 2018b",
                "[BOLD] 48.53",
                "48.13",
                "573",
                "48.47",
                "48.19",
                "773",
                "33.50",
                "32.63",
                "988",
                "37.60",
                "37.33",
                "808"
            ],
            [
                "Noise-aware Alignment",
                "[BOLD] 48.53",
                "[BOLD] 48.20",
                "471",
                "[BOLD] 49.67",
                "[BOLD] 48.89",
                "568",
                "[BOLD] 33.98",
                "[BOLD] 33.68",
                "502",
                "[BOLD] 38.40",
                "[BOLD] 37.79",
                "551"
            ]
        ],
        "question": "Is it true that In addition, the noise-aware model is more stable and therefore requires fewer iterations to converge?",
        "answer_label": "yes"
    },
    {
        "id": "91e9f947-9e27-4fe2-9913-b45c570f1d05",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
        "table_column_names": [
            "# of Heads",
            "Accuracy",
            "Val. Loss",
            "Effect"
        ],
        "table_content_values": [
            [
                "1",
                "89.44%",
                "0.2811",
                "-6.84%"
            ],
            [
                "2",
                "91.20%",
                "0.2692",
                "-5.08%"
            ],
            [
                "4",
                "93.85%",
                "0.2481",
                "-2.43%"
            ],
            [
                "8",
                "96.02%",
                "0.2257",
                "-0.26%"
            ],
            [
                "10",
                "96.28%",
                "0.2197",
                "[EMPTY]"
            ],
            [
                "16",
                "96.32%",
                "0.2190",
                "+0.04"
            ]
        ],
        "question": "Is it true that This shows that more attention heads, thereby attending to multiple different contexts at once, does not necessarily lead to state-of-the-art results?",
        "answer_label": "no"
    },
    {
        "id": "24a53156-9a34-46d3-8bd8-acd2a28bdb82",
        "table_caption": "Deriving Machine Attention from Human Rationales Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.",
        "table_column_names": [
            "Source",
            "Target",
            "Svm",
            "Ra-Svm\u2021",
            "Ra-Cnn\u2021",
            "Trans\u2020",
            "Ra-Trans\u2021\u2020",
            "Ours\u2021\u2020",
            "Oracle\u2020"
        ],
        "table_content_values": [
            [
                "Beer aroma+palate",
                "Beer look",
                "74.41",
                "74.83",
                "74.94",
                "72.75",
                "76.41",
                "[BOLD] 79.53",
                "80.29"
            ],
            [
                "Beer look+palate",
                "Beer aroma",
                "68.57",
                "69.23",
                "67.55",
                "69.92",
                "76.45",
                "[BOLD] 77.94",
                "78.11"
            ],
            [
                "Beer look+aroma",
                "Beer palate",
                "63.88",
                "67.82",
                "65.72",
                "74.66",
                "73.40",
                "[BOLD] 75.24",
                "75.50"
            ]
        ],
        "question": "Is it true that Our model (OURS) does not obtain substantial gains in accuracy over the baselines across all three target aspects?",
        "answer_label": "no"
    },
    {
        "id": "2a8dabe5-b9db-45df-a2d3-8f7ad54d75d2",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 5: Performance of different agents on the neural user simulator.",
        "table_column_names": [
            "Method",
            "VHUS Turns",
            "VHUS Inform",
            "VHUS Match",
            "VHUS Success"
        ],
        "table_content_values": [
            [
                "ACER",
                "22.35",
                "55.13",
                "33.08",
                "18.6"
            ],
            [
                "PPO",
                "[BOLD] 19.23",
                "[BOLD] 56.31",
                "33.08",
                "18.3"
            ],
            [
                "ALDM",
                "26.90",
                "54.37",
                "24.15",
                "16.4"
            ],
            [
                "GDPL",
                "22.43",
                "52.58",
                "[BOLD] 36.21",
                "[BOLD] 19.7"
            ]
        ],
        "question": "Is it true that In comparison, GDPL is still comparable with ACER and PPO, obtains a better match rate, and even achieves higher task success?",
        "answer_label": "yes"
    },
    {
        "id": "893265ca-c355-4c56-914b-a7e0fc559077",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 4: Model performance on the COPA test set (Overall), on Easy instances with superficial cues, and on Hard instances without superficial cues. p-values according to Approximate Randomization Tests Noreen (1989), with \u2217 indicating a significant difference between performance on Easy and Hard p<5%. Methods are pointwise mutual information (PMI), word frequency provided by the wordfreq package Speer et al. (2018), pretrained language model (LM), and next-sentence prediction (NSP).",
        "table_column_names": [
            "Model",
            "Method",
            "Training Data",
            "Overall",
            "Easy",
            "Hard",
            "p-value (%)"
        ],
        "table_content_values": [
            [
                "goodwin-etal-2012-utdhlt",
                "PMI",
                "unsupervised",
                "61.8",
                "64.7",
                "60.0",
                "19.8"
            ],
            [
                "gordon_commonsense_2011-1",
                "PMI",
                "unsupervised",
                "65.4",
                "65.8",
                "65.2",
                "83.5"
            ],
            [
                "sasaki-etal-2017-handling",
                "PMI",
                "unsupervised",
                "71.4",
                "75.3",
                "69.0",
                "4.8\u2217"
            ],
            [
                "Word frequency",
                "wordfreq",
                "COPA",
                "53.5",
                "57.4",
                "51.3",
                "9.8"
            ],
            [
                "BERT-large-FT",
                "LM, NSP",
                "COPA",
                "76.5 (\u00b1 2.7)",
                "83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)",
                "0.0\u2217"
            ],
            [
                "RoBERTa-large-FT",
                "LM",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)",
                "0.0\u2217"
            ]
        ],
        "question": "Is it true that However, BERT's improvements over previous work can be almost entirely attributed to high accuracy on the Easy subset: on this subset, finetuned BERT-large improves 8.6 percent over the model by (Sasaki et al., 2017) (83.9% vs. 75.3%), but on the Hard subset, the improvement is only 2.9 percent (71.9% vs. 69.0%)?",
        "answer_label": "yes"
    },
    {
        "id": "81206d38-8fec-4a92-95c9-b53a0785ab95",
        "table_caption": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 2: Precisions on the Wikidata dataset.",
        "table_column_names": [
            "Recall",
            "0.1",
            "0.2",
            "0.3",
            "AUC"
        ],
        "table_content_values": [
            [
                "Rank+ExATT",
                "0.584",
                "0.535",
                "0.487",
                "0.392"
            ],
            [
                "PCNN+ATT (m)",
                "0.365",
                "0.317",
                "0.213",
                "0.204"
            ],
            [
                "PCNN+ATT (1)",
                "0.665",
                "0.517",
                "0.413",
                "0.396"
            ],
            [
                "Our Model",
                "0.650",
                "0.519",
                "0.422",
                "[BOLD] 0.405"
            ]
        ],
        "question": "Is it true that We observe that our model exhibits the best performances?",
        "answer_label": "yes"
    },
    {
        "id": "202d9083-e874-49ff-8926-97f4f3f5bc91",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
        "table_column_names": [
            "<bold>Model</bold>",
            "REF \u21d2 GEN <bold>ENT</bold>",
            "REF \u21d2 GEN <bold>CON</bold>",
            "REF \u21d2 GEN <bold>NEU</bold>"
        ],
        "table_content_values": [
            [
                "S2S",
                "38.45",
                "11.17",
                "50.38"
            ],
            [
                "G2S-GIN",
                "49.78",
                "9.80",
                "40.42"
            ],
            [
                "G2S-GAT",
                "49.48",
                "8.09",
                "42.43"
            ],
            [
                "G2S-GGNN",
                "51.32",
                "8.82",
                "39.86"
            ],
            [
                "[EMPTY]",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF"
            ],
            [
                "<bold>Model</bold>",
                "<bold>ENT</bold>",
                "<bold>CON</bold>",
                "<bold>NEU</bold>"
            ],
            [
                "S2S",
                "73.79",
                "12.75",
                "13.46"
            ],
            [
                "G2S-GIN",
                "76.27",
                "10.65",
                "13.08"
            ],
            [
                "G2S-GAT",
                "77.54",
                "8.54",
                "13.92"
            ],
            [
                "G2S-GGNN",
                "77.64",
                "9.64",
                "12.72"
            ]
        ],
        "question": "Is it true that G2S models generate sentences that contradict the reference sentences more?",
        "answer_label": "no"
    },
    {
        "id": "8d5740c5-abf5-47c2-beb2-4150cb29d77f",
        "table_caption": "The MeMAD Submission to the WMT18 Multimodal Translation Task Table 3: Using automatically translated image captions and domain labels (BLEU% scores). A is short for Amun, T for Transformer.",
        "table_column_names": [
            "[EMPTY]",
            "en-fr",
            "flickr16",
            "flickr17",
            "mscoco17"
        ],
        "table_content_values": [
            [
                "A",
                "subs1M [ITALIC]  [ITALIC] H+MS-COCO",
                "66.3",
                "60.5",
                "52.1"
            ],
            [
                "A",
                "+domain-tuned",
                "66.8",
                "60.6",
                "52.0"
            ],
            [
                "A",
                "+labels",
                "[BOLD] 67.2",
                "60.4",
                "51.7"
            ],
            [
                "T",
                "subs1M [ITALIC]  [ITALIC] LM+MS-COCO",
                "66.9",
                "60.3",
                "[BOLD] 52.8"
            ],
            [
                "T",
                "+labels",
                "[BOLD] 67.2",
                "[BOLD] 60.9",
                "52.7"
            ],
            [
                "[EMPTY]",
                "en-de",
                "flickr16",
                "flickr17",
                "mscoco17"
            ],
            [
                "A",
                "subs1M [ITALIC]  [ITALIC] H+MS-COCO",
                "43.1",
                "39.0",
                "35.1"
            ],
            [
                "A",
                "+domain-tuned",
                "43.9",
                "39.4",
                "35.8"
            ],
            [
                "A",
                "+labels",
                "43.2",
                "39.3",
                "34.3"
            ],
            [
                "T",
                "subs1M [ITALIC]  [ITALIC] LM+MS-COCO",
                "[BOLD] 44.4",
                "39.4",
                "35.0"
            ],
            [
                "T",
                "+labels",
                "44.1",
                "[BOLD] 39.8",
                "[BOLD] 36.5"
            ]
        ],
        "question": "Is it true that [CONTINUE] For Marian amun, the effect is negligible as we can see in Table 3?",
        "answer_label": "yes"
    },
    {
        "id": "cad7b2c8-2a39-4cb7-84fc-68280ec753d8",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that For example, on AMR17, the ensemble model of Seq2SeqB is 1 BLEU point higher than the single DCGCN model?",
        "answer_label": "no"
    },
    {
        "id": "5a6b1a51-03be-40dd-9c08-02b7829a9750",
        "table_caption": "Neural End-to-End Learning for Computational Argumentation Mining Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by \u201c:\u201d. Layers from which tasks feed are indicated by respective numbers.",
        "table_column_names": [
            "[EMPTY]",
            "C-F1 100%",
            "C-F1 50%",
            "R-F1 100%",
            "R-F1 50%",
            "F1 100%",
            "F1 50%"
        ],
        "table_content_values": [
            [
                "Y-3",
                "49.59",
                "65.37",
                "26.28",
                "37.00",
                "34.35",
                "47.25"
            ],
            [
                "Y-3:Y<italic>C</italic>-1",
                "54.71",
                "66.84",
                "28.44",
                "37.35",
                "37.40",
                "47.92"
            ],
            [
                "Y-3:Y<italic>R</italic>-1",
                "51.32",
                "66.49",
                "26.92",
                "37.18",
                "35.31",
                "47.69"
            ],
            [
                "Y-3:Y<italic>C</italic>-3",
                "<bold>54.58</bold>",
                "67.66",
                "<bold>30.22</bold>",
                "<bold>40.30</bold>",
                "<bold>38.90</bold>",
                "<bold>50.51</bold>"
            ],
            [
                "Y-3:Y<italic>R</italic>-3",
                "53.31",
                "66.71",
                "26.65",
                "35.86",
                "35.53",
                "46.64"
            ],
            [
                "Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2",
                "52.95",
                "<bold>67.84</bold>",
                "27.90",
                "39.71",
                "36.54",
                "50.09"
            ],
            [
                "Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3",
                "54.55",
                "67.60",
                "28.30",
                "38.26",
                "37.26",
                "48.86"
            ]
        ],
        "question": "Is it true that Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally weaker: [CONTINUE] as in Eq?",
        "answer_label": "yes"
    },
    {
        "id": "a8563f9b-8e6c-4b68-b8e5-7d8c75675538",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Original",
                "TGen\u2212",
                "63.37",
                "7.7188",
                "41.99",
                "68.53",
                "1.9355",
                "00.06",
                "15.77",
                "00.11",
                "15.94"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen",
                "66.41",
                "8.5565",
                "45.07",
                "69.17",
                "2.2253",
                "00.14",
                "04.11",
                "00.03",
                "04.27"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen+",
                "67.06",
                "8.5871",
                "45.83",
                "69.73",
                "2.2681",
                "00.04",
                "01.75",
                "00.01",
                "01.80"
            ],
            [
                "Original",
                "[BOLD] Original",
                "SC-LSTM",
                "39.11",
                "5.6704",
                "36.83",
                "50.02",
                "0.6045",
                "02.79",
                "18.90",
                "09.79",
                "31.51"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen\u2212",
                "65.87",
                "8.6400",
                "44.20",
                "67.51",
                "2.1710",
                "00.20",
                "00.56",
                "00.21",
                "00.97"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen",
                "66.24",
                "8.6889",
                "44.66",
                "67.85",
                "2.2181",
                "00.10",
                "00.02",
                "00.00",
                "00.12"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen+",
                "65.97",
                "8.6630",
                "44.45",
                "67.59",
                "2.1855",
                "00.02",
                "00.00",
                "00.00",
                "00.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "SC-LSTM",
                "38.52",
                "5.7125",
                "37.45",
                "48.50",
                "0.4343",
                "03.85",
                "17.39",
                "08.12",
                "29.37"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen\u2212",
                "66.28",
                "8.5202",
                "43.96",
                "67.83",
                "2.1375",
                "00.14",
                "02.26",
                "00.22",
                "02.61"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen",
                "67.00",
                "8.6889",
                "44.97",
                "68.19",
                "2.2228",
                "00.06",
                "00.44",
                "00.03",
                "00.53"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen+",
                "66.74",
                "8.6649",
                "44.84",
                "67.95",
                "2.2018",
                "00.00",
                "00.21",
                "00.03",
                "00.24"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen\u2212",
                "64.40",
                "7.9692",
                "42.81",
                "68.87",
                "2.0563",
                "00.01",
                "13.08",
                "00.00",
                "13.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen",
                "66.23",
                "8.5578",
                "45.12",
                "68.87",
                "2.2548",
                "00.04",
                "03.04",
                "00.00",
                "03.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen+",
                "65.96",
                "8.5238",
                "45.49",
                "68.79",
                "2.2456",
                "00.00",
                "01.44",
                "00.00",
                "01.45"
            ]
        ],
        "question": "Is it true that Again, one possible explanation is that cleaning the missing slots provided more complex training examples?",
        "answer_label": "yes"
    },
    {
        "id": "20f98547-11fd-48bd-a892-284b3df13a83",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 6: ROUGE scores for models trained and tested on the Multi-News dataset.",
        "table_column_names": [
            "[BOLD] Method",
            "[BOLD] R-1",
            "[BOLD] R-2",
            "[BOLD] R-SU"
        ],
        "table_content_values": [
            [
                "First-1",
                "26.83",
                "7.25",
                "6.46"
            ],
            [
                "First-2",
                "35.99",
                "10.17",
                "12.06"
            ],
            [
                "First-3",
                "39.41",
                "11.77",
                "14.51"
            ],
            [
                "LexRank Erkan and Radev ( 2004 )",
                "38.27",
                "12.70",
                "13.20"
            ],
            [
                "TextRank Mihalcea and Tarau ( 2004 )",
                "38.44",
                "13.10",
                "13.50"
            ],
            [
                "MMR Carbonell and Goldstein ( 1998 )",
                "38.77",
                "11.98",
                "12.91"
            ],
            [
                "PG-Original Lebanoff et\u00a0al. ( 2018 )",
                "41.85",
                "12.91",
                "16.46"
            ],
            [
                "PG-MMR Lebanoff et\u00a0al. ( 2018 )",
                "40.55",
                "12.36",
                "15.87"
            ],
            [
                "PG-BRNN Gehrmann et\u00a0al. ( 2018 )",
                "42.80",
                "14.19",
                "16.75"
            ],
            [
                "CopyTransformer Gehrmann et\u00a0al. ( 2018 )",
                "[BOLD] 43.57",
                "14.03",
                "17.37"
            ],
            [
                "Hi-MAP (Our Model)",
                "43.47",
                "[BOLD] 14.89",
                "[BOLD] 17.41"
            ]
        ],
        "question": "Is it true that The Transformer performs best in terms of R-1 while Hi-MAP does not outperform it on R-2 and R-SU?",
        "answer_label": "no"
    },
    {
        "id": "e76cbbbb-e973-4cdf-9ab0-a1a39fec7cfc",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
        "table_column_names": [
            "<bold>Model</bold>",
            "R",
            "MUC P",
            "<italic>F</italic>1",
            "R",
            "B3 P",
            "<italic>F</italic>1",
            "R",
            "CEAF-<italic>e</italic> P",
            "<italic>F</italic>1",
            "CoNLL <italic>F</italic>1"
        ],
        "table_content_values": [
            [
                "<bold>Baselines</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Cluster+Lemma",
                "76.5",
                "79.9",
                "78.1",
                "71.7",
                "85",
                "77.8",
                "75.5",
                "71.7",
                "73.6",
                "76.5"
            ],
            [
                "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
                "71",
                "75",
                "73",
                "71",
                "78",
                "74",
                "-",
                "-",
                "64",
                "73"
            ],
            [
                "KCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>)",
                "67",
                "71",
                "69",
                "71",
                "67",
                "69",
                "71",
                "67",
                "69",
                "69"
            ],
            [
                "Cluster+KCP",
                "68.4",
                "79.3",
                "73.4",
                "67.2",
                "87.2",
                "75.9",
                "77.4",
                "66.4",
                "71.5",
                "73.6"
            ],
            [
                "<bold>Model Variants</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Disjoint",
                "75.5",
                "83.6",
                "79.4",
                "75.4",
                "86",
                "80.4",
                "80.3",
                "71.9",
                "75.9",
                "78.5"
            ],
            [
                "Joint",
                "77.6",
                "84.5",
                "80.9",
                "76.1",
                "85.1",
                "80.3",
                "81",
                "73.8",
                "77.3",
                "<bold>79.5</bold>"
            ]
        ],
        "question": "Is it true that [CONTINUE] Our model achieves state-of-the-art results, outperforming previous models by 9.9 CoNLL F1 points on events?",
        "answer_label": "no"
    },
    {
        "id": "319f8071-f254-4ea5-9d12-2d01758e5686",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Acc",
            "[BOLD] F1",
            "[BOLD] AUC"
        ],
        "table_content_values": [
            [
                "Most Frequent Class",
                "64.2",
                "39.1",
                "0.500"
            ],
            [
                "Logistic Regression",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Sentiment \u2013 MPQA",
                "64.2",
                "39.1",
                "0.499"
            ],
            [
                "Sentiment \u2013 NRC",
                "63.9",
                "42.2",
                "0.599"
            ],
            [
                "Sentiment \u2013 V&B",
                "68.9",
                "60.0",
                "0.696"
            ],
            [
                "Sentiment \u2013 VADER",
                "66.0",
                "54.2",
                "0.654"
            ],
            [
                "Sentiment \u2013 Stanford",
                "68.0",
                "55.6",
                "0.696"
            ],
            [
                "Complaint Specific (all)",
                "65.7",
                "55.2",
                "0.634"
            ],
            [
                "Request",
                "64.2",
                "39.1",
                "0.583"
            ],
            [
                "Intensifiers",
                "64.5",
                "47.3",
                "0.639"
            ],
            [
                "Downgraders",
                "65.4",
                "49.8",
                "0.615"
            ],
            [
                "Temporal References",
                "64.2",
                "43.7",
                "0.535"
            ],
            [
                "Pronoun Types",
                "64.1",
                "39.1",
                "0.545"
            ],
            [
                "POS Bigrams",
                "72.2",
                "66.8",
                "0.756"
            ],
            [
                "LIWC",
                "71.6",
                "65.8",
                "0.784"
            ],
            [
                "Word2Vec Clusters",
                "67.7",
                "58.3",
                "0.738"
            ],
            [
                "Bag-of-Words",
                "79.8",
                "77.5",
                "0.866"
            ],
            [
                "All Features",
                "[BOLD] 80.5",
                "[BOLD] 78.0",
                "[BOLD] 0.873"
            ],
            [
                "Neural Networks",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "MLP",
                "78.3",
                "76.2",
                "0.845"
            ],
            [
                "LSTM",
                "80.2",
                "77.0",
                "0.864"
            ]
        ],
        "question": "Is it true that However, best predictive performance is obtained using bag-of-word features, reaching an F1 of up to 77.5 and AUC of 0.866?",
        "answer_label": "yes"
    },
    {
        "id": "99e5b5e6-f575-4873-b1b5-cf1ecc803e78",
        "table_caption": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 2: Precisions on the Wikidata dataset.",
        "table_column_names": [
            "Recall",
            "0.1",
            "0.2",
            "0.3",
            "AUC"
        ],
        "table_content_values": [
            [
                "Rank+ExATT",
                "0.584",
                "0.535",
                "0.487",
                "0.392"
            ],
            [
                "PCNN+ATT (m)",
                "0.365",
                "0.317",
                "0.213",
                "0.204"
            ],
            [
                "PCNN+ATT (1)",
                "0.665",
                "0.517",
                "0.413",
                "0.396"
            ],
            [
                "Our Model",
                "0.650",
                "0.519",
                "0.422",
                "[BOLD] 0.405"
            ]
        ],
        "question": "Is it true that We show the precision numbers for some particular recalls as well as the AUC in Table 2, where PCNN+ATT (1) refers to train sentences with two entities and one relation label, PCNN+ATT (m) refers to train sentences with four entities7 and two relation labels. However, our model does not outperform the other models, as evidenced by the lower AUC score?",
        "answer_label": "no"
    },
    {
        "id": "78a80fec-1eb9-49ce-9e20-ffc00a53a2b1",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Method",
            "SUBJ",
            "CR",
            "MR",
            "MPQA",
            "MRPC",
            "TREC",
            "SICK-E",
            "SST2",
            "SST5",
            "STS-B",
            "SICK-R"
        ],
        "table_content_values": [
            [
                "CBOW/784",
                "90.0",
                "[BOLD] 79.2",
                "[BOLD] 74.0",
                "87.1",
                "71.6",
                "85.6",
                "78.9",
                "78.5",
                "42.1",
                "61.0",
                "[BOLD] 78.1"
            ],
            [
                "CMOW/784",
                "87.5",
                "73.4",
                "70.6",
                "[BOLD] 87.3",
                "69.6",
                "[BOLD] 88.0",
                "77.2",
                "74.7",
                "37.9",
                "56.5",
                "76.2"
            ],
            [
                "Hybrid",
                "[BOLD] 90.2",
                "78.7",
                "73.7",
                "[BOLD] 87.3",
                "[BOLD] 72.7",
                "87.6",
                "[BOLD] 79.4",
                "[BOLD] 79.6",
                "[BOLD] 43.3",
                "[BOLD] 63.4",
                "77.8"
            ],
            [
                "cmp. CBOW",
                "+0.2%",
                "-0.6%",
                "-0.4%",
                "+0.2%",
                "+1.5%",
                "+2.3%",
                "+0.6%",
                "+1.4%",
                "+2.9%",
                "+3.9%",
                "-0.4%"
            ],
            [
                "cmp. CMOW",
                "+3.1%",
                "+7.2%",
                "+4.4%",
                "+0%",
                "+4.5%",
                "-0.5%",
                "+2.9%",
                "+6.7%",
                "+14.3",
                "+12.2%",
                "+2.1%"
            ]
        ],
        "question": "Is it true that [CONTINUE] However, CMOW does not in general supersede CBOW embeddings?",
        "answer_label": "yes"
    },
    {
        "id": "6a39e1b2-806a-4c81-953d-0a1db9b7d962",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
        "table_column_names": [
            "[BOLD] Training data",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] Disfl"
        ],
        "table_content_values": [
            [
                "Original",
                "0",
                "22",
                "0",
                "14"
            ],
            [
                "Cleaned added",
                "0",
                "23",
                "0",
                "14"
            ],
            [
                "Cleaned missing",
                "0",
                "1",
                "0",
                "2"
            ],
            [
                "Cleaned",
                "0",
                "0",
                "0",
                "5"
            ]
        ],
        "question": "Is it true that All fluency problems we found were very slight, but added and wrong-valued slots were still found, so missed slots are not the only problem?",
        "answer_label": "no"
    },
    {
        "id": "66c277ce-a633-4441-911f-9346c0b73c74",
        "table_caption": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task Table 4: Image-caption ranking results for Japanese (MS-COCO)",
        "table_column_names": [
            "[EMPTY]",
            "Image to Text R@1",
            "Image to Text R@5",
            "Image to Text R@10",
            "Image to Text Mr",
            "Text to Image R@1",
            "Text to Image R@5",
            "Text to Image R@10",
            "Text to Image Mr",
            "Alignment"
        ],
        "table_content_values": [
            [
                "[BOLD] symmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Mono",
                "42.7",
                "77.7",
                "88.5",
                "2",
                "33.1",
                "69.8",
                "84.3",
                "3",
                "-"
            ],
            [
                "FME",
                "40.7",
                "77.7",
                "88.3",
                "2",
                "30.0",
                "68.9",
                "83.1",
                "3",
                "92.70%"
            ],
            [
                "AME",
                "[BOLD] 50.2",
                "[BOLD] 85.6",
                "[BOLD] 93.1",
                "[BOLD] 1",
                "[BOLD] 40.2",
                "[BOLD] 76.7",
                "[BOLD] 87.8",
                "[BOLD] 2",
                "82.54%"
            ],
            [
                "[BOLD] asymmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Mono",
                "49.9",
                "83.4",
                "93.7",
                "2",
                "39.7",
                "76.5",
                "88.3",
                "[BOLD] 2",
                "-"
            ],
            [
                "FME",
                "48.8",
                "81.9",
                "91.9",
                "2",
                "37.0",
                "74.8",
                "87.0",
                "[BOLD] 2",
                "92.70%"
            ],
            [
                "AME",
                "[BOLD] 55.5",
                "[BOLD] 87.9",
                "[BOLD] 95.2",
                "[BOLD] 1",
                "[BOLD] 44.9",
                "[BOLD] 80.7",
                "[BOLD] 89.3",
                "[BOLD] 2",
                "84.99%"
            ]
        ],
        "question": "Is it true that For the Japanese captions, AME reaches 6.25% and 3.66% better results on average compared to monolingual model in symmetric and asymmetric modes, respectively?",
        "answer_label": "yes"
    },
    {
        "id": "f480c688-06c4-459b-affc-8737fc822e2b",
        "table_caption": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 4: Results of Self-Play Evaluation.",
        "table_column_names": [
            "System",
            "TGPC Succ. (%)",
            "TGPC #Turns",
            "CWC Succ. (%)",
            "CWC #Turns"
        ],
        "table_content_values": [
            [
                "Retrieval\u00a0",
                "7.16",
                "4.17",
                "0",
                "-"
            ],
            [
                "Retrieval-Stgy\u00a0",
                "47.80",
                "6.7",
                "44.6",
                "7.42"
            ],
            [
                "PMI\u00a0",
                "35.36",
                "6.38",
                "47.4",
                "5.29"
            ],
            [
                "Neural\u00a0",
                "54.76",
                "4.73",
                "47.6",
                "5.16"
            ],
            [
                "Kernel\u00a0",
                "62.56",
                "4.65",
                "53.2",
                "4.08"
            ],
            [
                "DKRN (ours)",
                "[BOLD] 89.0",
                "5.02",
                "[BOLD] 84.4",
                "4.20"
            ]
        ],
        "question": "Is it true that This table refutes the effectiveness of our approach?",
        "answer_label": "no"
    },
    {
        "id": "0cf1197a-668e-4bba-a1b1-ccc35b2fd72f",
        "table_caption": "Zero-Shot Grounding of Objects from Natural Language Queries Table 3: Category-wise performance with the default split of Flickr30k Entities.",
        "table_column_names": [
            "Method",
            "Overall",
            "people",
            "clothing",
            "bodyparts",
            "animals",
            "vehicles",
            "instruments",
            "scene",
            "other"
        ],
        "table_content_values": [
            [
                "QRC - VGG(det)",
                "60.21",
                "75.08",
                "55.9",
                "20.27",
                "73.36",
                "68.95",
                "45.68",
                "65.27",
                "38.8"
            ],
            [
                "CITE - VGG(det)",
                "61.89",
                "[BOLD] 75.95",
                "58.50",
                "30.78",
                "[BOLD] 77.03",
                "[BOLD] 79.25",
                "48.15",
                "58.78",
                "43.24"
            ],
            [
                "ZSGNet - VGG (cls)",
                "60.12",
                "72.52",
                "60.57",
                "38.51",
                "63.61",
                "64.47",
                "49.59",
                "64.66",
                "41.09"
            ],
            [
                "ZSGNet - Res50 (cls)",
                "[BOLD] 63.39",
                "73.87",
                "[BOLD] 66.18",
                "[BOLD] 45.27",
                "73.79",
                "71.38",
                "[BOLD] 58.54",
                "[BOLD] 66.49",
                "[BOLD] 45.53"
            ]
        ],
        "question": "Is it true that [CONTINUE] As these models use object detectors pretrained on Pascal-VOC , they have somewhat higher performance on classes that are common to both Flickr30k and Pascal-VOC (\"animals\", \"people\" and \"vehicles\"), however, the ZSGNet model with Res50 (cls) performs better than the other models on all categories?",
        "answer_label": "no"
    },
    {
        "id": "681f0a81-e820-45f2-9d6b-64137a6a6c7c",
        "table_caption": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection Table 3: Relative frequency of rephrases correctly predicted as disfluent for disfluencies that contain a content word in both the reparandum and repair (content-content), either the reparandum or repair (content-function) or in neither. Percentages in parentheses show the fraction of tokens belong to each category.",
        "table_column_names": [
            "[BOLD] Type",
            "[BOLD] Reparandum Length  [BOLD] 1-2",
            "[BOLD] Reparandum Length  [BOLD] 3-5"
        ],
        "table_content_values": [
            [
                "content-content",
                "0.61 (30%)",
                "0.58 (52%)"
            ],
            [
                "content-function",
                "0.77 (20%)",
                "0.66 (17%)"
            ],
            [
                "function-function",
                "0.83 (50%)",
                "0.80 (32%)"
            ]
        ],
        "question": "Is it true that We found that rephrase disfluencies that contain content words are harder for the model to detect, compared to rephrases with function words only, and error increases for longer disfluencies?",
        "answer_label": "yes"
    },
    {
        "id": "249235b7-0bb2-431e-b55c-3adcbc63a9d2",
        "table_caption": "Filling Conversation Ellipsis for Better Social Dialog Understanding Table 6: Dialog act prediction performance using different selection methods.",
        "table_column_names": [
            "[BOLD] Selection Method",
            "[BOLD] Prec.(%)",
            "[BOLD] Rec.(%)",
            "[BOLD] F1(%)"
        ],
        "table_content_values": [
            [
                "Max Logits",
                "80.19",
                "80.50",
                "79.85"
            ],
            [
                "Add Logits",
                "81.30",
                "81.28",
                "80.85"
            ],
            [
                "Add Logits+Expert",
                "[BOLD] 81.30",
                "[BOLD] 81.41",
                "[BOLD] 80.90"
            ],
            [
                "Concat Hidden",
                "80.24",
                "80.04",
                "79.65"
            ],
            [
                "Max Hidden",
                "80.30",
                "80.04",
                "79.63"
            ],
            [
                "Add Hidden",
                "80.82",
                "80.28",
                "80.08"
            ]
        ],
        "question": "Is it true that We can see from Table 6 that empirically adding logits from two models after classifiers performs the best?",
        "answer_label": "yes"
    },
    {
        "id": "5eb5ab6e-0556-435d-b7c3-f73a75086415",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that Similarly, excluding the direction aggregation module leads to a performance drop to 24.6 BLEU points?",
        "answer_label": "yes"
    },
    {
        "id": "2a748b24-0923-494a-b41b-5b290c77df35",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
        "table_column_names": [
            "Topic Name",
            "Size",
            "TF-IDF ARI",
            "WMD ARI",
            "Sent2vec ARI",
            "Doc2vec ARI",
            "BERT ARI",
            "[ITALIC] OD-w2v ARI",
            "[ITALIC] OD-d2v ARI",
            "TF-IDF  [ITALIC] Sil.",
            "WMD  [ITALIC] Sil.",
            "Sent2vec  [ITALIC] Sil.",
            "Doc2vec  [ITALIC] Sil.",
            "BERT  [ITALIC] Sil.",
            "[ITALIC] OD-w2v  [ITALIC] Sil.",
            "[ITALIC] OD-d2v  [ITALIC] Sil."
        ],
        "table_content_values": [
            [
                "Affirmative Action",
                "81",
                "-0.07",
                "-0.02",
                "0.03",
                "-0.01",
                "-0.02",
                "[BOLD] 0.14",
                "[ITALIC] 0.02",
                "0.01",
                "0.01",
                "-0.01",
                "-0.02",
                "-0.04",
                "[BOLD] 0.06",
                "[ITALIC] 0.01"
            ],
            [
                "Atheism",
                "116",
                "[BOLD] 0.19",
                "0.07",
                "0.00",
                "0.03",
                "-0.01",
                "0.11",
                "[ITALIC] 0.16",
                "0.02",
                "0.01",
                "0.02",
                "0.01",
                "0.01",
                "[ITALIC] 0.05",
                "[BOLD] 0.07"
            ],
            [
                "Austerity Measures",
                "20",
                "[ITALIC] 0.04",
                "[ITALIC] 0.04",
                "-0.01",
                "-0.05",
                "0.04",
                "[BOLD] 0.21",
                "-0.01",
                "0.06",
                "0.07",
                "0.05",
                "-0.03",
                "0.10",
                "[BOLD] 0.19",
                "0.1"
            ],
            [
                "Democratization",
                "76",
                "0.02",
                "-0.01",
                "0.00",
                "[ITALIC] 0.09",
                "-0.01",
                "[BOLD] 0.11",
                "0.07",
                "0.01",
                "0.01",
                "0.02",
                "0.02",
                "0.03",
                "[BOLD] 0.16",
                "[ITALIC] 0.11"
            ],
            [
                "Education Voucher Scheme",
                "30",
                "[BOLD] 0.25",
                "0.12",
                "0.08",
                "-0.02",
                "0.04",
                "0.13",
                "[ITALIC] 0.19",
                "0.01",
                "0.01",
                "0.01",
                "-0.01",
                "0.02",
                "[ITALIC] 0.38",
                "[BOLD] 0.40"
            ],
            [
                "Gambling",
                "60",
                "-0.06",
                "-0.01",
                "-0.02",
                "0.04",
                "0.09",
                "[ITALIC] 0.35",
                "[BOLD] 0.39",
                "0.01",
                "0.02",
                "0.03",
                "0.01",
                "0.09",
                "[BOLD] 0.30",
                "[ITALIC] 0.22"
            ],
            [
                "Housing",
                "30",
                "0.01",
                "-0.01",
                "-0.01",
                "-0.02",
                "0.08",
                "[BOLD] 0.27",
                "0.01",
                "0.02",
                "0.03",
                "0.03",
                "0.01",
                "0.11",
                "[BOLD] 0.13",
                "[ITALIC] 0.13"
            ],
            [
                "Hydroelectric Dams",
                "110",
                "[BOLD] 0.47",
                "[ITALIC] 0.45",
                "[ITALIC] 0.45",
                "-0.01",
                "0.38",
                "0.35",
                "0.14",
                "0.04",
                "0.08",
                "0.12",
                "0.01",
                "0.19",
                "[BOLD] 0.26",
                "[ITALIC] 0.09"
            ],
            [
                "Intellectual Property",
                "66",
                "0.01",
                "0.01",
                "0.00",
                "0.03",
                "0.03",
                "[ITALIC] 0.05",
                "[BOLD] 0.14",
                "0.01",
                "[ITALIC] 0.04",
                "0.03",
                "0.01",
                "0.03",
                "[ITALIC] 0.04",
                "[BOLD] 0.12"
            ],
            [
                "Keystone pipeline",
                "18",
                "0.01",
                "0.01",
                "0.00",
                "-0.13",
                "[BOLD] 0.07",
                "-0.01",
                "[BOLD] 0.07",
                "-0.01",
                "-0.03",
                "-0.03",
                "-0.07",
                "0.03",
                "[BOLD] 0.05",
                "[ITALIC] 0.02"
            ],
            [
                "Monarchy",
                "61",
                "-0.04",
                "0.01",
                "0.00",
                "0.03",
                "-0.02",
                "[BOLD] 0.15",
                "[BOLD] 0.15",
                "0.01",
                "0.02",
                "0.02",
                "0.01",
                "0.01",
                "[BOLD] 0.11",
                "[ITALIC] 0.09"
            ],
            [
                "National Service",
                "33",
                "0.14",
                "-0.03",
                "-0.01",
                "0.02",
                "0.01",
                "[ITALIC] 0.31",
                "[BOLD] 0.39",
                "0.02",
                "0.04",
                "0.02",
                "0.01",
                "0.02",
                "[BOLD] 0.25",
                "[BOLD] 0.25"
            ],
            [
                "One-child policy China",
                "67",
                "-0.05",
                "0.01",
                "[BOLD] 0.11",
                "-0.02",
                "0.02",
                "[BOLD] 0.11",
                "0.01",
                "0.01",
                "0.02",
                "[ITALIC] 0.04",
                "-0.01",
                "0.03",
                "[BOLD] 0.07",
                "-0.02"
            ],
            [
                "Open-source Software",
                "48",
                "-0.02",
                "-0.01",
                "[ITALIC] 0.05",
                "0.01",
                "0.12",
                "[BOLD] 0.09",
                "-0.02",
                "0.01",
                "-0.01",
                "0.00",
                "-0.02",
                "0.03",
                "[BOLD] 0.18",
                "0.01"
            ],
            [
                "Pornography",
                "52",
                "-0.02",
                "0.01",
                "0.01",
                "-0.02",
                "-0.01",
                "[BOLD] 0.41",
                "[BOLD] 0.41",
                "0.01",
                "0.01",
                "0.02",
                "-0.01",
                "0.03",
                "[BOLD] 0.47",
                "[ITALIC] 0.41"
            ],
            [
                "Seanad Abolition",
                "25",
                "0.23",
                "0.09",
                "-0.01",
                "-0.01",
                "0.03",
                "[ITALIC] 0.32",
                "[BOLD] 0.54",
                "0.02",
                "0.01",
                "-0.01",
                "-0.03",
                "-0.04",
                "[ITALIC] 0.15",
                "[BOLD] 0.31"
            ],
            [
                "Trades Unions",
                "19",
                "[ITALIC] 0.44",
                "[ITALIC] 0.44",
                "[BOLD] 0.60",
                "-0.05",
                "0.44",
                "[ITALIC] 0.44",
                "0.29",
                "0.1",
                "0.17",
                "0.21",
                "0.01",
                "0.26",
                "[BOLD] 0.48",
                "[ITALIC] 0.32"
            ],
            [
                "Video Games",
                "72",
                "-0.01",
                "0.01",
                "0.12",
                "0.01",
                "0.08",
                "[ITALIC] 0.40",
                "[BOLD] 0.56",
                "0.01",
                "0.01",
                "0.06",
                "0.01",
                "0.05",
                "[ITALIC] 0.32",
                "[BOLD] 0.42"
            ],
            [
                "Average",
                "54.67",
                "0.09",
                "0.07",
                "0.08",
                "0.01",
                "0.08",
                "[BOLD] 0.22",
                "[ITALIC] 0.20",
                "0.02",
                "0.03",
                "0.04",
                "-0.01",
                "0.05",
                "[BOLD] 0.20",
                "[ITALIC] 0.17"
            ]
        ],
        "question": "Is it true that The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are not statistically significant (paired t-test) with respect to baselines at significance level 0.005?",
        "answer_label": "no"
    },
    {
        "id": "e4c0799b-5c10-4ff0-a19a-4f43a14dc253",
        "table_caption": "Filling Conversation Ellipsis for Better Social Dialog Understanding Table 6: Dialog act prediction performance using different selection methods.",
        "table_column_names": [
            "[BOLD] Selection Method",
            "[BOLD] Prec.(%)",
            "[BOLD] Rec.(%)",
            "[BOLD] F1(%)"
        ],
        "table_content_values": [
            [
                "Max Logits",
                "80.19",
                "80.50",
                "79.85"
            ],
            [
                "Add Logits",
                "81.30",
                "81.28",
                "80.85"
            ],
            [
                "Add Logits+Expert",
                "[BOLD] 81.30",
                "[BOLD] 81.41",
                "[BOLD] 80.90"
            ],
            [
                "Concat Hidden",
                "80.24",
                "80.04",
                "79.65"
            ],
            [
                "Max Hidden",
                "80.30",
                "80.04",
                "79.63"
            ],
            [
                "Add Hidden",
                "80.82",
                "80.28",
                "80.08"
            ]
        ],
        "question": "Is it true that We can see from Table 6 that empirically adding logits from two models after classifiers does not perform the best?",
        "answer_label": "no"
    },
    {
        "id": "6f1ef6d3-b841-4e1e-ab21-64d50093f881",
        "table_caption": "Sparse and Structured Visual Attention Table 1: Automatic evaluation of caption generation on MSCOCO and Flickr30k.",
        "table_column_names": [
            "[EMPTY]",
            "MSCOCO spice",
            "MSCOCO cider",
            "MSCOCO rouge [ITALIC] L",
            "MSCOCO bleu4",
            "MSCOCO meteor",
            "MSCOCO rep\u2193",
            "Flickr30k spice",
            "Flickr30k cider",
            "Flickr30k rouge [ITALIC] L",
            "Flickr30k bleu4",
            "Flickr30k meteor",
            "Flickr30k rep\u2193"
        ],
        "table_content_values": [
            [
                "softmax",
                "18.4",
                "0.967",
                "52.9",
                "29.9",
                "24.9",
                "3.76",
                "13.5",
                "0.443",
                "44.2",
                "19.9",
                "19.1",
                "6.09"
            ],
            [
                "sparsemax",
                "[BOLD] 18.9",
                "[BOLD] 0.990",
                "[BOLD] 53.5",
                "[BOLD] 31.5",
                "[BOLD] 25.3",
                "3.69",
                "[BOLD] 13.7",
                "[BOLD] 0.444",
                "[BOLD] 44.3",
                "[BOLD] 20.7",
                "[BOLD] 19.3",
                "5.84"
            ],
            [
                "TVmax",
                "18.5",
                "0.974",
                "53.1",
                "29.9",
                "25.1",
                "[BOLD] 3.17",
                "13.3",
                "0.438",
                "44.2",
                "20.5",
                "19.0",
                "[BOLD] 3.97"
            ]
        ],
        "question": "Is it true that As can be seen in Table 1, softmax achieves better results overall when compared with sparsemax and TVMAX, indicating that the use of selective attention does not necessarily lead to better captions?",
        "answer_label": "no"
    },
    {
        "id": "f2719604-1c66-4880-9cef-38422fcdc053",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that The ensemble approach based on combining five DCGCN models initialized with different random seeds achieves a BLEU score of 30.4 and a CHRF++ score of 59.6?",
        "answer_label": "yes"
    },
    {
        "id": "c1eced18-5360-4a8e-af31-06277e4a832e",
        "table_caption": "Syntactic Dependency Representations in Neural Relation Classification Table 1: Effect of using the shortest dependency path on each relation type.",
        "table_column_names": [
            "[BOLD] Relation",
            "[BOLD] best F1 (in 5-fold) without sdp",
            "[BOLD] best F1 (in 5-fold) with sdp",
            "[BOLD] Diff."
        ],
        "table_content_values": [
            [
                "USAGE",
                "60.34",
                "80.24",
                "+ 19.90"
            ],
            [
                "MODEL-FEATURE",
                "48.89",
                "70.00",
                "+ 21.11"
            ],
            [
                "PART_WHOLE",
                "29.51",
                "70.27",
                "+40.76"
            ],
            [
                "TOPIC",
                "45.80",
                "91.26",
                "+45.46"
            ],
            [
                "RESULT",
                "54.35",
                "81.58",
                "+27.23"
            ],
            [
                "COMPARE",
                "20.00",
                "61.82",
                "+ 41.82"
            ],
            [
                "macro-averaged",
                "50.10",
                "76.10",
                "+26.00"
            ]
        ],
        "question": "Is it true that We find that the effect of syntactic structure varies between the different relation types?",
        "answer_label": "yes"
    },
    {
        "id": "a63f189d-7408-49c3-bc3c-2b89a01e30cf",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 1: Test accuracy (ACC) on SNLI task. \u201c#Params\u201d: the parameter number of Base. Base and LN denote the baseline model and layer normalization respectively. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti. Best results are highlighted in bold.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "Base ACC",
            "Base Time",
            "+LN ACC",
            "+LN Time",
            "+BERT ACC",
            "+BERT Time",
            "+LN+BERT ACC",
            "+LN+BERT Time"
        ],
        "table_content_values": [
            [
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "Rockt\u00e4schel et\u00a0al. ( 2016 )",
                "250K",
                "83.50",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-",
                "-"
            ],
            [
                "This",
                "LSTM",
                "8.36M",
                "84.27",
                "0.262",
                "86.03",
                "0.432",
                "89.95",
                "0.544",
                "[BOLD] 90.49",
                "0.696"
            ],
            [
                "This",
                "GRU",
                "6.41M",
                "[BOLD] 85.71",
                "0.245",
                "[BOLD] 86.05",
                "0.419",
                "[BOLD] 90.29",
                "0.529",
                "90.10",
                "0.695"
            ],
            [
                "This",
                "ATR",
                "2.87M",
                "84.88",
                "0.210",
                "85.81",
                "0.307",
                "90.00",
                "0.494",
                "90.28",
                "0.580"
            ],
            [
                "Work",
                "SRU",
                "5.48M",
                "84.28",
                "0.258",
                "85.32",
                "0.283",
                "89.98",
                "0.543",
                "90.09",
                "0.555"
            ],
            [
                "[EMPTY]",
                "LRN",
                "4.25M",
                "84.88",
                "[BOLD] 0.209",
                "85.06",
                "[BOLD] 0.223",
                "89.98",
                "[BOLD] 0.488",
                "89.93",
                "[BOLD] 0.506"
            ]
        ],
        "question": "Is it true that LRN is still the fastest model, outperforming other recurrent units by 8%\u223c27%?",
        "answer_label": "yes"
    },
    {
        "id": "97d49102-06a2-4887-84ba-121e1a200ed6",
        "table_caption": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance Table 5: Comparison on hard and soft alignments.",
        "table_column_names": [
            "Metrics",
            "cs-en",
            "de-en",
            "fi-en",
            "lv-en"
        ],
        "table_content_values": [
            [
                "RUSE",
                "0.624",
                "0.644",
                "0.750",
                "0.697"
            ],
            [
                "Hmd-F1 + BERT",
                "0.655",
                "0.681",
                "0.821",
                "0.712"
            ],
            [
                "Hmd-Recall + BERT",
                "0.651",
                "0.658",
                "0.788",
                "0.681"
            ],
            [
                "Hmd-Prec + BERT",
                "0.624",
                "0.669",
                "0.817",
                "0.707"
            ],
            [
                "Wmd-unigram + BERT",
                "0.651",
                "0.686",
                "<bold>0.823</bold>",
                "0.710"
            ],
            [
                "Wmd-bigram + BERT",
                "<bold>0.665</bold>",
                "<bold>0.688</bold>",
                "0.821",
                "<bold>0.712</bold>"
            ]
        ],
        "question": "Is it true that We also observe that WMD-UNIGRAMS slightly outperforms WMD-BIGRAMS on 3 out of 4 language pairs?",
        "answer_label": "no"
    },
    {
        "id": "9a337795-1c06-4d0e-91f3-3ec46743dc82",
        "table_caption": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 3: Results of Turn-level Evaluation.",
        "table_column_names": [
            "Dataset",
            "System",
            "Keyword Prediction  [ITALIC] Rw@1",
            "Keyword Prediction  [ITALIC] Rw@3",
            "Keyword Prediction  [ITALIC] Rw@5",
            "Keyword Prediction P@1",
            "Response Retrieval  [ITALIC] R20@1",
            "Response Retrieval  [ITALIC] R20@3",
            "Response Retrieval  [ITALIC] R20@5",
            "Response Retrieval MRR"
        ],
        "table_content_values": [
            [
                "TGPC",
                "Retrieval\u00a0",
                "-",
                "-",
                "-",
                "-",
                "0.5063",
                "0.7615",
                "0.8676",
                "0.6589"
            ],
            [
                "TGPC",
                "PMI\u00a0",
                "0.0585",
                "0.1351",
                "0.1872",
                "0.0871",
                "0.5441",
                "0.7839",
                "0.8716",
                "0.6847"
            ],
            [
                "TGPC",
                "Neural\u00a0",
                "0.0708",
                "0.1438",
                "0.1820",
                "0.1321",
                "0.5311",
                "0.7905",
                "0.8800",
                "0.6822"
            ],
            [
                "TGPC",
                "Kernel\u00a0",
                "0.0632",
                "0.1377",
                "0.1798",
                "0.1172",
                "0.5386",
                "0.8012",
                "0.8924",
                "0.6877"
            ],
            [
                "TGPC",
                "DKRN (ours)",
                "[BOLD] 0.0909",
                "[BOLD] 0.1903",
                "[BOLD] 0.2477",
                "[BOLD] 0.1685",
                "[BOLD] 0.5729",
                "[BOLD] 0.8132",
                "[BOLD] 0.8966",
                "[BOLD] 0.7110"
            ],
            [
                "CWC",
                "Retrieval\u00a0",
                "-",
                "-",
                "-",
                "-",
                "0.5785",
                "0.8101",
                "0.8999",
                "0.7141"
            ],
            [
                "CWC",
                "PMI\u00a0",
                "0.0555",
                "0.1001",
                "0.1212",
                "0.0969",
                "0.5945",
                "0.8185",
                "0.9054",
                "0.7257"
            ],
            [
                "CWC",
                "Neural\u00a0",
                "0.0654",
                "0.1194",
                "0.1450",
                "0.1141",
                "0.6044",
                "0.8233",
                "0.9085",
                "0.7326"
            ],
            [
                "CWC",
                "Kernel\u00a0",
                "0.0592",
                "0.1113",
                "0.1337",
                "0.1011",
                "0.6017",
                "0.8234",
                "0.9087",
                "0.7320"
            ],
            [
                "CWC",
                "DKRN (ours)",
                "[BOLD] 0.0680",
                "[BOLD] 0.1254",
                "[BOLD] 0.1548",
                "[BOLD] 0.1185",
                "[BOLD] 0.6324",
                "[BOLD] 0.8416",
                "[BOLD] 0.9183",
                "[BOLD] 0.7533"
            ]
        ],
        "question": "Is it true that Our approach DKRN outperforms all state-of-the-art methods in terms of all metrics on both datasets with two tasks?",
        "answer_label": "yes"
    },
    {
        "id": "84d8d7a2-2811-41a4-b5a3-1777e9b8af8a",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 5: Performance of different agents on the neural user simulator.",
        "table_column_names": [
            "Method",
            "VHUS Turns",
            "VHUS Inform",
            "VHUS Match",
            "VHUS Success"
        ],
        "table_content_values": [
            [
                "ACER",
                "22.35",
                "55.13",
                "33.08",
                "18.6"
            ],
            [
                "PPO",
                "[BOLD] 19.23",
                "[BOLD] 56.31",
                "33.08",
                "18.3"
            ],
            [
                "ALDM",
                "26.90",
                "54.37",
                "24.15",
                "16.4"
            ],
            [
                "GDPL",
                "22.43",
                "52.58",
                "[BOLD] 36.21",
                "[BOLD] 19.7"
            ]
        ],
        "question": "Is it true that ALDM even gets worse performance than ACER and PPO?",
        "answer_label": "yes"
    },
    {
        "id": "c9d32538-fe25-40c4-a010-0b5e2a167331",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
        "table_column_names": [
            "[BOLD] Emoji alias",
            "[BOLD] N",
            "[BOLD] emoji #",
            "[BOLD] emoji %",
            "[BOLD] no-emoji #",
            "[BOLD] no-emoji %",
            "[BOLD] \u0394%"
        ],
        "table_content_values": [
            [
                "mask",
                "163",
                "154",
                "94.48",
                "134",
                "82.21",
                "- 12.27"
            ],
            [
                "two_hearts",
                "87",
                "81",
                "93.10",
                "77",
                "88.51",
                "- 4.59"
            ],
            [
                "heart_eyes",
                "122",
                "109",
                "89.34",
                "103",
                "84.43",
                "- 4.91"
            ],
            [
                "heart",
                "267",
                "237",
                "88.76",
                "235",
                "88.01",
                "- 0.75"
            ],
            [
                "rage",
                "92",
                "78",
                "84.78",
                "66",
                "71.74",
                "- 13.04"
            ],
            [
                "cry",
                "116",
                "97",
                "83.62",
                "83",
                "71.55",
                "- 12.07"
            ],
            [
                "sob",
                "490",
                "363",
                "74.08",
                "345",
                "70.41",
                "- 3.67"
            ],
            [
                "unamused",
                "167",
                "121",
                "72.46",
                "116",
                "69.46",
                "- 3.00"
            ],
            [
                "weary",
                "204",
                "140",
                "68.63",
                "139",
                "68.14",
                "- 0.49"
            ],
            [
                "joy",
                "978",
                "649",
                "66.36",
                "629",
                "64.31",
                "- 2.05"
            ],
            [
                "sweat_smile",
                "111",
                "73",
                "65.77",
                "75",
                "67.57",
                "1.80"
            ],
            [
                "confused",
                "77",
                "46",
                "59.74",
                "48",
                "62.34",
                "2.60"
            ]
        ],
        "question": "Is it true that Contrary to intuition, the sob emoji contributes more than cry, despite representing a stronger emotion?",
        "answer_label": "no"
    },
    {
        "id": "0988097c-eeaa-4876-91cc-424a0e4d7f65",
        "table_caption": "Assessing Gender Bias in Machine Translation \u2013 A Case Study with Google Translate Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table",
        "table_column_names": [
            "Category",
            "Female (%)",
            "Male (%)",
            "Neutral (%)"
        ],
        "table_content_values": [
            [
                "Office and administrative support",
                "11.015",
                "58.812",
                "16.954"
            ],
            [
                "Architecture and engineering",
                "2.299",
                "72.701",
                "10.92"
            ],
            [
                "Farming, fishing, and forestry",
                "12.179",
                "62.179",
                "14.744"
            ],
            [
                "Management",
                "11.232",
                "66.667",
                "12.681"
            ],
            [
                "Community and social service",
                "20.238",
                "62.5",
                "10.119"
            ],
            [
                "Healthcare support",
                "25.0",
                "43.75",
                "17.188"
            ],
            [
                "Sales and related",
                "8.929",
                "62.202",
                "16.964"
            ],
            [
                "Installation, maintenance, and repair",
                "5.22",
                "58.333",
                "17.125"
            ],
            [
                "Transportation and material moving",
                "8.81",
                "62.976",
                "17.5"
            ],
            [
                "Legal",
                "11.905",
                "72.619",
                "10.714"
            ],
            [
                "Business and financial operations",
                "7.065",
                "67.935",
                "15.58"
            ],
            [
                "Life, physical, and social science",
                "5.882",
                "73.284",
                "10.049"
            ],
            [
                "Arts, design, entertainment, sports, and media",
                "10.36",
                "67.342",
                "11.486"
            ],
            [
                "Education, training, and library",
                "23.485",
                "53.03",
                "9.091"
            ],
            [
                "Building and grounds cleaning and maintenance",
                "12.5",
                "68.333",
                "11.667"
            ],
            [
                "Personal care and service",
                "18.939",
                "49.747",
                "18.434"
            ],
            [
                "Healthcare practitioners and technical",
                "22.674",
                "51.744",
                "15.116"
            ],
            [
                "Production",
                "14.331",
                "51.199",
                "18.245"
            ],
            [
                "Computer and mathematical",
                "4.167",
                "66.146",
                "14.062"
            ],
            [
                "Construction and extraction",
                "8.578",
                "61.887",
                "17.525"
            ],
            [
                "Protective service",
                "8.631",
                "65.179",
                "12.5"
            ],
            [
                "Food preparation and serving related",
                "21.078",
                "58.333",
                "17.647"
            ],
            [
                "Total",
                "11.76",
                "58.93",
                "15.939"
            ]
        ],
        "question": "Is it true that What we have found is that Google Translate does not always translate sentences with male pronouns with greater probability than it does either with female or gender-neutral pronouns, as evidenced by the data in Table 6?",
        "answer_label": "no"
    },
    {
        "id": "59876715-8c94-4df5-8027-281cc74e8292",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
        "table_column_names": [
            "VS.",
            "Efficiency W",
            "Efficiency D",
            "Efficiency L",
            "Quality W",
            "Quality D",
            "Quality L",
            "Success W",
            "Success D",
            "Success L"
        ],
        "table_content_values": [
            [
                "ACER",
                "55",
                "25",
                "20",
                "44",
                "32",
                "24",
                "52",
                "30",
                "18"
            ],
            [
                "PPO",
                "74",
                "13",
                "13",
                "56",
                "26",
                "18",
                "59",
                "31",
                "10"
            ],
            [
                "ALDM",
                "69",
                "19",
                "12",
                "49",
                "25",
                "26",
                "61",
                "24",
                "15"
            ]
        ],
        "question": "Is it true that Among all the baselines, GDPL obtains the most preference against PPO?",
        "answer_label": "yes"
    },
    {
        "id": "0633fe26-997d-4980-b1c3-69077f797d1e",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. \u201cPref%\u201d: in how many percentage of documents a system receives the higher human rating.",
        "table_column_names": [
            "Reward",
            "R-1",
            "R-2",
            "R-L",
            "Human",
            "Pref%"
        ],
        "table_content_values": [
            [
                "R-L (original)",
                "40.9",
                "17.8",
                "38.5",
                "1.75",
                "15"
            ],
            [
                "Learned (ours)",
                "39.2",
                "17.4",
                "37.5",
                "[BOLD] 2.20",
                "[BOLD] 75"
            ]
        ],
        "question": "Is it true that It is clear from Table 5 that using the learned reward does not help the RL-based system generate summaries with significantly higher human ratings?",
        "answer_label": "no"
    },
    {
        "id": "98be01ed-6aba-4c93-9b7b-d19e39a83138",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 6: F1 score on CoNLL-2003 English NER task. \u201c#Params\u201d: the parameter number in NER task. LSTM* denotes the reported result\u00a0Lample et\u00a0al. (2016).",
        "table_column_names": [
            "Model",
            "#Params",
            "NER"
        ],
        "table_content_values": [
            [
                "LSTM*",
                "-",
                "90.94"
            ],
            [
                "LSTM",
                "245K",
                "[BOLD] 89.61"
            ],
            [
                "GRU",
                "192K",
                "89.35"
            ],
            [
                "ATR",
                "87K",
                "88.46"
            ],
            [
                "SRU",
                "161K",
                "88.89"
            ],
            [
                "LRN",
                "129K",
                "88.56"
            ]
        ],
        "question": "Is it true that As shown in Table 6, the performance of LRN is significantly lower than that of LSTM and GRU (-1.05 and -0.79)?",
        "answer_label": "no"
    },
    {
        "id": "043020a0-d7f8-48bf-b2be-50dbed48a648",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).",
        "table_column_names": [
            "Model",
            "#Params",
            "Base",
            "+Elmo"
        ],
        "table_content_values": [
            [
                "rnet*",
                "-",
                "71.1/79.5",
                "-/-"
            ],
            [
                "LSTM",
                "2.67M",
                "[BOLD] 70.46/78.98",
                "75.17/82.79"
            ],
            [
                "GRU",
                "2.31M",
                "70.41/ [BOLD] 79.15",
                "75.81/83.12"
            ],
            [
                "ATR",
                "1.59M",
                "69.73/78.70",
                "75.06/82.76"
            ],
            [
                "SRU",
                "2.44M",
                "69.27/78.41",
                "74.56/82.50"
            ],
            [
                "LRN",
                "2.14M",
                "70.11/78.83",
                "[BOLD] 76.14/ [BOLD] 83.83"
            ]
        ],
        "question": "Is it true that Table 4 lists the EM/F1 score of different models?",
        "answer_label": "yes"
    },
    {
        "id": "e09138ac-6dc6-45f0-a0e3-2fc276e7b93a",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that Excluding the direction aggregation module does not lead to a performance drop to 24.6 BLEU points?",
        "answer_label": "no"
    },
    {
        "id": "239b62aa-ebc4-4f36-aa92-d44876612730",
        "table_caption": "What do Deep Networks Like to Read? Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.",
        "table_column_names": [
            "[EMPTY]",
            "<bold>RNN</bold>",
            "<bold>CNN</bold>",
            "<bold>DAN</bold>"
        ],
        "table_content_values": [
            [
                "Positive",
                "+9.7",
                "+4.3",
                "+<bold>23.6</bold>"
            ],
            [
                "Negative",
                "+6.9",
                "+5.5",
                "+<bold>16.1</bold>"
            ],
            [
                "Flipped to Positive",
                "+20.2",
                "+24.9",
                "+27.4"
            ],
            [
                "Flipped to Negative",
                "+31.5",
                "+28.6",
                "+19.3"
            ]
        ],
        "question": "Is it true that We see a varying increase in sentiment value across all three models after finetuning, indicating that the framework is not always able to pick up on words that are indicative of sentiment?",
        "answer_label": "no"
    },
    {
        "id": "531171f1-fe4b-4849-81ec-36b06b6eb36f",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 2: Throughput for processing the TreeLSTM model on our recursive framework, Fold\u2019s folding technique, and TensorFlow\u2019s iterative approach, with the Large Movie Review dataset. The recursive approach performs the best on inference with efficient parallel execution of tree nodes, while the folding technique shows better performance on training thanks to its GPU exploitation.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Inference",
            "Throughput (instances/s) Training",
            "Throughput (instances/s) Training",
            "Throughput (instances/s) Training"
        ],
        "table_content_values": [
            [
                "Batch size",
                "Iter",
                "Recur",
                "Fold",
                "Iter",
                "Recur",
                "Fold"
            ],
            [
                "1",
                "19.2",
                "81.4",
                "16.5",
                "2.5",
                "4.8",
                "9.0"
            ],
            [
                "10",
                "49.3",
                "217.9",
                "52.2",
                "4.0",
                "4.2",
                "37.5"
            ],
            [
                "25",
                "72.1",
                "269.9",
                "61.6",
                "5.5",
                "3.6",
                "54.7"
            ]
        ],
        "question": "Is it true that The amount of resources is sufficient for executing forward computations, and therefore our framework outperforms the folding technique for the inference task with up to 4.93x faster throughput?",
        "answer_label": "yes"
    },
    {
        "id": "a0b9120d-320d-4547-967f-d8b3eb9529f2",
        "table_caption": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations Table 3: Cue and token distribution in the conversational negation corpus.",
        "table_column_names": [
            "Total negation cues",
            "2921"
        ],
        "table_content_values": [
            [
                "True negation cues",
                "2674"
            ],
            [
                "False negation cues",
                "247"
            ],
            [
                "Average scope length",
                "2.9"
            ],
            [
                "Average sentence length",
                "13.6"
            ],
            [
                "Average tweet length",
                "22.3"
            ]
        ],
        "question": "Is it true that The average number of tokens per tweet is 22.3, per sentence is 13.6 and average scope length is 2.9?",
        "answer_label": "yes"
    },
    {
        "id": "c0e45fc9-0434-4421-bdaf-7b40f7afac29",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that In LDC2017T10, G2S-GGNN achieves a BLEU score of 27.87, which is 3.33 points higher than Damonte and Cohen (2019), a state-of-the-art model that does not employ external information?",
        "answer_label": "yes"
    },
    {
        "id": "6592737a-49c3-4723-b433-e554703165cd",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that [CONTINUE] G2S-GIN has a better performance in handling graphs with node out-degrees higher than 9?",
        "answer_label": "yes"
    },
    {
        "id": "499719d6-acd1-40d1-8962-3bd945f7691c",
        "table_caption": "Deriving Machine Attention from Human Rationales Table 4: Accuracy of transferring between domains. Models with \u2020 use labeled data from source domains and unlabeled data from the target domain. Models with \u2021 use human rationales on the target task.",
        "table_column_names": [
            "Source",
            "Target",
            "Svm",
            "Ra-Svm\u2021",
            "Ra-Cnn\u2021",
            "Trans\u2020",
            "Ra-Trans\u2021\u2020",
            "Ours\u2021\u2020",
            "Oracle\u2020"
        ],
        "table_content_values": [
            [
                "Beer look + Beer aroma + Beer palate",
                "Hotel location",
                "78.65",
                "79.09",
                "79.28",
                "80.42",
                "82.10",
                "[BOLD] 84.52",
                "85.43"
            ],
            [
                "Beer look + Beer aroma + Beer palate",
                "Hotel cleanliness",
                "86.44",
                "86.68",
                "89.01",
                "86.95",
                "87.15",
                "[BOLD] 90.66",
                "92.09"
            ],
            [
                "Beer look + Beer aroma + Beer palate",
                "Hotel service",
                "85.34",
                "86.61",
                "87.91",
                "87.37",
                "86.40",
                "[BOLD] 89.93",
                "92.42"
            ]
        ],
        "question": "Is it true that The error reduction over the best baseline is only 5.09% on average?",
        "answer_label": "no"
    },
    {
        "id": "56344d9a-7b9f-4606-ac3b-ef1134c5db28",
        "table_caption": "What do Deep Networks Like to Read? Table 1: Example sentences of the different classifiers compared to the original on SST-2. We report further examples in the Appendix. <u> use for <UNK>.",
        "table_column_names": [
            "Orig",
            "<u> turns in a <u> screenplay that <u> at the edges ; it \u2019s so clever you want to hate it ."
        ],
        "table_content_values": [
            [
                "DAN",
                "<u> turns in a <u> screenplay screenplay screenplay of <u> edges edges edges shapes so clever easy want hate hate hate hate hate hate hate hate hate hate"
            ],
            [
                "CNN",
                "she turns on a on ( ( in in the the the edges \u2019s so clever \u201c want to hate it \u201d"
            ],
            [
                "RNN",
                "<u> turns in a <u> screenplay was <u> <u> <u> edges edges edges curves <u> clever clever you want hate hate it ."
            ]
        ],
        "question": "Is it true that In contrast, DAN masks out punctuation and determiners using words indicative of the class label (i.e?",
        "answer_label": "yes"
    },
    {
        "id": "62f2b650-5f5f-46e6-8570-bd6ff0013ea0",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that TF has the best values of recall and f-measure for all corpora except the English version of TED Talks, where DF has the best value of recall and HClust has the best value of f-measure?",
        "answer_label": "no"
    },
    {
        "id": "f3feea83-901a-4779-ab3b-bbd37a7b2ad8",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 4: Number of tweets on the test set with and without emoji and hashtags. The number between parentheses is the proportion of tweets classified correctly.",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Present",
            "[BOLD] Not Present"
        ],
        "table_content_values": [
            [
                "Emoji",
                "4805 (76.6%)",
                "23952 (68.0%)"
            ],
            [
                "Hashtags",
                "2122 (70.5%)",
                "26635 (69.4%)"
            ]
        ],
        "question": "Is it true that [CONTINUE] Tweets containing emoji seem to be easier for the model to classify than those without?",
        "answer_label": "yes"
    },
    {
        "id": "8e8f27d4-b0f5-43ef-b7ea-55a8d400fc5e",
        "table_caption": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
        "table_column_names": [
            "Model",
            "Belief State Type",
            "System Action Type",
            "System Action Form",
            "Inform (%)",
            "Success (%)",
            "BLEU",
            "Combined Score"
        ],
        "table_content_values": [
            [
                "1. Seq2Seq + Attention ",
                "oracle",
                "-",
                "-",
                "71.3",
                "61.0",
                "[BOLD] 18.9",
                "85.1"
            ],
            [
                "2. Seq2Seq + Copy",
                "oracle",
                "-",
                "-",
                "86.2",
                "[BOLD] 72.0",
                "15.7",
                "94.8"
            ],
            [
                "3. MD-Sequicity",
                "oracle",
                "-",
                "-",
                "[BOLD] 86.6",
                "71.6",
                "16.8",
                "[BOLD] 95.9"
            ],
            [
                "4. SFN + RL (Mehri et al. mehri2019structured)",
                "oracle",
                "generated",
                "one-hot",
                "82.7",
                "72.1",
                "16.3",
                "93.7"
            ],
            [
                "5. HDSA ",
                "oracle",
                "generated",
                "graph",
                "82.9",
                "68.9",
                "[BOLD] 23.6",
                "99.5"
            ],
            [
                "6. DAMD",
                "oracle",
                "generated",
                "span",
                "[BOLD] 89.5",
                "75.8",
                "18.3",
                "100.9"
            ],
            [
                "7. DAMD + multi-action data augmentation",
                "oracle",
                "generated",
                "span",
                "89.2",
                "[BOLD] 77.9",
                "18.6",
                "[BOLD] 102.2"
            ],
            [
                "8. SFN + RL (Mehri et al. mehri2019structured)",
                "oracle",
                "oracle",
                "one-hot",
                "-",
                "-",
                "29.0",
                "106.0"
            ],
            [
                "9. HDSA ",
                "oracle",
                "oracle",
                "graph",
                "87.9",
                "78.0",
                "[BOLD] 30.4",
                "113.4"
            ],
            [
                "10. DAMD + multi-action data augmentation",
                "oracle",
                "oracle",
                "span",
                "[BOLD] 95.4",
                "[BOLD] 87.2",
                "27.3",
                "[BOLD] 118.5"
            ],
            [
                "11. SFN + RL (Mehri et al. mehri2019structured)",
                "generated",
                "generated",
                "one-hot",
                "73.8",
                "58.6",
                "[BOLD] 16.9",
                "83.0"
            ],
            [
                "12. DAMD + multi-action data augmentation",
                "generated",
                "generated",
                "span",
                "[BOLD] 76.3",
                "[BOLD] 60.4",
                "16.6",
                "[BOLD] 85.0"
            ]
        ],
        "question": "Is it true that [CONTINUE] The relative lower BLEU score of our DAMD model compared to other models with different system action forms suggests that it does not outperform them in terms of inform and success rates, [CONTINUE] While we find applying our data augmentation achieves a limited improvement on combined score (6 vs 7), it is not enough to make up for the lower BLEU score, [CONTINUE] Moreover, even if a model has access to ground truth system action, the model does not necessarily improve its task performance?",
        "answer_label": "no"
    },
    {
        "id": "8da915c7-59a0-473f-9ae4-dc07094a27f0",
        "table_caption": "Improving Generalization by Incorporating Coverage in Natural Language Inference Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
        "table_column_names": [
            "[EMPTY]",
            "in-domain SQuAD",
            "in-domain SQuAD",
            "out-of-domain QA-SRL",
            "out-of-domain QA-SRL"
        ],
        "table_content_values": [
            [
                "[EMPTY]",
                "EM",
                "F1",
                "EM",
                "F1"
            ],
            [
                "MQAN",
                "31.76",
                "75.37",
                "<bold>10.99</bold>",
                "50.10"
            ],
            [
                "+coverage",
                "<bold>32.67</bold>",
                "<bold>76.83</bold>",
                "10.63",
                "<bold>50.89</bold>"
            ],
            [
                "BIDAF (ELMO)",
                "70.43",
                "79.76",
                "28.35",
                "49.98"
            ],
            [
                "+coverage",
                "<bold>71.07</bold>",
                "<bold>80.15</bold>",
                "<bold>30.58</bold>",
                "<bold>52.43</bold>"
            ]
        ],
        "question": "Is it true that Table 3 shows the impact of coverage for decreasing generalization across these two datasets that belong to the two similar tasks of reading comprehension and QA-SRL?",
        "answer_label": "no"
    },
    {
        "id": "1ad45c40-5107-4f03-9741-d43fe4bf9bb5",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 6: Results of non-fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
        "table_column_names": [
            "Model",
            "Training data",
            "Overall",
            "Easy",
            "Hard"
        ],
        "table_content_values": [
            [
                "BERT-large",
                "B-COPA",
                "70.5 (\u00b1 2.5)",
                "72.6 (\u00b1 2.3)",
                "[BOLD] 69.1 (\u00b1 2.7)"
            ],
            [
                "BERT-large",
                "B-COPA (50%)",
                "69.9 (\u00b1 1.9)",
                "71.2 (\u00b1 1.3)",
                "69.0 (\u00b1 3.5)"
            ],
            [
                "BERT-large",
                "COPA",
                "[BOLD] 71.7 (\u00b1 0.5)",
                "[BOLD] 80.5 (\u00b1 0.4)",
                "66.3 (\u00b1 0.8)"
            ],
            [
                "RoBERTa-large",
                "B-COPA",
                "[BOLD] 76.7 (\u00b1 0.8)",
                "73.3 (\u00b1 1.5)",
                "[BOLD] 78.8 (\u00b1 2.0)"
            ],
            [
                "RoBERTa-large",
                "B-COPA (50%)",
                "72.4 (\u00b1 2.0)",
                "72.1 (\u00b1 1.7)",
                "72.6 (\u00b1 2.1)"
            ],
            [
                "RoBERTa-large",
                "COPA",
                "76.4 (\u00b1 0.7)",
                "[BOLD] 79.6 (\u00b1 1.0)",
                "74.4 (\u00b1 1.1)"
            ],
            [
                "BERT-base-NSP",
                "None",
                "[BOLD] 66.4",
                "66.2",
                "[BOLD] 66.7"
            ],
            [
                "BERT-large-NSP",
                "None",
                "65.0",
                "[BOLD] 66.9",
                "62.1"
            ]
        ],
        "question": "Is it true that The relatively high accuracies of BERT-large, RoBERTa-large and BERT-*-NSP show that these pretrained models are already well-equipped to perform this task \"out-of-the-box\"?",
        "answer_label": "yes"
    },
    {
        "id": "754e6967-568c-467b-8192-79e841cef788",
        "table_caption": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance Table 4: Pearson correlation with system-level human judgments on MSCOCO dataset. \u2019M\u2019 and \u2019P\u2019 are short names.",
        "table_column_names": [
            "Setting",
            "Metric",
            "M1",
            "M2"
        ],
        "table_content_values": [
            [
                "Baselines",
                "LEIC(*)",
                "<bold>0.939</bold>",
                "<bold>0.949</bold>"
            ],
            [
                "Baselines",
                "METEOR",
                "0.606",
                "0.594"
            ],
            [
                "Baselines",
                "SPICE",
                "0.759",
                "0.750"
            ],
            [
                "Baselines",
                "BERTScore-Recall",
                "0.809",
                "0.749"
            ],
            [
                "Sent-Mover",
                "SMD + W2V",
                "0.683",
                "0.668"
            ],
            [
                "Sent-Mover",
                "SMD + ELMO + P",
                "0.709",
                "0.712"
            ],
            [
                "Sent-Mover",
                "SMD + BERT + P",
                "0.723",
                "0.747"
            ],
            [
                "Sent-Mover",
                "SMD + BERT + M + P",
                "0.789",
                "0.784"
            ],
            [
                "Word-Mover",
                "Wmd-1 + W2V",
                "0.728",
                "0.764"
            ],
            [
                "Word-Mover",
                "Wmd-1 + ELMO + P",
                "0.753",
                "0.775"
            ],
            [
                "Word-Mover",
                "Wmd-1 + BERT + P",
                "0.780",
                "0.790"
            ],
            [
                "Word-Mover",
                "Wmd-1 + BERT + M + P",
                "<bold>0.813</bold>",
                "<bold>0.810</bold>"
            ],
            [
                "Word-Mover",
                "Wmd-2 + BERT + M + P",
                "0.812",
                "0.808"
            ]
        ],
        "question": "Is it true that Table 4: Word mover metrics outperform all baselines except for the supervised metric LEIC, which uses more information by considering both images and texts?",
        "answer_label": "yes"
    },
    {
        "id": "7a5a63e4-676b-42ae-8599-591b3465f476",
        "table_caption": "Syntactic Dependency Representations in Neural Relation Classification Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
        "table_column_names": [
            "[BOLD] Representation",
            "[BOLD] Hyper parameters Filter size",
            "[BOLD] Hyper parameters Num. Feature maps",
            "[BOLD] Hyper parameters Activation func.",
            "[BOLD] Hyper parameters L2 Reg.",
            "[BOLD] Hyper parameters Learning rate",
            "[BOLD] Hyper parameters Dropout Prob.",
            "[BOLD] F1.(avg. in 5-fold) with default values",
            "[BOLD] F1.(avg. in 5-fold) with optimal values"
        ],
        "table_content_values": [
            [
                "CoNLL08",
                "4-5",
                "1000",
                "Softplus",
                "1.15e+01",
                "1.13e-03",
                "1",
                "73.34",
                "74.49"
            ],
            [
                "SB",
                "4-5",
                "806",
                "Sigmoid",
                "8.13e-02",
                "1.79e-03",
                "0.87",
                "72.83",
                "[BOLD] 75.05"
            ],
            [
                "UD v1.3",
                "5",
                "716",
                "Softplus",
                "1.66e+00",
                "9.63E-04",
                "1",
                "68.93",
                "69.57"
            ]
        ],
        "question": "Is it true that We see that the optimized parameter settings are consistent across the different representations, showing that tuning is not necessary for these types of comparisons?",
        "answer_label": "no"
    },
    {
        "id": "c2398213-ef73-4862-8f2d-48b607c14e26",
        "table_caption": "Recent Advances in Natural Language Inference:A Survey of Benchmarks, Resources, and Approaches Table 2: Comparison of exact-match accuracy achieved on selected benchmarks by a random or majority-choice baseline, various neural contextual embedding models, and humans. ELMo refers to the highest-performing listed approach using ELMo embeddings. Best system performance on each benchmark in bold. Information extracted from leaderboards (linked to in the first column) at time of writing (October 2019), and original papers for benchmarks introduced in Section\u00a02.",
        "table_column_names": [
            "[BOLD] Benchmark",
            "[BOLD]  Simple Baseline ",
            "[BOLD] ELMo",
            "[BOLD] GPT",
            "[BOLD] BERT",
            "[BOLD] MT-DNN",
            "[BOLD] XLNet",
            "[BOLD] RoBERTa",
            "[BOLD] ALBERT",
            "[BOLD] Human"
        ],
        "table_content_values": [
            [
                "[BOLD] CLOTH",
                "25.0",
                "70.7",
                "\u2013",
                "[BOLD] 86.0",
                "\u2013",
                "\u2013",
                "\u2013",
                "\u2013",
                "85.9"
            ],
            [
                "[BOLD] Cosmos QA",
                "\u2013",
                "\u2013",
                "54.5",
                "67.1",
                "\u2013",
                "\u2013",
                "\u2013",
                "\u2013",
                "94.0"
            ],
            [
                "[BOLD] DREAM",
                "33.4",
                "59.5",
                "55.5",
                "66.8",
                "\u2013",
                "[BOLD] 72.0",
                "\u2013",
                "\u2013",
                "95.5"
            ],
            [
                "[BOLD] GLUE",
                "\u2013",
                "70.0",
                "\u2013",
                "80.5",
                "87.6",
                "88.4",
                "88.5",
                "[BOLD] 89.4",
                "87.1"
            ],
            [
                "[BOLD] HellaSWAG",
                "25.0",
                "33.3",
                "41.7",
                "47.3",
                "\u2013",
                "\u2013",
                "[BOLD] 85.2",
                "[EMPTY]",
                "95.6"
            ],
            [
                "[BOLD] MC-TACO",
                "17.4",
                "26.4",
                "\u2013",
                "42.7",
                "\u2013",
                "\u2013",
                "[BOLD] 43.6",
                "\u2013",
                "75.8"
            ],
            [
                "[BOLD] RACE",
                "24.9",
                "\u2013",
                "59.0",
                "72.0",
                "\u2013",
                "81.8",
                "83.2",
                "[BOLD] 89.4",
                "94.5"
            ],
            [
                "[BOLD] SciTail",
                "60.3",
                "\u2013",
                "88.3",
                "\u2013",
                "94.1",
                "\u2013",
                "\u2013",
                "\u2013",
                "\u2013"
            ],
            [
                "[BOLD] SQuAD 1.1",
                "1.3",
                "81.0",
                "\u2013",
                "87.4",
                "\u2013",
                "[BOLD] 89.9",
                "\u2013",
                "\u2013",
                "82.3"
            ],
            [
                "[BOLD] SQuAD 2.0",
                "48.9",
                "63.4",
                "\u2013",
                "80.8",
                "\u2013",
                "86.3",
                "86.8",
                "[BOLD] 89.7",
                "86.9"
            ],
            [
                "[BOLD] SuperGLUE",
                "47.1",
                "\u2013",
                "\u2013",
                "69.0",
                "\u2013",
                "\u2013",
                "[BOLD] 84.6",
                "\u2013",
                "89.8"
            ],
            [
                "[BOLD] SWAG",
                "25.0",
                "59.1",
                "78.0",
                "86.3",
                "87.1",
                "\u2013",
                "[BOLD] 89.9",
                "\u2013",
                "88.0"
            ]
        ],
        "question": "Is it true that The most representative models are only BERT and its variants?",
        "answer_label": "no"
    },
    {
        "id": "e752385e-f0a0-4fa9-b573-8198a4d3bf24",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that The relative improvement averaged over all tasks is 8%?",
        "answer_label": "yes"
    },
    {
        "id": "8b062e9b-8f83-4dd6-b370-1a476c743858",
        "table_caption": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] ROUGE-1  [BOLD] R (%)",
            "[BOLD] ROUGE-1  [BOLD] P (%)",
            "[BOLD] ROUGE-1  [BOLD] F (%)",
            "[BOLD] ROUGE-2  [BOLD] R (%)",
            "[BOLD] ROUGE-2  [BOLD] P (%)",
            "[BOLD] ROUGE-2  [BOLD] F (%)",
            "[BOLD] Sentence-Level  [BOLD] R (%)",
            "[BOLD] Sentence-Level  [BOLD] P (%)",
            "[BOLD] Sentence-Level  [BOLD] F (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] ILP",
                "24.5",
                "41.1",
                "29.3\u00b10.5",
                "7.9",
                "15.0",
                "9.9\u00b10.5",
                "13.6",
                "22.6",
                "15.6\u00b10.4"
            ],
            [
                "[BOLD] Sum-Basic",
                "28.4",
                "44.4",
                "33.1\u00b10.5",
                "8.5",
                "15.6",
                "10.4\u00b10.4",
                "14.7",
                "22.9",
                "16.7\u00b10.5"
            ],
            [
                "[BOLD] KL-Sum",
                "39.5",
                "34.6",
                "35.5\u00b10.5",
                "13.0",
                "12.7",
                "12.3\u00b10.5",
                "15.2",
                "21.1",
                "16.3\u00b10.5"
            ],
            [
                "[BOLD] LexRank",
                "42.1",
                "39.5",
                "38.7\u00b10.5",
                "14.7",
                "15.3",
                "14.2\u00b10.5",
                "14.3",
                "21.5",
                "16.0\u00b10.5"
            ],
            [
                "[BOLD] MEAD",
                "45.5",
                "36.5",
                "38.5\u00b1 0.5",
                "17.9",
                "14.9",
                "15.4\u00b10.5",
                "27.8",
                "29.2",
                "26.8\u00b10.5"
            ],
            [
                "[BOLD] SVM",
                "19.0",
                "48.8",
                "24.7\u00b10.8",
                "7.5",
                "21.1",
                "10.0\u00b10.5",
                "32.7",
                "34.3",
                "31.4\u00b10.4"
            ],
            [
                "[BOLD] LogReg",
                "26.9",
                "34.5",
                "28.7\u00b10.6",
                "6.4",
                "9.9",
                "7.3\u00b10.4",
                "12.2",
                "14.9",
                "12.7\u00b10.5"
            ],
            [
                "[BOLD] LogReg [ITALIC] r",
                "28.0",
                "34.8",
                "29.4\u00b10.6",
                "6.9",
                "10.4",
                "7.8\u00b10.4",
                "12.1",
                "14.5",
                "12.5\u00b10.5"
            ],
            [
                "[BOLD] HAN",
                "31.0",
                "42.8",
                "33.7\u00b10.7",
                "11.2",
                "17.8",
                "12.7\u00b10.5",
                "26.9",
                "34.1",
                "32.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT",
                "32.2",
                "42.4",
                "34.4\u00b10.7",
                "11.5",
                "17.5",
                "12.9\u00b10.5",
                "29.6",
                "35.8",
                "32.2\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU",
                "32.1",
                "42.1",
                "33.8\u00b10.7",
                "11.6",
                "17.6",
                "12.9\u00b10.5",
                "30.1",
                "35.6",
                "32.3\u00b10.5"
            ],
            [
                "[BOLD] HAN [ITALIC] r",
                "38.1",
                "40.5",
                "[BOLD] 37.8\u00b10.5",
                "14.0",
                "17.1",
                "[BOLD] 14.7\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.5",
                "16.8",
                "[BOLD] 14.4\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.6",
                "16.9",
                "[BOLD] 14.4\u00b10.5",
                "33.9",
                "33.8",
                "[BOLD] 33.8\u00b10.5"
            ]
        ],
        "question": "Is it true that The HAN models outperform MEAD in terms of sentence prediction?",
        "answer_label": "yes"
    },
    {
        "id": "782d2043-8a43-4dc8-9989-3d1e544a66c8",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that Despite joint training, our hybrid model does not learn to pick up the best features from CBOW and CMOW simultaneously?",
        "answer_label": "no"
    },
    {
        "id": "f161e252-7353-46ef-97f7-408e240403cf",
        "table_caption": "Towards Scalable and Reliable Capsule Networksfor Challenging NLP Applications Table 2: Comparisons of our NLP-Cap approach and baselines on two text classification benchmarks, where \u2019-\u2019 denotes methods that failed to scale due to memory issues.",
        "table_column_names": [
            "<bold>Datasets</bold>",
            "<bold>Metrics</bold>",
            "<bold>FastXML</bold>",
            "<bold>PD-Sparse</bold>",
            "<bold>FastText</bold>",
            "<bold>Bow-CNN</bold>",
            "<bold>CNN-Kim</bold>",
            "<bold>XML-CNN</bold>",
            "<bold>Cap-Zhao</bold>",
            "<bold>NLP-Cap</bold>",
            "<bold>Impv</bold>"
        ],
        "table_content_values": [
            [
                "RCV1",
                "PREC@1",
                "94.62",
                "95.16",
                "95.40",
                "96.40",
                "93.54",
                "96.86",
                "96.63",
                "<bold>97.05</bold>",
                "+0.20%"
            ],
            [
                "RCV1",
                "PREC@3",
                "78.40",
                "79.46",
                "79.96",
                "81.17",
                "76.15",
                "81.11",
                "81.02",
                "<bold>81.27</bold>",
                "+0.20%"
            ],
            [
                "RCV1",
                "PREC@5",
                "54.82",
                "55.61",
                "55.64",
                "<bold>56.74</bold>",
                "52.94",
                "56.07",
                "56.12",
                "56.33",
                "-0.72%"
            ],
            [
                "[EMPTY]",
                "NDCG@1",
                "94.62",
                "95.16",
                "95.40",
                "96.40",
                "93.54",
                "96.88",
                "96.63",
                "<bold>97.05</bold>",
                "+0.20%"
            ],
            [
                "[EMPTY]",
                "NDCG@3",
                "89.21",
                "90.29",
                "90.95",
                "92.04",
                "87.26",
                "92.22",
                "92.31",
                "<bold>92.47</bold>",
                "+0.17%"
            ],
            [
                "[EMPTY]",
                "NDCG@5",
                "90.27",
                "91.29",
                "91.68",
                "92.89",
                "88.20",
                "92.63",
                "92.75",
                "<bold>93.11</bold>",
                "+0.52%"
            ],
            [
                "EUR-Lex",
                "PREC@1",
                "68.12",
                "72.10",
                "71.51",
                "64.99",
                "68.35",
                "75.65",
                "-",
                "<bold>80.20</bold>",
                "+6.01%"
            ],
            [
                "EUR-Lex",
                "PREC@3",
                "57.93",
                "57.74",
                "60.37",
                "51.68",
                "54.45",
                "61.81",
                "-",
                "<bold>65.48</bold>",
                "+5.93%"
            ],
            [
                "EUR-Lex",
                "PREC@5",
                "48.97",
                "47.48",
                "50.41",
                "42.32",
                "44.07",
                "50.90",
                "-",
                "<bold>52.83</bold>",
                "+3.79%"
            ],
            [
                "[EMPTY]",
                "NDCG@1",
                "68.12",
                "72.10",
                "71.51",
                "64.99",
                "68.35",
                "75.65",
                "-",
                "<bold>80.20</bold>",
                "+6.01%"
            ],
            [
                "[EMPTY]",
                "NDCG@3",
                "60.66",
                "61.33",
                "63.32",
                "55.03",
                "59.81",
                "66.71",
                "-",
                "<bold>71.11</bold>",
                "+6.59%"
            ],
            [
                "[EMPTY]",
                "NDCG@5",
                "56.42",
                "55.93",
                "58.56",
                "49.92",
                "57.99",
                "64.45",
                "-",
                "<bold>68.80</bold>",
                "+6.75%"
            ]
        ],
        "question": "Is it true that In Table 2, we can see that our capsule-based approach does not bring a noticeable margin over the strong baselines on EUR-Lex, and only competitive results on RCV1?",
        "answer_label": "no"
    },
    {
        "id": "6d1edb41-733b-4b4f-9eee-201b52781e80",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 2: Experiment 1",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.001",
                "0.003",
                "-20.818",
                "***",
                "0.505"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.083",
                "0.048",
                "101.636",
                "***",
                "1.724"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.001",
                "0.001",
                "0.035",
                "[EMPTY]",
                "1.001"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.023",
                "0.012",
                "64.418",
                "***",
                "1.993"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.002",
                "0.001",
                "4.047",
                "***",
                "1.120"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.049",
                "0.019",
                "120.986",
                "***",
                "2.573"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.173",
                "0.065",
                "243.285",
                "***",
                "2.653"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.032",
                "0.023",
                "39.483",
                "***",
                "1.396"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.111",
                "0.061",
                "122.707",
                "***",
                "1.812"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.178",
                "0.080",
                "211.319",
                "***",
                "2.239"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.028",
                "0.015",
                "63.131",
                "***",
                "1.854"
            ]
        ],
        "question": "Is it true that In most cases the racial disparities persist, although they are generally smaller in magnitude and in some cases the direction even changes?",
        "answer_label": "yes"
    },
    {
        "id": "3b3489c6-cb2f-4a1a-b462-ec4844bcfbf2",
        "table_caption": "Giving Attention to the Unexpected:Using Prosody Innovations in Disfluency Detection Table 4: F1 scores on disfluency detection when using a single set of features (text-only, raw prosody features or innovation features), with early fusion and late fusion. \u201cRaw\u201d indicates the usage of original prosodic features (Section 3.2), while \u201cinnovations\u201d indicate the usage of innovation features (Section 3.3).",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Model",
            "[BOLD] dev mean",
            "[BOLD] dev best",
            "[BOLD] test mean",
            "[BOLD] test best",
            "[ITALIC] \u03b1"
        ],
        "table_content_values": [
            [
                "single",
                "text",
                "86.54",
                "86.80",
                "86.47",
                "86.96",
                "\u2013"
            ],
            [
                "single",
                "raw",
                "35.00",
                "37.33",
                "35.78",
                "37.70",
                "\u2013"
            ],
            [
                "single",
                "innovations",
                "80.86",
                "81.51",
                "80.28",
                "82.15",
                "\u2013"
            ],
            [
                "early",
                "text + raw",
                "86.46",
                "86.65",
                "86.24",
                "86.53",
                "\u2013"
            ],
            [
                "early",
                "text + innovations",
                "86.53",
                "86.77",
                "86.54",
                "87.00",
                "\u2013"
            ],
            [
                "early",
                "text + raw + innovations",
                "86.35",
                "86.69",
                "86.55",
                "86.44",
                "\u2013"
            ],
            [
                "late",
                "text + raw",
                "86.71",
                "87.05",
                "86.35",
                "86.71",
                "0.2"
            ],
            [
                "late",
                "text + innovations",
                "[BOLD] 86.98",
                "[BOLD] 87.48",
                "[BOLD] 86.68",
                "[BOLD] 87.02",
                "0.5"
            ],
            [
                "late",
                "text + raw + innovations",
                "86.95",
                "87.30",
                "86.60",
                "86.87",
                "0.5"
            ]
        ],
        "question": "Is it true that The interpolation weight \u03b1 for the late fusion experiments is low when innovations are used, which further indicates that innovation features are not useful in overall prediction?",
        "answer_label": "no"
    },
    {
        "id": "c3215ba9-f925-4d0e-9090-74e533d5c180",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.",
        "table_column_names": [
            "[EMPTY]",
            "Italian Original",
            "Italian Debiased",
            "Italian English",
            "Italian Reduction",
            "German Original",
            "German Debiased",
            "German English",
            "German Reduction"
        ],
        "table_content_values": [
            [
                "Same Gender",
                "0.442",
                "0.434",
                "0.424",
                "\u2013",
                "0.491",
                "0.478",
                "0.446",
                "\u2013"
            ],
            [
                "Different Gender",
                "0.385",
                "0.421",
                "0.415",
                "\u2013",
                "0.415",
                "0.435",
                "0.403",
                "\u2013"
            ],
            [
                "difference",
                "0.057",
                "0.013",
                "0.009",
                "[BOLD] 91.67%",
                "0.076",
                "0.043",
                "0.043",
                "[BOLD] 100%"
            ]
        ],
        "question": "Is it true that As expected, in both languages, the difference between the average of the two sets with the debiased embeddings is much higher?",
        "answer_label": "no"
    },
    {
        "id": "a5f84823-7cbf-4c56-9c4c-a5c245145720",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that On the same dataset, our results are not as competitive as Damonte and Cohen (2019)?",
        "answer_label": "no"
    },
    {
        "id": "ce51a0ff-e0da-4d8c-84db-e4cabdbaac82",
        "table_caption": "Deriving Machine Attention from Human Rationales Table 3: Accuracy of transferring between aspects. Models with \u2020 use labeled data from source aspects. Models with \u2021 use human rationales on the target aspect.",
        "table_column_names": [
            "Source",
            "Target",
            "Svm",
            "Ra-Svm\u2021",
            "Ra-Cnn\u2021",
            "Trans\u2020",
            "Ra-Trans\u2021\u2020",
            "Ours\u2021\u2020",
            "Oracle\u2020"
        ],
        "table_content_values": [
            [
                "Beer aroma+palate",
                "Beer look",
                "74.41",
                "74.83",
                "74.94",
                "72.75",
                "76.41",
                "[BOLD] 79.53",
                "80.29"
            ],
            [
                "Beer look+palate",
                "Beer aroma",
                "68.57",
                "69.23",
                "67.55",
                "69.92",
                "76.45",
                "[BOLD] 77.94",
                "78.11"
            ],
            [
                "Beer look+aroma",
                "Beer palate",
                "63.88",
                "67.82",
                "65.72",
                "74.66",
                "73.40",
                "[BOLD] 75.24",
                "75.50"
            ]
        ],
        "question": "Is it true that It does not match the performance of ORACLE, with a difference of up to 6.29% absolute difference?",
        "answer_label": "no"
    },
    {
        "id": "eb9d6e8f-d389-49e1-a146-61202625fda6",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "36.85",
                "5.3782",
                "35.14",
                "55.01",
                "1.6016",
                "00.34",
                "09.81",
                "00.15",
                "10.31"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen",
                "39.23",
                "6.0217",
                "36.97",
                "55.52",
                "1.7623",
                "00.40",
                "03.59",
                "00.07",
                "04.05"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen+",
                "40.25",
                "6.1448",
                "37.50",
                "56.19",
                "1.8181",
                "00.21",
                "01.99",
                "00.05",
                "02.24"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.88",
                "3.9310",
                "32.11",
                "39.90",
                "0.5036",
                "07.73",
                "17.76",
                "09.52",
                "35.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.19",
                "6.0543",
                "37.38",
                "55.88",
                "1.8104",
                "00.17",
                "01.31",
                "00.25",
                "01.72"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen",
                "40.73",
                "6.1711",
                "37.76",
                "56.09",
                "1.8518",
                "00.07",
                "00.72",
                "00.08",
                "00.87"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen+",
                "40.51",
                "6.1226",
                "37.61",
                "55.98",
                "1.8286",
                "00.02",
                "00.63",
                "00.06",
                "00.70"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.66",
                "3.9511",
                "32.93",
                "39.29",
                "0.3855",
                "07.89",
                "15.60",
                "08.44",
                "31.94"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.48",
                "6.0269",
                "37.26",
                "56.19",
                "1.7999",
                "00.43",
                "02.84",
                "00.26",
                "03.52"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen",
                "41.57",
                "6.2830",
                "37.99",
                "56.36",
                "1.8849",
                "00.37",
                "01.40",
                "00.09",
                "01.86"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen+",
                "41.56",
                "6.2700",
                "37.94",
                "56.38",
                "1.8827",
                "00.21",
                "01.04",
                "00.07",
                "01.31"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "35.99",
                "5.0734",
                "34.74",
                "54.79",
                "1.5259",
                "00.02",
                "11.58",
                "00.02",
                "11.62"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen",
                "40.07",
                "6.1243",
                "37.45",
                "55.81",
                "1.8026",
                "00.05",
                "03.23",
                "00.01",
                "03.29"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen+",
                "40.80",
                "6.2197",
                "37.86",
                "56.13",
                "1.8422",
                "00.01",
                "01.87",
                "00.01",
                "01.88"
            ]
        ],
        "question": "Is it true that The results in the table suggest that cleaning the missing slots did not provide more complex training examples?",
        "answer_label": "no"
    },
    {
        "id": "2eaca02a-6756-46bf-99d2-d597218b717d",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that This indicates that GINs cannot be employed in tasks where the distribution of node degrees has a long tail?",
        "answer_label": "no"
    },
    {
        "id": "fb295289-5470-4bd0-99a4-18c93946d800",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that The coverage mechanism is also effective in our models?",
        "answer_label": "yes"
    },
    {
        "id": "1818df5d-04f4-44b0-8e69-fd87724f010c",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 7: Sensitivity of BERT-large to superficial cues identified in \u00a72 (unit: 10\u22122). Cues with top-5 reduction are shown. SCOPA,SB_COPA indicate the mean contributions of BERT-large trained on COPA, and BERT-large trained on B-COPA, respectively.",
        "table_column_names": [
            "Cue",
            "[ITALIC] SCOPA",
            "[ITALIC] SB_COPA",
            "Diff.",
            "Prod."
        ],
        "table_content_values": [
            [
                "woman",
                "7.98",
                "4.84",
                "-3.14",
                "0.25"
            ],
            [
                "mother",
                "5.16",
                "3.95",
                "-1.21",
                "0.75"
            ],
            [
                "went",
                "6.00",
                "5.15",
                "-0.85",
                "0.73"
            ],
            [
                "down",
                "5.52",
                "4.93",
                "-0.58",
                "0.71"
            ],
            [
                "into",
                "4.07",
                "3.51",
                "-0.56",
                "0.40"
            ]
        ],
        "question": "Is it true that We observe that BERT trained on Balanced COPA is less sensitive to a few highly productive superficial cues than BERT trained on original COPA?",
        "answer_label": "yes"
    },
    {
        "id": "d588b47d-e4c7-49e6-8f76-0c5678b232ea",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
        "table_column_names": [
            "Model",
            "Training data",
            "Overall",
            "Easy",
            "Hard"
        ],
        "table_content_values": [
            [
                "BERT-large-FT",
                "B-COPA",
                "74.5 (\u00b1 0.7)",
                "74.7 (\u00b1 0.4)",
                "[BOLD] 74.4 (\u00b1 0.9)"
            ],
            [
                "BERT-large-FT",
                "B-COPA (50%)",
                "74.3 (\u00b1 2.2)",
                "76.8 (\u00b1 1.9)",
                "72.8 (\u00b1 3.1)"
            ],
            [
                "BERT-large-FT",
                "COPA",
                "[BOLD] 76.5 (\u00b1 2.7)",
                "[BOLD] 83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA",
                "[BOLD] 89.0 (\u00b1 0.3)",
                "88.9 (\u00b1 2.1)",
                "[BOLD] 89.0 (\u00b1 0.8)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA (50%)",
                "86.1 (\u00b1 2.2)",
                "87.4 (\u00b1 1.1)",
                "85.4 (\u00b1 2.9)"
            ],
            [
                "RoBERTa-large-FT",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "[BOLD] 91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)"
            ]
        ],
        "question": "Is it true that Moreover, training on B-COPA improves performance on the Hard subset, both when training with all 1000 instances in B-COPA, and when matching the training size of the original COPA (500 instances, B-COPA 50%)?",
        "answer_label": "yes"
    },
    {
        "id": "a2014e4b-6a45-4619-8130-4052f27e2307",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE VIII: Precision scores for the Semantic Analogy Test",
        "table_column_names": [
            "Questions Subset",
            "# of Questions Seen",
            "GloVe",
            "Word2Vec",
            "Proposed"
        ],
        "table_content_values": [
            [
                "All",
                "8783",
                "78.94",
                "81.03",
                "79.96"
            ],
            [
                "At least one",
                "1635",
                "67.58",
                "70.89",
                "67.89"
            ],
            [
                "concept word",
                "1635",
                "67.58",
                "70.89",
                "67.89"
            ],
            [
                "All concept words",
                "110",
                "77.27",
                "89.09",
                "83.64"
            ]
        ],
        "question": "Is it true that However, the greatest performance increase is not seen for the last scenario, which suggests that the semantic features captured by embeddings cannot be improved with a reasonable selection of the lexical resource from which the concept wordgroups were derived?",
        "answer_label": "no"
    },
    {
        "id": "8b27ad92-0b0c-4813-94fc-d817b8f9837d",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 7: Complaint prediction results using the original data set and distantly supervised data. All models are based on logistic regression with bag-of-word and Part-of-Speech tag features.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Acc",
            "[BOLD] F1",
            "[BOLD] AUC"
        ],
        "table_content_values": [
            [
                "Most Frequent Class",
                "64.2",
                "39.1",
                "0.500"
            ],
            [
                "LR-All Features \u2013 Original Data",
                "80.5",
                "78.0",
                "0.873"
            ],
            [
                "Dist. Supervision + Pooling",
                "77.2",
                "75.7",
                "0.853"
            ],
            [
                "Dist. Supervision + EasyAdapt",
                "[BOLD] 81.2",
                "[BOLD] 79.0",
                "[BOLD] 0.885"
            ]
        ],
        "question": "Is it true that [CONTINUE] However, simply pooling the data actually hurts predictive performance leading to a drop of more than 2 points in F1?",
        "answer_label": "yes"
    },
    {
        "id": "6580de06-3999-4498-95f2-ca6c5250638a",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
        "table_column_names": [
            "[BOLD] Dataset",
            "[BOLD] # pairs",
            "[BOLD] # words (doc)",
            "[BOLD] # sents (docs)",
            "[BOLD] # words (summary)",
            "[BOLD] # sents (summary)",
            "[BOLD] vocab size"
        ],
        "table_content_values": [
            [
                "Multi-News",
                "44,972/5,622/5,622",
                "2,103.49",
                "82.73",
                "263.66",
                "9.97",
                "666,515"
            ],
            [
                "DUC03+04",
                "320",
                "4,636.24",
                "173.15",
                "109.58",
                "2.88",
                "19,734"
            ],
            [
                "TAC 2011",
                "176",
                "4,695.70",
                "188.43",
                "99.70",
                "1.00",
                "24,672"
            ],
            [
                "CNNDM",
                "287,227/13,368/11,490",
                "810.57",
                "39.78",
                "56.20",
                "3.68",
                "717,951"
            ]
        ],
        "question": "Is it true that The total number of words in the concatenated inputs is shorter than other MDS datasets, as those consist of 10 input documents, but larger than SDS datasets, as expected?",
        "answer_label": "yes"
    },
    {
        "id": "74ffa35f-85bf-4c65-9d6a-082216a84d24",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1192",
                "0.0083",
                "0.0137",
                "0.0150",
                "0.0150",
                "0.0445",
                "0.0326"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1022",
                "0.0069",
                "0.0060",
                "0.0092",
                "0.0090",
                "0.0356",
                "0.0162"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5710",
                "0.1948",
                "0.3855",
                "0.5474",
                "0.4485",
                "[BOLD] 0.8052",
                "0.4058"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "[BOLD] 0.6304",
                "0.1870",
                "0.3250",
                "0.5312",
                "0.4576",
                "0.6064",
                "0.3698"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0037",
                "0.3278",
                "0.5941",
                "0.6486",
                "[BOLD] 0.6490",
                "0.0017",
                "0.0003"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0002",
                "0.1486",
                "0.4332",
                "[BOLD] 0.6467",
                "0.6332",
                "0.0967",
                "0.0003"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0002",
                "0.1562",
                "0.5157",
                "[BOLD] 0.7255",
                "0.5932",
                "0.0032",
                "0.0001"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "2.10-5",
                "0.0507",
                "0.4492",
                "[BOLD] 0.7000",
                "0.5887",
                "0.1390",
                "0.0002"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0073",
                "0.0162",
                "0.0268",
                "[BOLD] 0.0293",
                "[BOLD] 0.0293",
                "0.0033",
                "0.0006"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0004",
                "0.0132",
                "0.0118",
                "0.0181",
                "0.0179",
                "[BOLD] 0.0520",
                "0.0005"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0005",
                "0.1733",
                "0.4412",
                "[BOLD] 0.6240",
                "0.5109",
                "0.0064",
                "0.0002"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "4.10-5",
                "0.0798",
                "0.3771",
                "[BOLD] 0.6040",
                "0.5149",
                "0.2261",
                "0.0004"
            ]
        ],
        "question": "Is it true that When increasing the number of terms to 10,000, the DocSub models using TED Talks corpora performed better than when using Europarl corpora?",
        "answer_label": "no"
    },
    {
        "id": "76767541-0820-44a2-9a66-24bea826ecca",
        "table_caption": "Aligning Vector-spaces with Noisy Supervised Lexicons Table 1: Bilingual Experiment P@1. Numbers are based on 10 runs of each method. The En\u2192De, En\u2192Fi and En\u2192Es improvements are significant at p<0.05 according to ANOVA on the different runs.",
        "table_column_names": [
            "Method",
            "En\u2192It best",
            "En\u2192It avg",
            "En\u2192It iters",
            "En\u2192De best",
            "En\u2192De avg",
            "En\u2192De iters",
            "En\u2192Fi best",
            "En\u2192Fi avg",
            "En\u2192Fi iters",
            "En\u2192Es best",
            "En\u2192Es avg",
            "En\u2192Es iters"
        ],
        "table_content_values": [
            [
                "Artetxe et\u00a0al., 2018b",
                "[BOLD] 48.53",
                "48.13",
                "573",
                "48.47",
                "48.19",
                "773",
                "33.50",
                "32.63",
                "988",
                "37.60",
                "37.33",
                "808"
            ],
            [
                "Noise-aware Alignment",
                "[BOLD] 48.53",
                "[BOLD] 48.20",
                "471",
                "[BOLD] 49.67",
                "[BOLD] 48.89",
                "568",
                "[BOLD] 33.98",
                "[BOLD] 33.68",
                "502",
                "[BOLD] 38.40",
                "[BOLD] 37.79",
                "551"
            ]
        ],
        "question": "Is it true that Our model improves the results in the translation tasks?",
        "answer_label": "yes"
    },
    {
        "id": "41907607-9691-409a-9a0a-517d74061500",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "-{4} dense block",
                "24.8",
                "54.9"
            ],
            [
                "-{3, 4} dense blocks",
                "23.8",
                "54.1"
            ],
            [
                "-{2, 3, 4} dense blocks",
                "23.2",
                "53.1"
            ]
        ],
        "question": "Is it true that The full model does not give the best performance on the AMR15 dev set?",
        "answer_label": "no"
    },
    {
        "id": "050de577-add7-44d3-9d2c-7892c25a8464",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
        "table_column_names": [
            "Model",
            "Answerability Macro P/R/F",
            "# Answerable",
            "Answer Prec.",
            "Derivation Prec. RG-L (P/R/F)",
            "Derivation Prec. BL-4"
        ],
        "table_content_values": [
            [
                "Shortest Path",
                "54.8/55.5/53.2",
                "976",
                "3.6",
                "56.7/38.5/41.5",
                "31.3"
            ],
            [
                "PRKGC",
                "52.6/51.5/50.7",
                "1,021",
                "45.2",
                "40.7/60.7/44.7",
                "30.9"
            ],
            [
                "PRKGC+NS",
                "53.6/54.1/52.1",
                "980",
                "45.4",
                "42.2/61.6/46.1",
                "33.4"
            ]
        ],
        "question": "Is it true that [CONTINUE] Although the PRKGC model do not receive supervision about human-generated NLDs, paths with the maximum score match human-generated NLDs to some extent?",
        "answer_label": "yes"
    },
    {
        "id": "e37522d9-013e-410c-bf5b-57c5a6ad3d60",
        "table_caption": "Semantic Neural Machine Translation using AMR Table 4: BLEU scores of Dual2seq on the little prince data, when gold or automatic AMRs are available.",
        "table_column_names": [
            "AMR Anno.",
            "BLEU"
        ],
        "table_content_values": [
            [
                "Automatic",
                "16.8"
            ],
            [
                "Gold",
                "[BOLD] *17.5*"
            ]
        ],
        "question": "Is it true that The improvement from automatic AMR to gold AMR (+0.7 BLEU) is not significant, which shows that the translation quality of our model cannot be further improved with an increase of AMR parsing accuracy?",
        "answer_label": "no"
    },
    {
        "id": "de8ef478-5e2b-4df6-90f3-1e34c08c069a",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "-{4} dense block",
                "24.8",
                "54.9"
            ],
            [
                "-{3, 4} dense blocks",
                "23.8",
                "54.1"
            ],
            [
                "-{2, 3, 4} dense blocks",
                "23.2",
                "53.1"
            ]
        ],
        "question": "Is it true that In general, the performance of the model drops substantially as we remove more dense connections until it cannot converge without dense connections?",
        "answer_label": "yes"
    },
    {
        "id": "67898573-d093-4de1-90ce-1a972b3794a9",
        "table_caption": "Language Independent Sequence Labelling for Opinion Target Extraction Table 6: ABSA SemEval 2016: Comparison of multilingual results in terms of F1 scores.",
        "table_column_names": [
            "Language",
            "System",
            "F1"
        ],
        "table_content_values": [
            [
                "es",
                "GTI",
                "68.51"
            ],
            [
                "es",
                "L +  [BOLD] CW600 + W2VW300",
                "[BOLD] 69.92"
            ],
            [
                "es",
                "Baseline",
                "51.91"
            ],
            [
                "fr",
                "IIT-T",
                "66.67"
            ],
            [
                "fr",
                "L +  [BOLD] CW100",
                "[BOLD] 69.50"
            ],
            [
                "fr",
                "Baseline",
                "45.45"
            ],
            [
                "nl",
                "IIT-T",
                "56.99"
            ],
            [
                "nl",
                "L +  [BOLD] W2VW400",
                "[BOLD] 66.39"
            ],
            [
                "nl",
                "Baseline",
                "50.64"
            ],
            [
                "ru",
                "Danii.",
                "33.47"
            ],
            [
                "ru",
                "L +  [BOLD] CW500",
                "[BOLD] 65.53"
            ],
            [
                "ru",
                "Baseline",
                "49.31"
            ],
            [
                "tr",
                "L +  [BOLD] BW",
                "[BOLD] 60.22"
            ],
            [
                "tr",
                "Baseline",
                "41.86"
            ]
        ],
        "question": "Is it true that Table 6 shows that our system does not outperform the best previous approaches across the five languages?",
        "answer_label": "no"
    },
    {
        "id": "0945c766-61d1-41b1-96ea-8577746e2651",
        "table_caption": "Unsupervised Evaluation Metrics and Learning Criteriafor Non-Parallel Textual Transfer Table 4: Manual evaluation results (%) using models from Table\u00a02 (i.e., with roughly fixed Acc). > means \u201cbetter than\u201d. \u0394Sim=Sim(A)\u2212Sim(B), and \u0394PP=PP(A)\u2212PP(B) (note that lower PP generally means better fluency). Each row uses at least 120 sentence pairs. A cell is bold if it represents a model win of at least 10%.",
        "table_column_names": [
            "Dataset",
            "Models A",
            "Models B",
            "Transfer quality A>B",
            "Transfer quality B>A",
            "Transfer quality Tie",
            "Semantic preservation A>B",
            "Semantic preservation B>A",
            "Semantic preservation Tie",
            "Semantic preservation \u0394Sim",
            "Fluency A>B",
            "Fluency B>A",
            "Fluency Tie",
            "Fluency \u0394PP"
        ],
        "table_content_values": [
            [
                "[EMPTY]",
                "M0",
                "M2",
                "9.0",
                "6.0",
                "85.1",
                "1.5",
                "[BOLD] 25.4",
                "73.1",
                "-0.05",
                "10.4",
                "[BOLD] 23.9",
                "65.7",
                "0.9"
            ],
            [
                "Yelp",
                "M0",
                "M7",
                "9.6",
                "14.7",
                "75.8",
                "2.5",
                "[BOLD] 54.5",
                "42.9",
                "-0.09",
                "4.6",
                "[BOLD] 39.4",
                "56.1",
                "8.3"
            ],
            [
                "Yelp",
                "M6",
                "M7",
                "13.7",
                "11.6",
                "74.7",
                "16.0",
                "16.7",
                "67.4",
                "0.01",
                "10.3",
                "20.0",
                "69.7",
                "14.3"
            ],
            [
                "[EMPTY]",
                "M2",
                "M7",
                "5.8",
                "9.3",
                "84.9",
                "8.1",
                "[BOLD] 25.6",
                "66.3",
                "-0.04",
                "14.0",
                "[BOLD] 26.7",
                "59.3",
                "7.4"
            ],
            [
                "Literature",
                "M2",
                "M6",
                "4.2",
                "6.7",
                "89.2",
                "16.7",
                "20.8",
                "62.5",
                "0.01",
                "[BOLD] 40.8",
                "13.3",
                "45.8",
                "-13.3"
            ],
            [
                "Literature",
                "M6",
                "M7",
                "15.8",
                "13.3",
                "70.8",
                "[BOLD] 25.0",
                "9.2",
                "65.8",
                "0.03",
                "14.2",
                "20.8",
                "65.0",
                "14.2"
            ]
        ],
        "question": "Is it true that For example, on Yelp, large differences in human judgments of semantic preservation (M2>M0, M7>M0, M7>M2) also show the largest differences in Sim, while M6 and M7 have very similar human judgments and very similar Sim scores?",
        "answer_label": "yes"
    },
    {
        "id": "e500eb41-5d94-4380-b1a6-2df603c661c0",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 7: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in Portuguese.",
        "table_column_names": [
            "Corpus",
            "Metric",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "Europarl",
                "TotalTerms:",
                "980",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "996",
                "1,000"
            ],
            [
                "Europarl",
                "TotalRoots:",
                "79",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "Europarl",
                "NumberRels:",
                "1,527",
                "1,031",
                "1,049",
                "1,185",
                "1,093",
                "1,644",
                "999"
            ],
            [
                "Europarl",
                "MaxDepth:",
                "19",
                "902",
                "894",
                "784",
                "849",
                "6",
                "10"
            ],
            [
                "Europarl",
                "MinDepth:",
                "1",
                "902",
                "894",
                "784",
                "849",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgDepth:",
                "9.43",
                "902",
                "894",
                "784",
                "849",
                "2.73",
                "4.29"
            ],
            [
                "Europarl",
                "DepthCohesion:",
                "2.02",
                "1",
                "1",
                "1",
                "1",
                "2.19",
                "2.33"
            ],
            [
                "Europarl",
                "MaxWidth:",
                "27",
                "3",
                "3",
                "4",
                "3",
                "201",
                "58"
            ],
            [
                "Europarl",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgWidth:",
                "1.98",
                "1.03",
                "1.05",
                "1.19",
                "1.09",
                "6.25",
                "2.55"
            ],
            [
                "TED Talks",
                "TotalTerms:",
                "296",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000"
            ],
            [
                "TED Talks",
                "TotalRoots:",
                "101",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "NumberRels:",
                "291",
                "1,045",
                "1,229",
                "3,637",
                "4,284",
                "2,875",
                "999"
            ],
            [
                "TED Talks",
                "MaxDepth:",
                "10",
                "860",
                "727",
                "388",
                "354",
                "252",
                "17"
            ],
            [
                "TED Talks",
                "MinDepth:",
                "1",
                "860",
                "727",
                "388",
                "354",
                "249",
                "1"
            ],
            [
                "TED Talks",
                "AvgDepth:",
                "3.94",
                "860",
                "727",
                "388",
                "354",
                "250.43",
                "6.16"
            ],
            [
                "TED Talks",
                "DepthCohesion:",
                "2.54",
                "1",
                "1",
                "1",
                "1",
                "1.01",
                "2.76"
            ],
            [
                "TED Talks",
                "MaxWidth:",
                "37",
                "3",
                "79",
                "18",
                "13",
                "9",
                "41"
            ],
            [
                "TED Talks",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "AvgWidth:",
                "1.79",
                "1.05",
                "1.23",
                "3.64",
                "4.29",
                "2.94",
                "2.37"
            ]
        ],
        "question": "Is it true that The results for the Portuguese corpora are quite different from the ones generated by the English corpora, with terms without relations in Patt and DocSub, and DSim, SLQS, TF and DF generating shallow taxonomies, disproving the characteristics of each method?",
        "answer_label": "no"
    },
    {
        "id": "9a9e8bed-916f-41b2-b6c7-a9163e27d1ac",
        "table_caption": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] ROUGE-1  [BOLD] R (%)",
            "[BOLD] ROUGE-1  [BOLD] P (%)",
            "[BOLD] ROUGE-1  [BOLD] F (%)",
            "[BOLD] ROUGE-2  [BOLD] R (%)",
            "[BOLD] ROUGE-2  [BOLD] P (%)",
            "[BOLD] ROUGE-2  [BOLD] F (%)",
            "[BOLD] Sentence-Level  [BOLD] R (%)",
            "[BOLD] Sentence-Level  [BOLD] P (%)",
            "[BOLD] Sentence-Level  [BOLD] F (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] ILP",
                "24.5",
                "41.1",
                "29.3\u00b10.5",
                "7.9",
                "15.0",
                "9.9\u00b10.5",
                "13.6",
                "22.6",
                "15.6\u00b10.4"
            ],
            [
                "[BOLD] Sum-Basic",
                "28.4",
                "44.4",
                "33.1\u00b10.5",
                "8.5",
                "15.6",
                "10.4\u00b10.4",
                "14.7",
                "22.9",
                "16.7\u00b10.5"
            ],
            [
                "[BOLD] KL-Sum",
                "39.5",
                "34.6",
                "35.5\u00b10.5",
                "13.0",
                "12.7",
                "12.3\u00b10.5",
                "15.2",
                "21.1",
                "16.3\u00b10.5"
            ],
            [
                "[BOLD] LexRank",
                "42.1",
                "39.5",
                "38.7\u00b10.5",
                "14.7",
                "15.3",
                "14.2\u00b10.5",
                "14.3",
                "21.5",
                "16.0\u00b10.5"
            ],
            [
                "[BOLD] MEAD",
                "45.5",
                "36.5",
                "38.5\u00b1 0.5",
                "17.9",
                "14.9",
                "15.4\u00b10.5",
                "27.8",
                "29.2",
                "26.8\u00b10.5"
            ],
            [
                "[BOLD] SVM",
                "19.0",
                "48.8",
                "24.7\u00b10.8",
                "7.5",
                "21.1",
                "10.0\u00b10.5",
                "32.7",
                "34.3",
                "31.4\u00b10.4"
            ],
            [
                "[BOLD] LogReg",
                "26.9",
                "34.5",
                "28.7\u00b10.6",
                "6.4",
                "9.9",
                "7.3\u00b10.4",
                "12.2",
                "14.9",
                "12.7\u00b10.5"
            ],
            [
                "[BOLD] LogReg [ITALIC] r",
                "28.0",
                "34.8",
                "29.4\u00b10.6",
                "6.9",
                "10.4",
                "7.8\u00b10.4",
                "12.1",
                "14.5",
                "12.5\u00b10.5"
            ],
            [
                "[BOLD] HAN",
                "31.0",
                "42.8",
                "33.7\u00b10.7",
                "11.2",
                "17.8",
                "12.7\u00b10.5",
                "26.9",
                "34.1",
                "32.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT",
                "32.2",
                "42.4",
                "34.4\u00b10.7",
                "11.5",
                "17.5",
                "12.9\u00b10.5",
                "29.6",
                "35.8",
                "32.2\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU",
                "32.1",
                "42.1",
                "33.8\u00b10.7",
                "11.6",
                "17.6",
                "12.9\u00b10.5",
                "30.1",
                "35.6",
                "32.3\u00b10.5"
            ],
            [
                "[BOLD] HAN [ITALIC] r",
                "38.1",
                "40.5",
                "[BOLD] 37.8\u00b10.5",
                "14.0",
                "17.1",
                "[BOLD] 14.7\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.5",
                "16.8",
                "[BOLD] 14.4\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.6",
                "16.9",
                "[BOLD] 14.4\u00b10.5",
                "33.9",
                "33.8",
                "[BOLD] 33.8\u00b10.5"
            ]
        ],
        "question": "Is it true that We observe that the redundancy removal step is not necessary for the HAN models to achieve outstanding results?",
        "answer_label": "no"
    },
    {
        "id": "86dee411-f4cc-4ea5-b221-e4b7a4877884",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE VI: Correlations for Word Similarity Tests",
        "table_column_names": [
            "Dataset (EN-)",
            "GloVe",
            "Word2Vec",
            "OIWE-IPG",
            "SOV",
            "SPINE",
            "Word2Sense",
            "Proposed"
        ],
        "table_content_values": [
            [
                "WS-353-ALL",
                "0.612",
                "0.7156",
                "0.634",
                "0.622",
                "0.173",
                "0.690",
                "0.657"
            ],
            [
                "SIMLEX-999",
                "0.359",
                "0.3939",
                "0.295",
                "0.355",
                "0.090",
                "0.380",
                "0.381"
            ],
            [
                "VERB-143",
                "0.326",
                "0.4430",
                "0.255",
                "0.271",
                "0.293",
                "0.271",
                "0.348"
            ],
            [
                "SimVerb-3500",
                "0.193",
                "0.2856",
                "0.184",
                "0.197",
                "0.035",
                "0.234",
                "0.245"
            ],
            [
                "WS-353-REL",
                "0.578",
                "0.6457",
                "0.595",
                "0.578",
                "0.134",
                "0.695",
                "0.619"
            ],
            [
                "RW-STANF.",
                "0.378",
                "0.4858",
                "0.316",
                "0.373",
                "0.122",
                "0.390",
                "0.382"
            ],
            [
                "YP-130",
                "0.524",
                "0.5211",
                "0.353",
                "0.482",
                "0.169",
                "0.420",
                "0.589"
            ],
            [
                "MEN-TR-3k",
                "0.710",
                "0.7528",
                "0.684",
                "0.696",
                "0.298",
                "0.769",
                "0.725"
            ],
            [
                "RG-65",
                "0.768",
                "0.8051",
                "0.736",
                "0.732",
                "0.338",
                "0.761",
                "0.774"
            ],
            [
                "MTurk-771",
                "0.650",
                "0.6712",
                "0.593",
                "0.623",
                "0.199",
                "0.665",
                "0.671"
            ],
            [
                "WS-353-SIM",
                "0.682",
                "0.7883",
                "0.713",
                "0.702",
                "0.220",
                "0.720",
                "0.720"
            ],
            [
                "MC-30",
                "0.749",
                "0.8112",
                "0.799",
                "0.726",
                "0.330",
                "0.735",
                "0.776"
            ],
            [
                "MTurk-287",
                "0.649",
                "0.6645",
                "0.591",
                "0.631",
                "0.295",
                "0.674",
                "0.634"
            ],
            [
                "Average",
                "0.552",
                "0.6141",
                "0.519",
                "0.538",
                "0.207",
                "0.570",
                "0.579"
            ]
        ],
        "question": "Is it true that It should also be noted that scores obtained by SPINE are relatively low on some tests, but still acceptable, indicating that it has achieved its interpretability performance without sacrificing its semantic functions?",
        "answer_label": "no"
    },
    {
        "id": "6fa0512b-21f4-4bd1-86eb-c649baf8805f",
        "table_caption": "Adversarial Removal of Demographic Attributes from Text Data Table 6: Accuracies of the protected attribute with different encoders.",
        "table_column_names": [
            "[EMPTY]",
            "[EMPTY]",
            "Embedding Leaky",
            "Embedding Guarded"
        ],
        "table_content_values": [
            [
                "RNN",
                "Leaky",
                "64.5",
                "67.8"
            ],
            [
                "RNN",
                "Guarded",
                "59.3",
                "54.8"
            ]
        ],
        "question": "Is it true that [CONTINUE] Table 6 summarize the results, implying that the leakage is caused mainly by the RNN, and less by the Embedding Matrix?",
        "answer_label": "yes"
    },
    {
        "id": "a1404ba9-ad11-4c99-803f-71fc990d1c07",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 8: Performance of models in Macro F1 on tweets from each domain.",
        "table_column_names": [
            "[BOLD] Domain",
            "[BOLD] In-Domain",
            "[BOLD] Pooling",
            "[BOLD] EasyAdapt"
        ],
        "table_content_values": [
            [
                "Food & Beverage",
                "63.9",
                "60.9",
                "[BOLD] 83.1"
            ],
            [
                "Apparel",
                "[BOLD] 76.2",
                "71.1",
                "72.5"
            ],
            [
                "Retail",
                "58.8",
                "[BOLD] 79.7",
                "[BOLD] 79.7"
            ],
            [
                "Cars",
                "41.5",
                "77.8",
                "[BOLD] 80.9"
            ],
            [
                "Services",
                "65.2",
                "75.9",
                "[BOLD] 76.7"
            ],
            [
                "Software",
                "61.3",
                "73.4",
                "[BOLD] 78.7"
            ],
            [
                "Transport",
                "56.4",
                "[BOLD] 73.4",
                "69.8"
            ],
            [
                "Electronics",
                "66.2",
                "73.0",
                "[BOLD] 76.2"
            ],
            [
                "Other",
                "42.4",
                "[BOLD] 82.8",
                "[BOLD] 82.8"
            ]
        ],
        "question": "Is it true that Overall, predictive performance is high across all domains, with the exception of transport?",
        "answer_label": "yes"
    },
    {
        "id": "905c2475-f7a6-4b0f-aeca-1ba199c9fd50",
        "table_caption": "Evaluation of Greek Word Embeddings Table 4: Word similarity.",
        "table_column_names": [
            "Model",
            "Pearson",
            "p-value",
            "Pairs (unknown)"
        ],
        "table_content_values": [
            [
                "gr_def",
                "[BOLD] 0.6042",
                "3.1E-35",
                "2.3%"
            ],
            [
                "gr_neg10",
                "0.5973",
                "2.9E-34",
                "2.3%"
            ],
            [
                "cc.el.300",
                "0.5311",
                "1.7E-25",
                "4.9%"
            ],
            [
                "wiki.el",
                "0.5812",
                "2.2E-31",
                "4.5%"
            ],
            [
                "gr_cbow_def",
                "0.5232",
                "2.7E-25",
                "2.3%"
            ],
            [
                "gr_d300_nosub",
                "0.5889",
                "3.8E-33",
                "2.3%"
            ],
            [
                "gr_w2v_sg_n5",
                "0.5879",
                "4.4E-33",
                "2.3%"
            ]
        ],
        "question": "Is it true that According to Pearson correlation, gr cbow def model had the highest correlation with human ratings of similarity?",
        "answer_label": "no"
    },
    {
        "id": "5a1d0c5b-836f-4eef-85b7-ff4f0e532907",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 4: Human evaluation on extractive summaries. Our system receives significantly higher human ratings on average. \u201cBest%\u201d: in how many percentage of documents a system receives the highest human rating.",
        "table_column_names": [
            "[EMPTY]",
            "Ours",
            "Refresh",
            "ExtAbsRL"
        ],
        "table_content_values": [
            [
                "Avg. Human Rating",
                "[BOLD] 2.52",
                "2.27",
                "1.66"
            ],
            [
                "Best%",
                "[BOLD] 70.0",
                "33.3",
                "6.7"
            ]
        ],
        "question": "Is it true that Also, the average human rating for Refresh is significantly higher (p (cid:28) 0.01) than ExtAbsRL,?",
        "answer_label": "yes"
    },
    {
        "id": "25fb709f-c1cf-4153-b8f6-672f1f494623",
        "table_caption": "Solving Hard Coreference Problems Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.",
        "table_column_names": [
            "Schema",
            "AntePre(Test)",
            "AntePre(Train)"
        ],
        "table_content_values": [
            [
                "Type 1",
                "76.67",
                "86.79"
            ],
            [
                "Type 2",
                "79.55",
                "88.86"
            ],
            [
                "Type 1 (Cat1)",
                "90.26",
                "93.64"
            ],
            [
                "Type 2 (Cat2)",
                "83.38",
                "92.49"
            ]
        ],
        "question": "Is it true that These results do not use the best performing KnowComb system?",
        "answer_label": "no"
    },
    {
        "id": "768f70ec-749a-408f-a097-279e7b07e70f",
        "table_caption": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 2: Precisions on the Wikidata dataset.",
        "table_column_names": [
            "Recall",
            "0.1",
            "0.2",
            "0.3",
            "AUC"
        ],
        "table_content_values": [
            [
                "Rank+ExATT",
                "0.584",
                "0.535",
                "0.487",
                "0.392"
            ],
            [
                "PCNN+ATT (m)",
                "0.365",
                "0.317",
                "0.213",
                "0.204"
            ],
            [
                "PCNN+ATT (1)",
                "0.665",
                "0.517",
                "0.413",
                "0.396"
            ],
            [
                "Our Model",
                "0.650",
                "0.519",
                "0.422",
                "[BOLD] 0.405"
            ]
        ],
        "question": "Is it true that We observe that PCNN+ATT (1) exhibits the best performances?",
        "answer_label": "no"
    },
    {
        "id": "7df38698-8364-4d29-b26e-8e8631b92458",
        "table_caption": "Entity, Relation, and Event Extraction with Contextualized Span Representations Table 3: F1 scores on Relation.",
        "table_column_names": [
            "[EMPTY]",
            "ACE05",
            "SciERC",
            "WLPC"
        ],
        "table_content_values": [
            [
                "BERT + LSTM",
                "60.6",
                "40.3",
                "65.1"
            ],
            [
                "+RelProp",
                "61.9",
                "41.1",
                "65.3"
            ],
            [
                "+CorefProp",
                "59.7",
                "42.6",
                "-"
            ],
            [
                "BERT FineTune",
                "[BOLD] 62.1",
                "44.3",
                "65.4"
            ],
            [
                "+RelProp",
                "62.0",
                "43.0",
                "[BOLD] 65.5"
            ],
            [
                "+CorefProp",
                "60.0",
                "[BOLD] 45.3",
                "-"
            ]
        ],
        "question": "Is it true that [CONTINUE] Relation propagation (RelProp) improves relation extraction performance over pretrained BERT, but does not improve fine-tuned BERT?",
        "answer_label": "yes"
    },
    {
        "id": "4cc9d2c1-a18e-47de-aa39-4909ec936ba9",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
        "table_column_names": [
            "Topic Name",
            "Size",
            "TF-IDF ARI",
            "WMD ARI",
            "Sent2vec ARI",
            "Doc2vec ARI",
            "BERT ARI",
            "[ITALIC] OD-w2v ARI",
            "[ITALIC] OD-d2v ARI",
            "TF-IDF  [ITALIC] Sil.",
            "WMD  [ITALIC] Sil.",
            "Sent2vec  [ITALIC] Sil.",
            "Doc2vec  [ITALIC] Sil.",
            "BERT  [ITALIC] Sil.",
            "[ITALIC] OD-w2v  [ITALIC] Sil.",
            "[ITALIC] OD-d2v  [ITALIC] Sil."
        ],
        "table_content_values": [
            [
                "Affirmative Action",
                "81",
                "-0.07",
                "-0.02",
                "0.03",
                "-0.01",
                "-0.02",
                "[BOLD] 0.14",
                "[ITALIC] 0.02",
                "0.01",
                "0.01",
                "-0.01",
                "-0.02",
                "-0.04",
                "[BOLD] 0.06",
                "[ITALIC] 0.01"
            ],
            [
                "Atheism",
                "116",
                "[BOLD] 0.19",
                "0.07",
                "0.00",
                "0.03",
                "-0.01",
                "0.11",
                "[ITALIC] 0.16",
                "0.02",
                "0.01",
                "0.02",
                "0.01",
                "0.01",
                "[ITALIC] 0.05",
                "[BOLD] 0.07"
            ],
            [
                "Austerity Measures",
                "20",
                "[ITALIC] 0.04",
                "[ITALIC] 0.04",
                "-0.01",
                "-0.05",
                "0.04",
                "[BOLD] 0.21",
                "-0.01",
                "0.06",
                "0.07",
                "0.05",
                "-0.03",
                "0.10",
                "[BOLD] 0.19",
                "0.1"
            ],
            [
                "Democratization",
                "76",
                "0.02",
                "-0.01",
                "0.00",
                "[ITALIC] 0.09",
                "-0.01",
                "[BOLD] 0.11",
                "0.07",
                "0.01",
                "0.01",
                "0.02",
                "0.02",
                "0.03",
                "[BOLD] 0.16",
                "[ITALIC] 0.11"
            ],
            [
                "Education Voucher Scheme",
                "30",
                "[BOLD] 0.25",
                "0.12",
                "0.08",
                "-0.02",
                "0.04",
                "0.13",
                "[ITALIC] 0.19",
                "0.01",
                "0.01",
                "0.01",
                "-0.01",
                "0.02",
                "[ITALIC] 0.38",
                "[BOLD] 0.40"
            ],
            [
                "Gambling",
                "60",
                "-0.06",
                "-0.01",
                "-0.02",
                "0.04",
                "0.09",
                "[ITALIC] 0.35",
                "[BOLD] 0.39",
                "0.01",
                "0.02",
                "0.03",
                "0.01",
                "0.09",
                "[BOLD] 0.30",
                "[ITALIC] 0.22"
            ],
            [
                "Housing",
                "30",
                "0.01",
                "-0.01",
                "-0.01",
                "-0.02",
                "0.08",
                "[BOLD] 0.27",
                "0.01",
                "0.02",
                "0.03",
                "0.03",
                "0.01",
                "0.11",
                "[BOLD] 0.13",
                "[ITALIC] 0.13"
            ],
            [
                "Hydroelectric Dams",
                "110",
                "[BOLD] 0.47",
                "[ITALIC] 0.45",
                "[ITALIC] 0.45",
                "-0.01",
                "0.38",
                "0.35",
                "0.14",
                "0.04",
                "0.08",
                "0.12",
                "0.01",
                "0.19",
                "[BOLD] 0.26",
                "[ITALIC] 0.09"
            ],
            [
                "Intellectual Property",
                "66",
                "0.01",
                "0.01",
                "0.00",
                "0.03",
                "0.03",
                "[ITALIC] 0.05",
                "[BOLD] 0.14",
                "0.01",
                "[ITALIC] 0.04",
                "0.03",
                "0.01",
                "0.03",
                "[ITALIC] 0.04",
                "[BOLD] 0.12"
            ],
            [
                "Keystone pipeline",
                "18",
                "0.01",
                "0.01",
                "0.00",
                "-0.13",
                "[BOLD] 0.07",
                "-0.01",
                "[BOLD] 0.07",
                "-0.01",
                "-0.03",
                "-0.03",
                "-0.07",
                "0.03",
                "[BOLD] 0.05",
                "[ITALIC] 0.02"
            ],
            [
                "Monarchy",
                "61",
                "-0.04",
                "0.01",
                "0.00",
                "0.03",
                "-0.02",
                "[BOLD] 0.15",
                "[BOLD] 0.15",
                "0.01",
                "0.02",
                "0.02",
                "0.01",
                "0.01",
                "[BOLD] 0.11",
                "[ITALIC] 0.09"
            ],
            [
                "National Service",
                "33",
                "0.14",
                "-0.03",
                "-0.01",
                "0.02",
                "0.01",
                "[ITALIC] 0.31",
                "[BOLD] 0.39",
                "0.02",
                "0.04",
                "0.02",
                "0.01",
                "0.02",
                "[BOLD] 0.25",
                "[BOLD] 0.25"
            ],
            [
                "One-child policy China",
                "67",
                "-0.05",
                "0.01",
                "[BOLD] 0.11",
                "-0.02",
                "0.02",
                "[BOLD] 0.11",
                "0.01",
                "0.01",
                "0.02",
                "[ITALIC] 0.04",
                "-0.01",
                "0.03",
                "[BOLD] 0.07",
                "-0.02"
            ],
            [
                "Open-source Software",
                "48",
                "-0.02",
                "-0.01",
                "[ITALIC] 0.05",
                "0.01",
                "0.12",
                "[BOLD] 0.09",
                "-0.02",
                "0.01",
                "-0.01",
                "0.00",
                "-0.02",
                "0.03",
                "[BOLD] 0.18",
                "0.01"
            ],
            [
                "Pornography",
                "52",
                "-0.02",
                "0.01",
                "0.01",
                "-0.02",
                "-0.01",
                "[BOLD] 0.41",
                "[BOLD] 0.41",
                "0.01",
                "0.01",
                "0.02",
                "-0.01",
                "0.03",
                "[BOLD] 0.47",
                "[ITALIC] 0.41"
            ],
            [
                "Seanad Abolition",
                "25",
                "0.23",
                "0.09",
                "-0.01",
                "-0.01",
                "0.03",
                "[ITALIC] 0.32",
                "[BOLD] 0.54",
                "0.02",
                "0.01",
                "-0.01",
                "-0.03",
                "-0.04",
                "[ITALIC] 0.15",
                "[BOLD] 0.31"
            ],
            [
                "Trades Unions",
                "19",
                "[ITALIC] 0.44",
                "[ITALIC] 0.44",
                "[BOLD] 0.60",
                "-0.05",
                "0.44",
                "[ITALIC] 0.44",
                "0.29",
                "0.1",
                "0.17",
                "0.21",
                "0.01",
                "0.26",
                "[BOLD] 0.48",
                "[ITALIC] 0.32"
            ],
            [
                "Video Games",
                "72",
                "-0.01",
                "0.01",
                "0.12",
                "0.01",
                "0.08",
                "[ITALIC] 0.40",
                "[BOLD] 0.56",
                "0.01",
                "0.01",
                "0.06",
                "0.01",
                "0.05",
                "[ITALIC] 0.32",
                "[BOLD] 0.42"
            ],
            [
                "Average",
                "54.67",
                "0.09",
                "0.07",
                "0.08",
                "0.01",
                "0.08",
                "[BOLD] 0.22",
                "[ITALIC] 0.20",
                "0.02",
                "0.03",
                "0.04",
                "-0.01",
                "0.05",
                "[BOLD] 0.20",
                "[ITALIC] 0.17"
            ]
        ],
        "question": "Is it true that [CONTINUE] In the exceptional case of \"Hydroelectric Dams\" dataset, the opinion distance OD performs particularly bad compared to TF-IDF?",
        "answer_label": "yes"
    },
    {
        "id": "c0ce7d66-9d67-4ba9-831c-145cdbef0f49",
        "table_caption": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context Table 2: Comparison of response generation results on MultiWOZ. The oracle/generated denotes either using ground truth or generated results. The results are grouped according to whether and how system action is modeled.",
        "table_column_names": [
            "Model",
            "Belief State Type",
            "System Action Type",
            "System Action Form",
            "Inform (%)",
            "Success (%)",
            "BLEU",
            "Combined Score"
        ],
        "table_content_values": [
            [
                "1. Seq2Seq + Attention ",
                "oracle",
                "-",
                "-",
                "71.3",
                "61.0",
                "[BOLD] 18.9",
                "85.1"
            ],
            [
                "2. Seq2Seq + Copy",
                "oracle",
                "-",
                "-",
                "86.2",
                "[BOLD] 72.0",
                "15.7",
                "94.8"
            ],
            [
                "3. MD-Sequicity",
                "oracle",
                "-",
                "-",
                "[BOLD] 86.6",
                "71.6",
                "16.8",
                "[BOLD] 95.9"
            ],
            [
                "4. SFN + RL (Mehri et al. mehri2019structured)",
                "oracle",
                "generated",
                "one-hot",
                "82.7",
                "72.1",
                "16.3",
                "93.7"
            ],
            [
                "5. HDSA ",
                "oracle",
                "generated",
                "graph",
                "82.9",
                "68.9",
                "[BOLD] 23.6",
                "99.5"
            ],
            [
                "6. DAMD",
                "oracle",
                "generated",
                "span",
                "[BOLD] 89.5",
                "75.8",
                "18.3",
                "100.9"
            ],
            [
                "7. DAMD + multi-action data augmentation",
                "oracle",
                "generated",
                "span",
                "89.2",
                "[BOLD] 77.9",
                "18.6",
                "[BOLD] 102.2"
            ],
            [
                "8. SFN + RL (Mehri et al. mehri2019structured)",
                "oracle",
                "oracle",
                "one-hot",
                "-",
                "-",
                "29.0",
                "106.0"
            ],
            [
                "9. HDSA ",
                "oracle",
                "oracle",
                "graph",
                "87.9",
                "78.0",
                "[BOLD] 30.4",
                "113.4"
            ],
            [
                "10. DAMD + multi-action data augmentation",
                "oracle",
                "oracle",
                "span",
                "[BOLD] 95.4",
                "[BOLD] 87.2",
                "27.3",
                "[BOLD] 118.5"
            ],
            [
                "11. SFN + RL (Mehri et al. mehri2019structured)",
                "generated",
                "generated",
                "one-hot",
                "73.8",
                "58.6",
                "[BOLD] 16.9",
                "83.0"
            ],
            [
                "12. DAMD + multi-action data augmentation",
                "generated",
                "generated",
                "span",
                "[BOLD] 76.3",
                "[BOLD] 60.4",
                "16.6",
                "[BOLD] 85.0"
            ]
        ],
        "question": "Is it true that [CONTINUE] The relative lower BLEU score [CONTINUE] Our DAMD model significantly outperforms other models with different system action forms in terms of inform and success rates, [CONTINUE] While we find applying our data augmentation achieves a limited improvement on combined score (6 vs 7), [CONTINUE] Moreover, if a model has access to ground truth system action, the model further improves its task performance?",
        "answer_label": "yes"
    },
    {
        "id": "78a8bdad-2657-45d7-88ef-60e249c81ea8",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.",
        "table_column_names": [
            "Model",
            "Val. Accuracy",
            "Loss",
            "Val. Loss",
            "Pretraining Time",
            "Finetuning Time"
        ],
        "table_content_values": [
            [
                "Siamese Networks",
                "77.42%",
                "0.5601",
                "0.5329",
                "[EMPTY]",
                "4m per epoch"
            ],
            [
                "BERT",
                "87.47%",
                "0.4655",
                "0.4419",
                "66 hours",
                "2m per epoch"
            ],
            [
                "GPT-2",
                "90.99%",
                "0.2172",
                "0.1826",
                "78 hours",
                "4m per epoch"
            ],
            [
                "ULMFiT",
                "91.59%",
                "0.3750",
                "0.1972",
                "11 hours",
                "2m per epoch"
            ],
            [
                "ULMFiT (no LM Finetuning)",
                "78.11%",
                "0.5512",
                "0.5409",
                "11 hours",
                "2m per epoch"
            ],
            [
                "BERT + Multitasking",
                "91.20%",
                "0.3155",
                "0.3023",
                "66 hours",
                "4m per epoch"
            ],
            [
                "GPT-2 + Multitasking",
                "96.28%",
                "0.2609",
                "0.2197",
                "78 hours",
                "5m per epoch"
            ]
        ],
        "question": "Is it true that GPT-2, on the other hand, finetuned to a final accuracy of 91.20%, only a 0.61% improvement over the performance of ULMFiT?",
        "answer_label": "no"
    },
    {
        "id": "05d0d281-1e18-4f77-932d-b89d635f6ca2",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section\u00a03).",
        "table_column_names": [
            "[BOLD] Dataset",
            "[BOLD] Part",
            "[BOLD] MRs",
            "[BOLD] Refs",
            "[BOLD] SER(%)"
        ],
        "table_content_values": [
            [
                "Original",
                "Train",
                "4,862",
                "42,061",
                "17.69"
            ],
            [
                "Original",
                "Dev",
                "547",
                "4,672",
                "11.42"
            ],
            [
                "Original",
                "Test",
                "630",
                "4,693",
                "11.49"
            ],
            [
                "[0.5pt/2pt] Cleaned",
                "Train",
                "8,362",
                "33,525",
                "(0.00)"
            ],
            [
                "[0.5pt/2pt] Cleaned",
                "Dev",
                "1,132",
                "4,299",
                "(0.00)"
            ],
            [
                "[0.5pt/2pt] Cleaned",
                "Test",
                "1,358",
                "4,693",
                "(0.00)"
            ]
        ],
        "question": "Is it true that This means that the cleaned dataset is more complex overall, with fewer references per MR and more diverse MRs?",
        "answer_label": "yes"
    },
    {
        "id": "4a080275-4fe7-4c3b-b13a-c21e16ec6d34",
        "table_caption": "Predicting Discourse Structure using Distant Supervision from Sentiment Table 3: Discourse structure prediction results; tested on RST-DTtest and Instr-DTtest. Subscripts in inter-domain evaluation sub-table indicate the training set. Best performance in the category is bold. Consistently best model for inter-domain discourse structure prediction is underlined",
        "table_column_names": [
            "Approach",
            "RST-DTtest",
            "Instr-DTtest"
        ],
        "table_content_values": [
            [
                "Right Branching",
                "54.64",
                "58.47"
            ],
            [
                "Left Branching",
                "53.73",
                "48.15"
            ],
            [
                "Hier. Right Branch.",
                "[BOLD] 70.82",
                "[BOLD] 67.86"
            ],
            [
                "Hier. Left Branch.",
                "70.58",
                "63.49"
            ],
            [
                "[BOLD] Intra-Domain Evaluation",
                "[BOLD] Intra-Domain Evaluation",
                "[BOLD] Intra-Domain Evaluation"
            ],
            [
                "HILDAHernault et al. ( 2010 )",
                "83.00",
                "\u2014"
            ],
            [
                "DPLPJi and Eisenstein ( 2014 )",
                "82.08",
                "\u2014"
            ],
            [
                "CODRAJoty et al. ( 2015 )",
                "83.84",
                "[BOLD] 82.88"
            ],
            [
                "Two-StageWang et al. ( 2017 )",
                "[BOLD] 86.00",
                "77.28"
            ],
            [
                "[BOLD] Inter-Domain Evaluation",
                "[BOLD] Inter-Domain Evaluation",
                "[BOLD] Inter-Domain Evaluation"
            ],
            [
                "Two-StageRST-DT",
                "\u00d7",
                "73.65"
            ],
            [
                "Two-StageInstr-DT",
                "74.48",
                "\u00d7"
            ],
            [
                "Two-StageOurs(avg)",
                "76.42",
                "[BOLD] 74.22"
            ],
            [
                "Two-StageOurs(max)",
                "[BOLD] 77.24",
                "73.12"
            ],
            [
                "Human Morey et al. ( 2017 )",
                "88.30",
                "\u2014"
            ]
        ],
        "question": "Is it true that The first set of results in Table 3 shows that the hierarchical right/left branching baselines dominate the completely right/left branching ones?",
        "answer_label": "yes"
    },
    {
        "id": "c1b4b874-ea14-4243-a9d8-1f7ce0e5c941",
        "table_caption": "Building a Production Model for Retrieval-Based Chatbots Table 9: An ablation study showing the effect of different model architectures and training regimes on performance on the proprietary help desk dataset.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Parameters",
            "[BOLD] Validation AUC@0.05",
            "[BOLD] Test AUC@0.05"
        ],
        "table_content_values": [
            [
                "Base",
                "8.0M",
                "[BOLD] 0.871",
                "0.816"
            ],
            [
                "4L SRU \u2192 2L LSTM",
                "7.3M",
                "0.864",
                "[BOLD] 0.829"
            ],
            [
                "4L SRU \u2192 2L SRU",
                "7.8M",
                "0.856",
                "[BOLD] 0.829"
            ],
            [
                "Flat \u2192 hierarchical",
                "12.4M",
                "0.825",
                "0.559"
            ],
            [
                "Cross entropy \u2192 hinge loss",
                "8.0M",
                "0.765",
                "0.693"
            ],
            [
                "6.6M \u2192 1M examples",
                "8.0M",
                "0.835",
                "0.694"
            ],
            [
                "6.6M \u2192 100K examples",
                "8.0M",
                "0.565",
                "0.417"
            ],
            [
                "200 \u2192 100 negatives",
                "8.0M",
                "0.864",
                "0.647"
            ],
            [
                "200 \u2192 10 negatives",
                "8.0M",
                "0.720",
                "0.412"
            ]
        ],
        "question": "Is it true that The model performs significantly better when trained with hinge loss instead of cross-entropy loss, indicating the importance of the loss function?",
        "answer_label": "no"
    },
    {
        "id": "2c6bda23-8b25-4510-a555-9234e71214ac",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
        "table_column_names": [
            "[BOLD] Emoji alias",
            "[BOLD] N",
            "[BOLD] emoji #",
            "[BOLD] emoji %",
            "[BOLD] no-emoji #",
            "[BOLD] no-emoji %",
            "[BOLD] \u0394%"
        ],
        "table_content_values": [
            [
                "mask",
                "163",
                "154",
                "94.48",
                "134",
                "82.21",
                "- 12.27"
            ],
            [
                "two_hearts",
                "87",
                "81",
                "93.10",
                "77",
                "88.51",
                "- 4.59"
            ],
            [
                "heart_eyes",
                "122",
                "109",
                "89.34",
                "103",
                "84.43",
                "- 4.91"
            ],
            [
                "heart",
                "267",
                "237",
                "88.76",
                "235",
                "88.01",
                "- 0.75"
            ],
            [
                "rage",
                "92",
                "78",
                "84.78",
                "66",
                "71.74",
                "- 13.04"
            ],
            [
                "cry",
                "116",
                "97",
                "83.62",
                "83",
                "71.55",
                "- 12.07"
            ],
            [
                "sob",
                "490",
                "363",
                "74.08",
                "345",
                "70.41",
                "- 3.67"
            ],
            [
                "unamused",
                "167",
                "121",
                "72.46",
                "116",
                "69.46",
                "- 3.00"
            ],
            [
                "weary",
                "204",
                "140",
                "68.63",
                "139",
                "68.14",
                "- 0.49"
            ],
            [
                "joy",
                "978",
                "649",
                "66.36",
                "629",
                "64.31",
                "- 2.05"
            ],
            [
                "sweat_smile",
                "111",
                "73",
                "65.77",
                "75",
                "67.57",
                "1.80"
            ],
            [
                "confused",
                "77",
                "46",
                "59.74",
                "48",
                "62.34",
                "2.60"
            ]
        ],
        "question": "Is it true that [CONTINUE] Finally, not all emoji are beneficial for this task?",
        "answer_label": "yes"
    },
    {
        "id": "4ee4134b-7e4a-41fb-9eb4-ef3dd0fef13f",
        "table_caption": "Using Structured Representation and Data: A Hybrid Model for Negation and Sentiment in Customer Service Conversations Table 7: Negation classifier performance for scope detection with gold cues and scope.",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Punctuation",
            "[BOLD] BiLSTM",
            "[BOLD] Proposed"
        ],
        "table_content_values": [
            [
                "In-scope (F)",
                "0.66",
                "0.88",
                "0.85"
            ],
            [
                "Out-scope (F)",
                "0.87",
                "0.97",
                "0.97"
            ],
            [
                "PCS",
                "0.52",
                "0.72",
                "0.72"
            ]
        ],
        "question": "Is it true that The results in Table 7 show that the method is comparable to state of the art BiLSTM model from (Fancellu et al., 2016) on gold negation cues for scope prediction?",
        "answer_label": "yes"
    },
    {
        "id": "27cbf083-1fc5-4938-9b87-bcce547cdec5",
        "table_caption": "Sparse and Structured Visual Attention Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
        "table_column_names": [
            "[EMPTY]",
            "Att. to image",
            "Att. to bounding boxes",
            "Test-Dev Yes/No",
            "Test-Dev Number",
            "Test-Dev Other",
            "Test-Dev Overall",
            "Test-Standard Yes/No",
            "Test-Standard Number",
            "Test-Standard Other",
            "Test-Standard Overall"
        ],
        "table_content_values": [
            [
                "softmax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "42.65",
                "55.74",
                "65.52",
                "83.55",
                "42.68",
                "56.01",
                "65.97"
            ],
            [
                "sparsemax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "43.19",
                "55.79",
                "65.60",
                "83.33",
                "42.99",
                "56.06",
                "65.94"
            ],
            [
                "soft-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.13",
                "43.53",
                "56.01",
                "65.76",
                "83.63",
                "43.24",
                "56.10",
                "66.11"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.10",
                "43.30",
                "56.14",
                "65.79",
                "83.66",
                "43.18",
                "56.21",
                "66.17"
            ],
            [
                "softmax",
                "[EMPTY]",
                "\u2713",
                "85.14",
                "49.59",
                "58.72",
                "68.57",
                "85.56",
                "49.54",
                "59.11",
                "69.04"
            ],
            [
                "sparsemax",
                "[EMPTY]",
                "\u2713",
                "[BOLD] 85.40",
                "[BOLD] 50.87",
                "58.67",
                "68.79",
                "[BOLD] 85.80",
                "50.18",
                "59.08",
                "69.19"
            ],
            [
                "softmax",
                "\u2713",
                "\u2713",
                "85.33",
                "50.49",
                "58.88",
                "68.82",
                "85.58",
                "50.42",
                "59.18",
                "69.17"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "\u2713",
                "85.35",
                "50.52",
                "[BOLD] 59.15",
                "[BOLD] 68.96",
                "85.72",
                "[BOLD] 50.66",
                "[BOLD] 59.22",
                "[BOLD] 69.28"
            ]
        ],
        "question": "Is it true that As can be seen in the results presented in Table 3, the models using softmax and sparsemax in the output attention layer outperform the models using TVMAX?",
        "answer_label": "no"
    },
    {
        "id": "c0d4f7e7-545a-4108-a282-9c2355965cb2",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 3: Results on LDC2015E86 test set when models are trained with additional Gigaword data.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>External</bold>",
            "<bold>BLEU</bold>"
        ],
        "table_content_values": [
            [
                "Konstas et al. (2017)",
                "200K",
                "27.40"
            ],
            [
                "Song et al. (2018)",
                "200K",
                "28.20"
            ],
            [
                "Guo et al. (2019)",
                "200K",
                "31.60"
            ],
            [
                "G2S-GGNN",
                "200K",
                "<bold>32.23</bold>"
            ]
        ],
        "question": "Is it true that G2S-GGNN outperforms others with the same amount of Gigaword sentences (200K), achieving a 32.23 BLEU score, as shown in Table 3?",
        "answer_label": "yes"
    },
    {
        "id": "1a8e67b2-88be-49d5-bb39-f4d96cdb3495",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 2: Experiment 1",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.001",
                "0.003",
                "-20.818",
                "***",
                "0.505"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.083",
                "0.048",
                "101.636",
                "***",
                "1.724"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.001",
                "0.001",
                "0.035",
                "[EMPTY]",
                "1.001"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.023",
                "0.012",
                "64.418",
                "***",
                "1.993"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.002",
                "0.001",
                "4.047",
                "***",
                "1.120"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.049",
                "0.019",
                "120.986",
                "***",
                "2.573"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.173",
                "0.065",
                "243.285",
                "***",
                "2.653"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.032",
                "0.023",
                "39.483",
                "***",
                "1.396"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.111",
                "0.061",
                "122.707",
                "***",
                "1.812"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.178",
                "0.080",
                "211.319",
                "***",
                "2.239"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.028",
                "0.015",
                "63.131",
                "***",
                "1.854"
            ]
        ],
        "question": "Is it true that For Waseem (2016) we see that there is no significant difference in the estimated rates at which tweets are clas [CONTINUE] sified as racist across groups, although the rates remain low?",
        "answer_label": "yes"
    },
    {
        "id": "c256c279-dab2-4c45-b0e9-b49660868f5f",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
        "table_column_names": [
            "[EMPTY]",
            "Prec.",
            "Rec.",
            "F1"
        ],
        "table_content_values": [
            [
                "(A1) BiLSTM-CNN",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "(A2) Standard attention",
                "0.466",
                "0.638",
                "0.539"
            ],
            [
                "(A3) Window size ( [ITALIC] ws)=5",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "(A4) Window size ( [ITALIC] ws)=10",
                "0.510",
                "0.640",
                "0.568"
            ],
            [
                "(A5) Softmax",
                "0.490",
                "0.658",
                "0.562"
            ],
            [
                "(A6) Max-pool",
                "0.492",
                "0.600",
                "0.541"
            ]
        ],
        "question": "Is it true that Increasing the window size to 10 reduces the F1 score marginally (A3\u2212A4)?",
        "answer_label": "yes"
    },
    {
        "id": "df4ef506-a5a6-4e77-8a07-2b8a0f630696",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.",
        "table_column_names": [
            "[BOLD] Language pair",
            "[BOLD] Model type",
            "[BOLD] Oracle model",
            "[BOLD] Decoder configuration  [BOLD] Uniform",
            "[BOLD] Decoder configuration  [BOLD] BI + IS"
        ],
        "table_content_values": [
            [
                "es-en",
                "Unadapted",
                "36.4",
                "34.7",
                "36.6"
            ],
            [
                "es-en",
                "No-reg",
                "36.6",
                "34.8",
                "-"
            ],
            [
                "es-en",
                "EWC",
                "37.0",
                "36.3",
                "[BOLD] 37.2"
            ],
            [
                "en-de",
                "Unadapted",
                "36.4",
                "26.8",
                "38.8"
            ],
            [
                "en-de",
                "No-reg",
                "41.7",
                "31.8",
                "-"
            ],
            [
                "en-de",
                "EWC",
                "42.1",
                "38.6",
                "[BOLD] 42.0"
            ]
        ],
        "question": "Is it true that [CONTINUE] BI+IS decoding with single-domain trained models achieves gains over both the naive uniform approach and over oracle single-domain models?",
        "answer_label": "yes"
    },
    {
        "id": "440cc321-d9fd-4f0c-8f3f-21bcaee949e5",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "-{4} dense block",
                "24.8",
                "54.9"
            ],
            [
                "-{3, 4} dense blocks",
                "23.8",
                "54.1"
            ],
            [
                "-{2, 3, 4} dense blocks",
                "23.2",
                "53.1"
            ]
        ],
        "question": "Is it true that These results indicate that dense connections do not play a significant role in our model?",
        "answer_label": "no"
    },
    {
        "id": "0dacc112-f2b4-4fcb-9e05-98368e576c3e",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
        "table_column_names": [
            "Model",
            "Training data",
            "Overall",
            "Easy",
            "Hard"
        ],
        "table_content_values": [
            [
                "BERT-large-FT",
                "B-COPA",
                "74.5 (\u00b1 0.7)",
                "74.7 (\u00b1 0.4)",
                "[BOLD] 74.4 (\u00b1 0.9)"
            ],
            [
                "BERT-large-FT",
                "B-COPA (50%)",
                "74.3 (\u00b1 2.2)",
                "76.8 (\u00b1 1.9)",
                "72.8 (\u00b1 3.1)"
            ],
            [
                "BERT-large-FT",
                "COPA",
                "[BOLD] 76.5 (\u00b1 2.7)",
                "[BOLD] 83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA",
                "[BOLD] 89.0 (\u00b1 0.3)",
                "88.9 (\u00b1 2.1)",
                "[BOLD] 89.0 (\u00b1 0.8)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA (50%)",
                "86.1 (\u00b1 2.2)",
                "87.4 (\u00b1 1.1)",
                "85.4 (\u00b1 2.9)"
            ],
            [
                "RoBERTa-large-FT",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "[BOLD] 91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)"
            ]
        ],
        "question": "Is it true that Note that training on B-COPA 50% exposes the model to lexically less diverse training instances than the original COPA due to the high overlap between mirrored alternatives [CONTINUE] These results show that once superficial cues [CONTINUE] are removed, the models are not able to learn the task to a high degree?",
        "answer_label": "no"
    },
    {
        "id": "7f929dc9-7327-40e8-9352-1767a83b1a2f",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
        "table_column_names": [
            "Model",
            "#Params",
            "BLEU",
            "Train",
            "Decode"
        ],
        "table_content_values": [
            [
                "GNMT",
                "-",
                "24.61",
                "-",
                "-"
            ],
            [
                "GRU",
                "206M",
                "26.28",
                "2.67",
                "45.35"
            ],
            [
                "ATR",
                "122M",
                "25.70",
                "1.33",
                "[BOLD] 34.40"
            ],
            [
                "SRU",
                "170M",
                "25.91",
                "1.34",
                "42.84"
            ],
            [
                "LRN",
                "143M",
                "26.26",
                "[BOLD] 0.99",
                "36.50"
            ],
            [
                "oLRN",
                "164M",
                "[BOLD] 26.73",
                "1.15",
                "40.19"
            ]
        ],
        "question": "Is it true that [CONTINUE] however, oLRN yields the best BLEU score of 26.73, outperforming GRU (+0.45 BLEU)?",
        "answer_label": "yes"
    },
    {
        "id": "59fe28ba-af08-4f7c-aa03-c61c11dbe6f2",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
        "table_column_names": [
            "[EMPTY]",
            "WN-N P",
            "WN-N R",
            "WN-N F",
            "WN-V P",
            "WN-V R",
            "WN-V F",
            "VN P",
            "VN R",
            "VN F"
        ],
        "table_content_values": [
            [
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2"
            ],
            [
                "type",
                ".700",
                ".654",
                ".676",
                ".535",
                ".474",
                ".503",
                ".327",
                ".309",
                ".318"
            ],
            [
                "x+POS",
                ".699",
                ".651",
                ".674",
                ".544",
                ".472",
                ".505",
                ".339",
                ".312",
                ".325"
            ],
            [
                "lemma",
                ".706",
                ".660",
                ".682",
                ".576",
                ".520",
                ".547",
                ".384",
                ".360",
                ".371"
            ],
            [
                "x+POS",
                "<bold>.710</bold>",
                "<bold>.662</bold>",
                "<bold>.685</bold>",
                "<bold>.589</bold>",
                "<bold>.529</bold>",
                "<bold>.557</bold>",
                "<bold>.410</bold>",
                "<bold>.389</bold>",
                "<bold>.399</bold>"
            ],
            [
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep"
            ],
            [
                "type",
                ".712",
                ".661",
                ".686",
                ".545",
                ".457",
                ".497",
                ".324",
                ".296",
                ".310"
            ],
            [
                "x+POS",
                ".715",
                ".659",
                ".686",
                ".560",
                ".464",
                ".508",
                ".349",
                ".320",
                ".334"
            ],
            [
                "lemma",
                "<bold>.725</bold>",
                "<bold>.668</bold>",
                "<bold>.696</bold>",
                ".591",
                ".512",
                ".548",
                ".408",
                ".371",
                ".388"
            ],
            [
                "x+POS",
                ".722",
                ".666",
                ".693",
                "<bold>.609</bold>",
                "<bold>.527</bold>",
                "<bold>.565</bold>",
                "<bold>.412</bold>",
                "<bold>.381</bold>",
                "<bold>.396</bold>"
            ]
        ],
        "question": "Is it true that Lemma-based targets with POS disambiguation perform best on WN-N when dependency-based contexts are used; the difference to lemmatized targets without disambiguation is statistically significant (p < .1)?",
        "answer_label": "no"
    },
    {
        "id": "b4829db1-041f-4a7e-9371-9042d2584441",
        "table_caption": "Modulated Self-attention Convolutional Network for VQA Table 1: Experiments run on a ResNet-34. Numbers following S (stages) and B (blocks) indicate where SA (self-attention) modules are put. Parameters count concerns only SA and are in millions (M).",
        "table_column_names": [
            "[BOLD] ResNet-34",
            "[BOLD] Eval set %",
            "[BOLD] #param"
        ],
        "table_content_values": [
            [
                "Baseline (No SA)Anderson et al. ( 2018 )",
                "55.00",
                "0M"
            ],
            [
                "SA (S: 1,2,3 - B: 1)",
                "55.11",
                "} 0.107M"
            ],
            [
                "SA (S: 1,2,3 - B: 2)",
                "55.17",
                "} 0.107M"
            ],
            [
                "[BOLD] SA (S: 1,2,3 - B: 3)",
                "[BOLD] 55.27",
                "} 0.107M"
            ]
        ],
        "question": "Is it true that [CONTINUE] We empirically found that self-attention was the most efficient in the 3rd stage?",
        "answer_label": "yes"
    },
    {
        "id": "7be9b83d-f973-4655-9a2c-39eb8160b687",
        "table_caption": "Building a Production Model for Retrieval-Based Chatbots Table 5: Recall@k for random, frequency, and clustering whitelists of different sizes. The \u201c+\u201d indicates that the true response is added to the whitelist.",
        "table_column_names": [
            "[BOLD] Whitelist",
            "[BOLD] R@1",
            "[BOLD] R@3",
            "[BOLD] R@5",
            "[BOLD] R@10",
            "[BOLD] BLEU"
        ],
        "table_content_values": [
            [
                "Random 10K+",
                "0.252",
                "0.400",
                "0.472",
                "0.560",
                "37.71"
            ],
            [
                "Frequency 10K+",
                "0.257",
                "0.389",
                "0.455",
                "0.544",
                "41.34"
            ],
            [
                "Clustering 10K+",
                "0.230",
                "0.376",
                "0.447",
                "0.541",
                "37.59"
            ],
            [
                "Random 1K+",
                "0.496",
                "0.663",
                "0.728",
                "0.805",
                "59.28"
            ],
            [
                "Frequency 1K+",
                "0.513",
                "0.666",
                "0.726",
                "0.794",
                "67.05"
            ],
            [
                "Clustering 1K+",
                "0.481",
                "0.667",
                "0.745",
                "0.835",
                "61.88"
            ],
            [
                "Frequency 10K",
                "0.136",
                "0.261",
                "0.327",
                "0.420",
                "30.46"
            ],
            [
                "Clustering 10K",
                "0.164",
                "0.292",
                "0.360",
                "0.457",
                "31.47"
            ],
            [
                "Frequency 1K",
                "0.273",
                "0.465",
                "0.550",
                "0.658",
                "47.13"
            ],
            [
                "Clustering 1K",
                "0.331",
                "0.542",
                "0.650",
                "0.782",
                "49.26"
            ]
        ],
        "question": "Is it true that The results in Table 5 show that the frequency whitelists perform better than the random and clustering whitelists when the true response is added?",
        "answer_label": "no"
    },
    {
        "id": "68d6065c-868c-40ac-b3a9-14218014c2c1",
        "table_caption": "Solving Hard Coreference Problems Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.",
        "table_column_names": [
            "Schema",
            "AntePre(Test)",
            "AntePre(Train)"
        ],
        "table_content_values": [
            [
                "Type 1",
                "76.67",
                "86.79"
            ],
            [
                "Type 2",
                "79.55",
                "88.86"
            ],
            [
                "Type 1 (Cat1)",
                "90.26",
                "93.64"
            ],
            [
                "Type 2 (Cat2)",
                "83.38",
                "92.49"
            ]
        ],
        "question": "Is it true that These results use the best performing KnowComb system?",
        "answer_label": "yes"
    },
    {
        "id": "dee030ab-36f2-4c8f-a9f2-297aac021d8f",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.",
        "table_column_names": [
            "Model",
            "Val. Accuracy",
            "Loss",
            "Val. Loss",
            "Pretraining Time",
            "Finetuning Time"
        ],
        "table_content_values": [
            [
                "Siamese Networks",
                "77.42%",
                "0.5601",
                "0.5329",
                "[EMPTY]",
                "4m per epoch"
            ],
            [
                "BERT",
                "87.47%",
                "0.4655",
                "0.4419",
                "66 hours",
                "2m per epoch"
            ],
            [
                "GPT-2",
                "90.99%",
                "0.2172",
                "0.1826",
                "78 hours",
                "4m per epoch"
            ],
            [
                "ULMFiT",
                "91.59%",
                "0.3750",
                "0.1972",
                "11 hours",
                "2m per epoch"
            ],
            [
                "ULMFiT (no LM Finetuning)",
                "78.11%",
                "0.5512",
                "0.5409",
                "11 hours",
                "2m per epoch"
            ],
            [
                "BERT + Multitasking",
                "91.20%",
                "0.3155",
                "0.3023",
                "66 hours",
                "4m per epoch"
            ],
            [
                "GPT-2 + Multitasking",
                "96.28%",
                "0.2609",
                "0.2197",
                "78 hours",
                "5m per epoch"
            ]
        ],
        "question": "Is it true that BERT achieved a final accuracy of 91.20%, now marginally comparable to ULMFiT's full performance?",
        "answer_label": "yes"
    },
    {
        "id": "a50811fb-3024-4844-b141-56da2fa21184",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. \u2020 denotes the previous best state-of-the-art model.",
        "table_column_names": [
            "Model",
            "NYT10 Prec.",
            "NYT10 Rec.",
            "NYT10 F1",
            "NYT11 Prec.",
            "NYT11 Rec.",
            "NYT11 F1"
        ],
        "table_content_values": [
            [
                "CNN zeng2014relation",
                "0.413",
                "0.591",
                "0.486",
                "0.444",
                "0.625",
                "0.519"
            ],
            [
                "PCNN zeng2015distant",
                "0.380",
                "[BOLD] 0.642",
                "0.477",
                "0.446",
                "0.679",
                "0.538\u2020"
            ],
            [
                "EA huang2016attention",
                "0.443",
                "0.638",
                "0.523\u2020",
                "0.419",
                "0.677",
                "0.517"
            ],
            [
                "BGWA jat2018attention",
                "0.364",
                "0.632",
                "0.462",
                "0.417",
                "[BOLD] 0.692",
                "0.521"
            ],
            [
                "BiLSTM-CNN",
                "0.490",
                "0.507",
                "0.498",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "Our model",
                "[BOLD] 0.541",
                "0.595",
                "[BOLD] 0.566*",
                "[BOLD] 0.507",
                "0.652",
                "[BOLD] 0.571*"
            ]
        ],
        "question": "Is it true that Our model does not outperform the previous state-of-the-art models on both datasets in terms of F1 score?",
        "answer_label": "no"
    },
    {
        "id": "fad1f689-0127-4c3a-bf86-258d0de6f364",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 4: Consolidated experiment results. The first section shows finetuning results for base transfer learning methods and the baseline siamese network. The second section shows results for ULMFiT without Language Model Finetuning. The last section shows finetuning results for transformer methods augmented with multitasking heads. BERT and GPT-2 were finetuned for three epochs in all cases and ULMFiT was finetuned for 5 during classifier finetuning.",
        "table_column_names": [
            "Model",
            "Val. Accuracy",
            "Loss",
            "Val. Loss",
            "Pretraining Time",
            "Finetuning Time"
        ],
        "table_content_values": [
            [
                "Siamese Networks",
                "77.42%",
                "0.5601",
                "0.5329",
                "[EMPTY]",
                "4m per epoch"
            ],
            [
                "BERT",
                "87.47%",
                "0.4655",
                "0.4419",
                "66 hours",
                "2m per epoch"
            ],
            [
                "GPT-2",
                "90.99%",
                "0.2172",
                "0.1826",
                "78 hours",
                "4m per epoch"
            ],
            [
                "ULMFiT",
                "91.59%",
                "0.3750",
                "0.1972",
                "11 hours",
                "2m per epoch"
            ],
            [
                "ULMFiT (no LM Finetuning)",
                "78.11%",
                "0.5512",
                "0.5409",
                "11 hours",
                "2m per epoch"
            ],
            [
                "BERT + Multitasking",
                "91.20%",
                "0.3155",
                "0.3023",
                "66 hours",
                "4m per epoch"
            ],
            [
                "GPT-2 + Multitasking",
                "96.28%",
                "0.2609",
                "0.2197",
                "78 hours",
                "5m per epoch"
            ]
        ],
        "question": "Is it true that GPT-2, on the other hand, finetuned to a final accuracy of 96.28%, a full 4.69% improvement over the performance of ULMFiT?",
        "answer_label": "yes"
    },
    {
        "id": "53cdc8d3-bd81-4d5d-8572-79b80c41480d",
        "table_caption": "Neural End-to-End Learning for Computational Argumentation Mining Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by \u201c:\u201d. Layers from which tasks feed are indicated by respective numbers.",
        "table_column_names": [
            "[EMPTY]",
            "C-F1 100%",
            "C-F1 50%",
            "R-F1 100%",
            "R-F1 50%",
            "F1 100%",
            "F1 50%"
        ],
        "table_content_values": [
            [
                "Y-3",
                "49.59",
                "65.37",
                "26.28",
                "37.00",
                "34.35",
                "47.25"
            ],
            [
                "Y-3:Y<italic>C</italic>-1",
                "54.71",
                "66.84",
                "28.44",
                "37.35",
                "37.40",
                "47.92"
            ],
            [
                "Y-3:Y<italic>R</italic>-1",
                "51.32",
                "66.49",
                "26.92",
                "37.18",
                "35.31",
                "47.69"
            ],
            [
                "Y-3:Y<italic>C</italic>-3",
                "<bold>54.58</bold>",
                "67.66",
                "<bold>30.22</bold>",
                "<bold>40.30</bold>",
                "<bold>38.90</bold>",
                "<bold>50.51</bold>"
            ],
            [
                "Y-3:Y<italic>R</italic>-3",
                "53.31",
                "66.71",
                "26.65",
                "35.86",
                "35.53",
                "46.64"
            ],
            [
                "Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2",
                "52.95",
                "<bold>67.84</bold>",
                "27.90",
                "39.71",
                "36.54",
                "50.09"
            ],
            [
                "Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3",
                "54.55",
                "67.60",
                "28.30",
                "38.26",
                "37.26",
                "48.86"
            ]
        ],
        "question": "Is it true that We find that when we train STagBL with only its main task\u2014with label set [CONTINUE] In Y contrast, when we include the 'natural subtasks' \"C\" (label [CONTINUE] performance increases typically by a few percentage points?",
        "answer_label": "yes"
    },
    {
        "id": "0ed421ff-4061-441d-bbdd-6f1cdc44ca0b",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that When we increase the DCGCN blocks from 1 to 4, the model performance does not necessarily increase on AMR15 development set?",
        "answer_label": "no"
    },
    {
        "id": "af3ebe5e-4d9c-48e9-83d1-6039796679bf",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 5: Test BLEU for 2-model es-en and 3-model en-de unadapted model ensembling, compared to oracle unadapted model chosen if test domain is known. Uniform ensembling generally underperforms the oracle, while BI+IS outperforms the oracle.",
        "table_column_names": [
            "[BOLD] Decoder configuration",
            "[BOLD] es-en  [BOLD] Health",
            "[BOLD] es-en  [BOLD] Bio",
            "[BOLD] en-de  [BOLD] News",
            "[BOLD] en-de  [BOLD] TED",
            "[BOLD] en-de  [BOLD] IT"
        ],
        "table_content_values": [
            [
                "Oracle model",
                "35.9",
                "36.1",
                "37.8",
                "24.1",
                "39.6"
            ],
            [
                "Uniform",
                "33.1",
                "36.4",
                "21.9",
                "18.4",
                "38.9"
            ],
            [
                "Identity-BI",
                "35.0",
                "36.6",
                "32.7",
                "25.3",
                "42.6"
            ],
            [
                "BI",
                "35.9",
                "36.5",
                "38.0",
                "26.1",
                "[BOLD] 44.7"
            ],
            [
                "IS",
                "[BOLD] 36.0",
                "36.8",
                "37.5",
                "25.6",
                "43.3"
            ],
            [
                "BI + IS",
                "[BOLD] 36.0",
                "[BOLD] 36.9",
                "[BOLD] 38.4",
                "[BOLD] 26.4",
                "[BOLD] 44.7"
            ]
        ],
        "question": "Is it true that Table 5 shows improvements on data without domain labelling using our adaptive decoding schemes with unadapted models trained only on one domain [CONTINUE] Uniform ensembling under-performs all oracle models except es-en Bio, especially on general domains?",
        "answer_label": "yes"
    },
    {
        "id": "ffc551ba-ac96-4d34-ab5f-fec1d0e20a57",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 7: Return distribution of GDPL on each metric. The first row counts the dialog sessions that get the full score of the corresponding metric, and the results of the rest sessions are included in the second row.",
        "table_column_names": [
            "Type",
            "Inform Mean",
            "Inform Num",
            "Match Mean",
            "Match Num",
            "Success Mean",
            "Success Num"
        ],
        "table_content_values": [
            [
                "Full",
                "8.413",
                "903",
                "10.59",
                "450",
                "11.18",
                "865"
            ],
            [
                "Other",
                "-99.95",
                "76",
                "-48.15",
                "99",
                "-71.62",
                "135"
            ]
        ],
        "question": "Is it true that It can be observed that the learned reward function does not have good interpretability in that the reward is positive when the dialog gets a full score on each metric, and negative otherwise?",
        "answer_label": "no"
    },
    {
        "id": "26382fa1-036e-4dcf-bf81-08d8763ef416",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 4: Experiment 2, t= \u201cb*tch\u201d",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.010",
                "0.010",
                "-0.632",
                "[EMPTY]",
                "0.978"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.963",
                "0.944",
                "20.064",
                "***",
                "1.020"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.011",
                "0.011",
                "-1.254",
                "[EMPTY]",
                "0.955"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.349",
                "0.290",
                "28.803",
                "***",
                "1.203"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.012",
                "0.012",
                "-0.162",
                "[EMPTY]",
                "0.995"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.017",
                "0.015",
                "4.698",
                "***",
                "1.152"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.988",
                "0.991",
                "-6.289",
                "***",
                "0.997"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.099",
                "0.091",
                "6.273",
                "***",
                "1.091"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.074",
                "0.027",
                "46.054",
                "***",
                "2.728"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.925",
                "0.968",
                "-41.396",
                "***",
                "0.956"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.010",
                "0.010",
                "0.000",
                "[EMPTY]",
                "1.000"
            ]
        ],
        "question": "Is it true that The Waseem and Hovy (2016) classifier is particularly sensitive to the word \"b*tch\" with 96% of black-aligned and 94% of white-aligned [CONTINUE] tweets predicted to belong to this class?",
        "answer_label": "yes"
    },
    {
        "id": "335c8128-859a-4bb2-808d-c48b428dd5d0",
        "table_caption": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
        "table_column_names": [
            "ID LSTM-800",
            "5-fold CV 70.56",
            "\u0394 0.66",
            "Single model 67.54",
            "\u0394 0.78",
            "Ensemble 67.65",
            "\u0394 0.30"
        ],
        "table_content_values": [
            [
                "LSTM-400",
                "70.50",
                "0.60",
                "[BOLD] 67.59",
                "0.83",
                "[BOLD] 68.00",
                "0.65"
            ],
            [
                "IN-TITLE",
                "70.11",
                "0.21",
                "[EMPTY]",
                "[EMPTY]",
                "67.52",
                "0.17"
            ],
            [
                "[BOLD] SUBMISSION",
                "69.90",
                "\u2013",
                "66.76",
                "\u2013",
                "67.35",
                "\u2013"
            ],
            [
                "NO-HIGHWAY",
                "69.72",
                "\u22120.18",
                "66.42",
                "\u22120.34",
                "66.64",
                "\u22120.71"
            ],
            [
                "NO-OVERLAPS",
                "69.46",
                "\u22120.44",
                "65.07",
                "\u22121.69",
                "66.47",
                "\u22120.88"
            ],
            [
                "LSTM-400-DROPOUT",
                "69.45",
                "\u22120.45",
                "65.53",
                "\u22121.23",
                "67.28",
                "\u22120.07"
            ],
            [
                "NO-TRANSLATIONS",
                "69.42",
                "\u22120.48",
                "65.92",
                "\u22120.84",
                "67.23",
                "\u22120.12"
            ],
            [
                "NO-ELMO-FINETUNING",
                "67.71",
                "\u22122.19",
                "65.16",
                "\u22121.60",
                "65.42",
                "\u22121.93"
            ]
        ],
        "question": "Is it true that [CONTINUE] The 'alternating' LSTM layout we chose for our submission actually outperformed the 'traditional' one in terms of both single model and ensemble performance?",
        "answer_label": "no"
    },
    {
        "id": "1d4b85e2-79a4-463d-873b-f6bdcef4fa2f",
        "table_caption": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.",
        "table_column_names": [
            "Feature",
            "LR P",
            "LR R",
            "LR F1",
            "SVM P",
            "SVM R",
            "SVM F1",
            "ANN P",
            "ANN R",
            "ANN F1"
        ],
        "table_content_values": [
            [
                "+BoW",
                "0.93",
                "0.91",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+BoC (Wiki-PubMed-PMC)",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.91",
                "0.91",
                "[BOLD] 0.91"
            ],
            [
                "+BoC (GloVe)",
                "0.93",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+ASM",
                "0.90",
                "0.85",
                "0.88",
                "0.90",
                "0.86",
                "0.88",
                "0.89",
                "0.89",
                "0.89"
            ],
            [
                "+Sentence Embeddings(SEs)",
                "0.89",
                "0.89",
                "0.89",
                "0.90",
                "0.86",
                "0.88",
                "0.88",
                "0.88",
                "0.88"
            ],
            [
                "+BoC(Wiki-PubMed-PMC)+SEs",
                "0.92",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ]
        ],
        "question": "Is it true that [CONTINUE] Lin-SVM outperforms other classifiers in extracting most relations?",
        "answer_label": "yes"
    },
    {
        "id": "3df48964-f174-4875-97f2-dee5dfb515c5",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that [CONTINUE] Though ALDM obtains a higher inform F1 and match rate than PPO, it does not get a significant improvement [CONTINUE] on task success [CONTINUE] Ablation test is investigated in Table 3?",
        "answer_label": "no"
    },
    {
        "id": "07dfd438-80f3-41d8-ba09-1f769f983131",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 2: Test error (ERR) on document classification task. \u201c#Params\u201d: the parameter number in AmaPolar task. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "AmaPolar ERR",
            "AmaPolar Time",
            "Yahoo ERR",
            "Yahoo Time",
            "AmaFull ERR",
            "AmaFull Time",
            "YelpPolar ERR",
            "YelpPolar Time"
        ],
        "table_content_values": [
            [
                "Zhang et\u00a0al. ( 2015 )",
                "Zhang et\u00a0al. ( 2015 )",
                "-",
                "6.10",
                "-",
                "29.16",
                "-",
                "40.57",
                "-",
                "5.26",
                "-"
            ],
            [
                "This",
                "LSTM",
                "227K",
                "[BOLD] 4.37",
                "0.947",
                "[BOLD] 24.62",
                "1.332",
                "37.22",
                "1.003",
                "3.58",
                "1.362"
            ],
            [
                "This",
                "GRU",
                "176K",
                "4.39",
                "0.948",
                "24.68",
                "1.242",
                "[BOLD] 37.20",
                "0.982",
                "[BOLD] 3.47",
                "1.230"
            ],
            [
                "This",
                "ATR",
                "74K",
                "4.78",
                "0.867",
                "25.33",
                "1.117",
                "38.54",
                "0.836",
                "4.00",
                "1.124"
            ],
            [
                "Work",
                "SRU",
                "194K",
                "4.95",
                "0.919",
                "24.78",
                "1.394",
                "38.23",
                "0.907",
                "3.99",
                "1.310"
            ],
            [
                "[EMPTY]",
                "LRN",
                "151K",
                "4.98",
                "[BOLD] 0.731",
                "25.07",
                "[BOLD] 1.038",
                "38.42",
                "[BOLD] 0.788",
                "3.98",
                "[BOLD] 1.022"
            ]
        ],
        "question": "Is it true that LRN does not accelerate the training over LSTM and SRU by about 20%?",
        "answer_label": "no"
    },
    {
        "id": "1d89289c-47a5-4052-8b51-a516aead51a8",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Feature",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Feature",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams"
            ],
            [
                "not",
                ".154",
                "[URL]",
                ".150"
            ],
            [
                "my",
                ".131",
                "!",
                ".082"
            ],
            [
                "working",
                ".124",
                "he",
                ".069"
            ],
            [
                "still",
                ".123",
                "thank",
                ".067"
            ],
            [
                "on",
                ".119",
                ",",
                ".064"
            ],
            [
                "can\u2019t",
                ".113",
                "love",
                ".064"
            ],
            [
                "service",
                ".112",
                "lol",
                ".061"
            ],
            [
                "customer",
                ".109",
                "you",
                ".060"
            ],
            [
                "why",
                ".108",
                "great",
                ".058"
            ],
            [
                "website",
                ".107",
                "win",
                ".058"
            ],
            [
                "no",
                ".104",
                "\u2019",
                ".058"
            ],
            [
                "?",
                ".098",
                "she",
                ".054"
            ],
            [
                "fix",
                ".093",
                ":",
                ".053"
            ],
            [
                "won\u2019t",
                ".092",
                "that",
                ".053"
            ],
            [
                "been",
                ".090",
                "more",
                ".052"
            ],
            [
                "issue",
                ".089",
                "it",
                ".052"
            ],
            [
                "days",
                ".088",
                "would",
                ".051"
            ],
            [
                "error",
                ".087",
                "him",
                ".047"
            ],
            [
                "is",
                ".084",
                "life",
                ".046"
            ],
            [
                "charged",
                ".083",
                "good",
                ".046"
            ],
            [
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)"
            ],
            [
                "VBN",
                ".141",
                "UH",
                ".104"
            ],
            [
                "$",
                ".118",
                "NNP",
                ".098"
            ],
            [
                "VBZ",
                ".114",
                "PRP",
                ".076"
            ],
            [
                "NN_VBZ",
                ".114",
                "HT",
                ".076"
            ],
            [
                "PRP$",
                ".107",
                "PRP_.",
                ".076"
            ],
            [
                "PRP$_NN",
                ".105",
                "PRP_RB",
                ".067"
            ],
            [
                "VBG",
                ".093",
                "NNP_NNP",
                ".062"
            ],
            [
                "CD",
                ".092",
                "VBP_PRP",
                ".054"
            ],
            [
                "WRB_VBZ",
                ".084",
                "JJ",
                ".053"
            ],
            [
                "VBZ_VBN",
                ".084",
                "DT_JJ",
                ".051"
            ]
        ],
        "question": "Is it true that [CONTINUE] Across unigrams, part-of-speech patterns and word clusters, we see a distinctive pattern emerging around pronoun usage?",
        "answer_label": "yes"
    },
    {
        "id": "4366020a-5cdf-4758-aa6a-8185b337656e",
        "table_caption": "Racial Bias in Hate Speech and Abusive Language Detection Datasets Table 4: Experiment 2, t= \u201cb*tch\u201d",
        "table_column_names": [
            "Dataset",
            "Class",
            "\u02c6 [ITALIC] piblack",
            "\u02c6 [ITALIC] piwhite",
            "[ITALIC] t",
            "[ITALIC] p",
            "\u02c6 [ITALIC] piblack\u02c6 [ITALIC] piwhite"
        ],
        "table_content_values": [
            [
                "[ITALIC] Waseem and Hovy",
                "Racism",
                "0.010",
                "0.010",
                "-0.632",
                "[EMPTY]",
                "0.978"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.963",
                "0.944",
                "20.064",
                "***",
                "1.020"
            ],
            [
                "[ITALIC] Waseem",
                "Racism",
                "0.011",
                "0.011",
                "-1.254",
                "[EMPTY]",
                "0.955"
            ],
            [
                "[EMPTY]",
                "Sexism",
                "0.349",
                "0.290",
                "28.803",
                "***",
                "1.203"
            ],
            [
                "[EMPTY]",
                "Racism and sexism",
                "0.012",
                "0.012",
                "-0.162",
                "[EMPTY]",
                "0.995"
            ],
            [
                "[ITALIC] Davidson et al.",
                "Hate",
                "0.017",
                "0.015",
                "4.698",
                "***",
                "1.152"
            ],
            [
                "[EMPTY]",
                "Offensive",
                "0.988",
                "0.991",
                "-6.289",
                "***",
                "0.997"
            ],
            [
                "[ITALIC] Golbeck et al.",
                "Harassment",
                "0.099",
                "0.091",
                "6.273",
                "***",
                "1.091"
            ],
            [
                "[ITALIC] Founta et al.",
                "Hate",
                "0.074",
                "0.027",
                "46.054",
                "***",
                "2.728"
            ],
            [
                "[EMPTY]",
                "Abusive",
                "0.925",
                "0.968",
                "-41.396",
                "***",
                "0.956"
            ],
            [
                "[EMPTY]",
                "Spam",
                "0.010",
                "0.010",
                "0.000",
                "[EMPTY]",
                "1.000"
            ]
        ],
        "question": "Is it true that The Waseem and Hovy (2016) classifier is not particularly sensitive to the word \"b*tch\" with only 1% of black-aligned and 1% of white-aligned tweets predicted to belong to this class?",
        "answer_label": "no"
    },
    {
        "id": "5e7dfbf4-9542-4f80-a83b-44c4f4753db7",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
        "table_column_names": [
            "[BOLD] Dataset",
            "[BOLD] # pairs",
            "[BOLD] # words (doc)",
            "[BOLD] # sents (docs)",
            "[BOLD] # words (summary)",
            "[BOLD] # sents (summary)",
            "[BOLD] vocab size"
        ],
        "table_content_values": [
            [
                "Multi-News",
                "44,972/5,622/5,622",
                "2,103.49",
                "82.73",
                "263.66",
                "9.97",
                "666,515"
            ],
            [
                "DUC03+04",
                "320",
                "4,636.24",
                "173.15",
                "109.58",
                "2.88",
                "19,734"
            ],
            [
                "TAC 2011",
                "176",
                "4,695.70",
                "188.43",
                "99.70",
                "1.00",
                "24,672"
            ],
            [
                "CNNDM",
                "287,227/13,368/11,490",
                "810.57",
                "39.78",
                "56.20",
                "3.68",
                "717,951"
            ]
        ],
        "question": "Is it true that The total number of words in the concatenated inputs is longer than other MDS datasets, as those consist of 10 input documents, but shorter than SDS datasets, as expected?",
        "answer_label": "no"
    },
    {
        "id": "5d749e06-1775-42bd-b28f-d106eab9163f",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 4: Results of the ablation study on the LDC2017T10 development set.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>",
            "<bold>Size</bold>"
        ],
        "table_content_values": [
            [
                "biLSTM",
                "22.50",
                "30.42",
                "57.6M"
            ],
            [
                "<italic>GEt</italic> + biLSTM",
                "26.33",
                "32.62",
                "59.6M"
            ],
            [
                "<italic>GEb</italic> + biLSTM",
                "26.12",
                "32.49",
                "59.6M"
            ],
            [
                "<italic>GEt</italic> + <italic>GEb</italic> + biLSTM",
                "27.37",
                "33.30",
                "61.7M"
            ]
        ],
        "question": "Is it true that The complete model has slightly more parameters than the model without graph encoders (57.6M vs 61.7M)?",
        "answer_label": "yes"
    },
    {
        "id": "c6b5330f-3246-4e62-a3c8-78aeae05a8f5",
        "table_caption": "Syntactic Dependency Representations in Neural Relation Classification Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
        "table_column_names": [
            "[BOLD] Representation",
            "[BOLD] Hyper parameters Filter size",
            "[BOLD] Hyper parameters Num. Feature maps",
            "[BOLD] Hyper parameters Activation func.",
            "[BOLD] Hyper parameters L2 Reg.",
            "[BOLD] Hyper parameters Learning rate",
            "[BOLD] Hyper parameters Dropout Prob.",
            "[BOLD] F1.(avg. in 5-fold) with default values",
            "[BOLD] F1.(avg. in 5-fold) with optimal values"
        ],
        "table_content_values": [
            [
                "CoNLL08",
                "4-5",
                "1000",
                "Softplus",
                "1.15e+01",
                "1.13e-03",
                "1",
                "73.34",
                "74.49"
            ],
            [
                "SB",
                "4-5",
                "806",
                "Sigmoid",
                "8.13e-02",
                "1.79e-03",
                "0.87",
                "72.83",
                "[BOLD] 75.05"
            ],
            [
                "UD v1.3",
                "5",
                "716",
                "Softplus",
                "1.66e+00",
                "9.63E-04",
                "1",
                "68.93",
                "69.57"
            ]
        ],
        "question": "Is it true that The results furthermore show that the sdps based on the Stanford Basic (SB) representation provide the best performance, followed by the CoNLL08 representation?",
        "answer_label": "yes"
    },
    {
        "id": "0a6b428c-3dd6-45e7-82d2-a04db14738d7",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 3: Performance comparison of our model with different values of m on the two datasets.",
        "table_column_names": [
            "[ITALIC] m",
            "NYT10 Prec.",
            "NYT10 Rec.",
            "NYT10 F1",
            "NYT11 Prec.",
            "NYT11 Rec.",
            "NYT11 F1"
        ],
        "table_content_values": [
            [
                "1",
                "0.541",
                "0.595",
                "[BOLD] 0.566",
                "0.495",
                "0.621",
                "0.551"
            ],
            [
                "2",
                "0.521",
                "0.597",
                "0.556",
                "0.482",
                "0.656",
                "0.555"
            ],
            [
                "3",
                "0.490",
                "0.617",
                "0.547",
                "0.509",
                "0.633",
                "0.564"
            ],
            [
                "4",
                "0.449",
                "0.623",
                "0.522",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "5",
                "0.467",
                "0.609",
                "0.529",
                "0.488",
                "0.677",
                "0.567"
            ]
        ],
        "question": "Is it true that On the NYT11 dataset, m = 4 gives the best performance?",
        "answer_label": "yes"
    },
    {
        "id": "0ee1d441-a931-45f7-ae8d-11c809364782",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 5: The effect of the number of layers inside DCGCN sub-blocks on the AMR15 development set.",
        "table_column_names": [
            "[ITALIC] Block",
            "[ITALIC] n",
            "[ITALIC] m",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "1",
                "1",
                "1",
                "17.6",
                "48.3"
            ],
            [
                "1",
                "1",
                "2",
                "19.2",
                "50.3"
            ],
            [
                "1",
                "2",
                "1",
                "18.4",
                "49.1"
            ],
            [
                "1",
                "1",
                "3",
                "19.6",
                "49.4"
            ],
            [
                "1",
                "3",
                "1",
                "20.0",
                "50.5"
            ],
            [
                "1",
                "3",
                "3",
                "21.4",
                "51.0"
            ],
            [
                "1",
                "3",
                "6",
                "21.8",
                "51.7"
            ],
            [
                "1",
                "6",
                "3",
                "21.7",
                "51.5"
            ],
            [
                "1",
                "6",
                "6",
                "22.0",
                "52.1"
            ],
            [
                "2",
                "3",
                "6",
                "[BOLD] 23.5",
                "53.3"
            ],
            [
                "2",
                "6",
                "3",
                "23.3",
                "[BOLD] 53.4"
            ],
            [
                "2",
                "6",
                "6",
                "22.0",
                "52.1"
            ]
        ],
        "question": "Is it true that We observe that the three settings (n=6, m=3), (n=3, m=6) and (n=6, m=6) give similar results for both 1 DCGCN block and 2 DCGCN blocks?",
        "answer_label": "yes"
    },
    {
        "id": "90528389-92f2-4f21-8903-6700b00bcec4",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that In some cases it seems to make no difference in results, e.g., Europarl in Portuguese which did not increase the precision from P=0.5984 in DF to P=0.6109 in TF, as well as the recall from R=0.5184 in DF to R=0.6727 in TF, resulting in no increase of f-measure from F=0.5555 in DF to F=0.6403 in TF?",
        "answer_label": "no"
    },
    {
        "id": "96697120-9b1f-4d1a-b6f8-dcbc3571a596",
        "table_caption": "IIIDYT at IEST 2018: Implicit Emotion Classification With Deep Contextualized Word Representations Table 5: Fine grained performance on tweets containing emoji, and the effect of removing them.",
        "table_column_names": [
            "[BOLD] Emoji alias",
            "[BOLD] N",
            "[BOLD] emoji #",
            "[BOLD] emoji %",
            "[BOLD] no-emoji #",
            "[BOLD] no-emoji %",
            "[BOLD] \u0394%"
        ],
        "table_content_values": [
            [
                "mask",
                "163",
                "154",
                "94.48",
                "134",
                "82.21",
                "- 12.27"
            ],
            [
                "two_hearts",
                "87",
                "81",
                "93.10",
                "77",
                "88.51",
                "- 4.59"
            ],
            [
                "heart_eyes",
                "122",
                "109",
                "89.34",
                "103",
                "84.43",
                "- 4.91"
            ],
            [
                "heart",
                "267",
                "237",
                "88.76",
                "235",
                "88.01",
                "- 0.75"
            ],
            [
                "rage",
                "92",
                "78",
                "84.78",
                "66",
                "71.74",
                "- 13.04"
            ],
            [
                "cry",
                "116",
                "97",
                "83.62",
                "83",
                "71.55",
                "- 12.07"
            ],
            [
                "sob",
                "490",
                "363",
                "74.08",
                "345",
                "70.41",
                "- 3.67"
            ],
            [
                "unamused",
                "167",
                "121",
                "72.46",
                "116",
                "69.46",
                "- 3.00"
            ],
            [
                "weary",
                "204",
                "140",
                "68.63",
                "139",
                "68.14",
                "- 0.49"
            ],
            [
                "joy",
                "978",
                "649",
                "66.36",
                "629",
                "64.31",
                "- 2.05"
            ],
            [
                "sweat_smile",
                "111",
                "73",
                "65.77",
                "75",
                "67.57",
                "1.80"
            ],
            [
                "confused",
                "77",
                "46",
                "59.74",
                "48",
                "62.34",
                "2.60"
            ]
        ],
        "question": "Is it true that [CONTINUE] The most interesting ones are mask, rage, and cry, which significantly decrease accuracy?",
        "answer_label": "no"
    },
    {
        "id": "f27a4cd3-518d-4311-b4e9-c12bfdeb6b12",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
        "table_column_names": [
            "Model",
            "Training data",
            "Overall",
            "Easy",
            "Hard"
        ],
        "table_content_values": [
            [
                "BERT-large-FT",
                "B-COPA",
                "74.5 (\u00b1 0.7)",
                "74.7 (\u00b1 0.4)",
                "[BOLD] 74.4 (\u00b1 0.9)"
            ],
            [
                "BERT-large-FT",
                "B-COPA (50%)",
                "74.3 (\u00b1 2.2)",
                "76.8 (\u00b1 1.9)",
                "72.8 (\u00b1 3.1)"
            ],
            [
                "BERT-large-FT",
                "COPA",
                "[BOLD] 76.5 (\u00b1 2.7)",
                "[BOLD] 83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA",
                "[BOLD] 89.0 (\u00b1 0.3)",
                "88.9 (\u00b1 2.1)",
                "[BOLD] 89.0 (\u00b1 0.8)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA (50%)",
                "86.1 (\u00b1 2.2)",
                "87.4 (\u00b1 1.1)",
                "85.4 (\u00b1 2.9)"
            ],
            [
                "RoBERTa-large-FT",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "[BOLD] 91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)"
            ]
        ],
        "question": "Is it true that Note that training on B-COPA 50% exposes the model to lexically less diverse training instances than the original COPA due to the high overlap between mirrored alternatives [CONTINUE] These results show that once superficial cues [CONTINUE] are removed, the models are able to learn the task to a high degree?",
        "answer_label": "yes"
    },
    {
        "id": "ea497540-0fb6-42d3-971a-c539b056ba98",
        "table_caption": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.",
        "table_column_names": [
            "Feature",
            "LR P",
            "LR R",
            "LR F1",
            "SVM P",
            "SVM R",
            "SVM F1",
            "ANN P",
            "ANN R",
            "ANN F1"
        ],
        "table_content_values": [
            [
                "+BoW",
                "0.93",
                "0.91",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+BoC (Wiki-PubMed-PMC)",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.91",
                "0.91",
                "[BOLD] 0.91"
            ],
            [
                "+BoC (GloVe)",
                "0.93",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+ASM",
                "0.90",
                "0.85",
                "0.88",
                "0.90",
                "0.86",
                "0.88",
                "0.89",
                "0.89",
                "0.89"
            ],
            [
                "+Sentence Embeddings(SEs)",
                "0.89",
                "0.89",
                "0.89",
                "0.90",
                "0.86",
                "0.88",
                "0.88",
                "0.88",
                "0.88"
            ],
            [
                "+BoC(Wiki-PubMed-PMC)+SEs",
                "0.92",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ]
        ],
        "question": "Is it true that Word embeddings derived from Wiki-PubMed-PMC outperform GloVe-based embeddings (Table 1)?",
        "answer_label": "yes"
    },
    {
        "id": "afb8c8d5-9e11-40ab-92fb-f60a4ab66649",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.",
        "table_column_names": [
            "Uni",
            "POS",
            "0 87.9",
            "1 92.0",
            "2 91.7",
            "3 91.8",
            "4 91.9"
        ],
        "table_content_values": [
            [
                "Uni",
                "SEM",
                "81.8",
                "87.8",
                "87.4",
                "87.6",
                "88.2"
            ],
            [
                "Bi",
                "POS",
                "87.9",
                "93.3",
                "92.9",
                "93.2",
                "92.8"
            ],
            [
                "Bi",
                "SEM",
                "81.9",
                "91.3",
                "90.8",
                "91.9",
                "91.9"
            ],
            [
                "Res",
                "POS",
                "87.9",
                "92.5",
                "91.9",
                "92.0",
                "92.4"
            ],
            [
                "Res",
                "SEM",
                "81.9",
                "88.2",
                "87.5",
                "87.6",
                "88.5"
            ]
        ],
        "question": "Is it true that Comparing POS and SEM tagging (Table 5), we note that higher layer representations improve SEM tagging, while POS tagging peaks at layer 1. we noticed small but consistent improvements in both translation (+0.9 BLEU) and POS and SEM tagging (up to +0.6% accuracy) when using features extracted from an NMT model trained with residual connections (Table 5)?",
        "answer_label": "yes"
    },
    {
        "id": "2373a5b5-05cc-45ca-9e6c-5323513811b8",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.",
        "table_column_names": [
            "Corpus",
            "Metric",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "Europarl",
                "TotalTerms:",
                "957",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "836",
                "1,000"
            ],
            [
                "Europarl",
                "TotalRoots:",
                "44",
                "1",
                "1",
                "1",
                "1",
                "43",
                "1"
            ],
            [
                "Europarl",
                "NumberRels:",
                "1,588",
                "1,025",
                "1,028",
                "1,185",
                "1,103",
                "1,184",
                "999"
            ],
            [
                "Europarl",
                "MaxDepth:",
                "21",
                "921",
                "901",
                "788",
                "835",
                "8",
                "15"
            ],
            [
                "Europarl",
                "MinDepth:",
                "1",
                "921",
                "901",
                "788",
                "835",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgDepth:",
                "11.82",
                "921",
                "901",
                "788",
                "835",
                "3.05",
                "8.46"
            ],
            [
                "Europarl",
                "DepthCohesion:",
                "1.78",
                "1",
                "1",
                "1",
                "1",
                "2.62",
                "1.77"
            ],
            [
                "Europarl",
                "MaxWidth:",
                "20",
                "2",
                "3",
                "4",
                "3",
                "88",
                "41"
            ],
            [
                "Europarl",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgWidth:",
                "1.99",
                "1.03",
                "1.03",
                "1.19",
                "1.10",
                "4.20",
                "2.38"
            ],
            [
                "TED Talks",
                "TotalTerms:",
                "476",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000"
            ],
            [
                "TED Talks",
                "TotalRoots:",
                "164",
                "2",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "NumberRels:",
                "521",
                "1,029",
                "1,331",
                "3,025",
                "3,438",
                "3,802",
                "1,009"
            ],
            [
                "TED Talks",
                "MaxDepth:",
                "16",
                "915",
                "658",
                "454",
                "395",
                "118",
                "12"
            ],
            [
                "TED Talks",
                "MinDepth:",
                "1",
                "913",
                "658",
                "454",
                "395",
                "110",
                "1"
            ],
            [
                "TED Talks",
                "AvgDepth:",
                "5.82",
                "914",
                "658",
                "454",
                "395",
                "112.24",
                "5.95"
            ],
            [
                "TED Talks",
                "DepthCohesion:",
                "2.75",
                "1",
                "1",
                "1",
                "1",
                "1.05",
                "2.02"
            ],
            [
                "TED Talks",
                "MaxWidth:",
                "25",
                "2",
                "77",
                "13",
                "12",
                "66",
                "98"
            ],
            [
                "TED Talks",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "AvgWidth:",
                "1.83",
                "1.03",
                "1.36",
                "3.03",
                "3.44",
                "6.64",
                "2.35"
            ]
        ],
        "question": "Is it true that [CONTINUE] As we can observe in Table 6, limiting the number of terms to 1,000, Patt and DocSub do not to generate relations for all terms?",
        "answer_label": "yes"
    },
    {
        "id": "db25a77b-8a01-4cdf-9925-4697cd1d307f",
        "table_caption": "What do Deep Networks Like to Read? Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.",
        "table_column_names": [
            "[EMPTY]",
            "<bold>RNN</bold>",
            "<bold>CNN</bold>",
            "<bold>DAN</bold>"
        ],
        "table_content_values": [
            [
                "Positive",
                "+9.7",
                "+4.3",
                "+<bold>23.6</bold>"
            ],
            [
                "Negative",
                "+6.9",
                "+5.5",
                "+<bold>16.1</bold>"
            ],
            [
                "Flipped to Positive",
                "+20.2",
                "+24.9",
                "+27.4"
            ],
            [
                "Flipped to Negative",
                "+31.5",
                "+28.6",
                "+19.3"
            ]
        ],
        "question": "Is it true that This is especially true in the case of DAN where we see a decrease as the decoder repeatedly predicts words having low sentiment value?",
        "answer_label": "no"
    },
    {
        "id": "b5d5b9ea-69ce-4b98-99de-831145e49e2b",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 4: Test BLEU for en-de adaptive training, with sequential adaptation to a third task. EWC-tuned models give the best performance on each domain.",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Training scheme",
            "[BOLD] News",
            "[BOLD] TED",
            "[BOLD] IT"
        ],
        "table_content_values": [
            [
                "1",
                "News",
                "37.8",
                "25.3",
                "35.3"
            ],
            [
                "2",
                "TED",
                "23.7",
                "24.1",
                "14.4"
            ],
            [
                "3",
                "IT",
                "1.6",
                "1.8",
                "39.6"
            ],
            [
                "4",
                "News and TED",
                "38.2",
                "25.5",
                "35.4"
            ],
            [
                "5",
                "1 then TED, No-reg",
                "30.6",
                "[BOLD] 27.0",
                "22.1"
            ],
            [
                "6",
                "1 then TED, L2",
                "37.9",
                "26.7",
                "31.8"
            ],
            [
                "7",
                "1 then TED, EWC",
                "[BOLD] 38.3",
                "[BOLD] 27.0",
                "33.1"
            ],
            [
                "8",
                "5 then IT, No-reg",
                "8.0",
                "6.9",
                "56.3"
            ],
            [
                "9",
                "6 then IT, L2",
                "32.3",
                "22.6",
                "56.9"
            ],
            [
                "10",
                "7 then IT, EWC",
                "35.8",
                "24.6",
                "[BOLD] 57.0"
            ]
        ],
        "question": "Is it true that However, EWC outperforms no-reg and L2 on News, not only reducing forgetting but giving 0.5 BLEU improvement over the baseline News model?",
        "answer_label": "yes"
    },
    {
        "id": "d85b8a88-a37f-4803-a198-3a7032d6e695",
        "table_caption": "Suggestion Mining from Online Reviews using ULMFiT Table 3: Performance of different models on the provided train and test dataset for Sub Task A.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] F1 (train)",
            "[BOLD] F1 (test)"
        ],
        "table_content_values": [
            [
                "[BOLD] Multinomial Naive Bayes (using Count Vectorizer)",
                "0.641",
                "0.517"
            ],
            [
                "[BOLD] Logistic Regression (using Count Vectorizer)",
                "0.679",
                "0.572"
            ],
            [
                "[BOLD] SVM (Linear Kernel) (using TfIdf Vectorizer)",
                "0.695",
                "0.576"
            ],
            [
                "[BOLD] LSTM (128 LSTM Units)",
                "0.731",
                "0.591"
            ],
            [
                "[BOLD] Provided Baseline",
                "0.720",
                "0.267"
            ],
            [
                "[BOLD] ULMFit*",
                "0.861",
                "0.701"
            ]
        ],
        "question": "Is it true that [CONTINUE] The Logistic Regression model achieved the best results with a F1-score of 0.679 on the training dataset and a F1-score of 0.572 on the test dataset?",
        "answer_label": "no"
    },
    {
        "id": "773f8b3d-676b-44d0-b830-93b964c3976c",
        "table_caption": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.",
        "table_column_names": [
            "[EMPTY]",
            "dev perp \u2193",
            "dev acc \u2191",
            "dev wer \u2193",
            "test perp \u2193",
            "test acc \u2191",
            "test wer \u2193"
        ],
        "table_content_values": [
            [
                "Spanish-only-LM",
                "329.68",
                "26.6",
                "30.47",
                "322.26",
                "25.1",
                "29.62"
            ],
            [
                "English-only-LM",
                "320.92",
                "29.3",
                "32.02",
                "314.04",
                "30.3",
                "32.51"
            ],
            [
                "All:CS-last-LM",
                "76.64",
                "47.8",
                "14.56",
                "76.97",
                "49.2",
                "14.13"
            ],
            [
                "All:Shuffled-LM",
                "68.00",
                "51.8",
                "13.64",
                "68.72",
                "51.4",
                "13.89"
            ],
            [
                "CS-only-LM",
                "43.20",
                "60.7",
                "12.60",
                "43.42",
                "57.9",
                "12.18"
            ],
            [
                "CS-only+vocab-LM",
                "45.61",
                "61.0",
                "12.56",
                "45.79",
                "58.8",
                "12.49"
            ],
            [
                "Fine-Tuned-LM",
                "39.76",
                "66.9",
                "10.71",
                "40.11",
                "65.4",
                "10.17"
            ],
            [
                "CS-only-disc",
                "\u2013",
                "72.0",
                "6.35",
                "\u2013",
                "70.5",
                "6.70"
            ],
            [
                "Fine-Tuned-disc",
                "\u2013",
                "[BOLD] 74.2",
                "[BOLD] 5.85",
                "\u2013",
                "[BOLD] 75.5",
                "[BOLD] 5.59"
            ]
        ],
        "question": "Is it true that We gain further improvement by adding monolingual data and get an even higher accuracy of 75.5%, which is 10.1 points higher than the best language model?",
        "answer_label": "yes"
    },
    {
        "id": "5e806a3c-f8ff-469f-9d27-e6fc37a34a3d",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
        "table_column_names": [
            "[BOLD] Model",
            "R",
            "MUC P",
            "[ITALIC] F1",
            "R",
            "B3 P",
            "[ITALIC] F1",
            "R",
            "CEAF- [ITALIC] e P",
            "[ITALIC] F1",
            "CoNLL  [ITALIC] F1"
        ],
        "table_content_values": [
            [
                "[BOLD] Baselines",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Cluster+Lemma",
                "76.5",
                "79.9",
                "78.1",
                "71.7",
                "85",
                "77.8",
                "75.5",
                "71.7",
                "73.6",
                "76.5"
            ],
            [
                "CV Cybulska and Vossen ( 2015a )",
                "71",
                "75",
                "73",
                "71",
                "78",
                "74",
                "-",
                "-",
                "64",
                "73"
            ],
            [
                "KCP Kenyon-Dean et\u00a0al. ( 2018 )",
                "67",
                "71",
                "69",
                "71",
                "67",
                "69",
                "71",
                "67",
                "69",
                "69"
            ],
            [
                "Cluster+KCP",
                "68.4",
                "79.3",
                "73.4",
                "67.2",
                "87.2",
                "75.9",
                "77.4",
                "66.4",
                "71.5",
                "73.6"
            ],
            [
                "[BOLD] Model Variants",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Disjoint",
                "75.5",
                "83.6",
                "79.4",
                "75.4",
                "86",
                "80.4",
                "80.3",
                "71.9",
                "75.9",
                "78.5"
            ],
            [
                "Joint",
                "77.6",
                "84.5",
                "80.9",
                "76.1",
                "85.1",
                "80.3",
                "81",
                "73.8",
                "77.3",
                "[BOLD] 79.5"
            ]
        ],
        "question": "Is it true that Our joint model outperforms all the base [CONTINUE] lines with a gap of 10.5 CoNLL F1 points from the last published results (KCP), while surpassing our strong lemma baseline by 3 points?",
        "answer_label": "yes"
    },
    {
        "id": "cf78ecf8-180e-4067-9f5f-5091c236de7d",
        "table_caption": "Building a Production Model for Retrieval-Based Chatbots Table 7: Results of the human evaluation of the responses produced by our model. A response is acceptable if it is either good or great. Note: Numbers may not add up to 100% due to rounding.",
        "table_column_names": [
            "[BOLD] Whitelist",
            "[BOLD] Great",
            "[BOLD] Good",
            "[BOLD] Bad",
            "[BOLD] Accept"
        ],
        "table_content_values": [
            [
                "Freq. 1K",
                "54%",
                "26%",
                "20%",
                "80%"
            ],
            [
                "Cluster. 1K",
                "55%",
                "21%",
                "23%",
                "77%"
            ],
            [
                "Freq. 10K",
                "56%",
                "24%",
                "21%",
                "80%"
            ],
            [
                "Cluster. 10K",
                "57%",
                "23%",
                "20%",
                "80%"
            ],
            [
                "Real response",
                "60%",
                "24%",
                "16%",
                "84%"
            ]
        ],
        "question": "Is it true that Interestingly, the size and type of whitelist have a significant effect on performance, indicating that all the whitelists do not contain responses appropriate to a variety of conversational contexts?",
        "answer_label": "no"
    },
    {
        "id": "3f8320d5-deeb-473d-ba46-1fead5ed6bde",
        "table_caption": "Syntactic Dependency Representations in Neural Relation Classification Table 1: Effect of using the shortest dependency path on each relation type.",
        "table_column_names": [
            "[BOLD] Relation",
            "[BOLD] best F1 (in 5-fold) without sdp",
            "[BOLD] best F1 (in 5-fold) with sdp",
            "[BOLD] Diff."
        ],
        "table_content_values": [
            [
                "USAGE",
                "60.34",
                "80.24",
                "+ 19.90"
            ],
            [
                "MODEL-FEATURE",
                "48.89",
                "70.00",
                "+ 21.11"
            ],
            [
                "PART_WHOLE",
                "29.51",
                "70.27",
                "+40.76"
            ],
            [
                "TOPIC",
                "45.80",
                "91.26",
                "+45.46"
            ],
            [
                "RESULT",
                "54.35",
                "81.58",
                "+27.23"
            ],
            [
                "COMPARE",
                "20.00",
                "61.82",
                "+ 41.82"
            ],
            [
                "macro-averaged",
                "50.10",
                "76.10",
                "+26.00"
            ]
        ],
        "question": "Is it true that We find that the effect of syntactic structure is consistent across the different relation types?",
        "answer_label": "no"
    },
    {
        "id": "a5751137-2fe5-4016-8932-c418dc82cae4",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Feature",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Feature",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams"
            ],
            [
                "not",
                ".154",
                "[URL]",
                ".150"
            ],
            [
                "my",
                ".131",
                "!",
                ".082"
            ],
            [
                "working",
                ".124",
                "he",
                ".069"
            ],
            [
                "still",
                ".123",
                "thank",
                ".067"
            ],
            [
                "on",
                ".119",
                ",",
                ".064"
            ],
            [
                "can\u2019t",
                ".113",
                "love",
                ".064"
            ],
            [
                "service",
                ".112",
                "lol",
                ".061"
            ],
            [
                "customer",
                ".109",
                "you",
                ".060"
            ],
            [
                "why",
                ".108",
                "great",
                ".058"
            ],
            [
                "website",
                ".107",
                "win",
                ".058"
            ],
            [
                "no",
                ".104",
                "\u2019",
                ".058"
            ],
            [
                "?",
                ".098",
                "she",
                ".054"
            ],
            [
                "fix",
                ".093",
                ":",
                ".053"
            ],
            [
                "won\u2019t",
                ".092",
                "that",
                ".053"
            ],
            [
                "been",
                ".090",
                "more",
                ".052"
            ],
            [
                "issue",
                ".089",
                "it",
                ".052"
            ],
            [
                "days",
                ".088",
                "would",
                ".051"
            ],
            [
                "error",
                ".087",
                "him",
                ".047"
            ],
            [
                "is",
                ".084",
                "life",
                ".046"
            ],
            [
                "charged",
                ".083",
                "good",
                ".046"
            ],
            [
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)"
            ],
            [
                "VBN",
                ".141",
                "UH",
                ".104"
            ],
            [
                "$",
                ".118",
                "NNP",
                ".098"
            ],
            [
                "VBZ",
                ".114",
                "PRP",
                ".076"
            ],
            [
                "NN_VBZ",
                ".114",
                "HT",
                ".076"
            ],
            [
                "PRP$",
                ".107",
                "PRP_.",
                ".076"
            ],
            [
                "PRP$_NN",
                ".105",
                "PRP_RB",
                ".067"
            ],
            [
                "VBG",
                ".093",
                "NNP_NNP",
                ".062"
            ],
            [
                "CD",
                ".092",
                "VBP_PRP",
                ".054"
            ],
            [
                "WRB_VBZ",
                ".084",
                "JJ",
                ".053"
            ],
            [
                "VBZ_VBN",
                ".084",
                "DT_JJ",
                ".051"
            ]
        ],
        "question": "Is it true that [CONTINUE] In addition, the presence of verbs in past participle (VBN) is the most distinctive part-of-speech pattern of complaints?",
        "answer_label": "yes"
    },
    {
        "id": "813352e2-948e-4c73-b239-4420a1634970",
        "table_caption": "Keyphrase Generation for Scientific Articles using GANs Table 2: \u03b1-nDCG@5 metrics",
        "table_column_names": [
            "Model",
            "Inspec",
            "Krapivin",
            "NUS",
            "KP20k"
        ],
        "table_content_values": [
            [
                "Catseq",
                "0.87803",
                "0.781",
                "0.82118",
                "0.804"
            ],
            [
                "Catseq-RL",
                "0.8602",
                "[BOLD] 0.786",
                "0.83",
                "0.809"
            ],
            [
                "GAN",
                "[BOLD] 0.891",
                "0.771",
                "[BOLD] 0.853",
                "[BOLD] 0.85"
            ]
        ],
        "question": "Is it true that The difference is most prevalent in KP20k, the largest of the four datasets, where our GAN model (at 0.85) is only marginally better than both the other baseline models?",
        "answer_label": "no"
    },
    {
        "id": "d2534ed4-c340-4211-8610-924f9fb9c445",
        "table_caption": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task Table 5: Textual similarity scores (asymmetric, Multi30k).",
        "table_column_names": [
            "[EMPTY]",
            "EN \u2192 DE R@1",
            "EN \u2192 DE R@5",
            "EN \u2192 DE R@10",
            "DE \u2192 EN R@1",
            "DE \u2192 EN R@5",
            "DE \u2192 EN R@10"
        ],
        "table_content_values": [
            [
                "FME",
                "51.4",
                "76.4",
                "84.5",
                "46.9",
                "71.2",
                "79.1"
            ],
            [
                "AME",
                "[BOLD] 51.7",
                "[BOLD] 76.7",
                "[BOLD] 85.1",
                "[BOLD] 49.1",
                "[BOLD] 72.6",
                "[BOLD] 80.5"
            ]
        ],
        "question": "Is it true that FME outperforms the AME model, confirming the importance of word embeddings adaptation?",
        "answer_label": "no"
    },
    {
        "id": "bf52da4c-3af3-4cc3-9e6d-19e0744ef2fe",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.",
        "table_column_names": [
            "Corpus",
            "Metric",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "Europarl",
                "TotalTerms:",
                "957",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "836",
                "1,000"
            ],
            [
                "Europarl",
                "TotalRoots:",
                "44",
                "1",
                "1",
                "1",
                "1",
                "43",
                "1"
            ],
            [
                "Europarl",
                "NumberRels:",
                "1,588",
                "1,025",
                "1,028",
                "1,185",
                "1,103",
                "1,184",
                "999"
            ],
            [
                "Europarl",
                "MaxDepth:",
                "21",
                "921",
                "901",
                "788",
                "835",
                "8",
                "15"
            ],
            [
                "Europarl",
                "MinDepth:",
                "1",
                "921",
                "901",
                "788",
                "835",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgDepth:",
                "11.82",
                "921",
                "901",
                "788",
                "835",
                "3.05",
                "8.46"
            ],
            [
                "Europarl",
                "DepthCohesion:",
                "1.78",
                "1",
                "1",
                "1",
                "1",
                "2.62",
                "1.77"
            ],
            [
                "Europarl",
                "MaxWidth:",
                "20",
                "2",
                "3",
                "4",
                "3",
                "88",
                "41"
            ],
            [
                "Europarl",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgWidth:",
                "1.99",
                "1.03",
                "1.03",
                "1.19",
                "1.10",
                "4.20",
                "2.38"
            ],
            [
                "TED Talks",
                "TotalTerms:",
                "476",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000"
            ],
            [
                "TED Talks",
                "TotalRoots:",
                "164",
                "2",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "NumberRels:",
                "521",
                "1,029",
                "1,331",
                "3,025",
                "3,438",
                "3,802",
                "1,009"
            ],
            [
                "TED Talks",
                "MaxDepth:",
                "16",
                "915",
                "658",
                "454",
                "395",
                "118",
                "12"
            ],
            [
                "TED Talks",
                "MinDepth:",
                "1",
                "913",
                "658",
                "454",
                "395",
                "110",
                "1"
            ],
            [
                "TED Talks",
                "AvgDepth:",
                "5.82",
                "914",
                "658",
                "454",
                "395",
                "112.24",
                "5.95"
            ],
            [
                "TED Talks",
                "DepthCohesion:",
                "2.75",
                "1",
                "1",
                "1",
                "1",
                "1.05",
                "2.02"
            ],
            [
                "TED Talks",
                "MaxWidth:",
                "25",
                "2",
                "77",
                "13",
                "12",
                "66",
                "98"
            ],
            [
                "TED Talks",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "AvgWidth:",
                "1.83",
                "1.03",
                "1.36",
                "3.03",
                "3.44",
                "6.64",
                "2.35"
            ]
        ],
        "question": "Is it true that Patt model could not generate relations for all terms because terms must to be in a pattern in order to have their taxonomic relation identified?",
        "answer_label": "yes"
    },
    {
        "id": "96c086ca-dbf6-4f2f-b5ca-e65b06ea3b23",
        "table_caption": "Task-Oriented Dialog Systems that Consider Multiple Appropriate Responses under the Same Context Table 5: Human evaluation results. Models with data augmentation are noted as (+). App denotes the average appropriateness score.",
        "table_column_names": [
            "Model",
            "Diversity",
            "App",
            "Good%",
            "OK%",
            "Invalid%"
        ],
        "table_content_values": [
            [
                "DAMD",
                "3.12",
                "2.50",
                "56.5%",
                "[BOLD] 37.4%",
                "6.1%"
            ],
            [
                "DAMD (+)",
                "[BOLD] 3.65",
                "[BOLD] 2.53",
                "[BOLD] 63.0%",
                "27.1%",
                "9.9%"
            ],
            [
                "HDSA (+)",
                "2.14",
                "2.47",
                "57.5%",
                "32.5%",
                "[BOLD] 10.0%"
            ]
        ],
        "question": "Is it true that However, the slightly increased invalid response percentage for the DAMD (+) model compared to the HDSA (+) model suggests that data augmentation may not be the most effective approach. We also observe that HDSA (+) outperforms DAMD in both diversity and appropriateness scores?",
        "answer_label": "no"
    },
    {
        "id": "1354fd83-f529-48f9-9a42-981bb82374b2",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. \u201cPref%\u201d: in how many percentage of documents a system receives the higher human rating.",
        "table_column_names": [
            "Reward",
            "R-1",
            "R-2",
            "R-L",
            "Human",
            "Pref%"
        ],
        "table_content_values": [
            [
                "R-L (original)",
                "40.9",
                "17.8",
                "38.5",
                "1.75",
                "15"
            ],
            [
                "Learned (ours)",
                "39.2",
                "17.4",
                "37.5",
                "[BOLD] 2.20",
                "[BOLD] 75"
            ]
        ],
        "question": "Is it true that When using our learned reward, the generated summaries have significantly higher average human ratings than when using ROUGE as rewards?",
        "answer_label": "no"
    },
    {
        "id": "cbb555a9-2b08-4e5d-a9fe-216378072ded",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 4: Features associated with complaint and non-complaint tweets, sorted by Pearson correlation (r) computed between the normalized frequency of each feature and the complaint label across all tweets. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Feature",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Feature",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams",
                "[BOLD] Unigrams"
            ],
            [
                "not",
                ".154",
                "[URL]",
                ".150"
            ],
            [
                "my",
                ".131",
                "!",
                ".082"
            ],
            [
                "working",
                ".124",
                "he",
                ".069"
            ],
            [
                "still",
                ".123",
                "thank",
                ".067"
            ],
            [
                "on",
                ".119",
                ",",
                ".064"
            ],
            [
                "can\u2019t",
                ".113",
                "love",
                ".064"
            ],
            [
                "service",
                ".112",
                "lol",
                ".061"
            ],
            [
                "customer",
                ".109",
                "you",
                ".060"
            ],
            [
                "why",
                ".108",
                "great",
                ".058"
            ],
            [
                "website",
                ".107",
                "win",
                ".058"
            ],
            [
                "no",
                ".104",
                "\u2019",
                ".058"
            ],
            [
                "?",
                ".098",
                "she",
                ".054"
            ],
            [
                "fix",
                ".093",
                ":",
                ".053"
            ],
            [
                "won\u2019t",
                ".092",
                "that",
                ".053"
            ],
            [
                "been",
                ".090",
                "more",
                ".052"
            ],
            [
                "issue",
                ".089",
                "it",
                ".052"
            ],
            [
                "days",
                ".088",
                "would",
                ".051"
            ],
            [
                "error",
                ".087",
                "him",
                ".047"
            ],
            [
                "is",
                ".084",
                "life",
                ".046"
            ],
            [
                "charged",
                ".083",
                "good",
                ".046"
            ],
            [
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)",
                "[BOLD] POS (Unigrams and Bigrams)"
            ],
            [
                "VBN",
                ".141",
                "UH",
                ".104"
            ],
            [
                "$",
                ".118",
                "NNP",
                ".098"
            ],
            [
                "VBZ",
                ".114",
                "PRP",
                ".076"
            ],
            [
                "NN_VBZ",
                ".114",
                "HT",
                ".076"
            ],
            [
                "PRP$",
                ".107",
                "PRP_.",
                ".076"
            ],
            [
                "PRP$_NN",
                ".105",
                "PRP_RB",
                ".067"
            ],
            [
                "VBG",
                ".093",
                "NNP_NNP",
                ".062"
            ],
            [
                "CD",
                ".092",
                "VBP_PRP",
                ".054"
            ],
            [
                "WRB_VBZ",
                ".084",
                "JJ",
                ".053"
            ],
            [
                "VBZ_VBN",
                ".084",
                "DT_JJ",
                ".051"
            ]
        ],
        "question": "Is it true that [CONTINUE] Mentions of time are specific of complaints (been, still, on, days, Temporal References cluster)?",
        "answer_label": "yes"
    },
    {
        "id": "1ff5fc91-911a-4368-876e-b26811139368",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
        "table_column_names": [
            "[BOLD] Dataset",
            "[BOLD] # pairs",
            "[BOLD] # words (doc)",
            "[BOLD] # sents (docs)",
            "[BOLD] # words (summary)",
            "[BOLD] # sents (summary)",
            "[BOLD] vocab size"
        ],
        "table_content_values": [
            [
                "Multi-News",
                "44,972/5,622/5,622",
                "2,103.49",
                "82.73",
                "263.66",
                "9.97",
                "666,515"
            ],
            [
                "DUC03+04",
                "320",
                "4,636.24",
                "173.15",
                "109.58",
                "2.88",
                "19,734"
            ],
            [
                "TAC 2011",
                "176",
                "4,695.70",
                "188.43",
                "99.70",
                "1.00",
                "24,672"
            ],
            [
                "CNNDM",
                "287,227/13,368/11,490",
                "810.57",
                "39.78",
                "56.20",
                "3.68",
                "717,951"
            ]
        ],
        "question": "Is it true that Our summaries are notably shorter than in other works, about 260 words on average?",
        "answer_label": "no"
    },
    {
        "id": "d6d08181-0eff-4e32-8f7a-d1d0134e99c2",
        "table_caption": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.",
        "table_column_names": [
            "[EMPTY]",
            "dev perp \u2193",
            "dev acc \u2191",
            "dev wer \u2193",
            "test perp \u2193",
            "test acc \u2191",
            "test wer \u2193"
        ],
        "table_content_values": [
            [
                "Spanish-only-LM",
                "329.68",
                "26.6",
                "30.47",
                "322.26",
                "25.1",
                "29.62"
            ],
            [
                "English-only-LM",
                "320.92",
                "29.3",
                "32.02",
                "314.04",
                "30.3",
                "32.51"
            ],
            [
                "All:CS-last-LM",
                "76.64",
                "47.8",
                "14.56",
                "76.97",
                "49.2",
                "14.13"
            ],
            [
                "All:Shuffled-LM",
                "68.00",
                "51.8",
                "13.64",
                "68.72",
                "51.4",
                "13.89"
            ],
            [
                "CS-only-LM",
                "43.20",
                "60.7",
                "12.60",
                "43.42",
                "57.9",
                "12.18"
            ],
            [
                "CS-only+vocab-LM",
                "45.61",
                "61.0",
                "12.56",
                "45.79",
                "58.8",
                "12.49"
            ],
            [
                "Fine-Tuned-LM",
                "39.76",
                "66.9",
                "10.71",
                "40.11",
                "65.4",
                "10.17"
            ],
            [
                "CS-only-disc",
                "\u2013",
                "72.0",
                "6.35",
                "\u2013",
                "70.5",
                "6.70"
            ],
            [
                "Fine-Tuned-disc",
                "\u2013",
                "[BOLD] 74.2",
                "[BOLD] 5.85",
                "\u2013",
                "[BOLD] 75.5",
                "[BOLD] 5.59"
            ]
        ],
        "question": "Is it true that Note that using discriminative training, even with no additional monolingual data, leads to better performance than that of the best language model: the CS-ONLY-DISCRIMINATIVE model achieves an accuracy of 74.2%, 0.3 points less than the accuracy of the FINE-TUNED-LM model?",
        "answer_label": "no"
    },
    {
        "id": "4a849ae1-eaa2-49ab-b46d-23bba1169582",
        "table_caption": "On the difficulty of a distributional semantics of spoken language Table 1: Results on Synthetically Spoken COCO. The row labeled VGS is the visually supervised model from chrupala2017representations.",
        "table_column_names": [
            "[EMPTY]",
            "Recall@10 (%)",
            "Median rank",
            "RSAimage"
        ],
        "table_content_values": [
            [
                "VGS",
                "27",
                "6",
                "0.4"
            ],
            [
                "SegMatch",
                "[BOLD] 10",
                "[BOLD] 37",
                "[BOLD] 0.5"
            ],
            [
                "Audio2vec-U",
                "5",
                "105",
                "0.0"
            ],
            [
                "Audio2vec-C",
                "2",
                "647",
                "0.0"
            ],
            [
                "Mean MFCC",
                "1",
                "1,414",
                "0.0"
            ],
            [
                "Chance",
                "0",
                "3,955",
                "0.0"
            ]
        ],
        "question": "Is it true that It does not come close to VGS on paraphrase retrieval, and it does not correlate with the visual modality better?",
        "answer_label": "no"
    },
    {
        "id": "9cd68def-c73f-452d-add9-53415403de26",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that ACER and PPO do not obtain high performance in inform F1 and match rate?",
        "answer_label": "no"
    },
    {
        "id": "bead6cbe-98f0-4891-a5fc-d3de82369621",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "-{4} dense block",
                "24.8",
                "54.9"
            ],
            [
                "-{3, 4} dense blocks",
                "23.8",
                "54.1"
            ],
            [
                "-{2, 3, 4} dense blocks",
                "23.2",
                "53.1"
            ]
        ],
        "question": "Is it true that In general, the performance of the model does not drop substantially as we remove more dense connections?",
        "answer_label": "no"
    },
    {
        "id": "4c507350-fae9-4a57-8c67-cfddd0d800b6",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 9: Ablation study for modules used in the graph encoder and the LSTM decoder",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "Encoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Linear Combination",
                "23.7",
                "53.2"
            ],
            [
                "-Global Node",
                "24.2",
                "54.6"
            ],
            [
                "-Direction Aggregation",
                "24.6",
                "54.6"
            ],
            [
                "-Graph Attention",
                "24.9",
                "54.7"
            ],
            [
                "-Global Node&Linear Combination",
                "22.9",
                "52.4"
            ],
            [
                "Decoder Modules",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "-Coverage Mechanism",
                "23.8",
                "53.0"
            ]
        ],
        "question": "Is it true that After removing the graph attention module, our model gives 22.9 BLEU points?",
        "answer_label": "no"
    },
    {
        "id": "00cd7c4e-19f7-4e98-876a-c7c38277a86d",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 1: Data statistics comparison for the original E2E data and our cleaned version (number of distinct MRs, total number of textual references, SER as measured by our slot matching script, see Section\u00a03).",
        "table_column_names": [
            "[BOLD] Dataset",
            "[BOLD] Part",
            "[BOLD] MRs",
            "[BOLD] Refs",
            "[BOLD] SER(%)"
        ],
        "table_content_values": [
            [
                "Original",
                "Train",
                "4,862",
                "42,061",
                "17.69"
            ],
            [
                "Original",
                "Dev",
                "547",
                "4,672",
                "11.42"
            ],
            [
                "Original",
                "Test",
                "630",
                "4,693",
                "11.49"
            ],
            [
                "[0.5pt/2pt] Cleaned",
                "Train",
                "8,362",
                "33,525",
                "(0.00)"
            ],
            [
                "[0.5pt/2pt] Cleaned",
                "Dev",
                "1,132",
                "4,299",
                "(0.00)"
            ],
            [
                "[0.5pt/2pt] Cleaned",
                "Test",
                "1,358",
                "4,693",
                "(0.00)"
            ]
        ],
        "question": "Is it true that On the other hand, the number of distinct MRs rose sharply after reannotation; the MRs also have more variance in the number of attributes?",
        "answer_label": "yes"
    },
    {
        "id": "60340fc9-b2cd-46fc-b451-0981d1000f34",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Acc",
            "[BOLD] F1",
            "[BOLD] AUC"
        ],
        "table_content_values": [
            [
                "Most Frequent Class",
                "64.2",
                "39.1",
                "0.500"
            ],
            [
                "Logistic Regression",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Sentiment \u2013 MPQA",
                "64.2",
                "39.1",
                "0.499"
            ],
            [
                "Sentiment \u2013 NRC",
                "63.9",
                "42.2",
                "0.599"
            ],
            [
                "Sentiment \u2013 V&B",
                "68.9",
                "60.0",
                "0.696"
            ],
            [
                "Sentiment \u2013 VADER",
                "66.0",
                "54.2",
                "0.654"
            ],
            [
                "Sentiment \u2013 Stanford",
                "68.0",
                "55.6",
                "0.696"
            ],
            [
                "Complaint Specific (all)",
                "65.7",
                "55.2",
                "0.634"
            ],
            [
                "Request",
                "64.2",
                "39.1",
                "0.583"
            ],
            [
                "Intensifiers",
                "64.5",
                "47.3",
                "0.639"
            ],
            [
                "Downgraders",
                "65.4",
                "49.8",
                "0.615"
            ],
            [
                "Temporal References",
                "64.2",
                "43.7",
                "0.535"
            ],
            [
                "Pronoun Types",
                "64.1",
                "39.1",
                "0.545"
            ],
            [
                "POS Bigrams",
                "72.2",
                "66.8",
                "0.756"
            ],
            [
                "LIWC",
                "71.6",
                "65.8",
                "0.784"
            ],
            [
                "Word2Vec Clusters",
                "67.7",
                "58.3",
                "0.738"
            ],
            [
                "Bag-of-Words",
                "79.8",
                "77.5",
                "0.866"
            ],
            [
                "All Features",
                "[BOLD] 80.5",
                "[BOLD] 78.0",
                "[BOLD] 0.873"
            ],
            [
                "Neural Networks",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "MLP",
                "78.3",
                "76.2",
                "0.845"
            ],
            [
                "LSTM",
                "80.2",
                "77.0",
                "0.864"
            ]
        ],
        "question": "Is it true that The topical features such as the LIWC dictionaries (which combine syntactic and semantic information) and Word2Vec topics do not perform as well as the part of speech tags?",
        "answer_label": "no"
    },
    {
        "id": "1ff499d0-7570-4df6-9550-0be662ce31b5",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that In fact, DocSub had worse results in precision when using both Europarl and Ted Talks corpora in English, where DF reached best values of precision and f-measure?",
        "answer_label": "no"
    },
    {
        "id": "4d93d55b-d069-4e2d-b720-69b1ff950af3",
        "table_caption": "Argument Generation with Retrieval, Planning, and Realization Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
        "table_column_names": [
            "[EMPTY]",
            "[ITALIC] w/ System Retrieval  [BOLD] B-2",
            "[ITALIC] w/ System Retrieval  [BOLD] B-4",
            "[ITALIC] w/ System Retrieval  [BOLD] R-2",
            "[ITALIC] w/ System Retrieval  [BOLD] MTR",
            "[ITALIC] w/ System Retrieval  [BOLD] #Word",
            "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
        ],
        "table_content_values": [
            [
                "Human",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22"
            ],
            [
                "Retrieval",
                "7.55",
                "1.11",
                "8.64",
                "14.38",
                "123",
                "23",
                "10.97",
                "3.05",
                "23.49",
                "20.08",
                "140",
                "21"
            ],
            [
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Seq2seq",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15"
            ],
            [
                "Seq2seqAug",
                "8.26",
                "2.24",
                "13.79",
                "15.75",
                "78",
                "14",
                "10.98",
                "4.41",
                "22.97",
                "19.62",
                "71",
                "14"
            ],
            [
                "[ITALIC] w/o psg",
                "7.94",
                "2.28",
                "10.13",
                "15.71",
                "75",
                "12",
                "9.89",
                "3.34",
                "14.20",
                "18.40",
                "66",
                "12"
            ],
            [
                "H&W\u00a0Hua and Wang ( 2018 )",
                "3.64",
                "0.92",
                "8.83",
                "11.78",
                "51",
                "12",
                "8.51",
                "2.86",
                "18.89",
                "17.18",
                "58",
                "12"
            ],
            [
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "CANDELA",
                "12.02\u2217",
                "[BOLD] 2.99\u2217",
                "[BOLD] 14.93\u2217",
                "[BOLD] 16.92\u2217",
                "119",
                "22",
                "15.80\u2217",
                "[BOLD] 5.00\u2217",
                "[BOLD] 23.75",
                "[BOLD] 20.18",
                "116",
                "22"
            ],
            [
                "[ITALIC] w/o psg",
                "[BOLD] 12.33\u2217",
                "2.86\u2217",
                "14.53\u2217",
                "16.60\u2217",
                "123",
                "23",
                "[BOLD] 16.33\u2217",
                "4.98\u2217",
                "23.65",
                "19.94",
                "123",
                "23"
            ]
        ],
        "question": "Is it true that However, our model generates shorter sentences than human arguments, with about 15 words per sentence compared to 22 words per sentence for human arguments?",
        "answer_label": "no"
    },
    {
        "id": "136c3899-0818-4fac-a86a-9914176d9a8e",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] External",
            "B"
        ],
        "table_content_values": [
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "-",
                "22.0"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "-",
                "23.3"
            ],
            [
                "GCNSEQ (Damonte and Cohen,  2019 )",
                "-",
                "24.4"
            ],
            [
                "DCGCN(single)",
                "-",
                "25.9"
            ],
            [
                "DCGCN(ensemble)",
                "-",
                "[BOLD] 28.2"
            ],
            [
                "TSP (Song et al.,  2016 )",
                "ALL",
                "22.4"
            ],
            [
                "PBMT (Pourdamghani et al.,  2016 )",
                "ALL",
                "26.9"
            ],
            [
                "Tree2Str (Flanigan et al.,  2016 )",
                "ALL",
                "23.0"
            ],
            [
                "SNRG (Song et al.,  2017 )",
                "ALL",
                "25.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "0.2M",
                "27.4"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "0.2M",
                "28.2"
            ],
            [
                "DCGCN(single)",
                "0.1M",
                "29.0"
            ],
            [
                "DCGCN(single)",
                "0.2M",
                "[BOLD] 31.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "2M",
                "32.3"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "2M",
                "33.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "20M",
                "33.8"
            ],
            [
                "DCGCN(single)",
                "0.3M",
                "33.2"
            ],
            [
                "DCGCN(ensemble)",
                "0.3M",
                "[BOLD] 35.3"
            ]
        ],
        "question": "Is it true that These results show that our model is not as effective in terms of using automatically generated AMR graphs?",
        "answer_label": "no"
    },
    {
        "id": "4a0cb1cb-cb53-4f2f-a292-d8f09739ae8f",
        "table_caption": "Assessing Gender Bias in Machine Translation \u2013 A Case Study with Google Translate Table 6: Percentage of female, male and neutral gender pronouns obtained for each BLS occupation category, averaged over all occupations in said category and tested languages detailed in Table",
        "table_column_names": [
            "Category",
            "Female (%)",
            "Male (%)",
            "Neutral (%)"
        ],
        "table_content_values": [
            [
                "Office and administrative support",
                "11.015",
                "58.812",
                "16.954"
            ],
            [
                "Architecture and engineering",
                "2.299",
                "72.701",
                "10.92"
            ],
            [
                "Farming, fishing, and forestry",
                "12.179",
                "62.179",
                "14.744"
            ],
            [
                "Management",
                "11.232",
                "66.667",
                "12.681"
            ],
            [
                "Community and social service",
                "20.238",
                "62.5",
                "10.119"
            ],
            [
                "Healthcare support",
                "25.0",
                "43.75",
                "17.188"
            ],
            [
                "Sales and related",
                "8.929",
                "62.202",
                "16.964"
            ],
            [
                "Installation, maintenance, and repair",
                "5.22",
                "58.333",
                "17.125"
            ],
            [
                "Transportation and material moving",
                "8.81",
                "62.976",
                "17.5"
            ],
            [
                "Legal",
                "11.905",
                "72.619",
                "10.714"
            ],
            [
                "Business and financial operations",
                "7.065",
                "67.935",
                "15.58"
            ],
            [
                "Life, physical, and social science",
                "5.882",
                "73.284",
                "10.049"
            ],
            [
                "Arts, design, entertainment, sports, and media",
                "10.36",
                "67.342",
                "11.486"
            ],
            [
                "Education, training, and library",
                "23.485",
                "53.03",
                "9.091"
            ],
            [
                "Building and grounds cleaning and maintenance",
                "12.5",
                "68.333",
                "11.667"
            ],
            [
                "Personal care and service",
                "18.939",
                "49.747",
                "18.434"
            ],
            [
                "Healthcare practitioners and technical",
                "22.674",
                "51.744",
                "15.116"
            ],
            [
                "Production",
                "14.331",
                "51.199",
                "18.245"
            ],
            [
                "Computer and mathematical",
                "4.167",
                "66.146",
                "14.062"
            ],
            [
                "Construction and extraction",
                "8.578",
                "61.887",
                "17.525"
            ],
            [
                "Protective service",
                "8.631",
                "65.179",
                "12.5"
            ],
            [
                "Food preparation and serving related",
                "21.078",
                "58.333",
                "17.647"
            ],
            [
                "Total",
                "11.76",
                "58.93",
                "15.939"
            ]
        ],
        "question": "Is it true that Furthermore, this bias is seemingly not aggravated for fields suggested to be troubled by male stereotypes, such as life and physical sciences, architecture, engineering, computer science and mathematics?",
        "answer_label": "no"
    },
    {
        "id": "f240b702-2fe4-4303-8584-e97282356a54",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 2: Combined within- and cross-document entity coreference results on the ECB+ test set.",
        "table_column_names": [
            "[BOLD] Model",
            "R",
            "MUC P",
            "[ITALIC] F1",
            "R",
            "B3 P",
            "[ITALIC] F1",
            "R",
            "CEAF- [ITALIC] e P",
            "[ITALIC] F1",
            "CoNLL  [ITALIC] F1"
        ],
        "table_content_values": [
            [
                "Cluster+Lemma",
                "71.3",
                "83",
                "76.7",
                "53.4",
                "84.9",
                "65.6",
                "70.1",
                "52.5",
                "60",
                "67.4"
            ],
            [
                "Disjoint",
                "76.7",
                "80.8",
                "78.7",
                "63.2",
                "78.2",
                "69.9",
                "65.3",
                "58.3",
                "61.6",
                "70"
            ],
            [
                "Joint",
                "78.6",
                "80.9",
                "79.7",
                "65.5",
                "76.4",
                "70.5",
                "65.4",
                "61.3",
                "63.3",
                "[BOLD] 71.2"
            ]
        ],
        "question": "Is it true that Our joint model improves upon the strong lemma baseline by 3.8 points in CoNLL F1 score?",
        "answer_label": "yes"
    },
    {
        "id": "0ea80dd7-1266-4389-a316-5dea81a6c8e7",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 5: Performance breakdown of the PRKGC+NS model. Derivation Precision denotes ROUGE-L F1 of generated NLDs.",
        "table_column_names": [
            "# gold NLD steps",
            "Answer Prec.",
            "Derivation Prec."
        ],
        "table_content_values": [
            [
                "1",
                "79.2",
                "38.4"
            ],
            [
                "2",
                "64.4",
                "48.6"
            ],
            [
                "3",
                "62.3",
                "41.3"
            ]
        ],
        "question": "Is it true that As shown in Table 5, as the required derivation step increases, the PRKGC+NS model suffers from predicting answer entities and generating correct NLDs?",
        "answer_label": "yes"
    },
    {
        "id": "957b738e-152b-46ad-b45e-b4e422ebe50c",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 5: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts and selecting the best parent.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1038",
                "0.0170",
                "0.0490",
                "0.0641",
                "0.0641",
                "0.0613",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1282",
                "0.0291",
                "0.0410",
                "0.0270",
                "0.0270",
                "0.1154",
                "0.0661"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.6185",
                "0.3744",
                "0.4144",
                "0.4394",
                "0.4394",
                "[BOLD] 0.7553",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.6308",
                "0.4124",
                "0.4404",
                "0.4515",
                "0.4945",
                "[BOLD] 0.8609",
                "0.5295"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "[BOLD] 0.0021",
                "0.0004",
                "0.0011",
                "0.0014",
                "0.0014",
                "0.0013",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0011",
                "0.0008",
                "0.0011",
                "0.0008",
                "0.0008",
                "[BOLD] 0.0030",
                "0.0018"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0012",
                "0.0008",
                "0.0009",
                "0.0010",
                "0.0010",
                "[BOLD] 0.0016",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0003",
                "0.0009",
                "0.0009",
                "0.0010",
                "0.0010",
                "[BOLD] 0.0017",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "[BOLD] 0.0041",
                "0.0007",
                "0.0021",
                "0.0027",
                "0.0027",
                "0.0026",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0022",
                "0.0016",
                "0.0022",
                "0.0015",
                "0.0015",
                "[BOLD] 0.0058",
                "0.0036"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0024",
                "0.0016",
                "0.0018",
                "0.0019",
                "0.0019",
                "[BOLD] 0.0031",
                "0.0023"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0005",
                "0.0018",
                "0.0018",
                "0.0020",
                "0.0021",
                "[BOLD] 0.0034",
                "0.0022"
            ]
        ],
        "question": "Is it true that As filtering out multiple hypernyms might remove also correct relations, the recall values for all corpora are very low?",
        "answer_label": "yes"
    },
    {
        "id": "052c61e4-3626-4d15-b461-16728483b42f",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 4: KL-divergence between different dialog policy and the human dialog KL(\u03c0turns||pturns), where \u03c0turns denotes the discrete distribution over the number of dialog turns of simulated sessions between the policy \u03c0 and the agenda-based user simulator, and pturns for the real human-human dialog.",
        "table_column_names": [
            "GP-MBCM",
            "ACER",
            "PPO",
            "ALDM",
            "GDPL"
        ],
        "table_content_values": [
            [
                "1.666",
                "0.775",
                "0.639",
                "1.069",
                "[BOLD] 0.238"
            ]
        ],
        "question": "Is it true that Table 4 shows that GDPL has the largest KL-divergence to the human on the number of dialog turns over the baselines, which implies that GDPL behaves less like the human?",
        "answer_label": "no"
    },
    {
        "id": "94b2149c-0f67-46ca-823e-e03b57610d66",
        "table_caption": "Building a Production Model for Retrieval-Based Chatbots Table 7: Results of the human evaluation of the responses produced by our model. A response is acceptable if it is either good or great. Note: Numbers may not add up to 100% due to rounding.",
        "table_column_names": [
            "[BOLD] Whitelist",
            "[BOLD] Great",
            "[BOLD] Good",
            "[BOLD] Bad",
            "[BOLD] Accept"
        ],
        "table_content_values": [
            [
                "Freq. 1K",
                "54%",
                "26%",
                "20%",
                "80%"
            ],
            [
                "Cluster. 1K",
                "55%",
                "21%",
                "23%",
                "77%"
            ],
            [
                "Freq. 10K",
                "56%",
                "24%",
                "21%",
                "80%"
            ],
            [
                "Cluster. 10K",
                "57%",
                "23%",
                "20%",
                "80%"
            ],
            [
                "Real response",
                "60%",
                "24%",
                "16%",
                "84%"
            ]
        ],
        "question": "Is it true that Interestingly, the size and type of whitelist seem to have little effect on performance, indicating that all the whitelists contain responses appropriate to a variety of conversational contexts?",
        "answer_label": "yes"
    },
    {
        "id": "9eb45685-012a-4854-aa9a-db7991101942",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that [CONTINUE] It is perceptible that GDPL has better performance than GDPL-sess on the task success and is comparable regarding the dialog turns, [CONTINUE] GDPL also outperforms GDPL-discr?",
        "answer_label": "yes"
    },
    {
        "id": "99af5f47-1b08-4a45-8235-734b854449ad",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Type",
            "[BOLD] English-German #P",
            "[BOLD] English-German B",
            "[BOLD] English-German C",
            "[BOLD] English-Czech #P",
            "[BOLD] English-Czech B",
            "[BOLD] English-Czech C"
        ],
        "table_content_values": [
            [
                "BoW+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "12.2",
                "-",
                "-",
                "7.5",
                "-"
            ],
            [
                "CNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "13.7",
                "-",
                "-",
                "8.7",
                "-"
            ],
            [
                "BiRNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "16.1",
                "-",
                "-",
                "9.6",
                "-"
            ],
            [
                "PB-SMT (Beck et al.,  2018 )",
                "Single",
                "-",
                "12.8",
                "43.2",
                "-",
                "8.6",
                "36.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Single",
                "41.4M",
                "15.5",
                "40.8",
                "39.1M",
                "8.9",
                "33.8"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Single",
                "41.2M",
                "16.7",
                "42.4",
                "38.8M",
                "9.8",
                "33.3"
            ],
            [
                "DCGCN (ours)",
                "Single",
                "[BOLD]  29.7M",
                "[BOLD] 19.0",
                "[BOLD] 44.1",
                "[BOLD]  28.3M",
                "[BOLD] 12.1",
                "[BOLD] 37.1"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Ensemble",
                "207M",
                "19.0",
                "44.1",
                "195M",
                "11.3",
                "36.4"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Ensemble",
                "206M",
                "19.6",
                "45.1",
                "194M",
                "11.7",
                "35.9"
            ],
            [
                "DCGCN (ours)",
                "Ensemble",
                "[BOLD]  149M",
                "[BOLD] 20.5",
                "[BOLD] 45.8",
                "[BOLD]  142M",
                "[BOLD] 13.1",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that Our single model DCGCN(single) does not outperform all the single models, as it only achieves 19.0 and 12.1 BLEU points on the En-De and EnCs tasks, respectively?",
        "answer_label": "no"
    },
    {
        "id": "3c4ed041-3a0d-436a-a068-dccdb94ff06c",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 2: Test error (ERR) on document classification task. \u201c#Params\u201d: the parameter number in AmaPolar task. Time: time in seconds per training batch measured from 1k training steps on GeForce GTX 1080 Ti.",
        "table_column_names": [
            "Model",
            "Model",
            "#Params",
            "AmaPolar ERR",
            "AmaPolar Time",
            "Yahoo ERR",
            "Yahoo Time",
            "AmaFull ERR",
            "AmaFull Time",
            "YelpPolar ERR",
            "YelpPolar Time"
        ],
        "table_content_values": [
            [
                "Zhang et\u00a0al. ( 2015 )",
                "Zhang et\u00a0al. ( 2015 )",
                "-",
                "6.10",
                "-",
                "29.16",
                "-",
                "40.57",
                "-",
                "5.26",
                "-"
            ],
            [
                "This",
                "LSTM",
                "227K",
                "[BOLD] 4.37",
                "0.947",
                "[BOLD] 24.62",
                "1.332",
                "37.22",
                "1.003",
                "3.58",
                "1.362"
            ],
            [
                "This",
                "GRU",
                "176K",
                "4.39",
                "0.948",
                "24.68",
                "1.242",
                "[BOLD] 37.20",
                "0.982",
                "[BOLD] 3.47",
                "1.230"
            ],
            [
                "This",
                "ATR",
                "74K",
                "4.78",
                "0.867",
                "25.33",
                "1.117",
                "38.54",
                "0.836",
                "4.00",
                "1.124"
            ],
            [
                "Work",
                "SRU",
                "194K",
                "4.95",
                "0.919",
                "24.78",
                "1.394",
                "38.23",
                "0.907",
                "3.99",
                "1.310"
            ],
            [
                "[EMPTY]",
                "LRN",
                "151K",
                "4.98",
                "[BOLD] 0.731",
                "25.07",
                "[BOLD] 1.038",
                "38.42",
                "[BOLD] 0.788",
                "3.98",
                "[BOLD] 1.022"
            ]
        ],
        "question": "Is it true that [CONTINUE] LRN accelerates the training over LSTM and SRU by about 20%,?",
        "answer_label": "yes"
    },
    {
        "id": "1b72f46f-433f-431f-a9ed-6475f5455096",
        "table_caption": "Zero-Shot Grounding of Objects from Natural Language Queries Table 3: Category-wise performance with the default split of Flickr30k Entities.",
        "table_column_names": [
            "Method",
            "Overall",
            "people",
            "clothing",
            "bodyparts",
            "animals",
            "vehicles",
            "instruments",
            "scene",
            "other"
        ],
        "table_content_values": [
            [
                "QRC - VGG(det)",
                "60.21",
                "75.08",
                "55.9",
                "20.27",
                "73.36",
                "68.95",
                "45.68",
                "65.27",
                "38.8"
            ],
            [
                "CITE - VGG(det)",
                "61.89",
                "[BOLD] 75.95",
                "58.50",
                "30.78",
                "[BOLD] 77.03",
                "[BOLD] 79.25",
                "48.15",
                "58.78",
                "43.24"
            ],
            [
                "ZSGNet - VGG (cls)",
                "60.12",
                "72.52",
                "60.57",
                "38.51",
                "63.61",
                "64.47",
                "49.59",
                "64.66",
                "41.09"
            ],
            [
                "ZSGNet - Res50 (cls)",
                "[BOLD] 63.39",
                "73.87",
                "[BOLD] 66.18",
                "[BOLD] 45.27",
                "73.79",
                "71.38",
                "[BOLD] 58.54",
                "[BOLD] 66.49",
                "[BOLD] 45.53"
            ]
        ],
        "question": "Is it true that However, on the classes like \"clothing\" and \"bodyparts\" our model ZSGNet does not show much better performance?",
        "answer_label": "no"
    },
    {
        "id": "47157bc0-08a1-4857-952c-75b652a9ec42",
        "table_caption": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.",
        "table_column_names": [
            "[EMPTY]",
            "dev perp \u2193",
            "dev acc \u2191",
            "dev wer \u2193",
            "test perp \u2193",
            "test acc \u2191",
            "test wer \u2193"
        ],
        "table_content_values": [
            [
                "Spanish-only-LM",
                "329.68",
                "26.6",
                "30.47",
                "322.26",
                "25.1",
                "29.62"
            ],
            [
                "English-only-LM",
                "320.92",
                "29.3",
                "32.02",
                "314.04",
                "30.3",
                "32.51"
            ],
            [
                "All:CS-last-LM",
                "76.64",
                "47.8",
                "14.56",
                "76.97",
                "49.2",
                "14.13"
            ],
            [
                "All:Shuffled-LM",
                "68.00",
                "51.8",
                "13.64",
                "68.72",
                "51.4",
                "13.89"
            ],
            [
                "CS-only-LM",
                "43.20",
                "60.7",
                "12.60",
                "43.42",
                "57.9",
                "12.18"
            ],
            [
                "CS-only+vocab-LM",
                "45.61",
                "61.0",
                "12.56",
                "45.79",
                "58.8",
                "12.49"
            ],
            [
                "Fine-Tuned-LM",
                "39.76",
                "66.9",
                "10.71",
                "40.11",
                "65.4",
                "10.17"
            ],
            [
                "CS-only-disc",
                "\u2013",
                "72.0",
                "6.35",
                "\u2013",
                "70.5",
                "6.70"
            ],
            [
                "Fine-Tuned-disc",
                "\u2013",
                "[BOLD] 74.2",
                "[BOLD] 5.85",
                "\u2013",
                "[BOLD] 75.5",
                "[BOLD] 5.59"
            ]
        ],
        "question": "Is it true that Similarly, when using discriminative trainthe FINE-TUNED-DISCRIMINATIVE model ing, outperforms the CS-ONLY-DISCRIMINATIVE model?",
        "answer_label": "yes"
    },
    {
        "id": "ff3845c0-9328-4bc8-a651-072c91290d64",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. \u201cEn\u201d column is an English autoencoder. BLEU scores are given for reference.",
        "table_column_names": [
            "[ITALIC] k",
            "Ar",
            "Es",
            "Fr",
            "Ru",
            "Zh",
            "En"
        ],
        "table_content_values": [
            [
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy"
            ],
            [
                "0",
                "88.0",
                "87.9",
                "87.9",
                "87.8",
                "87.7",
                "87.4"
            ],
            [
                "1",
                "92.4",
                "91.9",
                "92.1",
                "92.1",
                "91.5",
                "89.4"
            ],
            [
                "2",
                "91.9",
                "91.8",
                "91.8",
                "91.8",
                "91.3",
                "88.3"
            ],
            [
                "3",
                "92.0",
                "92.3",
                "92.1",
                "91.6",
                "91.2",
                "87.9"
            ],
            [
                "4",
                "92.1",
                "92.4",
                "92.5",
                "92.0",
                "90.5",
                "86.9"
            ],
            [
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy"
            ],
            [
                "0",
                "81.9",
                "81.9",
                "81.8",
                "81.8",
                "81.8",
                "81.2"
            ],
            [
                "1",
                "87.9",
                "87.7",
                "87.8",
                "87.9",
                "87.7",
                "84.5"
            ],
            [
                "2",
                "87.4",
                "87.5",
                "87.4",
                "87.3",
                "87.2",
                "83.2"
            ],
            [
                "3",
                "87.8",
                "87.9",
                "87.9",
                "87.3",
                "87.3",
                "82.9"
            ],
            [
                "4",
                "88.3",
                "88.6",
                "88.4",
                "88.1",
                "87.7",
                "82.1"
            ],
            [
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU"
            ],
            [
                "[EMPTY]",
                "32.7",
                "49.1",
                "38.5",
                "34.2",
                "32.1",
                "96.6"
            ]
        ],
        "question": "Is it true that [CONTINUE] Turning to SEM tagging (Table 3, second block), representations from layers 1 through 4 boost the performance to around 87-88%, [CONTINUE] which is far above the UnsupEmb and MFT baselines?",
        "answer_label": "yes"
    },
    {
        "id": "54b945d8-03c2-4cd1-8058-4bc1bcb9d223",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that [CONTINUE] It is perceptible that GDPL-sess has better performance than GDPL on the task success and is comparable regarding the dialog turns, [CONTINUE] GDPL-discr also outperforms GDPL?",
        "answer_label": "no"
    },
    {
        "id": "2ca8cd4b-2fd2-4e9b-be58-dc16c5750fc9",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Balanced",
            "Throughput (instances/s) Moderate",
            "Throughput (instances/s) Linear"
        ],
        "table_content_values": [
            [
                "1",
                "46.7",
                "27.3",
                "7.6"
            ],
            [
                "10",
                "125.2",
                "78.2",
                "22.7"
            ],
            [
                "25",
                "129.7",
                "83.1",
                "45.4"
            ]
        ],
        "question": "Is it true that As a result, our implementation can train input data of balanced trees with greater throughput than input data of unbalanced trees, but the throughput of the linear dataset increases more significantly when the batch size increases from 1 to 25?",
        "answer_label": "no"
    },
    {
        "id": "30e73d0d-6f33-471a-87ca-8d240db19162",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.",
        "table_column_names": [
            "Model",
            "Encoder",
            "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1",
            "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
            "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
            "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
            "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1",
            "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
            "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
            "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
        ],
        "table_content_values": [
            [
                "MLP",
                "CNN-RNN",
                ".311",
                ".340",
                ".486",
                ".532",
                ".318",
                ".335",
                ".481",
                ".524"
            ],
            [
                "MLP",
                "PMeans-RNN",
                ".313",
                ".331",
                ".489",
                ".536",
                ".354",
                ".375",
                ".502",
                ".556"
            ],
            [
                "MLP",
                "BERT",
                "[BOLD] .487",
                "[BOLD] .526",
                "[BOLD] .544",
                "[BOLD] .597",
                "[BOLD] .505",
                "[BOLD] .531",
                "[BOLD] .556",
                "[BOLD] .608"
            ],
            [
                "SimRed",
                "CNN",
                ".340",
                ".392",
                ".470",
                ".515",
                ".396",
                ".443",
                ".499",
                ".549"
            ],
            [
                "SimRed",
                "PMeans",
                ".354",
                ".393",
                ".493",
                ".541",
                ".370",
                ".374",
                ".507",
                ".551"
            ],
            [
                "SimRed",
                "BERT",
                ".266",
                ".296",
                ".458",
                ".495",
                ".325",
                ".338",
                ".485",
                ".533"
            ],
            [
                "Peyrard and Gurevych ( 2018 )",
                "Peyrard and Gurevych ( 2018 )",
                ".177",
                ".189",
                ".271",
                ".306",
                ".175",
                ".186",
                ".268",
                ".174"
            ]
        ],
        "question": "Is it true that Specifically, BERT+MLP+Pref does not significantly outperform (p < 0.05) all the other models that do not use BERT+MLP?",
        "answer_label": "no"
    },
    {
        "id": "29c9462b-ac10-4436-83e3-afab19d30849",
        "table_caption": "Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.",
        "table_column_names": [
            "[EMPTY]",
            "DUC\u201901 <italic>R</italic>1",
            "DUC\u201901 <italic>R</italic>2",
            "DUC\u201902 <italic>R</italic>1",
            "DUC\u201902 <italic>R</italic>2",
            "DUC\u201904 <italic>R</italic>1",
            "DUC\u201904 <italic>R</italic>2"
        ],
        "table_content_values": [
            [
                "ICSI",
                "33.31",
                "7.33",
                "35.04",
                "8.51",
                "37.31",
                "9.36"
            ],
            [
                "PriorSum",
                "35.98",
                "7.89",
                "36.63",
                "8.97",
                "38.91",
                "10.07"
            ],
            [
                "TCSum",
                "<bold>36.45</bold>",
                "7.66",
                "36.90",
                "8.61",
                "38.27",
                "9.66"
            ],
            [
                "TCSum\u2212",
                "33.45",
                "6.07",
                "34.02",
                "7.39",
                "35.66",
                "8.66"
            ],
            [
                "SRSum",
                "36.04",
                "8.44",
                "<bold>38.93</bold>",
                "<bold>10.29</bold>",
                "39.29",
                "10.70"
            ],
            [
                "DeepTD",
                "28.74",
                "5.95",
                "31.63",
                "7.09",
                "33.57",
                "7.96"
            ],
            [
                "REAPER",
                "32.43",
                "6.84",
                "35.03",
                "8.11",
                "37.22",
                "8.64"
            ],
            [
                "RELIS",
                "34.73",
                "<bold>8.66</bold>",
                "37.11",
                "9.12",
                "<bold>39.34</bold>",
                "<bold>10.73</bold>"
            ]
        ],
        "question": "Is it true that [CONTINUE] RELIS significantly outperforms the other RL-based systems?",
        "answer_label": "yes"
    },
    {
        "id": "e5e0d1cc-b6b3-4ce5-bd6b-3a00628c32ba",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 3: Test BLEU for es-en adaptive training. EWC reduces forgetting compared to other fine-tuning methods, while offering the greatest improvement on the new domain.",
        "table_column_names": [
            "[EMPTY]",
            "[BOLD] Training scheme",
            "[BOLD] Health",
            "[BOLD] Bio"
        ],
        "table_content_values": [
            [
                "1",
                "Health",
                "[BOLD] 35.9",
                "33.1"
            ],
            [
                "2",
                "Bio",
                "29.6",
                "36.1"
            ],
            [
                "3",
                "Health and Bio",
                "35.8",
                "37.2"
            ],
            [
                "4",
                "1 then Bio, No-reg",
                "30.3",
                "36.6"
            ],
            [
                "5",
                "1 then Bio, L2",
                "35.1",
                "37.3"
            ],
            [
                "6",
                "1 then Bio, EWC",
                "35.2",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that We find EWC outperforms the L2 approach?",
        "answer_label": "yes"
    },
    {
        "id": "d13b3ebf-01ff-42c0-9b3e-999d9d28f9cd",
        "table_caption": "Distant Learning for Entity Linking with Automatic Noise Detection Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
        "table_column_names": [
            "System",
            "All P",
            "All R",
            "All F1",
            "In  [ITALIC] E+ P",
            "In  [ITALIC] E+ R",
            "In  [ITALIC] E+ F1"
        ],
        "table_content_values": [
            [
                "Name matching",
                "15.03",
                "15.03",
                "15.03",
                "29.13",
                "29.13",
                "29.13"
            ],
            [
                "MIL (model 1)",
                "35.87",
                "35.87",
                "35.87 \u00b10.72",
                "69.38",
                "69.38",
                "69.38 \u00b11.29"
            ],
            [
                "MIL-ND (model 2)",
                "37.42",
                "[BOLD] 37.42",
                "37.42 \u00b10.35",
                "72.50",
                "[BOLD] 72.50",
                "[BOLD] 72.50 \u00b10.68"
            ],
            [
                "[ITALIC] \u03c4MIL-ND (model 2)",
                "[BOLD] 38.91",
                "36.73",
                "[BOLD] 37.78 \u00b10.26",
                "[BOLD] 73.19",
                "71.15",
                "72.16 \u00b10.48"
            ],
            [
                "Supervised learning",
                "42.90",
                "42.90",
                "42.90 \u00b10.59",
                "83.12",
                "83.12",
                "83.12 \u00b11.15"
            ]
        ],
        "question": "Is it true that [CONTINUE] MIL-ND achieves higher precision, recall, and F1 than MIL, [CONTINUE] Using its confidence at test time (\u03c4 MIL-ND, 'All' setting) was also beneficial in terms of precision and F1 (it cannot possibly increase recall)?",
        "answer_label": "yes"
    },
    {
        "id": "da02f66d-8a1d-4ea0-aea6-e404a1c7037a",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Type",
            "[BOLD] English-German #P",
            "[BOLD] English-German B",
            "[BOLD] English-German C",
            "[BOLD] English-Czech #P",
            "[BOLD] English-Czech B",
            "[BOLD] English-Czech C"
        ],
        "table_content_values": [
            [
                "BoW+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "12.2",
                "-",
                "-",
                "7.5",
                "-"
            ],
            [
                "CNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "13.7",
                "-",
                "-",
                "8.7",
                "-"
            ],
            [
                "BiRNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "16.1",
                "-",
                "-",
                "9.6",
                "-"
            ],
            [
                "PB-SMT (Beck et al.,  2018 )",
                "Single",
                "-",
                "12.8",
                "43.2",
                "-",
                "8.6",
                "36.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Single",
                "41.4M",
                "15.5",
                "40.8",
                "39.1M",
                "8.9",
                "33.8"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Single",
                "41.2M",
                "16.7",
                "42.4",
                "38.8M",
                "9.8",
                "33.3"
            ],
            [
                "DCGCN (ours)",
                "Single",
                "[BOLD]  29.7M",
                "[BOLD] 19.0",
                "[BOLD] 44.1",
                "[BOLD]  28.3M",
                "[BOLD] 12.1",
                "[BOLD] 37.1"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Ensemble",
                "207M",
                "19.0",
                "44.1",
                "195M",
                "11.3",
                "36.4"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Ensemble",
                "206M",
                "19.6",
                "45.1",
                "194M",
                "11.7",
                "35.9"
            ],
            [
                "DCGCN (ours)",
                "Ensemble",
                "[BOLD]  149M",
                "[BOLD] 20.5",
                "[BOLD] 45.8",
                "[BOLD]  142M",
                "[BOLD] 13.1",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that PB-SMT is the phrase-based statistical machine translation model using Moses (Koehn et al., 2007)?",
        "answer_label": "yes"
    },
    {
        "id": "cd32feb8-8dd5-43fa-8568-60aaaffe74d8",
        "table_caption": "Low-supervision urgency detection and transfer in short crisis messages TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal",
        "table_column_names": [
            "System",
            "Accuracy",
            "Precision",
            "Recall",
            "F-Measure"
        ],
        "table_content_values": [
            [
                "Local",
                "63.97%",
                "64.27%",
                "64.50%",
                "63.93%"
            ],
            [
                "Manual",
                "64.25%",
                "[BOLD] 70.84%\u2217\u2217",
                "48.50%",
                "57.11%"
            ],
            [
                "Wiki",
                "67.25%",
                "66.51%",
                "69.50%",
                "67.76%"
            ],
            [
                "Local-Manual",
                "65.75%",
                "67.96%",
                "59.50%",
                "62.96%"
            ],
            [
                "Wiki-Local",
                "67.40%",
                "65.54%",
                "68.50%",
                "66.80%"
            ],
            [
                "Wiki-Manual",
                "67.75%",
                "70.38%",
                "63.00%",
                "65.79%"
            ],
            [
                "[ITALIC] Our Approach",
                "[BOLD] 69.25%\u2217\u2217\u2217",
                "68.76%",
                "[BOLD] 70.50%\u2217\u2217",
                "[BOLD] 69.44%\u2217\u2217\u2217"
            ]
        ],
        "question": "Is it true that Manual features reduce recall, but do not help the system to improve accuracy and precision?",
        "answer_label": "no"
    },
    {
        "id": "5f1f4b6d-1767-41f9-9314-c9dcd0205077",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1192",
                "0.0083",
                "0.0137",
                "0.0150",
                "0.0150",
                "0.0445",
                "0.0326"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1022",
                "0.0069",
                "0.0060",
                "0.0092",
                "0.0090",
                "0.0356",
                "0.0162"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5710",
                "0.1948",
                "0.3855",
                "0.5474",
                "0.4485",
                "[BOLD] 0.8052",
                "0.4058"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "[BOLD] 0.6304",
                "0.1870",
                "0.3250",
                "0.5312",
                "0.4576",
                "0.6064",
                "0.3698"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0037",
                "0.3278",
                "0.5941",
                "0.6486",
                "[BOLD] 0.6490",
                "0.0017",
                "0.0003"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0002",
                "0.1486",
                "0.4332",
                "[BOLD] 0.6467",
                "0.6332",
                "0.0967",
                "0.0003"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0002",
                "0.1562",
                "0.5157",
                "[BOLD] 0.7255",
                "0.5932",
                "0.0032",
                "0.0001"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "2.10-5",
                "0.0507",
                "0.4492",
                "[BOLD] 0.7000",
                "0.5887",
                "0.1390",
                "0.0002"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0073",
                "0.0162",
                "0.0268",
                "[BOLD] 0.0293",
                "[BOLD] 0.0293",
                "0.0033",
                "0.0006"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0004",
                "0.0132",
                "0.0118",
                "0.0181",
                "0.0179",
                "[BOLD] 0.0520",
                "0.0005"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0005",
                "0.1733",
                "0.4412",
                "[BOLD] 0.6240",
                "0.5109",
                "0.0064",
                "0.0002"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "4.10-5",
                "0.0798",
                "0.3771",
                "[BOLD] 0.6040",
                "0.5149",
                "0.2261",
                "0.0004"
            ]
        ],
        "question": "Is it true that The highest values of precision are achieved by DSim model, and the highest recalls are obtained by HClust and Patt models?",
        "answer_label": "no"
    },
    {
        "id": "736c33a1-48f2-4f67-939b-397fd82f51f2",
        "table_caption": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 6: Results of the Human Rating on CWC.",
        "table_column_names": [
            "[EMPTY]",
            "Ours Better(%)",
            "No Prefer(%)",
            "Ours Worse(%)"
        ],
        "table_content_values": [
            [
                "Retrieval-Stgy\u00a0",
                "[BOLD] 62",
                "22",
                "16"
            ],
            [
                "PMI\u00a0",
                "[BOLD] 54",
                "32",
                "14"
            ],
            [
                "Neural\u00a0",
                "[BOLD] 60",
                "22",
                "18"
            ],
            [
                "Kernel\u00a0",
                "[BOLD] 62",
                "26",
                "12"
            ]
        ],
        "question": "Is it true that Our agent outperforms the comparison agents with a large margin?",
        "answer_label": "yes"
    },
    {
        "id": "910499d2-85dd-428e-a7f5-268b24bfa673",
        "table_caption": "Semantic Neural Machine Translation using AMR Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.",
        "table_column_names": [
            "System",
            "NC-v11 BLEU",
            "NC-v11 TER\u2193",
            "NC-v11 Meteor",
            "Full BLEU",
            "Full TER\u2193",
            "Full Meteor"
        ],
        "table_content_values": [
            [
                "OpenNMT-tf",
                "15.1",
                "0.6902",
                "0.3040",
                "24.3",
                "0.5567",
                "0.4225"
            ],
            [
                "Transformer-tf",
                "17.1",
                "0.6647",
                "0.3578",
                "25.1",
                "0.5537",
                "0.4344"
            ],
            [
                "Seq2seq",
                "16.0",
                "0.6695",
                "0.3379",
                "23.7",
                "0.5590",
                "0.4258"
            ],
            [
                "Dual2seq-LinAMR",
                "17.3",
                "0.6530",
                "0.3612",
                "24.0",
                "0.5643",
                "0.4246"
            ],
            [
                "Duel2seq-SRL",
                "17.2",
                "0.6591",
                "0.3644",
                "23.8",
                "0.5626",
                "0.4223"
            ],
            [
                "Dual2seq-Dep",
                "17.8",
                "0.6516",
                "0.3673",
                "25.0",
                "0.5538",
                "0.4328"
            ],
            [
                "Dual2seq",
                "[BOLD] *19.2*",
                "[BOLD] 0.6305",
                "[BOLD] 0.3840",
                "[BOLD] *25.5*",
                "[BOLD] 0.5480",
                "[BOLD] 0.4376"
            ]
        ],
        "question": "Is it true that [CONTINUE] Dual2seq is not significantly better than Seq2seq in both settings, [CONTINUE] In particular, the improvement is much smaller under the small-scale setting (+3.2 BLEU) than that under the large-scale setting (+1.7 BLEU)?",
        "answer_label": "no"
    },
    {
        "id": "5b739ed7-b554-4969-8cb2-5d048179aeb5",
        "table_caption": "Revisiting Joint Modeling of Cross-documentEntity and Event Coreference Resolution Table 3: Combined within- and cross-document event coreference results on the ECB+ test set.",
        "table_column_names": [
            "<bold>Model</bold>",
            "R",
            "MUC P",
            "<italic>F</italic>1",
            "R",
            "B3 P",
            "<italic>F</italic>1",
            "R",
            "CEAF-<italic>e</italic> P",
            "<italic>F</italic>1",
            "CoNLL <italic>F</italic>1"
        ],
        "table_content_values": [
            [
                "<bold>Baselines</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Cluster+Lemma",
                "76.5",
                "79.9",
                "78.1",
                "71.7",
                "85",
                "77.8",
                "75.5",
                "71.7",
                "73.6",
                "76.5"
            ],
            [
                "CV Cybulska and Vossen (<ref id='bib-bib8'>2015a</ref>)",
                "71",
                "75",
                "73",
                "71",
                "78",
                "74",
                "-",
                "-",
                "64",
                "73"
            ],
            [
                "KCP Kenyon-Dean et\u00a0al. (<ref id='bib-bib14'>2018</ref>)",
                "67",
                "71",
                "69",
                "71",
                "67",
                "69",
                "71",
                "67",
                "69",
                "69"
            ],
            [
                "Cluster+KCP",
                "68.4",
                "79.3",
                "73.4",
                "67.2",
                "87.2",
                "75.9",
                "77.4",
                "66.4",
                "71.5",
                "73.6"
            ],
            [
                "<bold>Model Variants</bold>",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Disjoint",
                "75.5",
                "83.6",
                "79.4",
                "75.4",
                "86",
                "80.4",
                "80.3",
                "71.9",
                "75.9",
                "78.5"
            ],
            [
                "Joint",
                "77.6",
                "84.5",
                "80.9",
                "76.1",
                "85.1",
                "80.3",
                "81",
                "73.8",
                "77.3",
                "<bold>79.5</bold>"
            ]
        ],
        "question": "Is it true that [CONTINUE] Our model achieves state-of-the-art results, outperforming previous models by 10.5 CoNLL F1 points on events,?",
        "answer_label": "yes"
    },
    {
        "id": "b8a1ebb9-8375-438d-a7a3-af682c33ac69",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE VII: Precision scores for the Analogy Test",
        "table_column_names": [
            "Methods",
            "# dims",
            "Analg. (sem)",
            "Analg. (syn)",
            "Total"
        ],
        "table_content_values": [
            [
                "GloVe",
                "300",
                "78.94",
                "64.12",
                "70.99"
            ],
            [
                "Word2Vec",
                "300",
                "81.03",
                "66.11",
                "73.03"
            ],
            [
                "OIWE-IPG",
                "300",
                "19.99",
                "23.44",
                "21.84"
            ],
            [
                "SOV",
                "3000",
                "64.09",
                "46.26",
                "54.53"
            ],
            [
                "SPINE",
                "1000",
                "17.07",
                "8.68",
                "12.57"
            ],
            [
                "Word2Sense",
                "2250",
                "12.94",
                "19.44",
                "5.84"
            ],
            [
                "Proposed",
                "300",
                "79.96",
                "63.52",
                "71.15"
            ]
        ],
        "question": "Is it true that Our proposed method outperforms GloVe in semantic analogy test set and in overall results, while GloVe performs slightly better in syntactic test set?",
        "answer_label": "yes"
    },
    {
        "id": "7cead150-e3a2-4135-aacf-47c24848a499",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "-{4} dense block",
                "24.8",
                "54.9"
            ],
            [
                "-{3, 4} dense blocks",
                "23.8",
                "54.1"
            ],
            [
                "-{2, 3, 4} dense blocks",
                "23.2",
                "53.1"
            ]
        ],
        "question": "Is it true that Without using the dense connections in the last two blocks, the score drops to 23.8?",
        "answer_label": "yes"
    },
    {
        "id": "e2aba604-ba6c-4e5b-a15a-91c898f2453a",
        "table_caption": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure TABLE VI: Correlations for Word Similarity Tests",
        "table_column_names": [
            "Dataset (EN-)",
            "GloVe",
            "Word2Vec",
            "OIWE-IPG",
            "SOV",
            "SPINE",
            "Word2Sense",
            "Proposed"
        ],
        "table_content_values": [
            [
                "WS-353-ALL",
                "0.612",
                "0.7156",
                "0.634",
                "0.622",
                "0.173",
                "0.690",
                "0.657"
            ],
            [
                "SIMLEX-999",
                "0.359",
                "0.3939",
                "0.295",
                "0.355",
                "0.090",
                "0.380",
                "0.381"
            ],
            [
                "VERB-143",
                "0.326",
                "0.4430",
                "0.255",
                "0.271",
                "0.293",
                "0.271",
                "0.348"
            ],
            [
                "SimVerb-3500",
                "0.193",
                "0.2856",
                "0.184",
                "0.197",
                "0.035",
                "0.234",
                "0.245"
            ],
            [
                "WS-353-REL",
                "0.578",
                "0.6457",
                "0.595",
                "0.578",
                "0.134",
                "0.695",
                "0.619"
            ],
            [
                "RW-STANF.",
                "0.378",
                "0.4858",
                "0.316",
                "0.373",
                "0.122",
                "0.390",
                "0.382"
            ],
            [
                "YP-130",
                "0.524",
                "0.5211",
                "0.353",
                "0.482",
                "0.169",
                "0.420",
                "0.589"
            ],
            [
                "MEN-TR-3k",
                "0.710",
                "0.7528",
                "0.684",
                "0.696",
                "0.298",
                "0.769",
                "0.725"
            ],
            [
                "RG-65",
                "0.768",
                "0.8051",
                "0.736",
                "0.732",
                "0.338",
                "0.761",
                "0.774"
            ],
            [
                "MTurk-771",
                "0.650",
                "0.6712",
                "0.593",
                "0.623",
                "0.199",
                "0.665",
                "0.671"
            ],
            [
                "WS-353-SIM",
                "0.682",
                "0.7883",
                "0.713",
                "0.702",
                "0.220",
                "0.720",
                "0.720"
            ],
            [
                "MC-30",
                "0.749",
                "0.8112",
                "0.799",
                "0.726",
                "0.330",
                "0.735",
                "0.776"
            ],
            [
                "MTurk-287",
                "0.649",
                "0.6645",
                "0.591",
                "0.631",
                "0.295",
                "0.674",
                "0.634"
            ],
            [
                "Average",
                "0.552",
                "0.6141",
                "0.519",
                "0.538",
                "0.207",
                "0.570",
                "0.579"
            ]
        ],
        "question": "Is it true that It should also be noted that scores obtained by SPINE is unacceptably low on almost all tests indicating that it has achieved its interpretability performance at the cost of losing its semantic functions?",
        "answer_label": "yes"
    },
    {
        "id": "6ba6dcd8-17b3-4498-8a29-4345b38b7aa1",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 5: Performance of ExtAbsRL with different reward functions, measured in terms of ROUGE (center) and human judgements (right). Using our learned reward yields significantly (p=0.0057) higher average human rating. \u201cPref%\u201d: in how many percentage of documents a system receives the higher human rating.",
        "table_column_names": [
            "Reward",
            "R-1",
            "R-2",
            "R-L",
            "Human",
            "Pref%"
        ],
        "table_content_values": [
            [
                "R-L (original)",
                "40.9",
                "17.8",
                "38.5",
                "1.75",
                "15"
            ],
            [
                "Learned (ours)",
                "39.2",
                "17.4",
                "37.5",
                "[BOLD] 2.20",
                "[BOLD] 75"
            ]
        ],
        "question": "Is it true that [CONTINUE] It is clear from Table 5 that using the learned reward helps the RL-based system generate summaries with significantly higher human ratings?",
        "answer_label": "yes"
    },
    {
        "id": "ea35a87b-5630-4eb2-b60a-28894e2b6299",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 2: Averages of rankings of the words in same-gender pairs vs. different-gender pairs for Italian and German, along with their differences. Og stands for the original embeddings, Db for the debiased embeddings, and En for English. Each row presents the averages of pairs with the respective scores in SimLex-999 (0\u20134, 4\u20137, 7\u201310).",
        "table_column_names": [
            "[EMPTY]",
            "Italian Same-gender",
            "Italian Diff-Gender",
            "Italian difference",
            "German Same-gender",
            "German Diff-Gender",
            "German difference"
        ],
        "table_content_values": [
            [
                "7\u201310",
                "Og: 4884",
                "Og: 12947",
                "Og: 8063",
                "Og: 5925",
                "Og: 33604",
                "Og: 27679"
            ],
            [
                "7\u201310",
                "Db: 5523",
                "Db: 7312",
                "Db: 1789",
                "Db: 7653",
                "Db: 26071",
                "Db: 18418"
            ],
            [
                "7\u201310",
                "En: 6978",
                "En: 2467",
                "En: -4511",
                "En: 4517",
                "En: 8666",
                "En: 4149"
            ],
            [
                "4\u20137",
                "Og: 10954",
                "Og: 15838",
                "Og: 4884",
                "Og: 19271",
                "Og: 27256",
                "Og: 7985"
            ],
            [
                "4\u20137",
                "Db: 12037",
                "Db: 12564",
                "Db: 527",
                "Db: 24845",
                "Db: 22970",
                "Db: -1875"
            ],
            [
                "4\u20137",
                "En: 15891",
                "En: 17782",
                "En: 1891",
                "En: 13282",
                "En: 17649",
                "En: 4367"
            ],
            [
                "0\u20134",
                "Og: 23314",
                "Og: 35783",
                "Og: 12469",
                "Og: 50983",
                "Og: 85263",
                "Og: 34280"
            ],
            [
                "0\u20134",
                "Db: 26386",
                "Db: 28067",
                "Db: 1681",
                "Db: 60603",
                "Db: 79081",
                "Db: 18478"
            ],
            [
                "0\u20134",
                "En: 57278",
                "En: 53053",
                "En: -4225",
                "En: 41509",
                "En: 62929",
                "En: 21420"
            ]
        ],
        "question": "Is it true that As expected, the average ranking of samegender pairs is significantly lower than that of different-gender pairs, both for German and Italian, while the difference between the sets in English is much smaller?",
        "answer_label": "yes"
    },
    {
        "id": "dc8b32c8-ec58-46a5-85b9-4a7eb85ba965",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Balanced",
            "Throughput (instances/s) Moderate",
            "Throughput (instances/s) Linear"
        ],
        "table_content_values": [
            [
                "1",
                "46.7",
                "27.3",
                "7.6"
            ],
            [
                "10",
                "125.2",
                "78.2",
                "22.7"
            ],
            [
                "25",
                "129.7",
                "83.1",
                "45.4"
            ]
        ],
        "question": "Is it true that [CONTINUE] Another interesting fact in Table 1 is that the training throughput on the linear dataset scales better than the throughput on the balanced dataset, as the batch size increases?",
        "answer_label": "yes"
    },
    {
        "id": "a487bf03-88aa-46db-bf2b-25eba56b5a37",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 5: Results of fine-tuned models on Balanced COPA. Easy: instances with superficial cues, Hard: instances without superficial cues.",
        "table_column_names": [
            "Model",
            "Training data",
            "Overall",
            "Easy",
            "Hard"
        ],
        "table_content_values": [
            [
                "BERT-large-FT",
                "B-COPA",
                "74.5 (\u00b1 0.7)",
                "74.7 (\u00b1 0.4)",
                "[BOLD] 74.4 (\u00b1 0.9)"
            ],
            [
                "BERT-large-FT",
                "B-COPA (50%)",
                "74.3 (\u00b1 2.2)",
                "76.8 (\u00b1 1.9)",
                "72.8 (\u00b1 3.1)"
            ],
            [
                "BERT-large-FT",
                "COPA",
                "[BOLD] 76.5 (\u00b1 2.7)",
                "[BOLD] 83.9 (\u00b1 4.4)",
                "71.9 (\u00b1 2.5)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA",
                "[BOLD] 89.0 (\u00b1 0.3)",
                "88.9 (\u00b1 2.1)",
                "[BOLD] 89.0 (\u00b1 0.8)"
            ],
            [
                "RoBERTa-large-FT",
                "B-COPA (50%)",
                "86.1 (\u00b1 2.2)",
                "87.4 (\u00b1 1.1)",
                "85.4 (\u00b1 2.9)"
            ],
            [
                "RoBERTa-large-FT",
                "COPA",
                "87.7 (\u00b1 0.9)",
                "[BOLD] 91.6 (\u00b1 1.1)",
                "85.3 (\u00b1 2.0)"
            ]
        ],
        "question": "Is it true that However, training on B-COPA does not necessarily improve performance on the Hard subset, even when training with all 1000 instances in B-COPA, and when matching the training size of the original COPA (500 instances, B-COPA 50%)?",
        "answer_label": "no"
    },
    {
        "id": "211e093d-d629-48fa-bdba-ab688d36cc5b",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 6: Results on SimLex-999 and WordSim-353, in Italian and German, before and after debiasing.",
        "table_column_names": [
            "[EMPTY]",
            "Italian Orig",
            "Italian Debias",
            "German Orig",
            "German Debias"
        ],
        "table_content_values": [
            [
                "SimLex",
                "0.280",
                "[BOLD] 0.288",
                "0.343",
                "[BOLD] 0.356"
            ],
            [
                "WordSim",
                "0.548",
                "[BOLD] 0.577",
                "0.547",
                "[BOLD] 0.553"
            ]
        ],
        "question": "Is it true that In both cases, the new embeddings perform better than the original ones?",
        "answer_label": "yes"
    },
    {
        "id": "0fb25186-f020-4e5a-9e5f-e3d96653c844",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that In some cases it seems to make difference in results, e.g., Europarl in Portuguese which increased the precision from P=0.5984 in DF to P=0.6109 in TF, as well as the recall from R=0.5184 in DF to R=0.6727 in TF, resulting in an increase of f-measure from F=0.5555 in DF to F=0.6403 in TF?",
        "answer_label": "yes"
    },
    {
        "id": "a4e8cf0d-5a37-4d81-b804-b773d5b80be4",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that Under the same setting, our model does not consistently outperform graph encoders based on recurrent neural networks or gating mechanisms?",
        "answer_label": "no"
    },
    {
        "id": "209c036c-49d4-4305-82fd-7e422df199d1",
        "table_caption": "Neural End-to-End Learning for Computational Argumentation Mining Table 3: Performance of MTL sequence tagging approaches, essay level. Tasks separated by \u201c:\u201d. Layers from which tasks feed are indicated by respective numbers.",
        "table_column_names": [
            "[EMPTY]",
            "C-F1 100%",
            "C-F1 50%",
            "R-F1 100%",
            "R-F1 50%",
            "F1 100%",
            "F1 50%"
        ],
        "table_content_values": [
            [
                "Y-3",
                "49.59",
                "65.37",
                "26.28",
                "37.00",
                "34.35",
                "47.25"
            ],
            [
                "Y-3:Y<italic>C</italic>-1",
                "54.71",
                "66.84",
                "28.44",
                "37.35",
                "37.40",
                "47.92"
            ],
            [
                "Y-3:Y<italic>R</italic>-1",
                "51.32",
                "66.49",
                "26.92",
                "37.18",
                "35.31",
                "47.69"
            ],
            [
                "Y-3:Y<italic>C</italic>-3",
                "<bold>54.58</bold>",
                "67.66",
                "<bold>30.22</bold>",
                "<bold>40.30</bold>",
                "<bold>38.90</bold>",
                "<bold>50.51</bold>"
            ],
            [
                "Y-3:Y<italic>R</italic>-3",
                "53.31",
                "66.71",
                "26.65",
                "35.86",
                "35.53",
                "46.64"
            ],
            [
                "Y-3:Y<italic>C</italic>-1:Y<italic>R</italic>-2",
                "52.95",
                "<bold>67.84</bold>",
                "27.90",
                "39.71",
                "36.54",
                "50.09"
            ],
            [
                "Y-3:Y<italic>C</italic>-3:Y<italic>R</italic>-3",
                "54.55",
                "67.60",
                "28.30",
                "38.26",
                "37.26",
                "48.86"
            ]
        ],
        "question": "Is it true that We find that when we train STagBL with only its main task\u2014with label set [CONTINUE] In Y contrast, when we include the 'natural subtasks' \"C\" (label [CONTINUE] performance decreases typically by a few percentage points?",
        "answer_label": "no"
    },
    {
        "id": "b7183bab-8092-4b58-8b4c-32d184f4ece2",
        "table_caption": "Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.",
        "table_column_names": [
            "[EMPTY]",
            "DUC\u201901 <italic>R</italic>1",
            "DUC\u201901 <italic>R</italic>2",
            "DUC\u201902 <italic>R</italic>1",
            "DUC\u201902 <italic>R</italic>2",
            "DUC\u201904 <italic>R</italic>1",
            "DUC\u201904 <italic>R</italic>2"
        ],
        "table_content_values": [
            [
                "ICSI",
                "33.31",
                "7.33",
                "35.04",
                "8.51",
                "37.31",
                "9.36"
            ],
            [
                "PriorSum",
                "35.98",
                "7.89",
                "36.63",
                "8.97",
                "38.91",
                "10.07"
            ],
            [
                "TCSum",
                "<bold>36.45</bold>",
                "7.66",
                "36.90",
                "8.61",
                "38.27",
                "9.66"
            ],
            [
                "TCSum\u2212",
                "33.45",
                "6.07",
                "34.02",
                "7.39",
                "35.66",
                "8.66"
            ],
            [
                "SRSum",
                "36.04",
                "8.44",
                "<bold>38.93</bold>",
                "<bold>10.29</bold>",
                "39.29",
                "10.70"
            ],
            [
                "DeepTD",
                "28.74",
                "5.95",
                "31.63",
                "7.09",
                "33.57",
                "7.96"
            ],
            [
                "REAPER",
                "32.43",
                "6.84",
                "35.03",
                "8.11",
                "37.22",
                "8.64"
            ],
            [
                "RELIS",
                "34.73",
                "<bold>8.66</bold>",
                "37.11",
                "9.12",
                "<bold>39.34</bold>",
                "<bold>10.73</bold>"
            ]
        ],
        "question": "Is it true that At the same time, RELIS performs on par with neural-based TCSum and SRSum, while it requires significantly less data and time to train, as shown next?",
        "answer_label": "yes"
    },
    {
        "id": "ee82637b-643d-49e2-a8a8-f2553f993db0",
        "table_caption": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment Table 1: Results on belinkov2014exploring\u2019s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et\u00a0al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Sch\u00fctze (2015) on GloVe.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] Initialization",
            "[BOLD] Embedding",
            "[BOLD] Resources",
            "[BOLD] Test Acc."
        ],
        "table_content_values": [
            [
                "HPCD (full)",
                "Syntactic-SG",
                "Type",
                "WordNet, VerbNet",
                "88.7"
            ],
            [
                "LSTM-PP",
                "GloVe",
                "Type",
                "-",
                "84.3"
            ],
            [
                "LSTM-PP",
                "GloVe-retro",
                "Type",
                "WordNet",
                "84.8"
            ],
            [
                "OntoLSTM-PP",
                "GloVe-extended",
                "Token",
                "WordNet",
                "[BOLD] 89.7"
            ]
        ],
        "question": "Is it true that OntoLSTM-PP does not outperform HPCD (full), the previous best result on this dataset?",
        "answer_label": "no"
    },
    {
        "id": "acf36685-577f-4ce5-b514-630c07cd800c",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 2: Summary-level correlation of learned reward functions. All results are averaged over 5-fold cross validations. Unlike the metrics in Table\u00a01, all rewards in this table do not require reference summaries.",
        "table_column_names": [
            "Model",
            "Encoder",
            "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] \u03c1",
            "[ITALIC] Reg. loss (Eq. ( 1 ))  [ITALIC] r",
            "[ITALIC] Reg. loss (Eq. ( 1 )) G-Pre",
            "[ITALIC] Reg. loss (Eq. ( 1 )) G-Rec",
            "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] \u03c1",
            "[ITALIC] Pref. loss (Eq. ( 3 ))  [ITALIC] r",
            "[ITALIC] Pref. loss (Eq. ( 3 )) G-Pre",
            "[ITALIC] Pref. loss (Eq. ( 3 )) G-Rec"
        ],
        "table_content_values": [
            [
                "MLP",
                "CNN-RNN",
                ".311",
                ".340",
                ".486",
                ".532",
                ".318",
                ".335",
                ".481",
                ".524"
            ],
            [
                "MLP",
                "PMeans-RNN",
                ".313",
                ".331",
                ".489",
                ".536",
                ".354",
                ".375",
                ".502",
                ".556"
            ],
            [
                "MLP",
                "BERT",
                "[BOLD] .487",
                "[BOLD] .526",
                "[BOLD] .544",
                "[BOLD] .597",
                "[BOLD] .505",
                "[BOLD] .531",
                "[BOLD] .556",
                "[BOLD] .608"
            ],
            [
                "SimRed",
                "CNN",
                ".340",
                ".392",
                ".470",
                ".515",
                ".396",
                ".443",
                ".499",
                ".549"
            ],
            [
                "SimRed",
                "PMeans",
                ".354",
                ".393",
                ".493",
                ".541",
                ".370",
                ".374",
                ".507",
                ".551"
            ],
            [
                "SimRed",
                "BERT",
                ".266",
                ".296",
                ".458",
                ".495",
                ".325",
                ".338",
                ".485",
                ".533"
            ],
            [
                "Peyrard and Gurevych ( 2018 )",
                "Peyrard and Gurevych ( 2018 )",
                ".177",
                ".189",
                ".271",
                ".306",
                ".175",
                ".186",
                ".268",
                ".174"
            ]
        ],
        "question": "Is it true that Specifically, BERT+MLP+Pref significantly outperforms (p < 0.05) all the other models that do not use BERT+MLP,?",
        "answer_label": "yes"
    },
    {
        "id": "57547cfd-917f-4e7d-a554-236cf763e4a1",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 5: Group text features associated with tweets that are complaints and not complaints. Features are sorted by Pearson correlation (r) between their each feature\u2019s normalized frequency and the outcome. We restrict to only the top six categories for each feature type. All correlations are significant at p",
        "table_column_names": [
            "[BOLD] Complaints  [BOLD] Label",
            "[BOLD] Complaints  [BOLD] Words",
            "[BOLD] Complaints  [ITALIC] r",
            "[BOLD] Not Complaints  [BOLD] Label",
            "[BOLD] Not Complaints  [BOLD] Words",
            "[BOLD] Not Complaints  [ITALIC] r"
        ],
        "table_content_values": [
            [
                "[BOLD] LIWC Features",
                "[BOLD] LIWC Features",
                "[BOLD] LIWC Features",
                "[BOLD] LIWC Features",
                "[BOLD] LIWC Features",
                "[BOLD] LIWC Features"
            ],
            [
                "NEGATE",
                "not, no, can\u2019t, don\u2019t, never, nothing, doesn\u2019t, won\u2019t",
                ".271",
                "POSEMO",
                "thanks, love, thank, good, great, support, lol, win",
                ".185"
            ],
            [
                "RELATIV",
                "in, on, when, at, out, still, now, up, back, new",
                ".225",
                "AFFECT",
                "thanks, love, thank, good, great, support, lol",
                ".111"
            ],
            [
                "FUNCTION",
                "the, i, to, a, my, and, you, for, is, in",
                ".204",
                "SHEHE",
                "he, his, she, her, him, he\u2019s, himself",
                ".105"
            ],
            [
                "TIME",
                "when, still, now, back, new, never, after, then, waiting",
                ".186",
                "MALE",
                "he, his, man, him, sir, he\u2019s, son",
                ".086"
            ],
            [
                "DIFFER",
                "not, but, if, or, can\u2019t, really, than, other, haven\u2019t",
                ".169",
                "FEMALE",
                "she, her, girl, mom, ma, lady, mother, female, mrs",
                ".084"
            ],
            [
                "COGPROC",
                "not, but, how, if, all, why, or, any, need",
                ".132",
                "ASSENT",
                "yes, ok, awesome, okay, yeah, cool, absolutely, agree",
                ".080"
            ],
            [
                "[BOLD] Word2Vec Clusters",
                "[BOLD] Word2Vec Clusters",
                "[BOLD] Word2Vec Clusters",
                "[BOLD] Word2Vec Clusters",
                "[BOLD] Word2Vec Clusters",
                "[BOLD] Word2Vec Clusters"
            ],
            [
                "Cust. Service",
                "service, customer, contact, job, staff, assist, agent",
                ".136",
                "Gratitude",
                "thanks, thank, good, great, support, everyone, huge, proud",
                ".089"
            ],
            [
                "Order",
                "order, store, buy, free, delivery, available, package",
                ".128",
                "Family",
                "old, friend, family, mom, wife, husband, younger",
                ".063"
            ],
            [
                "Issues",
                "delayed, closed, between, outage, delay, road, accident",
                ".122",
                "Voting",
                "favorite, part, stars, model, vote, models, represent",
                ".060"
            ],
            [
                "Time Ref.",
                "been, yet, haven\u2019t, long, happened, yesterday, took",
                ".122",
                "Contests",
                "Christmas, gift, receive, entered, giveaway, enter, cards",
                ".058"
            ],
            [
                "Tech Parts",
                "battery, laptop, screen, warranty, desktop, printer",
                ".100",
                "Pets",
                "dogs, cat, dog, pet, shepherd, fluffy, treats",
                ".054"
            ],
            [
                "Access",
                "use, using, error, password, access, automatically, reset",
                ".098",
                "Christian",
                "god, shall, heaven, spirit, lord, belongs, soul, believers",
                ".053"
            ]
        ],
        "question": "Is it true that Several groups of words are much more likely to appear in a complaint, although not used to express complaints per se: about orders or deliveries (in the retail domain), about access (in complaints to service providers) and about parts of tech products (in tech)?",
        "answer_label": "yes"
    },
    {
        "id": "baaa1788-387d-4417-be35-6c9a092846ab",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
        "table_column_names": [
            "[EMPTY]",
            "Prec.",
            "Rec.",
            "F1"
        ],
        "table_content_values": [
            [
                "(A1) BiLSTM-CNN",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "(A2) Standard attention",
                "0.466",
                "0.638",
                "0.539"
            ],
            [
                "(A3) Window size ( [ITALIC] ws)=5",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "(A4) Window size ( [ITALIC] ws)=10",
                "0.510",
                "0.640",
                "0.568"
            ],
            [
                "(A5) Softmax",
                "0.490",
                "0.658",
                "0.562"
            ],
            [
                "(A6) Max-pool",
                "0.492",
                "0.600",
                "0.541"
            ]
        ],
        "question": "Is it true that Adding the dependency weight factor with a window size of 5 improves [CONTINUE] the F1 score by 3.2% (A3\u2212A2)?",
        "answer_label": "yes"
    },
    {
        "id": "69091183-93a9-443b-9f2b-248ee7ef89fc",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 6: The count of human preference on dialog session pairs that GDPL wins (W), draws with (D) or loses to (L) other methods based on different criteria. One method wins the other if the majority prefer the former one.",
        "table_column_names": [
            "VS.",
            "Efficiency W",
            "Efficiency D",
            "Efficiency L",
            "Quality W",
            "Quality D",
            "Quality L",
            "Success W",
            "Success D",
            "Success L"
        ],
        "table_content_values": [
            [
                "ACER",
                "55",
                "25",
                "20",
                "44",
                "32",
                "24",
                "52",
                "30",
                "18"
            ],
            [
                "PPO",
                "74",
                "13",
                "13",
                "56",
                "26",
                "18",
                "59",
                "31",
                "10"
            ],
            [
                "ALDM",
                "69",
                "19",
                "12",
                "49",
                "25",
                "26",
                "61",
                "24",
                "15"
            ]
        ],
        "question": "Is it true that GDPL does not outperform three baselines significantly in all aspects (sign test, p-value < 0.01), including the quality compared with ACER?",
        "answer_label": "no"
    },
    {
        "id": "f5f06ec5-0b30-4907-a0bd-d39536b0aae2",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] External",
            "B"
        ],
        "table_content_values": [
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "-",
                "22.0"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "-",
                "23.3"
            ],
            [
                "GCNSEQ (Damonte and Cohen,  2019 )",
                "-",
                "24.4"
            ],
            [
                "DCGCN(single)",
                "-",
                "25.9"
            ],
            [
                "DCGCN(ensemble)",
                "-",
                "[BOLD] 28.2"
            ],
            [
                "TSP (Song et al.,  2016 )",
                "ALL",
                "22.4"
            ],
            [
                "PBMT (Pourdamghani et al.,  2016 )",
                "ALL",
                "26.9"
            ],
            [
                "Tree2Str (Flanigan et al.,  2016 )",
                "ALL",
                "23.0"
            ],
            [
                "SNRG (Song et al.,  2017 )",
                "ALL",
                "25.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "0.2M",
                "27.4"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "0.2M",
                "28.2"
            ],
            [
                "DCGCN(single)",
                "0.1M",
                "29.0"
            ],
            [
                "DCGCN(single)",
                "0.2M",
                "[BOLD] 31.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "2M",
                "32.3"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "2M",
                "33.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "20M",
                "33.8"
            ],
            [
                "DCGCN(single)",
                "0.3M",
                "33.2"
            ],
            [
                "DCGCN(ensemble)",
                "0.3M",
                "[BOLD] 35.3"
            ]
        ],
        "question": "Is it true that These results show that our model is more effective in terms of using automatically generated AMR graphs?",
        "answer_label": "yes"
    },
    {
        "id": "6f90e958-7eef-46a6-8b4a-bfbc7d4b391f",
        "table_caption": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 3: Results on the dev set and on the test set. \u201cperp\u201d stands for perplexity, \u201cacc\u201d stands for accuracy (in percents), and \u201cwer\u201d stands for word-error-rate.",
        "table_column_names": [
            "[EMPTY]",
            "dev perp \u2193",
            "dev acc \u2191",
            "dev wer \u2193",
            "test perp \u2193",
            "test acc \u2191",
            "test wer \u2193"
        ],
        "table_content_values": [
            [
                "Spanish-only-LM",
                "329.68",
                "26.6",
                "30.47",
                "322.26",
                "25.1",
                "29.62"
            ],
            [
                "English-only-LM",
                "320.92",
                "29.3",
                "32.02",
                "314.04",
                "30.3",
                "32.51"
            ],
            [
                "All:CS-last-LM",
                "76.64",
                "47.8",
                "14.56",
                "76.97",
                "49.2",
                "14.13"
            ],
            [
                "All:Shuffled-LM",
                "68.00",
                "51.8",
                "13.64",
                "68.72",
                "51.4",
                "13.89"
            ],
            [
                "CS-only-LM",
                "43.20",
                "60.7",
                "12.60",
                "43.42",
                "57.9",
                "12.18"
            ],
            [
                "CS-only+vocab-LM",
                "45.61",
                "61.0",
                "12.56",
                "45.79",
                "58.8",
                "12.49"
            ],
            [
                "Fine-Tuned-LM",
                "39.76",
                "66.9",
                "10.71",
                "40.11",
                "65.4",
                "10.17"
            ],
            [
                "CS-only-disc",
                "\u2013",
                "72.0",
                "6.35",
                "\u2013",
                "70.5",
                "6.70"
            ],
            [
                "Fine-Tuned-disc",
                "\u2013",
                "[BOLD] 74.2",
                "[BOLD] 5.85",
                "\u2013",
                "[BOLD] 75.5",
                "[BOLD] 5.59"
            ]
        ],
        "question": "Is it true that Similarly, when using discriminative training, the CS-ONLY-DISCRIMINATIVE model outperforms the FINE-TUNED-DISCRIMINATIVE model?",
        "answer_label": "no"
    },
    {
        "id": "c12dfef6-a48b-4994-9d35-bac3557acb18",
        "table_caption": "What do Deep Networks Like to Read? Table 3: Sentiment score changes in SST-2. The numbers indicate the changes in percentage points with respect to the original sentence. The last two rows correspond to the case where negative labels are flipped to positive and vice versa. and indicate that the score increases in positive and negative sentiment.",
        "table_column_names": [
            "[EMPTY]",
            "<bold>RNN</bold>",
            "<bold>CNN</bold>",
            "<bold>DAN</bold>"
        ],
        "table_content_values": [
            [
                "Positive",
                "+9.7",
                "+4.3",
                "+<bold>23.6</bold>"
            ],
            [
                "Negative",
                "+6.9",
                "+5.5",
                "+<bold>16.1</bold>"
            ],
            [
                "Flipped to Positive",
                "+20.2",
                "+24.9",
                "+27.4"
            ],
            [
                "Flipped to Negative",
                "+31.5",
                "+28.6",
                "+19.3"
            ]
        ],
        "question": "Is it true that We see a constant increase in sentiment value in both directions across all three models after finetuning demonstrating that the framework is able to pick up on words that are indicative of sentiment?",
        "answer_label": "yes"
    },
    {
        "id": "f684361e-9ba9-42b7-b25d-ea65f81115a2",
        "table_caption": "Building a Production Model for Retrieval-Based Chatbots Table 6: Recall@1 versus coverage for frequency and clustering whitelists.",
        "table_column_names": [
            "[BOLD] Whitelist",
            "[BOLD] R@1",
            "[BOLD] Coverage"
        ],
        "table_content_values": [
            [
                "Frequency 10K",
                "0.136",
                "45.04%"
            ],
            [
                "Clustering 10K",
                "0.164",
                "38.38%"
            ],
            [
                "Frequency 1K",
                "0.273",
                "33.38%"
            ],
            [
                "Clustering 1K",
                "0.331",
                "23.28%"
            ]
        ],
        "question": "Is it true that While the clustering whitelists have higher recall, the frequency whitelists have higher coverage?",
        "answer_label": "yes"
    },
    {
        "id": "719df65c-e7e1-4d81-862e-9f799a929714",
        "table_caption": "Two Causal Principles for Improving Visual Dialog Table 2: Performance (NDCG%) of ablative studies on different models on VisDial v1.0 validation set. P2 indicates the most effective one (i.e., hidden dictionary learning) shown in Table\u00a01. Note that only applying P2 is implemented by the implementations in Section\u00a05 with the history shortcut.",
        "table_column_names": [
            "Model",
            "LF\u00a0",
            "HCIAE\u00a0",
            "CoAtt\u00a0",
            "RvA\u00a0"
        ],
        "table_content_values": [
            [
                "baseline",
                "57.21",
                "56.98",
                "56.46",
                "56.74"
            ],
            [
                "+P1",
                "61.88",
                "60.12",
                "60.27",
                "61.02"
            ],
            [
                "+P2",
                "72.65",
                "71.50",
                "71.41",
                "71.44"
            ],
            [
                "+P1+P2",
                "[BOLD] 73.63",
                "71.99",
                "71.87",
                "72.88"
            ]
        ],
        "question": "Is it true that In general, both of our principles can improve all the models in any ablative condition (i.e., P1, P2, P1+P2)?",
        "answer_label": "yes"
    },
    {
        "id": "25bcab8e-9441-4c2e-beda-8c47ffe86585",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
        "table_column_names": [
            "Model",
            "Answerability Macro P/R/F",
            "# Answerable",
            "Answer Prec.",
            "Derivation Prec. RG-L (P/R/F)",
            "Derivation Prec. BL-4"
        ],
        "table_content_values": [
            [
                "Shortest Path",
                "54.8/55.5/53.2",
                "976",
                "3.6",
                "56.7/38.5/41.5",
                "31.3"
            ],
            [
                "PRKGC",
                "52.6/51.5/50.7",
                "1,021",
                "45.2",
                "40.7/60.7/44.7",
                "30.9"
            ],
            [
                "PRKGC+NS",
                "53.6/54.1/52.1",
                "980",
                "45.4",
                "42.2/61.6/46.1",
                "33.4"
            ]
        ],
        "question": "Is it true that [CONTINUE] Supervising path attentions (the PRKGC+NS model) is indeed effective for improving the human interpretability of generated NLDs?",
        "answer_label": "yes"
    },
    {
        "id": "7994ec04-0d3b-4a69-8433-deafd2d52158",
        "table_caption": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
        "table_column_names": [
            "ID LSTM-800",
            "5-fold CV 70.56",
            "\u0394 0.66",
            "Single model 67.54",
            "\u0394 0.78",
            "Ensemble 67.65",
            "\u0394 0.30"
        ],
        "table_content_values": [
            [
                "LSTM-400",
                "70.50",
                "0.60",
                "[BOLD] 67.59",
                "0.83",
                "[BOLD] 68.00",
                "0.65"
            ],
            [
                "IN-TITLE",
                "70.11",
                "0.21",
                "[EMPTY]",
                "[EMPTY]",
                "67.52",
                "0.17"
            ],
            [
                "[BOLD] SUBMISSION",
                "69.90",
                "\u2013",
                "66.76",
                "\u2013",
                "67.35",
                "\u2013"
            ],
            [
                "NO-HIGHWAY",
                "69.72",
                "\u22120.18",
                "66.42",
                "\u22120.34",
                "66.64",
                "\u22120.71"
            ],
            [
                "NO-OVERLAPS",
                "69.46",
                "\u22120.44",
                "65.07",
                "\u22121.69",
                "66.47",
                "\u22120.88"
            ],
            [
                "LSTM-400-DROPOUT",
                "69.45",
                "\u22120.45",
                "65.53",
                "\u22121.23",
                "67.28",
                "\u22120.07"
            ],
            [
                "NO-TRANSLATIONS",
                "69.42",
                "\u22120.48",
                "65.92",
                "\u22120.84",
                "67.23",
                "\u22120.12"
            ],
            [
                "NO-ELMO-FINETUNING",
                "67.71",
                "\u22122.19",
                "65.16",
                "\u22121.60",
                "65.42",
                "\u22121.93"
            ]
        ],
        "question": "Is it true that Apart from the flipped results of the LSTM-800 and the LSTM-400, small differences in CV score are usually associated with large discrepancies in test set performance?",
        "answer_label": "no"
    },
    {
        "id": "8b8dabcd-08f8-434a-8cab-38912a86d4c9",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that The relative improvement averaged over all tasks is less than 8%?",
        "answer_label": "no"
    },
    {
        "id": "1cdd42db-f5b4-4e6e-989c-aa0d126f6ee8",
        "table_caption": "Building a Production Model for Retrieval-Based Chatbots Table 6: Recall@1 versus coverage for frequency and clustering whitelists.",
        "table_column_names": [
            "[BOLD] Whitelist",
            "[BOLD] R@1",
            "[BOLD] Coverage"
        ],
        "table_content_values": [
            [
                "Frequency 10K",
                "0.136",
                "45.04%"
            ],
            [
                "Clustering 10K",
                "0.164",
                "38.38%"
            ],
            [
                "Frequency 1K",
                "0.273",
                "33.38%"
            ],
            [
                "Clustering 1K",
                "0.331",
                "23.28%"
            ]
        ],
        "question": "Is it true that While the frequency whitelists have higher recall, the clustering whitelists have higher coverage?",
        "answer_label": "no"
    },
    {
        "id": "8e7135c9-5245-4dc2-a2ad-75c47cd2ee70",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
        "table_column_names": [
            "# of Heads",
            "Accuracy",
            "Val. Loss",
            "Effect"
        ],
        "table_content_values": [
            [
                "1",
                "89.44%",
                "0.2811",
                "-6.84%"
            ],
            [
                "2",
                "91.20%",
                "0.2692",
                "-5.08%"
            ],
            [
                "4",
                "93.85%",
                "0.2481",
                "-2.43%"
            ],
            [
                "8",
                "96.02%",
                "0.2257",
                "-0.26%"
            ],
            [
                "10",
                "96.28%",
                "0.2197",
                "[EMPTY]"
            ],
            [
                "16",
                "96.32%",
                "0.2190",
                "+0.04"
            ]
        ],
        "question": "Is it true that As shown in Table 6, reducing the number of attention heads severely decreases multitasking performance?",
        "answer_label": "yes"
    },
    {
        "id": "3926cb33-082d-4658-b949-24978f01cc9f",
        "table_caption": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment Table 1: Results on belinkov2014exploring\u2019s PPA test set. HPCD (full) is from the original paper, and it uses syntactic SkipGram. GloVe-retro is GloVe vectors retrofitted Faruqui et\u00a0al. (2015) to WordNet 3.1, and GloVe-extended refers to the synset embeddings obtained by running AutoExtend Rothe and Sch\u00fctze (2015) on GloVe.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] Initialization",
            "[BOLD] Embedding",
            "[BOLD] Resources",
            "[BOLD] Test Acc."
        ],
        "table_content_values": [
            [
                "HPCD (full)",
                "Syntactic-SG",
                "Type",
                "WordNet, VerbNet",
                "88.7"
            ],
            [
                "LSTM-PP",
                "GloVe",
                "Type",
                "-",
                "84.3"
            ],
            [
                "LSTM-PP",
                "GloVe-retro",
                "Type",
                "WordNet",
                "84.8"
            ],
            [
                "OntoLSTM-PP",
                "GloVe-extended",
                "Token",
                "WordNet",
                "[BOLD] 89.7"
            ]
        ],
        "question": "Is it true that Table 1 shows that our proposed token level embedding scheme OntoLSTM-PP does not outperform the better variant of our baseline LSTM-PP (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%?",
        "answer_label": "no"
    },
    {
        "id": "06a83460-1e39-4475-811c-5697454932b9",
        "table_caption": "Solving Hard Coreference Problems Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.",
        "table_column_names": [
            "System",
            "MUC",
            "BCUB",
            "CEAFe",
            "AVG"
        ],
        "table_content_values": [
            [
                "ACE",
                "ACE",
                "ACE",
                "ACE",
                "ACE"
            ],
            [
                "IlliCons",
                "[BOLD] 78.17",
                "81.64",
                "[BOLD] 78.45",
                "[BOLD] 79.42"
            ],
            [
                "KnowComb",
                "77.51",
                "[BOLD] 81.97",
                "77.44",
                "78.97"
            ],
            [
                "OntoNotes",
                "OntoNotes",
                "OntoNotes",
                "OntoNotes",
                "OntoNotes"
            ],
            [
                "IlliCons",
                "84.10",
                "[BOLD] 78.30",
                "[BOLD] 68.74",
                "[BOLD] 77.05"
            ],
            [
                "KnowComb",
                "[BOLD] 84.33",
                "78.02",
                "67.95",
                "76.76"
            ]
        ],
        "question": "Is it true that As hard coreference problems are rare in standard coreference datasets, we do not have significant performance improvement?",
        "answer_label": "yes"
    },
    {
        "id": "502a5b58-34d5-4304-a106-9b6ab93d3401",
        "table_caption": "Towards Universal Dialogue State Tracking Table 1: Joint goal accuracy on DSTC2 and WOZ 2.0 test set vs.\u00a0various approaches as reported in the literature.",
        "table_column_names": [
            "[BOLD] DST Models",
            "[BOLD] Joint Acc. DSTC2",
            "[BOLD] Joint Acc. WOZ 2.0"
        ],
        "table_content_values": [
            [
                "Delexicalisation-Based (DB) Model Mrk\u0161i\u0107 et\u00a0al. ( 2017 )",
                "69.1",
                "70.8"
            ],
            [
                "DB Model + Semantic Dictionary Mrk\u0161i\u0107 et\u00a0al. ( 2017 )",
                "72.9",
                "83.7"
            ],
            [
                "Scalable Multi-domain DST Rastogi et\u00a0al. ( 2017 )",
                "70.3",
                "-"
            ],
            [
                "MemN2N Perez and Liu ( 2017 )",
                "74.0",
                "-"
            ],
            [
                "PtrNet Xu and Hu ( 2018 )",
                "72.1",
                "-"
            ],
            [
                "Neural Belief Tracker: NBT-DNN Mrk\u0161i\u0107 et\u00a0al. ( 2017 )",
                "72.6",
                "84.4"
            ],
            [
                "Neural Belief Tracker: NBT-CNN Mrk\u0161i\u0107 et\u00a0al. ( 2017 )",
                "73.4",
                "84.2"
            ],
            [
                "Belief Tracking: Bi-LSTM Ramadan et\u00a0al. ( 2018 )",
                "-",
                "85.1"
            ],
            [
                "Belief Tracking: CNN Ramadan et\u00a0al. ( 2018 )",
                "-",
                "85.5"
            ],
            [
                "GLAD Zhong et\u00a0al. ( 2018 )",
                "74.5",
                "88.1"
            ],
            [
                "StateNet",
                "74.1",
                "87.8"
            ],
            [
                "StateNet_PS",
                "74.5",
                "88.2"
            ],
            [
                "[BOLD] StateNet_PSI",
                "[BOLD] 75.5",
                "[BOLD] 88.9"
            ]
        ],
        "question": "Is it true that StateNet PS outperforms StateNet, and StateNet PSI performs best among all 3 models?",
        "answer_label": "yes"
    },
    {
        "id": "3cdfdbe2-07d7-40fb-b7f9-ae12971c7575",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 7: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in Portuguese.",
        "table_column_names": [
            "Corpus",
            "Metric",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "Europarl",
                "TotalTerms:",
                "980",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "996",
                "1,000"
            ],
            [
                "Europarl",
                "TotalRoots:",
                "79",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "Europarl",
                "NumberRels:",
                "1,527",
                "1,031",
                "1,049",
                "1,185",
                "1,093",
                "1,644",
                "999"
            ],
            [
                "Europarl",
                "MaxDepth:",
                "19",
                "902",
                "894",
                "784",
                "849",
                "6",
                "10"
            ],
            [
                "Europarl",
                "MinDepth:",
                "1",
                "902",
                "894",
                "784",
                "849",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgDepth:",
                "9.43",
                "902",
                "894",
                "784",
                "849",
                "2.73",
                "4.29"
            ],
            [
                "Europarl",
                "DepthCohesion:",
                "2.02",
                "1",
                "1",
                "1",
                "1",
                "2.19",
                "2.33"
            ],
            [
                "Europarl",
                "MaxWidth:",
                "27",
                "3",
                "3",
                "4",
                "3",
                "201",
                "58"
            ],
            [
                "Europarl",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgWidth:",
                "1.98",
                "1.03",
                "1.05",
                "1.19",
                "1.09",
                "6.25",
                "2.55"
            ],
            [
                "TED Talks",
                "TotalTerms:",
                "296",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000"
            ],
            [
                "TED Talks",
                "TotalRoots:",
                "101",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "NumberRels:",
                "291",
                "1,045",
                "1,229",
                "3,637",
                "4,284",
                "2,875",
                "999"
            ],
            [
                "TED Talks",
                "MaxDepth:",
                "10",
                "860",
                "727",
                "388",
                "354",
                "252",
                "17"
            ],
            [
                "TED Talks",
                "MinDepth:",
                "1",
                "860",
                "727",
                "388",
                "354",
                "249",
                "1"
            ],
            [
                "TED Talks",
                "AvgDepth:",
                "3.94",
                "860",
                "727",
                "388",
                "354",
                "250.43",
                "6.16"
            ],
            [
                "TED Talks",
                "DepthCohesion:",
                "2.54",
                "1",
                "1",
                "1",
                "1",
                "1.01",
                "2.76"
            ],
            [
                "TED Talks",
                "MaxWidth:",
                "37",
                "3",
                "79",
                "18",
                "13",
                "9",
                "41"
            ],
            [
                "TED Talks",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "AvgWidth:",
                "1.79",
                "1.05",
                "1.23",
                "3.64",
                "4.29",
                "2.94",
                "2.37"
            ]
        ],
        "question": "Is it true that [CONTINUE] The results for the Portuguese corpora are quite similar to the ones generated by the English corpora, having terms without relations in Patt and DocSub, and DSim, SLQS, TF and DF generating deep taxonomies, affirming the characteristics of each method?",
        "answer_label": "yes"
    },
    {
        "id": "23b7092a-6047-4963-9ad7-0bd5b23ee3ec",
        "table_caption": "Variational Self-attention Model for Sentence Representation Table 2: Performance comparison with the state-of-art algorithms on the FNC-1 test dataset.",
        "table_column_names": [
            "Model",
            "Accuracy (%) agree",
            "Accuracy (%) disagree",
            "Accuracy (%) discuss",
            "Accuracy (%) unrelated",
            "Micro F1(%)"
        ],
        "table_content_values": [
            [
                "Average of Word2vec Embedding",
                "12.43",
                "01.30",
                "43.32",
                "74.24",
                "45.53"
            ],
            [
                "CNN-based Sentence Embedding",
                "24.54",
                "05.06",
                "53.24",
                "79.53",
                "81.72"
            ],
            [
                "RNN-based Sentence Embedding",
                "24.42",
                "05.42",
                "69.05",
                "65.34",
                "78.70"
            ],
            [
                "Self-attention Sentence Embedding",
                "23.53",
                "04.63",
                "63.59",
                "80.34",
                "80.11"
            ],
            [
                "Our model",
                "28.53",
                "10.43",
                "65.43",
                "82.43",
                "[BOLD] 83.54"
            ]
        ],
        "question": "Is it true that As for the micro F1 evaluation metric, our model achieves the highest performance (83.54%) on the FNC-1 testing subset?",
        "answer_label": "yes"
    },
    {
        "id": "c3076ba9-a8f1-40bc-ac14-40e17489112b",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 3: SEM and POS tagging accuracy using features extracted from the k-th encoding layer of 4-layered NMT models trained with different target languages. \u201cEn\u201d column is an English autoencoder. BLEU scores are given for reference.",
        "table_column_names": [
            "[ITALIC] k",
            "Ar",
            "Es",
            "Fr",
            "Ru",
            "Zh",
            "En"
        ],
        "table_content_values": [
            [
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy",
                "POS Tagging Accuracy"
            ],
            [
                "0",
                "88.0",
                "87.9",
                "87.9",
                "87.8",
                "87.7",
                "87.4"
            ],
            [
                "1",
                "92.4",
                "91.9",
                "92.1",
                "92.1",
                "91.5",
                "89.4"
            ],
            [
                "2",
                "91.9",
                "91.8",
                "91.8",
                "91.8",
                "91.3",
                "88.3"
            ],
            [
                "3",
                "92.0",
                "92.3",
                "92.1",
                "91.6",
                "91.2",
                "87.9"
            ],
            [
                "4",
                "92.1",
                "92.4",
                "92.5",
                "92.0",
                "90.5",
                "86.9"
            ],
            [
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy",
                "SEM Tagging Accuracy"
            ],
            [
                "0",
                "81.9",
                "81.9",
                "81.8",
                "81.8",
                "81.8",
                "81.2"
            ],
            [
                "1",
                "87.9",
                "87.7",
                "87.8",
                "87.9",
                "87.7",
                "84.5"
            ],
            [
                "2",
                "87.4",
                "87.5",
                "87.4",
                "87.3",
                "87.2",
                "83.2"
            ],
            [
                "3",
                "87.8",
                "87.9",
                "87.9",
                "87.3",
                "87.3",
                "82.9"
            ],
            [
                "4",
                "88.3",
                "88.6",
                "88.4",
                "88.1",
                "87.7",
                "82.1"
            ],
            [
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU",
                "BLEU"
            ],
            [
                "[EMPTY]",
                "32.7",
                "49.1",
                "38.5",
                "34.2",
                "32.1",
                "96.6"
            ]
        ],
        "question": "Is it true that [CONTINUE] we found that En-En encoder-decoders (that is, English autoencoders) produce poor representations for POS and SEM tagging (last column in Table 3)?",
        "answer_label": "yes"
    },
    {
        "id": "d1dff78c-4750-4fd2-b97c-a90c86c61345",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).",
        "table_column_names": [
            "Model",
            "#Params",
            "Base",
            "+Elmo"
        ],
        "table_content_values": [
            [
                "rnet*",
                "-",
                "71.1/79.5",
                "-/-"
            ],
            [
                "LSTM",
                "2.67M",
                "[BOLD] 70.46/78.98",
                "75.17/82.79"
            ],
            [
                "GRU",
                "2.31M",
                "70.41/ [BOLD] 79.15",
                "75.81/83.12"
            ],
            [
                "ATR",
                "1.59M",
                "69.73/78.70",
                "75.06/82.76"
            ],
            [
                "SRU",
                "2.44M",
                "69.27/78.41",
                "74.56/82.50"
            ],
            [
                "LRN",
                "2.14M",
                "70.11/78.83",
                "[BOLD] 76.14/ [BOLD] 83.83"
            ]
        ],
        "question": "Is it true that In this task, LRN outperforms ATR and SRU in terms of both EM and F1 score?",
        "answer_label": "yes"
    },
    {
        "id": "9bf4c3fe-a123-4e6c-98e3-14e4a13d4f09",
        "table_caption": "Language Modeling for Code-Switching:Evaluation, Integration of Monolingual Data, and Discriminative Training Table 5: Accuracy on the dev set and on the test set, according to the type of the gold sentence in the set: code-switched (CS) vs. monolingual (mono).",
        "table_column_names": [
            "[EMPTY]",
            "dev CS",
            "dev mono",
            "test CS",
            "test mono"
        ],
        "table_content_values": [
            [
                "CS-only-LM",
                "45.20",
                "65.87",
                "43.20",
                "62.80"
            ],
            [
                "Fine-Tuned-LM",
                "49.60",
                "72.67",
                "47.60",
                "71.33"
            ],
            [
                "CS-only-disc",
                "[BOLD] 75.60",
                "70.40",
                "70.80",
                "70.53"
            ],
            [
                "Fine-Tuned-disc",
                "70.80",
                "[BOLD] 74.40",
                "[BOLD] 75.33",
                "[BOLD] 75.87"
            ]
        ],
        "question": "Is it true that [CONTINUE] the FINE-TUNEDDISCRIMINATIVE model is able to prioritize the gold sentence better than all other models, under both conditions?",
        "answer_label": "yes"
    },
    {
        "id": "9bde8eb0-16a1-41f1-bde7-8d9066b32406",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 5: An ablation study on the effects of pretraining for multitasking-based and standard GPT-2 finetuning. Results show that pretraining greatly accounts for almost half of performance on both finetuning techniques. \u201cAcc. Inc.\u201d refers to the boost in performance contributed by the pretraining step. \u201c% of Perf.\u201d refers to the percentage of the total performance that the pretraining step contributes.",
        "table_column_names": [
            "Finetuning",
            "Pretrained?",
            "Accuracy",
            "Val. Loss",
            "Acc. Inc.",
            "% of Perf."
        ],
        "table_content_values": [
            [
                "Multitasking",
                "No",
                "53.61%",
                "0.7217",
                "-",
                "-"
            ],
            [
                "[EMPTY]",
                "Yes",
                "96.28%",
                "0.2197",
                "+42.67%",
                "44.32%"
            ],
            [
                "Standard",
                "No",
                "51.02%",
                "0.7024",
                "-",
                "-"
            ],
            [
                "[EMPTY]",
                "Yes",
                "90.99%",
                "0.1826",
                "+39.97%",
                "43.93%"
            ]
        ],
        "question": "Is it true that In Table 5, it can be seen that generative pretraining via language modeling does account for a considerable amount of performance, constituting 44.32% of the overall performance (a boost of 42.67% in accuracy) in the multitasking setup, and constituting 43.93% of the overall performance (a boost of 39.97%) in the standard finetuning setup?",
        "answer_label": "yes"
    },
    {
        "id": "1f092e69-62e3-4ec5-a7e8-17cc384fd74b",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 7: Cross-lingual embedding alignment in Italian and in German, before and after debiasing.",
        "table_column_names": [
            "[EMPTY]",
            "Italian \u2192 En",
            "Italian En \u2192",
            "German \u2192 En",
            "German En \u2192"
        ],
        "table_content_values": [
            [
                "Orig",
                "58.73",
                "59.68",
                "47.58",
                "50.48"
            ],
            [
                "Debias",
                "[BOLD] 60.03",
                "[BOLD] 60.96",
                "[BOLD] 47.89",
                "[BOLD] 51.76"
            ]
        ],
        "question": "Is it true that The results reported in Table 7 show that precision on BDI does not increase as a result of the reduced effect of grammatical gender on the embeddings for German and Italian?",
        "answer_label": "no"
    },
    {
        "id": "383dd023-7ed9-4ccf-ae4c-879ef4bdff0a",
        "table_caption": "Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model Table 3: Comparison of our Multi-News dataset to other MDS datasets as well as an SDS dataset used as training data for MDS (CNNDM). Training, validation and testing size splits (article(s) to summary) are provided when applicable. Statistics for multi-document inputs are calculated on the concatenation of all input sources.",
        "table_column_names": [
            "[BOLD] Dataset",
            "[BOLD] # pairs",
            "[BOLD] # words (doc)",
            "[BOLD] # sents (docs)",
            "[BOLD] # words (summary)",
            "[BOLD] # sents (summary)",
            "[BOLD] vocab size"
        ],
        "table_content_values": [
            [
                "Multi-News",
                "44,972/5,622/5,622",
                "2,103.49",
                "82.73",
                "263.66",
                "9.97",
                "666,515"
            ],
            [
                "DUC03+04",
                "320",
                "4,636.24",
                "173.15",
                "109.58",
                "2.88",
                "19,734"
            ],
            [
                "TAC 2011",
                "176",
                "4,695.70",
                "188.43",
                "99.70",
                "1.00",
                "24,672"
            ],
            [
                "CNNDM",
                "287,227/13,368/11,490",
                "810.57",
                "39.78",
                "56.20",
                "3.68",
                "717,951"
            ]
        ],
        "question": "Is it true that The number of examples in our Multi-News dataset is not significantly larger than previous MDS news data?",
        "answer_label": "no"
    },
    {
        "id": "697c2b0c-fe97-4d2c-b0fe-70e21db0f34d",
        "table_caption": "Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns Table 6: Performance of our baselines on the development set. Parallelism+URL tests the page-context setting; all other test the snippet-context setting. Bold indicates best performance in each setting.",
        "table_column_names": [
            "[EMPTY]",
            "M",
            "F",
            "B",
            "O"
        ],
        "table_content_values": [
            [
                "Random",
                "43.6",
                "39.3",
                "[ITALIC] 0.90",
                "41.5"
            ],
            [
                "Token Distance",
                "50.1",
                "42.4",
                "[ITALIC] 0.85",
                "46.4"
            ],
            [
                "Topical Entity",
                "51.5",
                "43.7",
                "[ITALIC] 0.85",
                "47.7"
            ],
            [
                "Syntactic Distance",
                "63.0",
                "56.2",
                "[ITALIC] 0.89",
                "59.7"
            ],
            [
                "Parallelism",
                "[BOLD] 67.1",
                "[BOLD] 63.1",
                "[ITALIC]  [BOLD] 0.94",
                "[BOLD] 65.2"
            ],
            [
                "Parallelism+URL",
                "[BOLD] 71.1",
                "[BOLD] 66.9",
                "[ITALIC]  [BOLD] 0.94",
                "[BOLD] 69.0"
            ],
            [
                "Transformer-Single",
                "58.6",
                "51.2",
                "[ITALIC] 0.87",
                "55.0"
            ],
            [
                "Transformer-Multi",
                "59.3",
                "52.9",
                "[ITALIC] 0.89",
                "56.2"
            ]
        ],
        "question": "Is it true that [CONTINUE] TRANSFORMER-MULTI is weaker than TRANSFORMER-SINGLE [CONTINUE] .2% overall decrease in performance compared to TRANSFORMER-SINGLE for the goldtwo-mention task?",
        "answer_label": "no"
    },
    {
        "id": "afd33f80-8078-43b6-bc31-857981266ce4",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that [CONTINUE] As we can observe in Table 3, Patt has the best values of precision for the English corpora while DocSub has the best values for the Portuguese corpora?",
        "answer_label": "yes"
    },
    {
        "id": "3ce60497-4677-4795-bb32-cfc18403af9e",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 3: Number of tweets annotated as complaints across the nine domains.",
        "table_column_names": [
            "[BOLD] Category",
            "[BOLD] Complaints",
            "[BOLD] Not Complaints"
        ],
        "table_content_values": [
            [
                "Food & Beverage",
                "95",
                "35"
            ],
            [
                "Apparel",
                "141",
                "117"
            ],
            [
                "Retail",
                "124",
                "75"
            ],
            [
                "Cars",
                "67",
                "25"
            ],
            [
                "Services",
                "207",
                "130"
            ],
            [
                "Software & Online Services",
                "189",
                "103"
            ],
            [
                "Transport",
                "139",
                "109"
            ],
            [
                "Electronics",
                "174",
                "112"
            ],
            [
                "Other",
                "96",
                "33"
            ],
            [
                "Total",
                "1232",
                "739"
            ]
        ],
        "question": "Is it true that In total, 739 tweets (37.6%) are complaints and 1,232 are not complaints (62.4%)?",
        "answer_label": "no"
    },
    {
        "id": "9c2d8eeb-0c25-43a9-80e9-93904819315d",
        "table_caption": "How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages? Table 4: Averages of similarities of pairs with same vs. different gender in Italian and German compared to English. The last row is the difference between the averages of the two sets. \u201cReduction\u201d stands for gap reduction when removing gender signals from the context.",
        "table_column_names": [
            "[EMPTY]",
            "Italian Original",
            "Italian Debiased",
            "Italian English",
            "Italian Reduction",
            "German Original",
            "German Debiased",
            "German English",
            "German Reduction"
        ],
        "table_content_values": [
            [
                "Same Gender",
                "0.442",
                "0.434",
                "0.424",
                "\u2013",
                "0.491",
                "0.478",
                "0.446",
                "\u2013"
            ],
            [
                "Different Gender",
                "0.385",
                "0.421",
                "0.415",
                "\u2013",
                "0.415",
                "0.435",
                "0.403",
                "\u2013"
            ],
            [
                "difference",
                "0.057",
                "0.013",
                "0.009",
                "[BOLD] 91.67%",
                "0.076",
                "0.043",
                "0.043",
                "[BOLD] 100%"
            ]
        ],
        "question": "Is it true that In German, we get a reduction of less than 100%?",
        "answer_label": "no"
    },
    {
        "id": "5f3db10e-3fe4-4526-b808-dad896f4ac6e",
        "table_caption": "Distant Learning for Entity Linking with Automatic Noise Detection Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
        "table_column_names": [
            "System",
            "All P",
            "All R",
            "All F1",
            "In  [ITALIC] E+ P",
            "In  [ITALIC] E+ R",
            "In  [ITALIC] E+ F1"
        ],
        "table_content_values": [
            [
                "Name matching",
                "15.03",
                "15.03",
                "15.03",
                "29.13",
                "29.13",
                "29.13"
            ],
            [
                "MIL (model 1)",
                "35.87",
                "35.87",
                "35.87 \u00b10.72",
                "69.38",
                "69.38",
                "69.38 \u00b11.29"
            ],
            [
                "MIL-ND (model 2)",
                "37.42",
                "[BOLD] 37.42",
                "37.42 \u00b10.35",
                "72.50",
                "[BOLD] 72.50",
                "[BOLD] 72.50 \u00b10.68"
            ],
            [
                "[ITALIC] \u03c4MIL-ND (model 2)",
                "[BOLD] 38.91",
                "36.73",
                "[BOLD] 37.78 \u00b10.26",
                "[BOLD] 73.19",
                "71.15",
                "72.16 \u00b10.48"
            ],
            [
                "Supervised learning",
                "42.90",
                "42.90",
                "42.90 \u00b10.59",
                "83.12",
                "83.12",
                "83.12 \u00b11.15"
            ]
        ],
        "question": "Is it true that Because all the test data points are valid for the 'In E+' setting, using the ND classifier had a slight negative effect on F1?",
        "answer_label": "yes"
    },
    {
        "id": "ed202752-8bac-401b-89ef-565c1be1319a",
        "table_caption": "When Choosing Plausible Alternatives, Clever Hans can be Clever Table 3: Results of human performance evaluation of the original COPA and Balanced COPA.",
        "table_column_names": [
            "Dataset",
            "Accuracy",
            "Fleiss\u2019 kappa  [ITALIC] k"
        ],
        "table_content_values": [
            [
                "Original COPA",
                "100.0",
                "0.973"
            ],
            [
                "Balanced COPA",
                "97.0",
                "0.798"
            ]
        ],
        "question": "Is it true that The human evaluation shows that our mirrored instances are comparable in difficulty to the original ones (see Table 3)?",
        "answer_label": "yes"
    },
    {
        "id": "f021eb05-ef24-40f8-9729-39dbcb2c1baf",
        "table_caption": "Syntactic Dependency Representations in Neural Relation Classification Table 2: Hyper parameter optimization results for each model with different representation. The max pooling strategy consistently performs better in all model variations.",
        "table_column_names": [
            "[BOLD] Representation",
            "[BOLD] Hyper parameters Filter size",
            "[BOLD] Hyper parameters Num. Feature maps",
            "[BOLD] Hyper parameters Activation func.",
            "[BOLD] Hyper parameters L2 Reg.",
            "[BOLD] Hyper parameters Learning rate",
            "[BOLD] Hyper parameters Dropout Prob.",
            "[BOLD] F1.(avg. in 5-fold) with default values",
            "[BOLD] F1.(avg. in 5-fold) with optimal values"
        ],
        "table_content_values": [
            [
                "CoNLL08",
                "4-5",
                "1000",
                "Softplus",
                "1.15e+01",
                "1.13e-03",
                "1",
                "73.34",
                "74.49"
            ],
            [
                "SB",
                "4-5",
                "806",
                "Sigmoid",
                "8.13e-02",
                "1.79e-03",
                "0.87",
                "72.83",
                "[BOLD] 75.05"
            ],
            [
                "UD v1.3",
                "5",
                "716",
                "Softplus",
                "1.66e+00",
                "9.63E-04",
                "1",
                "68.93",
                "69.57"
            ]
        ],
        "question": "Is it true that We see that the optimized parameter settings vary for the different representations, showing the importance of tuning for these types of comparisons?",
        "answer_label": "yes"
    },
    {
        "id": "82dbb1e6-82ef-4fef-860f-e26ee0e3b964",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that The models have better results when handling sentences with 20 or fewer tokens?",
        "answer_label": "yes"
    },
    {
        "id": "900aab18-0c84-4791-b0e8-3c2d4270ff79",
        "table_caption": "Entity, Relation, and Event Extraction with Contextualized Span Representations Table 3: F1 scores on Relation.",
        "table_column_names": [
            "[EMPTY]",
            "ACE05",
            "SciERC",
            "WLPC"
        ],
        "table_content_values": [
            [
                "BERT + LSTM",
                "60.6",
                "40.3",
                "65.1"
            ],
            [
                "+RelProp",
                "61.9",
                "41.1",
                "65.3"
            ],
            [
                "+CorefProp",
                "59.7",
                "42.6",
                "-"
            ],
            [
                "BERT FineTune",
                "[BOLD] 62.1",
                "44.3",
                "65.4"
            ],
            [
                "+RelProp",
                "62.0",
                "43.0",
                "[BOLD] 65.5"
            ],
            [
                "+CorefProp",
                "60.0",
                "[BOLD] 45.3",
                "-"
            ]
        ],
        "question": "Is it true that CorefProp does not improve relation extraction on SciERC?",
        "answer_label": "no"
    },
    {
        "id": "337b26da-3751-4e60-b0a0-f78b9af8cafe",
        "table_caption": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
        "table_column_names": [
            "ID LSTM-800",
            "5-fold CV 70.56",
            "\u0394 0.66",
            "Single model 67.54",
            "\u0394 0.78",
            "Ensemble 67.65",
            "\u0394 0.30"
        ],
        "table_content_values": [
            [
                "LSTM-400",
                "70.50",
                "0.60",
                "[BOLD] 67.59",
                "0.83",
                "[BOLD] 68.00",
                "0.65"
            ],
            [
                "IN-TITLE",
                "70.11",
                "0.21",
                "[EMPTY]",
                "[EMPTY]",
                "67.52",
                "0.17"
            ],
            [
                "[BOLD] SUBMISSION",
                "69.90",
                "\u2013",
                "66.76",
                "\u2013",
                "67.35",
                "\u2013"
            ],
            [
                "NO-HIGHWAY",
                "69.72",
                "\u22120.18",
                "66.42",
                "\u22120.34",
                "66.64",
                "\u22120.71"
            ],
            [
                "NO-OVERLAPS",
                "69.46",
                "\u22120.44",
                "65.07",
                "\u22121.69",
                "66.47",
                "\u22120.88"
            ],
            [
                "LSTM-400-DROPOUT",
                "69.45",
                "\u22120.45",
                "65.53",
                "\u22121.23",
                "67.28",
                "\u22120.07"
            ],
            [
                "NO-TRANSLATIONS",
                "69.42",
                "\u22120.48",
                "65.92",
                "\u22120.84",
                "67.23",
                "\u22120.12"
            ],
            [
                "NO-ELMO-FINETUNING",
                "67.71",
                "\u22122.19",
                "65.16",
                "\u22121.60",
                "65.42",
                "\u22121.93"
            ]
        ],
        "question": "Is it true that [CONTINUE] Perhaps the most striking thing about the ablation results is that the 'traditional' LSTM layout outsperformed the 'alternating' one we chose for our submission?",
        "answer_label": "yes"
    },
    {
        "id": "aff15db7-a64b-4e93-9e29-0a34989164f0",
        "table_caption": "Towards Quantifying the Distance between Opinions Table 6: Performance comparison of the distance measures on all 18 datasets. The semantic distance in opinion distance (OD) measure is computed via cosine distance over either Word2vec (OD-w2v with semantic distance threshold 0.6) or Doc2vec (OD-d2v with distance threshold 0.3) embeddings. Sil. refers to Silhouette Coefficient. The second best result is italicized and underlined. The ARI and Silhouette coefficients scores of both OD methods (OD-d2v and OD-w2v) are statistically significant (paired t-test) with respect to baselines at significance level 0.005.",
        "table_column_names": [
            "Topic Name",
            "Size",
            "TF-IDF ARI",
            "WMD ARI",
            "Sent2vec ARI",
            "Doc2vec ARI",
            "BERT ARI",
            "[ITALIC] OD-w2v ARI",
            "[ITALIC] OD-d2v ARI",
            "TF-IDF  [ITALIC] Sil.",
            "WMD  [ITALIC] Sil.",
            "Sent2vec  [ITALIC] Sil.",
            "Doc2vec  [ITALIC] Sil.",
            "BERT  [ITALIC] Sil.",
            "[ITALIC] OD-w2v  [ITALIC] Sil.",
            "[ITALIC] OD-d2v  [ITALIC] Sil."
        ],
        "table_content_values": [
            [
                "Affirmative Action",
                "81",
                "-0.07",
                "-0.02",
                "0.03",
                "-0.01",
                "-0.02",
                "[BOLD] 0.14",
                "[ITALIC] 0.02",
                "0.01",
                "0.01",
                "-0.01",
                "-0.02",
                "-0.04",
                "[BOLD] 0.06",
                "[ITALIC] 0.01"
            ],
            [
                "Atheism",
                "116",
                "[BOLD] 0.19",
                "0.07",
                "0.00",
                "0.03",
                "-0.01",
                "0.11",
                "[ITALIC] 0.16",
                "0.02",
                "0.01",
                "0.02",
                "0.01",
                "0.01",
                "[ITALIC] 0.05",
                "[BOLD] 0.07"
            ],
            [
                "Austerity Measures",
                "20",
                "[ITALIC] 0.04",
                "[ITALIC] 0.04",
                "-0.01",
                "-0.05",
                "0.04",
                "[BOLD] 0.21",
                "-0.01",
                "0.06",
                "0.07",
                "0.05",
                "-0.03",
                "0.10",
                "[BOLD] 0.19",
                "0.1"
            ],
            [
                "Democratization",
                "76",
                "0.02",
                "-0.01",
                "0.00",
                "[ITALIC] 0.09",
                "-0.01",
                "[BOLD] 0.11",
                "0.07",
                "0.01",
                "0.01",
                "0.02",
                "0.02",
                "0.03",
                "[BOLD] 0.16",
                "[ITALIC] 0.11"
            ],
            [
                "Education Voucher Scheme",
                "30",
                "[BOLD] 0.25",
                "0.12",
                "0.08",
                "-0.02",
                "0.04",
                "0.13",
                "[ITALIC] 0.19",
                "0.01",
                "0.01",
                "0.01",
                "-0.01",
                "0.02",
                "[ITALIC] 0.38",
                "[BOLD] 0.40"
            ],
            [
                "Gambling",
                "60",
                "-0.06",
                "-0.01",
                "-0.02",
                "0.04",
                "0.09",
                "[ITALIC] 0.35",
                "[BOLD] 0.39",
                "0.01",
                "0.02",
                "0.03",
                "0.01",
                "0.09",
                "[BOLD] 0.30",
                "[ITALIC] 0.22"
            ],
            [
                "Housing",
                "30",
                "0.01",
                "-0.01",
                "-0.01",
                "-0.02",
                "0.08",
                "[BOLD] 0.27",
                "0.01",
                "0.02",
                "0.03",
                "0.03",
                "0.01",
                "0.11",
                "[BOLD] 0.13",
                "[ITALIC] 0.13"
            ],
            [
                "Hydroelectric Dams",
                "110",
                "[BOLD] 0.47",
                "[ITALIC] 0.45",
                "[ITALIC] 0.45",
                "-0.01",
                "0.38",
                "0.35",
                "0.14",
                "0.04",
                "0.08",
                "0.12",
                "0.01",
                "0.19",
                "[BOLD] 0.26",
                "[ITALIC] 0.09"
            ],
            [
                "Intellectual Property",
                "66",
                "0.01",
                "0.01",
                "0.00",
                "0.03",
                "0.03",
                "[ITALIC] 0.05",
                "[BOLD] 0.14",
                "0.01",
                "[ITALIC] 0.04",
                "0.03",
                "0.01",
                "0.03",
                "[ITALIC] 0.04",
                "[BOLD] 0.12"
            ],
            [
                "Keystone pipeline",
                "18",
                "0.01",
                "0.01",
                "0.00",
                "-0.13",
                "[BOLD] 0.07",
                "-0.01",
                "[BOLD] 0.07",
                "-0.01",
                "-0.03",
                "-0.03",
                "-0.07",
                "0.03",
                "[BOLD] 0.05",
                "[ITALIC] 0.02"
            ],
            [
                "Monarchy",
                "61",
                "-0.04",
                "0.01",
                "0.00",
                "0.03",
                "-0.02",
                "[BOLD] 0.15",
                "[BOLD] 0.15",
                "0.01",
                "0.02",
                "0.02",
                "0.01",
                "0.01",
                "[BOLD] 0.11",
                "[ITALIC] 0.09"
            ],
            [
                "National Service",
                "33",
                "0.14",
                "-0.03",
                "-0.01",
                "0.02",
                "0.01",
                "[ITALIC] 0.31",
                "[BOLD] 0.39",
                "0.02",
                "0.04",
                "0.02",
                "0.01",
                "0.02",
                "[BOLD] 0.25",
                "[BOLD] 0.25"
            ],
            [
                "One-child policy China",
                "67",
                "-0.05",
                "0.01",
                "[BOLD] 0.11",
                "-0.02",
                "0.02",
                "[BOLD] 0.11",
                "0.01",
                "0.01",
                "0.02",
                "[ITALIC] 0.04",
                "-0.01",
                "0.03",
                "[BOLD] 0.07",
                "-0.02"
            ],
            [
                "Open-source Software",
                "48",
                "-0.02",
                "-0.01",
                "[ITALIC] 0.05",
                "0.01",
                "0.12",
                "[BOLD] 0.09",
                "-0.02",
                "0.01",
                "-0.01",
                "0.00",
                "-0.02",
                "0.03",
                "[BOLD] 0.18",
                "0.01"
            ],
            [
                "Pornography",
                "52",
                "-0.02",
                "0.01",
                "0.01",
                "-0.02",
                "-0.01",
                "[BOLD] 0.41",
                "[BOLD] 0.41",
                "0.01",
                "0.01",
                "0.02",
                "-0.01",
                "0.03",
                "[BOLD] 0.47",
                "[ITALIC] 0.41"
            ],
            [
                "Seanad Abolition",
                "25",
                "0.23",
                "0.09",
                "-0.01",
                "-0.01",
                "0.03",
                "[ITALIC] 0.32",
                "[BOLD] 0.54",
                "0.02",
                "0.01",
                "-0.01",
                "-0.03",
                "-0.04",
                "[ITALIC] 0.15",
                "[BOLD] 0.31"
            ],
            [
                "Trades Unions",
                "19",
                "[ITALIC] 0.44",
                "[ITALIC] 0.44",
                "[BOLD] 0.60",
                "-0.05",
                "0.44",
                "[ITALIC] 0.44",
                "0.29",
                "0.1",
                "0.17",
                "0.21",
                "0.01",
                "0.26",
                "[BOLD] 0.48",
                "[ITALIC] 0.32"
            ],
            [
                "Video Games",
                "72",
                "-0.01",
                "0.01",
                "0.12",
                "0.01",
                "0.08",
                "[ITALIC] 0.40",
                "[BOLD] 0.56",
                "0.01",
                "0.01",
                "0.06",
                "0.01",
                "0.05",
                "[ITALIC] 0.32",
                "[BOLD] 0.42"
            ],
            [
                "Average",
                "54.67",
                "0.09",
                "0.07",
                "0.08",
                "0.01",
                "0.08",
                "[BOLD] 0.22",
                "[ITALIC] 0.20",
                "0.02",
                "0.03",
                "0.04",
                "-0.01",
                "0.05",
                "[BOLD] 0.20",
                "[ITALIC] 0.17"
            ]
        ],
        "question": "Is it true that The semantic threshold for OD-d2v is set at 0.6 while for OD-w2v is set at 0.3?",
        "answer_label": "no"
    },
    {
        "id": "5c1296f4-0fdf-4ab1-9ef2-c8cb4e4a2e3f",
        "table_caption": "A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data Table 1: Performance of supervised learning models with different features.",
        "table_column_names": [
            "Feature",
            "LR P",
            "LR R",
            "LR F1",
            "SVM P",
            "SVM R",
            "SVM F1",
            "ANN P",
            "ANN R",
            "ANN F1"
        ],
        "table_content_values": [
            [
                "+BoW",
                "0.93",
                "0.91",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+BoC (Wiki-PubMed-PMC)",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.94",
                "0.92",
                "[BOLD] 0.93",
                "0.91",
                "0.91",
                "[BOLD] 0.91"
            ],
            [
                "+BoC (GloVe)",
                "0.93",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ],
            [
                "+ASM",
                "0.90",
                "0.85",
                "0.88",
                "0.90",
                "0.86",
                "0.88",
                "0.89",
                "0.89",
                "0.89"
            ],
            [
                "+Sentence Embeddings(SEs)",
                "0.89",
                "0.89",
                "0.89",
                "0.90",
                "0.86",
                "0.88",
                "0.88",
                "0.88",
                "0.88"
            ],
            [
                "+BoC(Wiki-PubMed-PMC)+SEs",
                "0.92",
                "0.92",
                "0.92",
                "0.94",
                "0.92",
                "0.93",
                "0.91",
                "0.91",
                "0.91"
            ]
        ],
        "question": "Is it true that Word embeddings derived from GloVe outperform Wiki-PubMed-PMC-based embeddings (Table 1)?",
        "answer_label": "no"
    },
    {
        "id": "2781ce47-903b-4c89-85a8-6153ef7c5707",
        "table_caption": "Localization of Fake News Detection via Multitask Transfer Learning Table 6: An ablation study on the effect of multiple heads in the attention mechanisms. The results show that increasing the number of heads improves performance, though this plateaus at 10 attention heads. All ablations use the multitask-based finetuning method. \u201cEffect\u201d refers to the increase or decrease of accuracy as the heads are removed. Note that 10 heads is the default used throughout the study.",
        "table_column_names": [
            "# of Heads",
            "Accuracy",
            "Val. Loss",
            "Effect"
        ],
        "table_content_values": [
            [
                "1",
                "89.44%",
                "0.2811",
                "-6.84%"
            ],
            [
                "2",
                "91.20%",
                "0.2692",
                "-5.08%"
            ],
            [
                "4",
                "93.85%",
                "0.2481",
                "-2.43%"
            ],
            [
                "8",
                "96.02%",
                "0.2257",
                "-0.26%"
            ],
            [
                "10",
                "96.28%",
                "0.2197",
                "[EMPTY]"
            ],
            [
                "16",
                "96.32%",
                "0.2190",
                "+0.04"
            ]
        ],
        "question": "Is it true that As shown in Table 6, increasing the number of attention heads does not necessarily improve multitasking performance?",
        "answer_label": "no"
    },
    {
        "id": "2220ceb0-019d-443e-89ee-f7ef12630a84",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that The performance of each approach that interacts with the agenda-based user simulator is shown in Table 3, with GDPL outperforming all other methods?",
        "answer_label": "no"
    },
    {
        "id": "08aab654-a0bc-4fc3-9ba0-5c1ae544fc69",
        "table_caption": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 4: Results of Self-Play Evaluation.",
        "table_column_names": [
            "System",
            "TGPC Succ. (%)",
            "TGPC #Turns",
            "CWC Succ. (%)",
            "CWC #Turns"
        ],
        "table_content_values": [
            [
                "Retrieval\u00a0",
                "7.16",
                "4.17",
                "0",
                "-"
            ],
            [
                "Retrieval-Stgy\u00a0",
                "47.80",
                "6.7",
                "44.6",
                "7.42"
            ],
            [
                "PMI\u00a0",
                "35.36",
                "6.38",
                "47.4",
                "5.29"
            ],
            [
                "Neural\u00a0",
                "54.76",
                "4.73",
                "47.6",
                "5.16"
            ],
            [
                "Kernel\u00a0",
                "62.56",
                "4.65",
                "53.2",
                "4.08"
            ],
            [
                "DKRN (ours)",
                "[BOLD] 89.0",
                "5.02",
                "[BOLD] 84.4",
                "4.20"
            ]
        ],
        "question": "Is it true that Although the average number of turns of our approach is slightly more than Kernel, our system obtains the highest success rate, significantly improving over other approaches?",
        "answer_label": "yes"
    },
    {
        "id": "de569012-eb52-4a91-b41c-4f97bd382305",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 8: Ablation study for density of connections on the dev set of AMR15. -{i} dense block denotes removing the dense connections in the i-th block.",
        "table_column_names": [
            "[BOLD] Model",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN4",
                "25.5",
                "55.4"
            ],
            [
                "-{4} dense block",
                "24.8",
                "54.9"
            ],
            [
                "-{3, 4} dense blocks",
                "23.8",
                "54.1"
            ],
            [
                "-{2, 3, 4} dense blocks",
                "23.2",
                "53.1"
            ]
        ],
        "question": "Is it true that Although these four models have the same number of layers, dense connections do not necessarily lead to better performance?",
        "answer_label": "no"
    },
    {
        "id": "2c5d4216-a6de-4d7c-ba81-4cde5a1639d8",
        "table_caption": "Improving Generalization by Incorporating Coverage in Natural Language Inference Table 3: Impact of using coverage for improving generalization across the datasets of similar tasks. Both models are trained on the SQuAD training data.",
        "table_column_names": [
            "[EMPTY]",
            "in-domain SQuAD",
            "in-domain SQuAD",
            "out-of-domain QA-SRL",
            "out-of-domain QA-SRL"
        ],
        "table_content_values": [
            [
                "[EMPTY]",
                "EM",
                "F1",
                "EM",
                "F1"
            ],
            [
                "MQAN",
                "31.76",
                "75.37",
                "<bold>10.99</bold>",
                "50.10"
            ],
            [
                "+coverage",
                "<bold>32.67</bold>",
                "<bold>76.83</bold>",
                "10.63",
                "<bold>50.89</bold>"
            ],
            [
                "BIDAF (ELMO)",
                "70.43",
                "79.76",
                "28.35",
                "49.98"
            ],
            [
                "+coverage",
                "<bold>71.07</bold>",
                "<bold>80.15</bold>",
                "<bold>30.58</bold>",
                "<bold>52.43</bold>"
            ]
        ],
        "question": "Is it true that Table 3 shows the impact of coverage for improving generalization across these two datasets that belong to the two similar tasks of reading comprehension and QA-SRL?",
        "answer_label": "yes"
    },
    {
        "id": "69d3706c-7e8f-4407-bb39-5eea75a9fb9c",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 2: Results evaluated on the original test set (averaged over 5 runs with different random initialisation). See Section\u00a05.1 for explanation of metrics. All numbers except NIST and ROUGE-L are percentages. Note that the numbers are not comparable to Table\u00a03 as the test set is different.",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Original",
                "TGen\u2212",
                "63.37",
                "7.7188",
                "41.99",
                "68.53",
                "1.9355",
                "00.06",
                "15.77",
                "00.11",
                "15.94"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen",
                "66.41",
                "8.5565",
                "45.07",
                "69.17",
                "2.2253",
                "00.14",
                "04.11",
                "00.03",
                "04.27"
            ],
            [
                "Original",
                "[BOLD] Original",
                "TGen+",
                "67.06",
                "8.5871",
                "45.83",
                "69.73",
                "2.2681",
                "00.04",
                "01.75",
                "00.01",
                "01.80"
            ],
            [
                "Original",
                "[BOLD] Original",
                "SC-LSTM",
                "39.11",
                "5.6704",
                "36.83",
                "50.02",
                "0.6045",
                "02.79",
                "18.90",
                "09.79",
                "31.51"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen\u2212",
                "65.87",
                "8.6400",
                "44.20",
                "67.51",
                "2.1710",
                "00.20",
                "00.56",
                "00.21",
                "00.97"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen",
                "66.24",
                "8.6889",
                "44.66",
                "67.85",
                "2.2181",
                "00.10",
                "00.02",
                "00.00",
                "00.12"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "TGen+",
                "65.97",
                "8.6630",
                "44.45",
                "67.59",
                "2.1855",
                "00.02",
                "00.00",
                "00.00",
                "00.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Original",
                "SC-LSTM",
                "38.52",
                "5.7125",
                "37.45",
                "48.50",
                "0.4343",
                "03.85",
                "17.39",
                "08.12",
                "29.37"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen\u2212",
                "66.28",
                "8.5202",
                "43.96",
                "67.83",
                "2.1375",
                "00.14",
                "02.26",
                "00.22",
                "02.61"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen",
                "67.00",
                "8.6889",
                "44.97",
                "68.19",
                "2.2228",
                "00.06",
                "00.44",
                "00.03",
                "00.53"
            ],
            [
                "Cleaned missing",
                "[BOLD] Original",
                "TGen+",
                "66.74",
                "8.6649",
                "44.84",
                "67.95",
                "2.2018",
                "00.00",
                "00.21",
                "00.03",
                "00.24"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen\u2212",
                "64.40",
                "7.9692",
                "42.81",
                "68.87",
                "2.0563",
                "00.01",
                "13.08",
                "00.00",
                "13.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen",
                "66.23",
                "8.5578",
                "45.12",
                "68.87",
                "2.2548",
                "00.04",
                "03.04",
                "00.00",
                "03.09"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Original",
                "TGen+",
                "65.96",
                "8.5238",
                "45.49",
                "68.79",
                "2.2456",
                "00.00",
                "01.44",
                "00.00",
                "01.45"
            ]
        ],
        "question": "Is it true that WOMs are slightly higher for TGen trained on the cleaned data, except for NIST, which gives more importance to matching less frequent n-grams?",
        "answer_label": "no"
    },
    {
        "id": "cc888efa-44f2-4095-adad-3055c1539c12",
        "table_caption": "Team EP at TAC 2018: Automating data extraction in systematic reviews of environmental agents Table 3: The estimation of impact of various design choices on the final result. The entries are sorted by the out-of-fold scores from CV. The SUBMISSION here uses score from ep_1 run for the single model and ep_2 for the ensemble performance.",
        "table_column_names": [
            "ID LSTM-800",
            "5-fold CV 70.56",
            "\u0394 0.66",
            "Single model 67.54",
            "\u0394 0.78",
            "Ensemble 67.65",
            "\u0394 0.30"
        ],
        "table_content_values": [
            [
                "LSTM-400",
                "70.50",
                "0.60",
                "[BOLD] 67.59",
                "0.83",
                "[BOLD] 68.00",
                "0.65"
            ],
            [
                "IN-TITLE",
                "70.11",
                "0.21",
                "[EMPTY]",
                "[EMPTY]",
                "67.52",
                "0.17"
            ],
            [
                "[BOLD] SUBMISSION",
                "69.90",
                "\u2013",
                "66.76",
                "\u2013",
                "67.35",
                "\u2013"
            ],
            [
                "NO-HIGHWAY",
                "69.72",
                "\u22120.18",
                "66.42",
                "\u22120.34",
                "66.64",
                "\u22120.71"
            ],
            [
                "NO-OVERLAPS",
                "69.46",
                "\u22120.44",
                "65.07",
                "\u22121.69",
                "66.47",
                "\u22120.88"
            ],
            [
                "LSTM-400-DROPOUT",
                "69.45",
                "\u22120.45",
                "65.53",
                "\u22121.23",
                "67.28",
                "\u22120.07"
            ],
            [
                "NO-TRANSLATIONS",
                "69.42",
                "\u22120.48",
                "65.92",
                "\u22120.84",
                "67.23",
                "\u22120.12"
            ],
            [
                "NO-ELMO-FINETUNING",
                "67.71",
                "\u22122.19",
                "65.16",
                "\u22121.60",
                "65.42",
                "\u22121.93"
            ]
        ],
        "question": "Is it true that [CONTINUE] Apart of the flipped results of the LSTM-800 and the LSTM-400, small differences in CV score are sometimes associated with large discrepancies in test set performance?",
        "answer_label": "yes"
    },
    {
        "id": "85ebe21b-ff40-4c23-b524-599d069dd7a5",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that This indicates that our architecture cannot learn to generate better signals for text generation?",
        "answer_label": "no"
    },
    {
        "id": "0f052b57-c133-422e-be0d-97281f7665a3",
        "table_caption": "Improving the Expressiveness of Deep Learning Frameworks with Recursion2footnote 22footnote 2 Table 1: Throughput for the TreeRNN model implemented with recursive dataflow graphs, using datasets of varying tree balancedness. The balanced dataset exhibits highest throughput thanks to the high degree of parallelization, but at the same time does not improve as well as the linear dataset when the batch size increases from 1 to 25, because there is only a small room of performance improvement left, w.r.t parallelization.",
        "table_column_names": [
            "Batch size",
            "Throughput (instances/s) Balanced",
            "Throughput (instances/s) Moderate",
            "Throughput (instances/s) Linear"
        ],
        "table_content_values": [
            [
                "1",
                "46.7",
                "27.3",
                "7.6"
            ],
            [
                "10",
                "125.2",
                "78.2",
                "22.7"
            ],
            [
                "25",
                "129.7",
                "83.1",
                "45.4"
            ]
        ],
        "question": "Is it true that On the contrary, for the linear dataset, the recursive implementation efficiently makes use of CPU resources and thus the performance gain provided by increasing the batch size is relatively low?",
        "answer_label": "no"
    },
    {
        "id": "dc829323-cd24-4c1e-a8aa-97dd288a0320",
        "table_caption": "Argument Generation with Retrieval, Planning, and Realization Table 3: Main results on argument generation. We report BLEU-2 (B-2), BLEU-4 (B-4), ROUGE-2 (R-2) recall, METEOR (MTR), and average number of words per argument and per sentence. Best scores are in bold. \u2217: statistically significantly better than all comparisons (randomization approximation test\u00a0Noreen (1989), p<0.0005). Input is the same for Seq2seq for both system and oracle setups.",
        "table_column_names": [
            "[EMPTY]",
            "[ITALIC] w/ System Retrieval  [BOLD] B-2",
            "[ITALIC] w/ System Retrieval  [BOLD] B-4",
            "[ITALIC] w/ System Retrieval  [BOLD] R-2",
            "[ITALIC] w/ System Retrieval  [BOLD] MTR",
            "[ITALIC] w/ System Retrieval  [BOLD] #Word",
            "[ITALIC] w/ System Retrieval  [BOLD] #Sent",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] B-4",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] R-2",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Word",
            "[ITALIC] w/ Oracle Retrieval  [BOLD] #Sent"
        ],
        "table_content_values": [
            [
                "Human",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22",
                "-",
                "-",
                "-",
                "-",
                "66",
                "22"
            ],
            [
                "Retrieval",
                "7.55",
                "1.11",
                "8.64",
                "14.38",
                "123",
                "23",
                "10.97",
                "3.05",
                "23.49",
                "20.08",
                "140",
                "21"
            ],
            [
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[BOLD] Comparisons",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Seq2seq",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15",
                "6.92",
                "2.13",
                "13.02",
                "15.08",
                "68",
                "15"
            ],
            [
                "Seq2seqAug",
                "8.26",
                "2.24",
                "13.79",
                "15.75",
                "78",
                "14",
                "10.98",
                "4.41",
                "22.97",
                "19.62",
                "71",
                "14"
            ],
            [
                "[ITALIC] w/o psg",
                "7.94",
                "2.28",
                "10.13",
                "15.71",
                "75",
                "12",
                "9.89",
                "3.34",
                "14.20",
                "18.40",
                "66",
                "12"
            ],
            [
                "H&W\u00a0Hua and Wang ( 2018 )",
                "3.64",
                "0.92",
                "8.83",
                "11.78",
                "51",
                "12",
                "8.51",
                "2.86",
                "18.89",
                "17.18",
                "58",
                "12"
            ],
            [
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[BOLD] Our Models",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "CANDELA",
                "12.02\u2217",
                "[BOLD] 2.99\u2217",
                "[BOLD] 14.93\u2217",
                "[BOLD] 16.92\u2217",
                "119",
                "22",
                "15.80\u2217",
                "[BOLD] 5.00\u2217",
                "[BOLD] 23.75",
                "[BOLD] 20.18",
                "116",
                "22"
            ],
            [
                "[ITALIC] w/o psg",
                "[BOLD] 12.33\u2217",
                "2.86\u2217",
                "14.53\u2217",
                "16.60\u2217",
                "123",
                "23",
                "[BOLD] 16.33\u2217",
                "4.98\u2217",
                "23.65",
                "19.94",
                "123",
                "23"
            ]
        ],
        "question": "Is it true that Under oracle setup, all models are notably improved due to the higher quality of reranked passages, and our model achieves statistically significantly better BLEU scores?",
        "answer_label": "yes"
    },
    {
        "id": "a22b9660-188b-4c29-b248-c811154705b7",
        "table_caption": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 3: Results of Turn-level Evaluation.",
        "table_column_names": [
            "Dataset",
            "System",
            "Keyword Prediction  [ITALIC] Rw@1",
            "Keyword Prediction  [ITALIC] Rw@3",
            "Keyword Prediction  [ITALIC] Rw@5",
            "Keyword Prediction P@1",
            "Response Retrieval  [ITALIC] R20@1",
            "Response Retrieval  [ITALIC] R20@3",
            "Response Retrieval  [ITALIC] R20@5",
            "Response Retrieval MRR"
        ],
        "table_content_values": [
            [
                "TGPC",
                "Retrieval\u00a0",
                "-",
                "-",
                "-",
                "-",
                "0.5063",
                "0.7615",
                "0.8676",
                "0.6589"
            ],
            [
                "TGPC",
                "PMI\u00a0",
                "0.0585",
                "0.1351",
                "0.1872",
                "0.0871",
                "0.5441",
                "0.7839",
                "0.8716",
                "0.6847"
            ],
            [
                "TGPC",
                "Neural\u00a0",
                "0.0708",
                "0.1438",
                "0.1820",
                "0.1321",
                "0.5311",
                "0.7905",
                "0.8800",
                "0.6822"
            ],
            [
                "TGPC",
                "Kernel\u00a0",
                "0.0632",
                "0.1377",
                "0.1798",
                "0.1172",
                "0.5386",
                "0.8012",
                "0.8924",
                "0.6877"
            ],
            [
                "TGPC",
                "DKRN (ours)",
                "[BOLD] 0.0909",
                "[BOLD] 0.1903",
                "[BOLD] 0.2477",
                "[BOLD] 0.1685",
                "[BOLD] 0.5729",
                "[BOLD] 0.8132",
                "[BOLD] 0.8966",
                "[BOLD] 0.7110"
            ],
            [
                "CWC",
                "Retrieval\u00a0",
                "-",
                "-",
                "-",
                "-",
                "0.5785",
                "0.8101",
                "0.8999",
                "0.7141"
            ],
            [
                "CWC",
                "PMI\u00a0",
                "0.0555",
                "0.1001",
                "0.1212",
                "0.0969",
                "0.5945",
                "0.8185",
                "0.9054",
                "0.7257"
            ],
            [
                "CWC",
                "Neural\u00a0",
                "0.0654",
                "0.1194",
                "0.1450",
                "0.1141",
                "0.6044",
                "0.8233",
                "0.9085",
                "0.7326"
            ],
            [
                "CWC",
                "Kernel\u00a0",
                "0.0592",
                "0.1113",
                "0.1337",
                "0.1011",
                "0.6017",
                "0.8234",
                "0.9087",
                "0.7320"
            ],
            [
                "CWC",
                "DKRN (ours)",
                "[BOLD] 0.0680",
                "[BOLD] 0.1254",
                "[BOLD] 0.1548",
                "[BOLD] 0.1185",
                "[BOLD] 0.6324",
                "[BOLD] 0.8416",
                "[BOLD] 0.9183",
                "[BOLD] 0.7533"
            ]
        ],
        "question": "Is it true that Our approach DKRN does not outperform all state-of-the-art methods in terms of all metrics on both datasets with two tasks?",
        "answer_label": "no"
    },
    {
        "id": "0018d644-5832-4e3f-ac9b-9b6069ff5550",
        "table_caption": "Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation Table 1: Overall performance comparing to the state-of-the-art methods with golden-standard entities.",
        "table_column_names": [
            "[BOLD] Method",
            "[BOLD] Trigger  [BOLD] Identification (%)",
            "[BOLD] Trigger  [BOLD] Identification (%)",
            "[BOLD] Trigger  [BOLD] Identification (%)",
            "[BOLD] Trigger  [BOLD] Classification (%)",
            "[BOLD] Trigger  [BOLD] Classification (%)",
            "[BOLD] Trigger  [BOLD] Classification (%)",
            "[BOLD] Argument  [BOLD] Identification (%)",
            "[BOLD] Argument  [BOLD] Identification (%)",
            "[BOLD] Argument  [BOLD] Identification (%)",
            "[BOLD] Argument  [BOLD] Role (%)",
            "[BOLD] Argument  [BOLD] Role (%)",
            "[BOLD] Argument  [BOLD] Role (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] Method",
                "[ITALIC] P",
                "[ITALIC] R",
                "[ITALIC] F1",
                "[ITALIC] P",
                "[ITALIC] R",
                "[ITALIC] F1",
                "[ITALIC] P",
                "[ITALIC] R",
                "[ITALIC] F1",
                "[ITALIC] P",
                "[ITALIC] R",
                "[ITALIC] F1"
            ],
            [
                "Cross-Event",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "68.7",
                "68.9",
                "68.8",
                "50.9",
                "49.7",
                "50.3",
                "45.1",
                "44.1",
                "44.6"
            ],
            [
                "JointBeam",
                "76.9",
                "65.0",
                "70.4",
                "73.7",
                "62.3",
                "67.5",
                "69.8",
                "47.9",
                "56.8",
                "64.7",
                "44.4",
                "52.7"
            ],
            [
                "DMCNN",
                "[BOLD] 80.4",
                "67.7",
                "73.5",
                "75.6",
                "63.6",
                "69.1",
                "68.8",
                "51.9",
                "59.1",
                "62.2",
                "46.9",
                "53.5"
            ],
            [
                "PSL",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "75.3",
                "64.4",
                "69.4",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "JRNN",
                "68.5",
                "[BOLD] 75.7",
                "71.9",
                "66.0",
                "[BOLD] 73.0",
                "69.3",
                "61.4",
                "64.2",
                "62.8",
                "54.2",
                "56.7",
                "55.4"
            ],
            [
                "dbRNN",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "74.1",
                "69.8",
                "71.9",
                "71.3",
                "64.5",
                "67.7",
                "66.2",
                "52.8",
                "58.7"
            ],
            [
                "[BOLD] JMEE",
                "80.2",
                "72.1",
                "[BOLD] 75.9",
                "[BOLD] 76.3",
                "71.3",
                "[BOLD] 73.7",
                "[BOLD] 71.4",
                "[BOLD] 65.6",
                "[BOLD] 68.4",
                "[BOLD] 66.8",
                "[BOLD] 54.9",
                "[BOLD] 60.3"
            ]
        ],
        "question": "Is it true that From the table, we can see that our JMEE framework achieves the best F1 scores for both trigger classification and argumentrelated subtasks among all the compared methods?",
        "answer_label": "yes"
    },
    {
        "id": "0c917759-6018-4282-9826-73b13410d748",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 7: Comparisons of different DCGCN models under almost the same parameter budget.",
        "table_column_names": [
            "[BOLD] Model",
            "D",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "DCGCN(1)",
                "300",
                "10.9M",
                "20.9",
                "52.0"
            ],
            [
                "DCGCN(2)",
                "180",
                "10.9M",
                "[BOLD] 22.2",
                "[BOLD] 52.3"
            ],
            [
                "DCGCN(2)",
                "240",
                "11.3M",
                "22.8",
                "52.8"
            ],
            [
                "DCGCN(4)",
                "180",
                "11.4M",
                "[BOLD] 23.4",
                "[BOLD] 53.4"
            ],
            [
                "DCGCN(1)",
                "420",
                "12.6M",
                "22.2",
                "52.4"
            ],
            [
                "DCGCN(2)",
                "300",
                "12.5M",
                "23.8",
                "53.8"
            ],
            [
                "DCGCN(3)",
                "240",
                "12.3M",
                "[BOLD] 23.9",
                "[BOLD] 54.1"
            ],
            [
                "DCGCN(2)",
                "360",
                "14.0M",
                "24.2",
                "[BOLD] 54.4"
            ],
            [
                "DCGCN(3)",
                "300",
                "14.0M",
                "[BOLD] 24.4",
                "54.2"
            ],
            [
                "DCGCN(2)",
                "420",
                "15.6M",
                "24.1",
                "53.7"
            ],
            [
                "DCGCN(4)",
                "300",
                "15.6M",
                "[BOLD] 24.6",
                "[BOLD] 54.8"
            ],
            [
                "DCGCN(3)",
                "420",
                "18.6M",
                "24.5",
                "54.6"
            ],
            [
                "DCGCN(4)",
                "360",
                "18.4M",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that For example, when both DCGCN1 and DCGCN2 are limited to 10.9M parameters, DCGCN1 obtains 20.9 BLEU points, which is higher than DCGCN2 (22.2)?",
        "answer_label": "no"
    },
    {
        "id": "b3e222d9-7bea-433d-bfad-a5e18e07af19",
        "table_caption": "Sparse and Structured Visual Attention Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
        "table_column_names": [
            "[EMPTY]",
            "Att. to image",
            "Att. to bounding boxes",
            "Test-Dev Yes/No",
            "Test-Dev Number",
            "Test-Dev Other",
            "Test-Dev Overall",
            "Test-Standard Yes/No",
            "Test-Standard Number",
            "Test-Standard Other",
            "Test-Standard Overall"
        ],
        "table_content_values": [
            [
                "softmax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "42.65",
                "55.74",
                "65.52",
                "83.55",
                "42.68",
                "56.01",
                "65.97"
            ],
            [
                "sparsemax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "43.19",
                "55.79",
                "65.60",
                "83.33",
                "42.99",
                "56.06",
                "65.94"
            ],
            [
                "soft-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.13",
                "43.53",
                "56.01",
                "65.76",
                "83.63",
                "43.24",
                "56.10",
                "66.11"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.10",
                "43.30",
                "56.14",
                "65.79",
                "83.66",
                "43.18",
                "56.21",
                "66.17"
            ],
            [
                "softmax",
                "[EMPTY]",
                "\u2713",
                "85.14",
                "49.59",
                "58.72",
                "68.57",
                "85.56",
                "49.54",
                "59.11",
                "69.04"
            ],
            [
                "sparsemax",
                "[EMPTY]",
                "\u2713",
                "[BOLD] 85.40",
                "[BOLD] 50.87",
                "58.67",
                "68.79",
                "[BOLD] 85.80",
                "50.18",
                "59.08",
                "69.19"
            ],
            [
                "softmax",
                "\u2713",
                "\u2713",
                "85.33",
                "50.49",
                "58.88",
                "68.82",
                "85.58",
                "50.42",
                "59.18",
                "69.17"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "\u2713",
                "85.35",
                "50.52",
                "[BOLD] 59.15",
                "[BOLD] 68.96",
                "85.72",
                "[BOLD] 50.66",
                "[BOLD] 59.22",
                "[BOLD] 69.28"
            ]
        ],
        "question": "Is it true that Thus, having sparse attention mechanisms in the self-attention layers is beneficial, but the biggest improvement is not necessarily obtained when using TVMAX in the output attention?",
        "answer_label": "no"
    },
    {
        "id": "4495c767-a361-43b9-8ebb-0290a8013b03",
        "table_caption": "Low-supervision urgency detection and transfer in short crisis messages TABLE IV: Results investigating RQ1 on the Nepal and Kerala datasets. (a) Nepal",
        "table_column_names": [
            "System",
            "Accuracy",
            "Precision",
            "Recall",
            "F-Measure"
        ],
        "table_content_values": [
            [
                "Local",
                "63.97%",
                "64.27%",
                "64.50%",
                "63.93%"
            ],
            [
                "Manual",
                "64.25%",
                "[BOLD] 70.84%\u2217\u2217",
                "48.50%",
                "57.11%"
            ],
            [
                "Wiki",
                "67.25%",
                "66.51%",
                "69.50%",
                "67.76%"
            ],
            [
                "Local-Manual",
                "65.75%",
                "67.96%",
                "59.50%",
                "62.96%"
            ],
            [
                "Wiki-Local",
                "67.40%",
                "65.54%",
                "68.50%",
                "66.80%"
            ],
            [
                "Wiki-Manual",
                "67.75%",
                "70.38%",
                "63.00%",
                "65.79%"
            ],
            [
                "[ITALIC] Our Approach",
                "[BOLD] 69.25%\u2217\u2217\u2217",
                "68.76%",
                "[BOLD] 70.50%\u2217\u2217",
                "[BOLD] 69.44%\u2217\u2217\u2217"
            ]
        ],
        "question": "Is it true that The results illustrate the lack of viability of urgency detection in low-supervision settings (with our approach yielding 69.44% F-Measure on Nepal, at 99% significance compared to the Local baseline), with different feature sets contributing differently to the four metrics?",
        "answer_label": "no"
    },
    {
        "id": "70c9a077-aea5-467e-97cd-520f06da7cd4",
        "table_caption": "Domain Adaptive Inference for Neural Machine Translation Table 7: Total BLEU for test data concatenated across domains. Results from 2-model es-en and 3-model en-de ensembles, compared to oracle model chosen if test domain is known. No-reg uniform corresponds to the approach of Freitag and Al-Onaizan (2016). BI+IS performs similarly to strong oracles with no test domain labeling.",
        "table_column_names": [
            "[BOLD] Language pair",
            "[BOLD] Model type",
            "[BOLD] Oracle model",
            "[BOLD] Decoder configuration  [BOLD] Uniform",
            "[BOLD] Decoder configuration  [BOLD] BI + IS"
        ],
        "table_content_values": [
            [
                "es-en",
                "Unadapted",
                "36.4",
                "34.7",
                "36.6"
            ],
            [
                "es-en",
                "No-reg",
                "36.6",
                "34.8",
                "-"
            ],
            [
                "es-en",
                "EWC",
                "37.0",
                "36.3",
                "[BOLD] 37.2"
            ],
            [
                "en-de",
                "Unadapted",
                "36.4",
                "26.8",
                "38.8"
            ],
            [
                "en-de",
                "No-reg",
                "41.7",
                "31.8",
                "-"
            ],
            [
                "en-de",
                "EWC",
                "42.1",
                "38.6",
                "[BOLD] 42.0"
            ]
        ],
        "question": "Is it true that Uniform no-reg ensembling outperforms unadapted uniform ensembling, since fine-tuning gives better in-domain performance?",
        "answer_label": "yes"
    },
    {
        "id": "550e39a7-dda1-4dac-ac92-76636384b64b",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 4: Exact match/F1-score on SQuad dataset. \u201c#Params\u201d: the parameter number of Base. rnet*: results published by\u00a0Wang et\u00a0al. (2017).",
        "table_column_names": [
            "Model",
            "#Params",
            "Base",
            "+Elmo"
        ],
        "table_content_values": [
            [
                "rnet*",
                "-",
                "71.1/79.5",
                "-/-"
            ],
            [
                "LSTM",
                "2.67M",
                "[BOLD] 70.46/78.98",
                "75.17/82.79"
            ],
            [
                "GRU",
                "2.31M",
                "70.41/ [BOLD] 79.15",
                "75.81/83.12"
            ],
            [
                "ATR",
                "1.59M",
                "69.73/78.70",
                "75.06/82.76"
            ],
            [
                "SRU",
                "2.44M",
                "69.27/78.41",
                "74.56/82.50"
            ],
            [
                "LRN",
                "2.14M",
                "70.11/78.83",
                "[BOLD] 76.14/ [BOLD] 83.83"
            ]
        ],
        "question": "Is it true that After integrating Elmo for contextual modeling, the performance of LRN reaches the best (76.1 [CONTINUE] EM and 83.83 F1), beating both GRU and LSTM (+0.33EM, +0.71F1)?",
        "answer_label": "yes"
    },
    {
        "id": "feb01e4b-e4e7-4f6c-a264-d2fb2e3f1962",
        "table_caption": "Sparse and Structured Visual Attention Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
        "table_column_names": [
            "[EMPTY]",
            "Att. to image",
            "Att. to bounding boxes",
            "Test-Dev Yes/No",
            "Test-Dev Number",
            "Test-Dev Other",
            "Test-Dev Overall",
            "Test-Standard Yes/No",
            "Test-Standard Number",
            "Test-Standard Other",
            "Test-Standard Overall"
        ],
        "table_content_values": [
            [
                "softmax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "42.65",
                "55.74",
                "65.52",
                "83.55",
                "42.68",
                "56.01",
                "65.97"
            ],
            [
                "sparsemax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "43.19",
                "55.79",
                "65.60",
                "83.33",
                "42.99",
                "56.06",
                "65.94"
            ],
            [
                "soft-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.13",
                "43.53",
                "56.01",
                "65.76",
                "83.63",
                "43.24",
                "56.10",
                "66.11"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.10",
                "43.30",
                "56.14",
                "65.79",
                "83.66",
                "43.18",
                "56.21",
                "66.17"
            ],
            [
                "softmax",
                "[EMPTY]",
                "\u2713",
                "85.14",
                "49.59",
                "58.72",
                "68.57",
                "85.56",
                "49.54",
                "59.11",
                "69.04"
            ],
            [
                "sparsemax",
                "[EMPTY]",
                "\u2713",
                "[BOLD] 85.40",
                "[BOLD] 50.87",
                "58.67",
                "68.79",
                "[BOLD] 85.80",
                "50.18",
                "59.08",
                "69.19"
            ],
            [
                "softmax",
                "\u2713",
                "\u2713",
                "85.33",
                "50.49",
                "58.88",
                "68.82",
                "85.58",
                "50.42",
                "59.18",
                "69.17"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "\u2713",
                "85.35",
                "50.52",
                "[BOLD] 59.15",
                "[BOLD] 68.96",
                "85.72",
                "[BOLD] 50.66",
                "[BOLD] 59.22",
                "[BOLD] 69.28"
            ]
        ],
        "question": "Is it true that Thus, having sparse attention mechanisms in the self-attention layers is beneficial, but the biggest improvement is obtained when using TVMAX in the output attention?",
        "answer_label": "yes"
    },
    {
        "id": "a2ea71dc-298d-49ba-8699-c91fbcf5159c",
        "table_caption": "Better Rewards Yield Better Summaries: Learning to Summarise Without References Table 1: Quality of reward metrics. G-Pre and G-Rec are the precision and recall rate of the \u201cgood\u201d summaries identified by the metrics, resp. All metrics here require reference summaries. We perform stemming and stop words removal as preprosessing, as they help increase the correlation. For InferSent, the embeddings of the reference/system summaries are obtained by averaging the embeddings of the sentences therein.",
        "table_column_names": [
            "Metric",
            "[ITALIC] \u03c1",
            "[ITALIC] r",
            "G-Pre",
            "G-Rec"
        ],
        "table_content_values": [
            [
                "ROUGE-1",
                ".290",
                ".304",
                ".392",
                ".428"
            ],
            [
                "ROUGE-2",
                ".259",
                ".278",
                ".408",
                ".444"
            ],
            [
                "ROUGE-L",
                ".274",
                ".297",
                ".390",
                ".426"
            ],
            [
                "ROUGE-SU4",
                ".282",
                ".279",
                ".404",
                ".440"
            ],
            [
                "BLEU-1",
                ".256",
                ".281",
                ".409",
                ".448"
            ],
            [
                "BLEU-2",
                ".301",
                ".312",
                ".411",
                ".446"
            ],
            [
                "BLEU-3",
                ".317",
                ".312",
                ".409",
                ".444"
            ],
            [
                "BLEU-4",
                ".311",
                ".307",
                ".409",
                ".446"
            ],
            [
                "BLEU-5",
                ".308",
                ".303",
                ".420",
                ".459"
            ],
            [
                "METEOR",
                ".305",
                ".285",
                ".409",
                ".444"
            ],
            [
                "InferSent-Cosine",
                "[BOLD] .329",
                "[BOLD] .339",
                ".417",
                ".460"
            ],
            [
                "BERT-Cosine",
                ".312",
                ".335",
                "[BOLD] .440",
                "[BOLD] .484"
            ]
        ],
        "question": "Is it true that More importantly, their G-Pre and G-Rec scores are all below .50, which means that more than half of the good summaries identified by the metrics are actually not good, and more than 50%?",
        "answer_label": "yes"
    },
    {
        "id": "9e01d648-e9ad-493e-979e-f695d4f329f4",
        "table_caption": "Solving Hard Coreference Problems Table 8: Performance results on ACE and OntoNotes datasets. Our system gets the same level of performance compared to a state-of-art general coreference system.",
        "table_column_names": [
            "System",
            "MUC",
            "BCUB",
            "CEAFe",
            "AVG"
        ],
        "table_content_values": [
            [
                "ACE",
                "ACE",
                "ACE",
                "ACE",
                "ACE"
            ],
            [
                "IlliCons",
                "[BOLD] 78.17",
                "81.64",
                "[BOLD] 78.45",
                "[BOLD] 79.42"
            ],
            [
                "KnowComb",
                "77.51",
                "[BOLD] 81.97",
                "77.44",
                "78.97"
            ],
            [
                "OntoNotes",
                "OntoNotes",
                "OntoNotes",
                "OntoNotes",
                "OntoNotes"
            ],
            [
                "IlliCons",
                "84.10",
                "[BOLD] 78.30",
                "[BOLD] 68.74",
                "[BOLD] 77.05"
            ],
            [
                "KnowComb",
                "[BOLD] 84.33",
                "78.02",
                "67.95",
                "76.76"
            ]
        ],
        "question": "Is it true that Our KnowComb system achieves the same level of performance as does the state-of-art general coreference system we base it on?",
        "answer_label": "yes"
    },
    {
        "id": "99b4876b-e7fd-48d6-b96a-d9f0c2fedb05",
        "table_caption": "Dynamic Knowledge Routing Network For Target-Guided Open-Domain Conversation Table 4: Results of Self-Play Evaluation.",
        "table_column_names": [
            "System",
            "TGPC Succ. (%)",
            "TGPC #Turns",
            "CWC Succ. (%)",
            "CWC #Turns"
        ],
        "table_content_values": [
            [
                "Retrieval\u00a0",
                "7.16",
                "4.17",
                "0",
                "-"
            ],
            [
                "Retrieval-Stgy\u00a0",
                "47.80",
                "6.7",
                "44.6",
                "7.42"
            ],
            [
                "PMI\u00a0",
                "35.36",
                "6.38",
                "47.4",
                "5.29"
            ],
            [
                "Neural\u00a0",
                "54.76",
                "4.73",
                "47.6",
                "5.16"
            ],
            [
                "Kernel\u00a0",
                "62.56",
                "4.65",
                "53.2",
                "4.08"
            ],
            [
                "DKRN (ours)",
                "[BOLD] 89.0",
                "5.02",
                "[BOLD] 84.4",
                "4.20"
            ]
        ],
        "question": "Is it true that Although the average number of turns of our approach is slightly more than Kernel, the success rate of our system is not significantly better than other approaches?",
        "answer_label": "no"
    },
    {
        "id": "b598c64e-ca52-4716-a6b8-4c78cbbc2195",
        "table_caption": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] ROUGE-1  [BOLD] R (%)",
            "[BOLD] ROUGE-1  [BOLD] P (%)",
            "[BOLD] ROUGE-1  [BOLD] F (%)",
            "[BOLD] ROUGE-2  [BOLD] R (%)",
            "[BOLD] ROUGE-2  [BOLD] P (%)",
            "[BOLD] ROUGE-2  [BOLD] F (%)",
            "[BOLD] Sentence-Level  [BOLD] R (%)",
            "[BOLD] Sentence-Level  [BOLD] P (%)",
            "[BOLD] Sentence-Level  [BOLD] F (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] ILP",
                "24.5",
                "41.1",
                "29.3\u00b10.5",
                "7.9",
                "15.0",
                "9.9\u00b10.5",
                "13.6",
                "22.6",
                "15.6\u00b10.4"
            ],
            [
                "[BOLD] Sum-Basic",
                "28.4",
                "44.4",
                "33.1\u00b10.5",
                "8.5",
                "15.6",
                "10.4\u00b10.4",
                "14.7",
                "22.9",
                "16.7\u00b10.5"
            ],
            [
                "[BOLD] KL-Sum",
                "39.5",
                "34.6",
                "35.5\u00b10.5",
                "13.0",
                "12.7",
                "12.3\u00b10.5",
                "15.2",
                "21.1",
                "16.3\u00b10.5"
            ],
            [
                "[BOLD] LexRank",
                "42.1",
                "39.5",
                "38.7\u00b10.5",
                "14.7",
                "15.3",
                "14.2\u00b10.5",
                "14.3",
                "21.5",
                "16.0\u00b10.5"
            ],
            [
                "[BOLD] MEAD",
                "45.5",
                "36.5",
                "38.5\u00b1 0.5",
                "17.9",
                "14.9",
                "15.4\u00b10.5",
                "27.8",
                "29.2",
                "26.8\u00b10.5"
            ],
            [
                "[BOLD] SVM",
                "19.0",
                "48.8",
                "24.7\u00b10.8",
                "7.5",
                "21.1",
                "10.0\u00b10.5",
                "32.7",
                "34.3",
                "31.4\u00b10.4"
            ],
            [
                "[BOLD] LogReg",
                "26.9",
                "34.5",
                "28.7\u00b10.6",
                "6.4",
                "9.9",
                "7.3\u00b10.4",
                "12.2",
                "14.9",
                "12.7\u00b10.5"
            ],
            [
                "[BOLD] LogReg [ITALIC] r",
                "28.0",
                "34.8",
                "29.4\u00b10.6",
                "6.9",
                "10.4",
                "7.8\u00b10.4",
                "12.1",
                "14.5",
                "12.5\u00b10.5"
            ],
            [
                "[BOLD] HAN",
                "31.0",
                "42.8",
                "33.7\u00b10.7",
                "11.2",
                "17.8",
                "12.7\u00b10.5",
                "26.9",
                "34.1",
                "32.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT",
                "32.2",
                "42.4",
                "34.4\u00b10.7",
                "11.5",
                "17.5",
                "12.9\u00b10.5",
                "29.6",
                "35.8",
                "32.2\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU",
                "32.1",
                "42.1",
                "33.8\u00b10.7",
                "11.6",
                "17.6",
                "12.9\u00b10.5",
                "30.1",
                "35.6",
                "32.3\u00b10.5"
            ],
            [
                "[BOLD] HAN [ITALIC] r",
                "38.1",
                "40.5",
                "[BOLD] 37.8\u00b10.5",
                "14.0",
                "17.1",
                "[BOLD] 14.7\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.5",
                "16.8",
                "[BOLD] 14.4\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.6",
                "16.9",
                "[BOLD] 14.4\u00b10.5",
                "33.9",
                "33.8",
                "[BOLD] 33.8\u00b10.5"
            ]
        ],
        "question": "Is it true that HAN models do not outperform both LogReg and SVM using the current set of features?",
        "answer_label": "no"
    },
    {
        "id": "2c2a4d97-aacc-474f-a381-6fb70c1daa1f",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Method",
            "SUBJ",
            "CR",
            "MR",
            "MPQA",
            "MRPC",
            "TREC",
            "SICK-E",
            "SST2",
            "SST5",
            "STS-B",
            "SICK-R"
        ],
        "table_content_values": [
            [
                "CBOW/784",
                "90.0",
                "[BOLD] 79.2",
                "[BOLD] 74.0",
                "87.1",
                "71.6",
                "85.6",
                "78.9",
                "78.5",
                "42.1",
                "61.0",
                "[BOLD] 78.1"
            ],
            [
                "CMOW/784",
                "87.5",
                "73.4",
                "70.6",
                "[BOLD] 87.3",
                "69.6",
                "[BOLD] 88.0",
                "77.2",
                "74.7",
                "37.9",
                "56.5",
                "76.2"
            ],
            [
                "Hybrid",
                "[BOLD] 90.2",
                "78.7",
                "73.7",
                "[BOLD] 87.3",
                "[BOLD] 72.7",
                "87.6",
                "[BOLD] 79.4",
                "[BOLD] 79.6",
                "[BOLD] 43.3",
                "[BOLD] 63.4",
                "77.8"
            ],
            [
                "cmp. CBOW",
                "+0.2%",
                "-0.6%",
                "-0.4%",
                "+0.2%",
                "+1.5%",
                "+2.3%",
                "+0.6%",
                "+1.4%",
                "+2.9%",
                "+3.9%",
                "-0.4%"
            ],
            [
                "cmp. CMOW",
                "+3.1%",
                "+7.2%",
                "+4.4%",
                "+0%",
                "+4.5%",
                "-0.5%",
                "+2.9%",
                "+6.7%",
                "+14.3",
                "+12.2%",
                "+2.1%"
            ]
        ],
        "question": "Is it true that On the TREC task, CBOW outperforms CMOW by 2.3 points?",
        "answer_label": "no"
    },
    {
        "id": "3f49d5e4-4226-44ed-b64b-6fd8e62e1cc5",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
        "table_column_names": [
            "<bold>Model</bold>",
            "REF \u21d2 GEN <bold>ENT</bold>",
            "REF \u21d2 GEN <bold>CON</bold>",
            "REF \u21d2 GEN <bold>NEU</bold>"
        ],
        "table_content_values": [
            [
                "S2S",
                "38.45",
                "11.17",
                "50.38"
            ],
            [
                "G2S-GIN",
                "49.78",
                "9.80",
                "40.42"
            ],
            [
                "G2S-GAT",
                "49.48",
                "8.09",
                "42.43"
            ],
            [
                "G2S-GGNN",
                "51.32",
                "8.82",
                "39.86"
            ],
            [
                "[EMPTY]",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF"
            ],
            [
                "<bold>Model</bold>",
                "<bold>ENT</bold>",
                "<bold>CON</bold>",
                "<bold>NEU</bold>"
            ],
            [
                "S2S",
                "73.79",
                "12.75",
                "13.46"
            ],
            [
                "G2S-GIN",
                "76.27",
                "10.65",
                "13.08"
            ],
            [
                "G2S-GAT",
                "77.54",
                "8.54",
                "13.92"
            ],
            [
                "G2S-GGNN",
                "77.64",
                "9.64",
                "12.72"
            ]
        ],
        "question": "Is it true that G2S-GGNN has 33.5% and 5.2% worse entailment performances than S2S, when REF entails GEN and GEN entails REF, respectively?",
        "answer_label": "no"
    },
    {
        "id": "6684d294-b666-4718-affe-953ad1c47f8b",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 2: Ratings of annotated NLDs by human judges.",
        "table_column_names": [
            "# steps",
            "Reachability",
            "Derivability Step 1",
            "Derivability Step 2",
            "Derivability Step 3"
        ],
        "table_content_values": [
            [
                "1",
                "3.0",
                "3.8",
                "-",
                "-"
            ],
            [
                "2",
                "2.8",
                "3.8",
                "3.7",
                "-"
            ],
            [
                "3",
                "2.3",
                "3.9",
                "3.8",
                "3.8"
            ]
        ],
        "question": "Is it true that The evaluation results shown in Table 2 indicate that the annotated NLDs are of low quality (Reachability), and each NLD is not properly derived from supporting documents (Derivability)?",
        "answer_label": "no"
    },
    {
        "id": "3af33c66-1cff-404c-be5f-b3aa9c3b3cf4",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
        "table_column_names": [
            "[BOLD] Training data",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] Disfl"
        ],
        "table_content_values": [
            [
                "Original",
                "0",
                "22",
                "0",
                "14"
            ],
            [
                "Cleaned added",
                "0",
                "23",
                "0",
                "14"
            ],
            [
                "Cleaned missing",
                "0",
                "1",
                "0",
                "2"
            ],
            [
                "Cleaned",
                "0",
                "0",
                "0",
                "5"
            ]
        ],
        "question": "Is it true that The results in Table 4 refute the findings of the automatic metrics: systems trained on the fully cleaned set or the set with cleaned missing slots do not have nearperfect performance, with the fully-cleaned one showing more errors than the other?",
        "answer_label": "no"
    },
    {
        "id": "232b4447-1be4-49ce-afe3-5bed802143ac",
        "table_caption": "A Lightweight Recurrent Network for Sequence Modeling Table 3: Case-insensitive tokenized BLEU score on WMT14 English-German translation task. Train: time in seconds per training batch measured from 0.2k training steps on Tesla P100. Decode: time in milliseconds used to decode one sentence measured on newstest2014 dataset.",
        "table_column_names": [
            "Model",
            "#Params",
            "BLEU",
            "Train",
            "Decode"
        ],
        "table_content_values": [
            [
                "GNMT",
                "-",
                "24.61",
                "-",
                "-"
            ],
            [
                "GRU",
                "206M",
                "26.28",
                "2.67",
                "45.35"
            ],
            [
                "ATR",
                "122M",
                "25.70",
                "1.33",
                "[BOLD] 34.40"
            ],
            [
                "SRU",
                "170M",
                "25.91",
                "1.34",
                "42.84"
            ],
            [
                "LRN",
                "143M",
                "26.26",
                "[BOLD] 0.99",
                "36.50"
            ],
            [
                "oLRN",
                "164M",
                "[BOLD] 26.73",
                "1.15",
                "40.19"
            ]
        ],
        "question": "Is it true that In addition, the training time results in Table 3 confirm the computational advantage of LRN over all other recurrent units, where LRN speeds up over ATR and SRU by approximately 25%?",
        "answer_label": "yes"
    },
    {
        "id": "3caabd87-48f9-4343-b91c-d8b677797b0e",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that A complementary behavior can be observed for H-CBOW, whose scores on Word Content are decreased?",
        "answer_label": "no"
    },
    {
        "id": "c9a135e8-f977-46e0-a2cb-10737ce5245c",
        "table_caption": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 2: Precisions on the Wikidata dataset.",
        "table_column_names": [
            "Recall",
            "0.1",
            "0.2",
            "0.3",
            "AUC"
        ],
        "table_content_values": [
            [
                "Rank+ExATT",
                "0.584",
                "0.535",
                "0.487",
                "0.392"
            ],
            [
                "PCNN+ATT (m)",
                "0.365",
                "0.317",
                "0.213",
                "0.204"
            ],
            [
                "PCNN+ATT (1)",
                "0.665",
                "0.517",
                "0.413",
                "0.396"
            ],
            [
                "Our Model",
                "0.650",
                "0.519",
                "0.422",
                "[BOLD] 0.405"
            ]
        ],
        "question": "Is it true that We show the precision numbers for some particular recalls as well as the AUC in Table 2, where PCNN+ATT (1) refers to train sentences with two entities and one relation label, PCNN+ATT (m) refers to train sentences with four entities7 and two relation labels?",
        "answer_label": "yes"
    },
    {
        "id": "92d74cff-f9f2-4fb1-adfb-4a10df43de9a",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that In contrast, our DCGCN models can be trained using a large number of layers?",
        "answer_label": "yes"
    },
    {
        "id": "29dedc61-018c-4f4a-985b-5998ec32dbe4",
        "table_caption": "Ontology-Aware Token Embeddings for Prepositional Phrase Attachment Table 3: Effect of removing sense priors and context sensitivity (attention) from the model.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] PPA Acc."
        ],
        "table_content_values": [
            [
                "full",
                "89.7"
            ],
            [
                "- sense priors",
                "88.4"
            ],
            [
                "- attention",
                "87.5"
            ]
        ],
        "question": "Is it true that The second row in Table 3 shows the test accuracy of a system trained without sense priors and the third row shows that removing attention from the model actually improved the accuracy, suggesting that context sensitivity is not necessary for good performance?",
        "answer_label": "no"
    },
    {
        "id": "38158ec1-ed12-4527-92c5-00f4c3f9312c",
        "table_caption": "Adversarial Removal of Demographic Attributes from Text Data Table 3: Performances on different datasets with an adversarial training. \u0394 is the difference between the attacker score and the corresponding adversary\u2019s accuracy.",
        "table_column_names": [
            "Data",
            "Task",
            "Protected Attribute",
            "Task Acc",
            "Leakage",
            "\u0394"
        ],
        "table_content_values": [
            [
                "Dial",
                "Sentiment",
                "Race",
                "64.7",
                "56.0",
                "5.0"
            ],
            [
                "[EMPTY]",
                "Mention",
                "Race",
                "81.5",
                "63.1",
                "9.2"
            ],
            [
                "PAN16",
                "Mention",
                "Gender",
                "75.6",
                "58.5",
                "8.0"
            ],
            [
                "[EMPTY]",
                "Mention",
                "Age",
                "72.5",
                "57.3",
                "6.9"
            ]
        ],
        "question": "Is it true that In all cases, the adversarial's success rate is around 50%, while the attacker's rate is substantially higher?",
        "answer_label": "yes"
    },
    {
        "id": "ef361478-7869-46cf-b2dd-3c136f8ec77a",
        "table_caption": "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks Table 5: POS and SEM tagging accuracy with features from different layers of 4-layer Uni/Bidirectional/Residual NMT encoders, averaged over all non-English target languages.",
        "table_column_names": [
            "Uni",
            "POS",
            "0 87.9",
            "1 92.0",
            "2 91.7",
            "3 91.8",
            "4 91.9"
        ],
        "table_content_values": [
            [
                "Uni",
                "SEM",
                "81.8",
                "87.8",
                "87.4",
                "87.6",
                "88.2"
            ],
            [
                "Bi",
                "POS",
                "87.9",
                "93.3",
                "92.9",
                "93.2",
                "92.8"
            ],
            [
                "Bi",
                "SEM",
                "81.9",
                "91.3",
                "90.8",
                "91.9",
                "91.9"
            ],
            [
                "Res",
                "POS",
                "87.9",
                "92.5",
                "91.9",
                "92.0",
                "92.4"
            ],
            [
                "Res",
                "SEM",
                "81.9",
                "88.2",
                "87.5",
                "87.6",
                "88.5"
            ]
        ],
        "question": "Is it true that We observe that POS tagging does benefit from features from the upper layers, while SEM tagging does not improve with layer 4 representations?",
        "answer_label": "no"
    },
    {
        "id": "ee25e11d-4b14-4114-8977-22a22aa4b799",
        "table_caption": "Sparse and Structured Visual Attention Table 2: Human evaluation results on MSCOCO.",
        "table_column_names": [
            "[EMPTY]",
            "caption",
            "attention relevance"
        ],
        "table_content_values": [
            [
                "softmax",
                "3.50",
                "3.38"
            ],
            [
                "sparsemax",
                "3.71",
                "3.89"
            ],
            [
                "TVmax",
                "[BOLD] 3.87",
                "[BOLD] 4.10"
            ]
        ],
        "question": "Is it true that The superior score on attention relevance shows that TVMAX is better at selecting the relevant features and its output is more interpretable?",
        "answer_label": "yes"
    },
    {
        "id": "72ae36b8-bb32-4a0d-9f55-10b95d234b1c",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 6: Comparisons with baselines. +RC denotes GCNs with residual connections. +RC+LA refers to GCNs with both residual connections and layer aggregations. DCGCNi represents our model with i blocks, containing i\u00d7(n+m) layers. The number of layers for each model is shown in parenthesis.",
        "table_column_names": [
            "[BOLD] GCN +RC (2)",
            "B 16.8",
            "C 48.1",
            "[BOLD] GCN +RC+LA (2)",
            "B 18.3",
            "C 47.9"
        ],
        "table_content_values": [
            [
                "+RC (4)",
                "18.4",
                "49.6",
                "+RC+LA (4)",
                "18.0",
                "51.1"
            ],
            [
                "+RC (6)",
                "19.9",
                "49.7",
                "+RC+LA (6)",
                "21.3",
                "50.8"
            ],
            [
                "+RC (9)",
                "[BOLD] 21.1",
                "50.5",
                "+RC+LA (9)",
                "[BOLD] 22.0",
                "52.6"
            ],
            [
                "+RC (10)",
                "20.7",
                "[BOLD] 50.7",
                "+RC+LA (10)",
                "21.2",
                "[BOLD] 52.9"
            ],
            [
                "DCGCN1 (9)",
                "22.9",
                "53.0",
                "DCGCN3 (27)",
                "24.8",
                "54.7"
            ],
            [
                "DCGCN2 (18)",
                "24.2",
                "54.4",
                "DCGCN4 (36)",
                "[BOLD] 25.5",
                "[BOLD] 55.4"
            ]
        ],
        "question": "Is it true that For example, DCGCN4 contains 36 layers and has the lowest performance on both datasets?",
        "answer_label": "no"
    },
    {
        "id": "68a6fee8-086e-407d-9604-300988692905",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 4: Results of manual error analysis of TGen on a sample of 100 instances from the original test set: total absolute numbers of errors we found (added, missed, wrong values, slight disfluencies).",
        "table_column_names": [
            "[BOLD] Training data",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] Disfl"
        ],
        "table_content_values": [
            [
                "Original",
                "0",
                "22",
                "0",
                "14"
            ],
            [
                "Cleaned added",
                "0",
                "23",
                "0",
                "14"
            ],
            [
                "Cleaned missing",
                "0",
                "1",
                "0",
                "2"
            ],
            [
                "Cleaned",
                "0",
                "0",
                "0",
                "5"
            ]
        ],
        "question": "Is it true that The systems trained on the original data or with cleaned added slots perform better in terms of both semantic accuracy and fluency?",
        "answer_label": "no"
    },
    {
        "id": "b0a81f4a-88ce-4478-bbbc-c06f440ff7db",
        "table_caption": "Sparse and Structured Visual Attention Table 3: Automatic evaluation of VQA on VQA-2.0. Sparse-TVmax and soft-TVmax correspond to using sparsemax or softmax on the image self-attention and TVmax on the output attention. Other models use softmax or sparsemax on self-attention and output attention.",
        "table_column_names": [
            "[EMPTY]",
            "Att. to image",
            "Att. to bounding boxes",
            "Test-Dev Yes/No",
            "Test-Dev Number",
            "Test-Dev Other",
            "Test-Dev Overall",
            "Test-Standard Yes/No",
            "Test-Standard Number",
            "Test-Standard Other",
            "Test-Standard Overall"
        ],
        "table_content_values": [
            [
                "softmax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "42.65",
                "55.74",
                "65.52",
                "83.55",
                "42.68",
                "56.01",
                "65.97"
            ],
            [
                "sparsemax",
                "\u2713",
                "[EMPTY]",
                "83.08",
                "43.19",
                "55.79",
                "65.60",
                "83.33",
                "42.99",
                "56.06",
                "65.94"
            ],
            [
                "soft-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.13",
                "43.53",
                "56.01",
                "65.76",
                "83.63",
                "43.24",
                "56.10",
                "66.11"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "[EMPTY]",
                "83.10",
                "43.30",
                "56.14",
                "65.79",
                "83.66",
                "43.18",
                "56.21",
                "66.17"
            ],
            [
                "softmax",
                "[EMPTY]",
                "\u2713",
                "85.14",
                "49.59",
                "58.72",
                "68.57",
                "85.56",
                "49.54",
                "59.11",
                "69.04"
            ],
            [
                "sparsemax",
                "[EMPTY]",
                "\u2713",
                "[BOLD] 85.40",
                "[BOLD] 50.87",
                "58.67",
                "68.79",
                "[BOLD] 85.80",
                "50.18",
                "59.08",
                "69.19"
            ],
            [
                "softmax",
                "\u2713",
                "\u2713",
                "85.33",
                "50.49",
                "58.88",
                "68.82",
                "85.58",
                "50.42",
                "59.18",
                "69.17"
            ],
            [
                "sparse-TVmax",
                "\u2713",
                "\u2713",
                "85.35",
                "50.52",
                "[BOLD] 59.15",
                "[BOLD] 68.96",
                "85.72",
                "[BOLD] 50.66",
                "[BOLD] 59.22",
                "[BOLD] 69.28"
            ]
        ],
        "question": "Is it true that Additionally, when using bounding box features, softmax outperforms sparsemax, showing that selecting only the bounding boxes of the relevant objects does not lead to a better answering capability?",
        "answer_label": "no"
    },
    {
        "id": "4a2c8295-40a1-49a0-9ac3-4d2275c74349",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 2: BLEU and METEOR scores on the test set of LDC2015E86 and LDC2017T10 datasets.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>BLEU</bold>",
            "<bold>METEOR</bold>"
        ],
        "table_content_values": [
            [
                "LDC2015E86",
                "LDC2015E86",
                "LDC2015E86"
            ],
            [
                "Konstas et al. (2017)",
                "22.00",
                "-"
            ],
            [
                "Song et al. (2018)",
                "23.28",
                "30.10"
            ],
            [
                "Cao et al. (2019)",
                "23.50",
                "-"
            ],
            [
                "Damonte et al.(2019)",
                "24.40",
                "23.60"
            ],
            [
                "Guo et al. (2019)",
                "<bold>25.70</bold>",
                "-"
            ],
            [
                "S2S",
                "22.55 \u00b1 0.17",
                "29.90 \u00b1 0.31"
            ],
            [
                "G2S-GIN",
                "22.93 \u00b1 0.20",
                "29.72 \u00b1 0.09"
            ],
            [
                "G2S-GAT",
                "23.42 \u00b1 0.16",
                "29.87 \u00b1 0.14"
            ],
            [
                "G2S-GGNN",
                "24.32 \u00b1 0.16",
                "<bold>30.53</bold> \u00b1 0.30"
            ],
            [
                "LDC2017T10",
                "LDC2017T10",
                "LDC2017T10"
            ],
            [
                "Back et al. (2018)",
                "23.30",
                "-"
            ],
            [
                "Song et al. (2018)",
                "24.86",
                "31.56"
            ],
            [
                "Damonte et al.(2019)",
                "24.54",
                "24.07"
            ],
            [
                "Cao et al. (2019)",
                "26.80",
                "-"
            ],
            [
                "Guo et al. (2019)",
                "27.60",
                "-"
            ],
            [
                "S2S",
                "22.73 \u00b1 0.18",
                "30.15 \u00b1 0.14"
            ],
            [
                "G2S-GIN",
                "26.90 \u00b1 0.19",
                "32.62 \u00b1 0.04"
            ],
            [
                "G2S-GAT",
                "26.72 \u00b1 0.20",
                "32.52 \u00b1 0.02"
            ],
            [
                "G2S-GGNN",
                "<bold>27.87</bold> \u00b1 0.15",
                "<bold>33.21</bold> \u00b1 0.15"
            ]
        ],
        "question": "Is it true that This indicates that our architecture can learn to generate better signals for text generation?",
        "answer_label": "yes"
    },
    {
        "id": "33575f0c-0627-4e2e-b5a6-668a9f447a3a",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 4: Main results on English-German and English-Czech datasets.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Type",
            "[BOLD] English-German #P",
            "[BOLD] English-German B",
            "[BOLD] English-German C",
            "[BOLD] English-Czech #P",
            "[BOLD] English-Czech B",
            "[BOLD] English-Czech C"
        ],
        "table_content_values": [
            [
                "BoW+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "12.2",
                "-",
                "-",
                "7.5",
                "-"
            ],
            [
                "CNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "13.7",
                "-",
                "-",
                "8.7",
                "-"
            ],
            [
                "BiRNN+GCN (Bastings et al.,  2017 )",
                "Single",
                "-",
                "16.1",
                "-",
                "-",
                "9.6",
                "-"
            ],
            [
                "PB-SMT (Beck et al.,  2018 )",
                "Single",
                "-",
                "12.8",
                "43.2",
                "-",
                "8.6",
                "36.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Single",
                "41.4M",
                "15.5",
                "40.8",
                "39.1M",
                "8.9",
                "33.8"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Single",
                "41.2M",
                "16.7",
                "42.4",
                "38.8M",
                "9.8",
                "33.3"
            ],
            [
                "DCGCN (ours)",
                "Single",
                "[BOLD]  29.7M",
                "[BOLD] 19.0",
                "[BOLD] 44.1",
                "[BOLD]  28.3M",
                "[BOLD] 12.1",
                "[BOLD] 37.1"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "Ensemble",
                "207M",
                "19.0",
                "44.1",
                "195M",
                "11.3",
                "36.4"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "Ensemble",
                "206M",
                "19.6",
                "45.1",
                "194M",
                "11.7",
                "35.9"
            ],
            [
                "DCGCN (ours)",
                "Ensemble",
                "[BOLD]  149M",
                "[BOLD] 20.5",
                "[BOLD] 45.8",
                "[BOLD]  142M",
                "[BOLD] 13.1",
                "[BOLD] 37.8"
            ]
        ],
        "question": "Is it true that In addition, our single model is comparable to the ensemble results of Seq2SeqB and GGNN2Seq, while the number of parameters of our models is only about 1/6 of theirs?",
        "answer_label": "yes"
    },
    {
        "id": "ba0b99c4-316c-4a92-b84e-5f072c99b79e",
        "table_caption": "Solving Hard Coreference Problems Table 10: Ablation Study of Knowledge Schemas on WinoCoref. The first line specifies the preformance for KnowComb with only Type 1 schema knowledge tested on all data while the third line specifies the preformance using the same model but tested on Cat1 data. The second line specifies the preformance results for KnowComb system with only Type 2 schema knowledge on all data while the fourth line specifies the preformance using the same model but tested on Cat2 data.",
        "table_column_names": [
            "Schema",
            "AntePre(Test)",
            "AntePre(Train)"
        ],
        "table_content_values": [
            [
                "Type 1",
                "76.67",
                "86.79"
            ],
            [
                "Type 2",
                "79.55",
                "88.86"
            ],
            [
                "Type 1 (Cat1)",
                "90.26",
                "93.64"
            ],
            [
                "Type 2 (Cat2)",
                "83.38",
                "92.49"
            ]
        ],
        "question": "Is it true that They showthat both Type 1 and Type 2 schema knowledge havehigher precision on Category 1 and Category 2 datainstances, respectively, compared to that on full data?",
        "answer_label": "yes"
    },
    {
        "id": "58155cc4-23ec-4f64-b7ec-568af0621eaa",
        "table_caption": "Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks Table 1: Results of thread summarization. \u2018HAN\u2019 models are our proposed approaches adapted from the hierarchical attention networks\u00a0[Yang et al.2016]. The models can be pretrained using unlabeled threads from TripAdvisor (\u2018T\u2019) and Ubuntuforum (\u2018U\u2019). r indicates a redundancy removal step is applied. We report the variance of F-scores across all threads (\u2018\u00b1\u2019). A redundancy removal step improves recall scores (shown in gray) of the HAN models and boosts performance.",
        "table_column_names": [
            "[BOLD] System",
            "[BOLD] ROUGE-1  [BOLD] R (%)",
            "[BOLD] ROUGE-1  [BOLD] P (%)",
            "[BOLD] ROUGE-1  [BOLD] F (%)",
            "[BOLD] ROUGE-2  [BOLD] R (%)",
            "[BOLD] ROUGE-2  [BOLD] P (%)",
            "[BOLD] ROUGE-2  [BOLD] F (%)",
            "[BOLD] Sentence-Level  [BOLD] R (%)",
            "[BOLD] Sentence-Level  [BOLD] P (%)",
            "[BOLD] Sentence-Level  [BOLD] F (%)"
        ],
        "table_content_values": [
            [
                "[BOLD] ILP",
                "24.5",
                "41.1",
                "29.3\u00b10.5",
                "7.9",
                "15.0",
                "9.9\u00b10.5",
                "13.6",
                "22.6",
                "15.6\u00b10.4"
            ],
            [
                "[BOLD] Sum-Basic",
                "28.4",
                "44.4",
                "33.1\u00b10.5",
                "8.5",
                "15.6",
                "10.4\u00b10.4",
                "14.7",
                "22.9",
                "16.7\u00b10.5"
            ],
            [
                "[BOLD] KL-Sum",
                "39.5",
                "34.6",
                "35.5\u00b10.5",
                "13.0",
                "12.7",
                "12.3\u00b10.5",
                "15.2",
                "21.1",
                "16.3\u00b10.5"
            ],
            [
                "[BOLD] LexRank",
                "42.1",
                "39.5",
                "38.7\u00b10.5",
                "14.7",
                "15.3",
                "14.2\u00b10.5",
                "14.3",
                "21.5",
                "16.0\u00b10.5"
            ],
            [
                "[BOLD] MEAD",
                "45.5",
                "36.5",
                "38.5\u00b1 0.5",
                "17.9",
                "14.9",
                "15.4\u00b10.5",
                "27.8",
                "29.2",
                "26.8\u00b10.5"
            ],
            [
                "[BOLD] SVM",
                "19.0",
                "48.8",
                "24.7\u00b10.8",
                "7.5",
                "21.1",
                "10.0\u00b10.5",
                "32.7",
                "34.3",
                "31.4\u00b10.4"
            ],
            [
                "[BOLD] LogReg",
                "26.9",
                "34.5",
                "28.7\u00b10.6",
                "6.4",
                "9.9",
                "7.3\u00b10.4",
                "12.2",
                "14.9",
                "12.7\u00b10.5"
            ],
            [
                "[BOLD] LogReg [ITALIC] r",
                "28.0",
                "34.8",
                "29.4\u00b10.6",
                "6.9",
                "10.4",
                "7.8\u00b10.4",
                "12.1",
                "14.5",
                "12.5\u00b10.5"
            ],
            [
                "[BOLD] HAN",
                "31.0",
                "42.8",
                "33.7\u00b10.7",
                "11.2",
                "17.8",
                "12.7\u00b10.5",
                "26.9",
                "34.1",
                "32.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT",
                "32.2",
                "42.4",
                "34.4\u00b10.7",
                "11.5",
                "17.5",
                "12.9\u00b10.5",
                "29.6",
                "35.8",
                "32.2\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU",
                "32.1",
                "42.1",
                "33.8\u00b10.7",
                "11.6",
                "17.6",
                "12.9\u00b10.5",
                "30.1",
                "35.6",
                "32.3\u00b10.5"
            ],
            [
                "[BOLD] HAN [ITALIC] r",
                "38.1",
                "40.5",
                "[BOLD] 37.8\u00b10.5",
                "14.0",
                "17.1",
                "[BOLD] 14.7\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainT [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.5",
                "16.8",
                "[BOLD] 14.4\u00b10.5",
                "32.5",
                "34.4",
                "[BOLD] 33.4\u00b10.5"
            ],
            [
                "[BOLD] HAN+pretrainU [ITALIC] r",
                "37.9",
                "40.4",
                "[BOLD] 37.6\u00b10.5",
                "13.6",
                "16.9",
                "[BOLD] 14.4\u00b10.5",
                "33.9",
                "33.8",
                "[BOLD] 33.8\u00b10.5"
            ]
        ],
        "question": "Is it true that [CONTINUE] We observe that the redundancy removal step is crucial for the HAN models to achieve outstanding results?",
        "answer_label": "yes"
    },
    {
        "id": "95543831-0ee0-4bea-a141-061abbf717e9",
        "table_caption": "Suggestion Mining from Online Reviews using ULMFiT Table 1: Dataset Distribution for Sub Task A - Task 9: Suggestion Mining from Online Reviews.",
        "table_column_names": [
            "[BOLD] Label",
            "[BOLD] Train",
            "[BOLD] Trial"
        ],
        "table_content_values": [
            [
                "[BOLD] Suggestion",
                "2085",
                "296"
            ],
            [
                "[BOLD] Non Suggestion",
                "6415",
                "296"
            ]
        ],
        "question": "Is it true that As evident from Table 1, there is a significant imbalance in the distribution of training instances that are suggestions and non-suggestions, 2https://www.uservoice.com/ [CONTINUE] For Sub Task A, the organizers shared a training and a validation dataset whose label distribution (suggestion or a non-suggestion) is presented in Table 1?",
        "answer_label": "yes"
    },
    {
        "id": "d84d4003-6890-4170-b3d7-c47ee7e05cf6",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 6: Metrics for taxonomies generated by models using the top 1,000 terms of each corpus in English.",
        "table_column_names": [
            "Corpus",
            "Metric",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "Europarl",
                "TotalTerms:",
                "957",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "836",
                "1,000"
            ],
            [
                "Europarl",
                "TotalRoots:",
                "44",
                "1",
                "1",
                "1",
                "1",
                "43",
                "1"
            ],
            [
                "Europarl",
                "NumberRels:",
                "1,588",
                "1,025",
                "1,028",
                "1,185",
                "1,103",
                "1,184",
                "999"
            ],
            [
                "Europarl",
                "MaxDepth:",
                "21",
                "921",
                "901",
                "788",
                "835",
                "8",
                "15"
            ],
            [
                "Europarl",
                "MinDepth:",
                "1",
                "921",
                "901",
                "788",
                "835",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgDepth:",
                "11.82",
                "921",
                "901",
                "788",
                "835",
                "3.05",
                "8.46"
            ],
            [
                "Europarl",
                "DepthCohesion:",
                "1.78",
                "1",
                "1",
                "1",
                "1",
                "2.62",
                "1.77"
            ],
            [
                "Europarl",
                "MaxWidth:",
                "20",
                "2",
                "3",
                "4",
                "3",
                "88",
                "41"
            ],
            [
                "Europarl",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "Europarl",
                "AvgWidth:",
                "1.99",
                "1.03",
                "1.03",
                "1.19",
                "1.10",
                "4.20",
                "2.38"
            ],
            [
                "TED Talks",
                "TotalTerms:",
                "476",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000",
                "1,000"
            ],
            [
                "TED Talks",
                "TotalRoots:",
                "164",
                "2",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "NumberRels:",
                "521",
                "1,029",
                "1,331",
                "3,025",
                "3,438",
                "3,802",
                "1,009"
            ],
            [
                "TED Talks",
                "MaxDepth:",
                "16",
                "915",
                "658",
                "454",
                "395",
                "118",
                "12"
            ],
            [
                "TED Talks",
                "MinDepth:",
                "1",
                "913",
                "658",
                "454",
                "395",
                "110",
                "1"
            ],
            [
                "TED Talks",
                "AvgDepth:",
                "5.82",
                "914",
                "658",
                "454",
                "395",
                "112.24",
                "5.95"
            ],
            [
                "TED Talks",
                "DepthCohesion:",
                "2.75",
                "1",
                "1",
                "1",
                "1",
                "1.05",
                "2.02"
            ],
            [
                "TED Talks",
                "MaxWidth:",
                "25",
                "2",
                "77",
                "13",
                "12",
                "66",
                "98"
            ],
            [
                "TED Talks",
                "MinWidth:",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1",
                "1"
            ],
            [
                "TED Talks",
                "AvgWidth:",
                "1.83",
                "1.03",
                "1.36",
                "3.03",
                "3.44",
                "6.64",
                "2.35"
            ]
        ],
        "question": "Is it true that For example, using relations generated by TF model using the Europarl corpus, we can understand the MaxDepth as having 788 terms with different values of term frequency, while having only 1 that share the same value of term frequency with other terms?",
        "answer_label": "no"
    },
    {
        "id": "fb51ce8e-cdc5-4c9d-8e17-c2043abea92e",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 2: Performance comparison of different models on the two datasets. * denotes a statistically significant improvement over the previous best state-of-the-art model with p<0.01 under the bootstrap paired t-test. \u2020 denotes the previous best state-of-the-art model.",
        "table_column_names": [
            "Model",
            "NYT10 Prec.",
            "NYT10 Rec.",
            "NYT10 F1",
            "NYT11 Prec.",
            "NYT11 Rec.",
            "NYT11 F1"
        ],
        "table_content_values": [
            [
                "CNN zeng2014relation",
                "0.413",
                "0.591",
                "0.486",
                "0.444",
                "0.625",
                "0.519"
            ],
            [
                "PCNN zeng2015distant",
                "0.380",
                "[BOLD] 0.642",
                "0.477",
                "0.446",
                "0.679",
                "0.538\u2020"
            ],
            [
                "EA huang2016attention",
                "0.443",
                "0.638",
                "0.523\u2020",
                "0.419",
                "0.677",
                "0.517"
            ],
            [
                "BGWA jat2018attention",
                "0.364",
                "0.632",
                "0.462",
                "0.417",
                "[BOLD] 0.692",
                "0.521"
            ],
            [
                "BiLSTM-CNN",
                "0.490",
                "0.507",
                "0.498",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "Our model",
                "[BOLD] 0.541",
                "0.595",
                "[BOLD] 0.566*",
                "[BOLD] 0.507",
                "0.652",
                "[BOLD] 0.571*"
            ]
        ],
        "question": "Is it true that Our model improves the precision scores on both datasets with good recall scores?",
        "answer_label": "yes"
    },
    {
        "id": "7a54fd92-fef9-4f12-8e39-7da48cdc52eb",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 3: Precision, recall and F-measure for methods using the top 1,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1173",
                "0.0366",
                "0.0503",
                "0.0554",
                "0.0548",
                "0.0443",
                "0.0761"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1125",
                "0.0301",
                "0.0382",
                "0.0425",
                "0.0441",
                "0.0710",
                "0.0664"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5163",
                "0.3330",
                "0.5257",
                "0.6109",
                "0.5984",
                "[BOLD] 0.7311",
                "0.5676"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.5387",
                "0.2907",
                "0.5300",
                "0.6117",
                "0.6159",
                "[BOLD] 0.6533",
                "0.5656"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0396",
                "0.3999",
                "0.5499",
                "[BOLD] 0.6045",
                "0.5887",
                "0.0023",
                "0.0017"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0018",
                "0.4442",
                "0.5377",
                "0.5657",
                "[BOLD] 0.6077",
                "0.2666",
                "0.0019"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0111",
                "0.3554",
                "0.5795",
                "[BOLD] 0.6727",
                "0.5184",
                "0.0053",
                "0.0012"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0004",
                "0.3142",
                "0.5484",
                "[BOLD] 0.6877",
                "0.5515",
                "0.4706",
                "0.0011"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0591",
                "0.0671",
                "0.0922",
                "[BOLD] 0.1015",
                "0.1003",
                "0.0044",
                "0.0033"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0035",
                "0.0564",
                "0.0713",
                "0.0791",
                "0.0822",
                "[BOLD] 0.1121",
                "0.0037"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0217",
                "0.3438",
                "0.5513",
                "[BOLD] 0.6403",
                "0.5555",
                "0.0105",
                "0.0024"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "0.0008",
                "0.3020",
                "0.5390",
                "[BOLD] 0.6475",
                "0.5819",
                "0.5471",
                "0.0022"
            ]
        ],
        "question": "Is it true that As we can observe in Table 3, Patt has the best values of precision for the English corpora while SLQS has the best values for the Portuguese corpora?",
        "answer_label": "no"
    },
    {
        "id": "a1761e4d-eabf-472c-a9bf-311759005b27",
        "table_caption": "Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.",
        "table_column_names": [
            "[EMPTY]",
            "DUC\u201901 <italic>R</italic>1",
            "DUC\u201901 <italic>R</italic>2",
            "DUC\u201902 <italic>R</italic>1",
            "DUC\u201902 <italic>R</italic>2",
            "DUC\u201904 <italic>R</italic>1",
            "DUC\u201904 <italic>R</italic>2"
        ],
        "table_content_values": [
            [
                "ICSI",
                "33.31",
                "7.33",
                "35.04",
                "8.51",
                "37.31",
                "9.36"
            ],
            [
                "PriorSum",
                "35.98",
                "7.89",
                "36.63",
                "8.97",
                "38.91",
                "10.07"
            ],
            [
                "TCSum",
                "<bold>36.45</bold>",
                "7.66",
                "36.90",
                "8.61",
                "38.27",
                "9.66"
            ],
            [
                "TCSum\u2212",
                "33.45",
                "6.07",
                "34.02",
                "7.39",
                "35.66",
                "8.66"
            ],
            [
                "SRSum",
                "36.04",
                "8.44",
                "<bold>38.93</bold>",
                "<bold>10.29</bold>",
                "39.29",
                "10.70"
            ],
            [
                "DeepTD",
                "28.74",
                "5.95",
                "31.63",
                "7.09",
                "33.57",
                "7.96"
            ],
            [
                "REAPER",
                "32.43",
                "6.84",
                "35.03",
                "8.11",
                "37.22",
                "8.64"
            ],
            [
                "RELIS",
                "34.73",
                "<bold>8.66</bold>",
                "37.11",
                "9.12",
                "<bold>39.34</bold>",
                "<bold>10.73</bold>"
            ]
        ],
        "question": "Is it true that RELIS does not significantly outperform the other RL-based systems?",
        "answer_label": "no"
    },
    {
        "id": "fc367a7a-61f9-4d01-80cb-71f58d303c05",
        "table_caption": "Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model Mrksic et al. (2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ Budzianowski et al. (2018).",
        "table_column_names": [
            "[BOLD] DST Models",
            "[BOLD] Joint Acc. WoZ 2.0",
            "[BOLD] Joint Acc. MultiWoZ",
            "[BOLD] ITC"
        ],
        "table_content_values": [
            [
                "Baselines Mrksic et al. ( 2017 )",
                "70.8%",
                "25.83%",
                "[ITALIC] O( [ITALIC] mn)"
            ],
            [
                "NBT-CNN Mrksic et al. ( 2017 )",
                "84.2%",
                "-",
                "[ITALIC] O( [ITALIC] mn)"
            ],
            [
                "StateNet_PSI Ren et al. ( 2018 )",
                "[BOLD] 88.9%",
                "-",
                "[ITALIC] O( [ITALIC] n)"
            ],
            [
                "GLAD Nouri and Hosseini-Asl ( 2018 )",
                "88.5%",
                "35.58%",
                "[ITALIC] O( [ITALIC] mn)"
            ],
            [
                "HyST (ensemble) Goel et al. ( 2019 )",
                "-",
                "44.22%",
                "[ITALIC] O( [ITALIC] n)"
            ],
            [
                "DSTRead (ensemble) Gao et al. ( 2019 )",
                "-",
                "42.12%",
                "[ITALIC] O( [ITALIC] n)"
            ],
            [
                "TRADE Wu et al. ( 2019 )",
                "-",
                "48.62%",
                "[ITALIC] O( [ITALIC] n)"
            ],
            [
                "COMER",
                "88.6%",
                "[BOLD] 48.79%",
                "[ITALIC] O(1)"
            ]
        ],
        "question": "Is it true that On the muli-domain dataset, MultiWoZ, our model achieves a joint goal accuracy of 48.79%, which marginally outperforms the previous state-of-the-art?",
        "answer_label": "yes"
    },
    {
        "id": "495a65cc-6f0c-4b6a-b19c-a3bd11784062",
        "table_caption": "Evaluating the Complementarity of Taxonomic Relation Extraction Methods Across Different Languages Table 4: Precision, recall and F-measure for methods using the top 10,000 words with the highest number of contexts.",
        "table_column_names": [
            "[EMPTY]",
            "Lang",
            "Corpus",
            "Patt",
            "DSim",
            "SLQS",
            "TF",
            "DF",
            "DocSub",
            "HClust"
        ],
        "table_content_values": [
            [
                "P",
                "EN",
                "Europarl",
                "[BOLD] 0.1192",
                "0.0083",
                "0.0137",
                "0.0150",
                "0.0150",
                "0.0445",
                "0.0326"
            ],
            [
                "P",
                "EN",
                "Ted Talks",
                "[BOLD] 0.1022",
                "0.0069",
                "0.0060",
                "0.0092",
                "0.0090",
                "0.0356",
                "0.0162"
            ],
            [
                "P",
                "PT",
                "Europarl",
                "0.5710",
                "0.1948",
                "0.3855",
                "0.5474",
                "0.4485",
                "[BOLD] 0.8052",
                "0.4058"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "[BOLD] 0.6304",
                "0.1870",
                "0.3250",
                "0.5312",
                "0.4576",
                "0.6064",
                "0.3698"
            ],
            [
                "R",
                "EN",
                "Europarl",
                "0.0037",
                "0.3278",
                "0.5941",
                "0.6486",
                "[BOLD] 0.6490",
                "0.0017",
                "0.0003"
            ],
            [
                "R",
                "EN",
                "Ted Talks",
                "0.0002",
                "0.1486",
                "0.4332",
                "[BOLD] 0.6467",
                "0.6332",
                "0.0967",
                "0.0003"
            ],
            [
                "R",
                "PT",
                "Europarl",
                "0.0002",
                "0.1562",
                "0.5157",
                "[BOLD] 0.7255",
                "0.5932",
                "0.0032",
                "0.0001"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "2.10-5",
                "0.0507",
                "0.4492",
                "[BOLD] 0.7000",
                "0.5887",
                "0.1390",
                "0.0002"
            ],
            [
                "F",
                "EN",
                "Europarl",
                "0.0073",
                "0.0162",
                "0.0268",
                "[BOLD] 0.0293",
                "[BOLD] 0.0293",
                "0.0033",
                "0.0006"
            ],
            [
                "F",
                "EN",
                "Ted Talks",
                "0.0004",
                "0.0132",
                "0.0118",
                "0.0181",
                "0.0179",
                "[BOLD] 0.0520",
                "0.0005"
            ],
            [
                "F",
                "PT",
                "Europarl",
                "0.0005",
                "0.1733",
                "0.4412",
                "[BOLD] 0.6240",
                "0.5109",
                "0.0064",
                "0.0002"
            ],
            [
                "[EMPTY]",
                "PT",
                "Ted Talks",
                "4.10-5",
                "0.0798",
                "0.3771",
                "[BOLD] 0.6040",
                "0.5149",
                "0.2261",
                "0.0004"
            ]
        ],
        "question": "Is it true that [CONTINUE] The lowest values of precision are achieved by DSim model, and the lowest recalls are obtained by HClust and Patt models?",
        "answer_label": "yes"
    },
    {
        "id": "9f1a619e-4ad3-401c-8c76-b44d8a33ef89",
        "table_caption": "Semantic Neural Machine Translation using AMR Table 3: Test performance. NC-v11 represents training only with the NC-v11 data, while Full means using the full training data. * represents significant Koehn (2004) result (p<0.01) over Seq2seq. \u2193 indicates the lower the better.",
        "table_column_names": [
            "System",
            "NC-v11 BLEU",
            "NC-v11 TER\u2193",
            "NC-v11 Meteor",
            "Full BLEU",
            "Full TER\u2193",
            "Full Meteor"
        ],
        "table_content_values": [
            [
                "OpenNMT-tf",
                "15.1",
                "0.6902",
                "0.3040",
                "24.3",
                "0.5567",
                "0.4225"
            ],
            [
                "Transformer-tf",
                "17.1",
                "0.6647",
                "0.3578",
                "25.1",
                "0.5537",
                "0.4344"
            ],
            [
                "Seq2seq",
                "16.0",
                "0.6695",
                "0.3379",
                "23.7",
                "0.5590",
                "0.4258"
            ],
            [
                "Dual2seq-LinAMR",
                "17.3",
                "0.6530",
                "0.3612",
                "24.0",
                "0.5643",
                "0.4246"
            ],
            [
                "Duel2seq-SRL",
                "17.2",
                "0.6591",
                "0.3644",
                "23.8",
                "0.5626",
                "0.4223"
            ],
            [
                "Dual2seq-Dep",
                "17.8",
                "0.6516",
                "0.3673",
                "25.0",
                "0.5538",
                "0.4328"
            ],
            [
                "Dual2seq",
                "[BOLD] *19.2*",
                "[BOLD] 0.6305",
                "[BOLD] 0.3840",
                "[BOLD] *25.5*",
                "[BOLD] 0.5480",
                "[BOLD] 0.4376"
            ]
        ],
        "question": "Is it true that Dual2seq-LinAMR shows much worse performance than our model and only slightly outperforms the Seq2seq baseline?",
        "answer_label": "yes"
    },
    {
        "id": "8f2d3f7f-1558-4d27-a479-4d02c594336f",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Method",
            "SUBJ",
            "CR",
            "MR",
            "MPQA",
            "MRPC",
            "TREC",
            "SICK-E",
            "SST2",
            "SST5",
            "STS-B",
            "SICK-R"
        ],
        "table_content_values": [
            [
                "CBOW/784",
                "90.0",
                "[BOLD] 79.2",
                "[BOLD] 74.0",
                "87.1",
                "71.6",
                "85.6",
                "78.9",
                "78.5",
                "42.1",
                "61.0",
                "[BOLD] 78.1"
            ],
            [
                "CMOW/784",
                "87.5",
                "73.4",
                "70.6",
                "[BOLD] 87.3",
                "69.6",
                "[BOLD] 88.0",
                "77.2",
                "74.7",
                "37.9",
                "56.5",
                "76.2"
            ],
            [
                "Hybrid",
                "[BOLD] 90.2",
                "78.7",
                "73.7",
                "[BOLD] 87.3",
                "[BOLD] 72.7",
                "87.6",
                "[BOLD] 79.4",
                "[BOLD] 79.6",
                "[BOLD] 43.3",
                "[BOLD] 63.4",
                "77.8"
            ],
            [
                "cmp. CBOW",
                "+0.2%",
                "-0.6%",
                "-0.4%",
                "+0.2%",
                "+1.5%",
                "+2.3%",
                "+0.6%",
                "+1.4%",
                "+2.9%",
                "+3.9%",
                "-0.4%"
            ],
            [
                "cmp. CMOW",
                "+3.1%",
                "+7.2%",
                "+4.4%",
                "+0%",
                "+4.5%",
                "-0.5%",
                "+2.9%",
                "+6.7%",
                "+14.3",
                "+12.2%",
                "+2.1%"
            ]
        ],
        "question": "Is it true that On 7 out of 11 supervised tasks, the joint model does not improve upon the better model, and on SST2, SST5, and MRPC the difference is less than 1 point?",
        "answer_label": "no"
    },
    {
        "id": "2aecaa7e-e91a-47ba-9108-97dd18064ff6",
        "table_caption": "Improving Generalization by Incorporating Coverage in Natural Language Inference Table 2: Impact of using coverage for improving generalization across different datasets of the same task (NLI). All models are trained on MultiNLI.",
        "table_column_names": [
            "[EMPTY]",
            "in-domain MultiNLI",
            "out-of-domain SNLI",
            "out-of-domain Glockner",
            "out-of-domain SICK"
        ],
        "table_content_values": [
            [
                "MQAN",
                "72.30",
                "60.91",
                "41.82",
                "53.95"
            ],
            [
                "+ coverage",
                "<bold>73.84</bold>",
                "<bold>65.38</bold>",
                "<bold>78.69</bold>",
                "<bold>54.55</bold>"
            ],
            [
                "ESIM (ELMO)",
                "80.04",
                "68.70",
                "60.21",
                "51.37"
            ],
            [
                "+ coverage",
                "<bold>80.38</bold>",
                "<bold>70.05</bold>",
                "<bold>67.47</bold>",
                "<bold>52.65</bold>"
            ]
        ],
        "question": "Is it true that [CONTINUE] The results show that coverage information considerably improves the generalization of both examined models across various NLI datasets?",
        "answer_label": "yes"
    },
    {
        "id": "faabddbe-aa9d-42f7-abf4-f242e5a01e76",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that Our single DCGCN model does not obtain better results than previous ensemble models?",
        "answer_label": "no"
    },
    {
        "id": "980f3548-a1c0-4e26-90d1-017f81573985",
        "table_caption": "Adversarial Removal of Demographic Attributes from Text Data Table 3: Performances on different datasets with an adversarial training. \u0394 is the difference between the attacker score and the corresponding adversary\u2019s accuracy.",
        "table_column_names": [
            "Data",
            "Task",
            "Protected Attribute",
            "Task Acc",
            "Leakage",
            "\u0394"
        ],
        "table_content_values": [
            [
                "Dial",
                "Sentiment",
                "Race",
                "64.7",
                "56.0",
                "5.0"
            ],
            [
                "[EMPTY]",
                "Mention",
                "Race",
                "81.5",
                "63.1",
                "9.2"
            ],
            [
                "PAN16",
                "Mention",
                "Gender",
                "75.6",
                "58.5",
                "8.0"
            ],
            [
                "[EMPTY]",
                "Mention",
                "Age",
                "72.5",
                "57.3",
                "6.9"
            ]
        ],
        "question": "Is it true that In all cases, the adversarial's success rate is higher than the attacker's rate, with a difference of at least 5%?",
        "answer_label": "no"
    },
    {
        "id": "968f23a7-c045-48ba-8bff-71a31abdd3d8",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 6: Entailment (ENT), contradiction (CON) and neutral (NEU) average percentages for the LDC2017T10 test set. (Top) The premise and the hypothesis are the generated (GEN) and reference (REF) sentences, respectively. (Bottom) The hypothesis and the premise are the generated (GEN) and reference (REF) sentences, respectively.",
        "table_column_names": [
            "<bold>Model</bold>",
            "REF \u21d2 GEN <bold>ENT</bold>",
            "REF \u21d2 GEN <bold>CON</bold>",
            "REF \u21d2 GEN <bold>NEU</bold>"
        ],
        "table_content_values": [
            [
                "S2S",
                "38.45",
                "11.17",
                "50.38"
            ],
            [
                "G2S-GIN",
                "49.78",
                "9.80",
                "40.42"
            ],
            [
                "G2S-GAT",
                "49.48",
                "8.09",
                "42.43"
            ],
            [
                "G2S-GGNN",
                "51.32",
                "8.82",
                "39.86"
            ],
            [
                "[EMPTY]",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF",
                "GEN \u21d2 REF"
            ],
            [
                "<bold>Model</bold>",
                "<bold>ENT</bold>",
                "<bold>CON</bold>",
                "<bold>NEU</bold>"
            ],
            [
                "S2S",
                "73.79",
                "12.75",
                "13.46"
            ],
            [
                "G2S-GIN",
                "76.27",
                "10.65",
                "13.08"
            ],
            [
                "G2S-GAT",
                "77.54",
                "8.54",
                "13.92"
            ],
            [
                "G2S-GGNN",
                "77.64",
                "9.64",
                "12.72"
            ]
        ],
        "question": "Is it true that This suggests that our models are capable of capturing better semantic information from the graph generating outputs semantically related to the reference sentences?",
        "answer_label": "yes"
    },
    {
        "id": "4a286ba2-cb10-4692-8016-513776cdc6b3",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
        "table_column_names": [
            "Model",
            "Answerability Macro P/R/F",
            "# Answerable",
            "Answer Prec.",
            "Derivation Prec. RG-L (P/R/F)",
            "Derivation Prec. BL-4"
        ],
        "table_content_values": [
            [
                "Shortest Path",
                "54.8/55.5/53.2",
                "976",
                "3.6",
                "56.7/38.5/41.5",
                "31.3"
            ],
            [
                "PRKGC",
                "52.6/51.5/50.7",
                "1,021",
                "45.2",
                "40.7/60.7/44.7",
                "30.9"
            ],
            [
                "PRKGC+NS",
                "53.6/54.1/52.1",
                "980",
                "45.4",
                "42.2/61.6/46.1",
                "33.4"
            ]
        ],
        "question": "Is it true that [CONTINUE] It also improves the generalization ability of question answering?",
        "answer_label": "yes"
    },
    {
        "id": "5e5549b7-46c9-4d2a-be76-7bf8049380a4",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
        "table_column_names": [
            "Model",
            "Answerability Macro P/R/F",
            "# Answerable",
            "Answer Prec.",
            "Derivation Prec. RG-L (P/R/F)",
            "Derivation Prec. BL-4"
        ],
        "table_content_values": [
            [
                "Shortest Path",
                "54.8/55.5/53.2",
                "976",
                "3.6",
                "56.7/38.5/41.5",
                "31.3"
            ],
            [
                "PRKGC",
                "52.6/51.5/50.7",
                "1,021",
                "45.2",
                "40.7/60.7/44.7",
                "30.9"
            ],
            [
                "PRKGC+NS",
                "53.6/54.1/52.1",
                "980",
                "45.4",
                "42.2/61.6/46.1",
                "33.4"
            ]
        ],
        "question": "Is it true that [CONTINUE] Yet, the PRKGC model do not give considerably good results, which indicates the non-triviality of RC-QEDE?",
        "answer_label": "yes"
    },
    {
        "id": "d87f2213-5235-4ce8-9871-44178ae506af",
        "table_caption": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task Table 4: Image-caption ranking results for Japanese (MS-COCO)",
        "table_column_names": [
            "[EMPTY]",
            "Image to Text R@1",
            "Image to Text R@5",
            "Image to Text R@10",
            "Image to Text Mr",
            "Text to Image R@1",
            "Text to Image R@5",
            "Text to Image R@10",
            "Text to Image Mr",
            "Alignment"
        ],
        "table_content_values": [
            [
                "[BOLD] symmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Mono",
                "42.7",
                "77.7",
                "88.5",
                "2",
                "33.1",
                "69.8",
                "84.3",
                "3",
                "-"
            ],
            [
                "FME",
                "40.7",
                "77.7",
                "88.3",
                "2",
                "30.0",
                "68.9",
                "83.1",
                "3",
                "92.70%"
            ],
            [
                "AME",
                "[BOLD] 50.2",
                "[BOLD] 85.6",
                "[BOLD] 93.1",
                "[BOLD] 1",
                "[BOLD] 40.2",
                "[BOLD] 76.7",
                "[BOLD] 87.8",
                "[BOLD] 2",
                "82.54%"
            ],
            [
                "[BOLD] asymmetric",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Mono",
                "49.9",
                "83.4",
                "93.7",
                "2",
                "39.7",
                "76.5",
                "88.3",
                "[BOLD] 2",
                "-"
            ],
            [
                "FME",
                "48.8",
                "81.9",
                "91.9",
                "2",
                "37.0",
                "74.8",
                "87.0",
                "[BOLD] 2",
                "92.70%"
            ],
            [
                "AME",
                "[BOLD] 55.5",
                "[BOLD] 87.9",
                "[BOLD] 95.2",
                "[BOLD] 1",
                "[BOLD] 44.9",
                "[BOLD] 80.7",
                "[BOLD] 89.3",
                "[BOLD] 2",
                "84.99%"
            ]
        ],
        "question": "Is it true that For the Japanese captions, AME does not reach better results on average compared to monolingual model in symmetric and asymmetric modes, respectively?",
        "answer_label": "no"
    },
    {
        "id": "554754d1-e781-421a-b781-05fba5c213dd",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 3: Main results on AMR15 with/without external Gigaword sentences as auto-parsed data are used. The number of parameters of our single model is 18.4M",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] External",
            "B"
        ],
        "table_content_values": [
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "-",
                "22.0"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "-",
                "23.3"
            ],
            [
                "GCNSEQ (Damonte and Cohen,  2019 )",
                "-",
                "24.4"
            ],
            [
                "DCGCN(single)",
                "-",
                "25.9"
            ],
            [
                "DCGCN(ensemble)",
                "-",
                "[BOLD] 28.2"
            ],
            [
                "TSP (Song et al.,  2016 )",
                "ALL",
                "22.4"
            ],
            [
                "PBMT (Pourdamghani et al.,  2016 )",
                "ALL",
                "26.9"
            ],
            [
                "Tree2Str (Flanigan et al.,  2016 )",
                "ALL",
                "23.0"
            ],
            [
                "SNRG (Song et al.,  2017 )",
                "ALL",
                "25.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "0.2M",
                "27.4"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "0.2M",
                "28.2"
            ],
            [
                "DCGCN(single)",
                "0.1M",
                "29.0"
            ],
            [
                "DCGCN(single)",
                "0.2M",
                "[BOLD] 31.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "2M",
                "32.3"
            ],
            [
                "GraphLSTM (Song et al.,  2018 )",
                "2M",
                "33.6"
            ],
            [
                "Seq2SeqK (Konstas et al.,  2017 )",
                "20M",
                "33.8"
            ],
            [
                "DCGCN(single)",
                "0.3M",
                "33.2"
            ],
            [
                "DCGCN(ensemble)",
                "0.3M",
                "[BOLD] 35.3"
            ]
        ],
        "question": "Is it true that When using the same amount of 0.2M data, the performance of DCGCN is not necessarily higher than Seq2SeqK and GraphLSTM?",
        "answer_label": "no"
    },
    {
        "id": "30ef8633-a942-41a8-8686-40d62e8d3848",
        "table_caption": "Effective Attention Modeling for Neural Relation Extraction Table 4: Effectiveness of model components (m=4) on the NYT11 dataset.",
        "table_column_names": [
            "[EMPTY]",
            "Prec.",
            "Rec.",
            "F1"
        ],
        "table_content_values": [
            [
                "(A1) BiLSTM-CNN",
                "0.473",
                "0.606",
                "0.531"
            ],
            [
                "(A2) Standard attention",
                "0.466",
                "0.638",
                "0.539"
            ],
            [
                "(A3) Window size ( [ITALIC] ws)=5",
                "0.507",
                "0.652",
                "[BOLD] 0.571"
            ],
            [
                "(A4) Window size ( [ITALIC] ws)=10",
                "0.510",
                "0.640",
                "0.568"
            ],
            [
                "(A5) Softmax",
                "0.490",
                "0.658",
                "0.562"
            ],
            [
                "(A6) Max-pool",
                "0.492",
                "0.600",
                "0.541"
            ]
        ],
        "question": "Is it true that Adding the dependency weight factor with a window size of 10 decreases the F1 score by 0.7% (A4\u2212A2)?",
        "answer_label": "no"
    },
    {
        "id": "71b4b4de-2559-490c-83ab-04da9dda769e",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that [CONTINUE] Regarding the probing tasks, we observe that CBOW embeddings better encode the linguistic properties of sentences than CMOW?",
        "answer_label": "no"
    },
    {
        "id": "931fadd7-9e3a-468e-a366-5b69a84720f6",
        "table_caption": "Unpaired Speech Enhancement by Acoustic and Adversarial Supervision for Speech Recognition TABLE I: WERs (%) and DCE of different speech enhancement methods on Librispeech + DEMAND test set",
        "table_column_names": [
            "Method",
            "WER (%)",
            "DCE"
        ],
        "table_content_values": [
            [
                "No enhancement",
                "17.3",
                "0.828"
            ],
            [
                "Wiener filter",
                "19.5",
                "0.722"
            ],
            [
                "Minimizing DCE",
                "15.8",
                "[BOLD] 0.269"
            ],
            [
                "FSEGAN",
                "14.9",
                "0.291"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=0)",
                "15.6",
                "0.330"
            ],
            [
                "AAS ( [ITALIC] wAC=1, [ITALIC] wAD=105)",
                "[BOLD] 14.4",
                "0.303"
            ],
            [
                "Clean speech",
                "5.7",
                "0.0"
            ]
        ],
        "question": "Is it true that [CONTINUE] In Librispeech + DEMAND, acoustic supervision (15.6%) and multi-task learning (14.4%) achieves a lower WER than minimizing DCE (15.8%) and FSEGAN (14.9%)?",
        "answer_label": "yes"
    },
    {
        "id": "cee6661e-b5b1-4fc8-b1cd-c9c014565b09",
        "table_caption": "Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation Table 3: Results of non-RL (top), cross-input (DeepTD) and input-specific (REAPER) RL approaches (middle) compared with RELIS.",
        "table_column_names": [
            "[EMPTY]",
            "DUC\u201901 <italic>R</italic>1",
            "DUC\u201901 <italic>R</italic>2",
            "DUC\u201902 <italic>R</italic>1",
            "DUC\u201902 <italic>R</italic>2",
            "DUC\u201904 <italic>R</italic>1",
            "DUC\u201904 <italic>R</italic>2"
        ],
        "table_content_values": [
            [
                "ICSI",
                "33.31",
                "7.33",
                "35.04",
                "8.51",
                "37.31",
                "9.36"
            ],
            [
                "PriorSum",
                "35.98",
                "7.89",
                "36.63",
                "8.97",
                "38.91",
                "10.07"
            ],
            [
                "TCSum",
                "<bold>36.45</bold>",
                "7.66",
                "36.90",
                "8.61",
                "38.27",
                "9.66"
            ],
            [
                "TCSum\u2212",
                "33.45",
                "6.07",
                "34.02",
                "7.39",
                "35.66",
                "8.66"
            ],
            [
                "SRSum",
                "36.04",
                "8.44",
                "<bold>38.93</bold>",
                "<bold>10.29</bold>",
                "39.29",
                "10.70"
            ],
            [
                "DeepTD",
                "28.74",
                "5.95",
                "31.63",
                "7.09",
                "33.57",
                "7.96"
            ],
            [
                "REAPER",
                "32.43",
                "6.84",
                "35.03",
                "8.11",
                "37.22",
                "8.64"
            ],
            [
                "RELIS",
                "34.73",
                "<bold>8.66</bold>",
                "37.11",
                "9.12",
                "<bold>39.34</bold>",
                "<bold>10.73</bold>"
            ]
        ],
        "question": "Is it true that At the same time, RELIS performs worse than neural-based TCSum and SRSum, while it requires significantly less data and time to train, as shown next?",
        "answer_label": "no"
    },
    {
        "id": "0d7db1a3-15a7-448f-a5ff-bee89e7168f1",
        "table_caption": "The MeMAD Submission to the WMT18 Multimodal Translation Task Table 6: Ablation experiments (BLEU% scores). The row subs3MLM detectron shows our best single model. Individual components or data choices are varied one by one. + stands for adding a component, and \u2212 for removing a component or data set. Multiple modifications are indicated by increasing the indentation.",
        "table_column_names": [
            "en-fr",
            "flickr16",
            "flickr17",
            "mscoco17"
        ],
        "table_content_values": [
            [
                "subs3M [ITALIC]  [ITALIC] LM detectron",
                "68.30",
                "62.45",
                "52.86"
            ],
            [
                "+ensemble-of-3",
                "68.72",
                "62.70",
                "53.06"
            ],
            [
                "\u2212visual features",
                "[BOLD] 68.74",
                "[BOLD] 62.71",
                "53.14"
            ],
            [
                "\u2212MS-COCO",
                "67.13",
                "61.17",
                "[BOLD] 53.34"
            ],
            [
                "\u2212multi-lingual",
                "68.21",
                "61.99",
                "52.40"
            ],
            [
                "subs6M [ITALIC]  [ITALIC] LM detectron",
                "68.29",
                "61.73",
                "53.05"
            ],
            [
                "subs3M [ITALIC]  [ITALIC] LM gn2048",
                "67.74",
                "61.78",
                "52.76"
            ],
            [
                "subs3M [ITALIC]  [ITALIC] LM text-only",
                "67.72",
                "61.75",
                "53.02"
            ],
            [
                "en-de",
                "flickr16",
                "flickr17",
                "mscoco17"
            ],
            [
                "subs3M [ITALIC]  [ITALIC] LM detectron",
                "45.09",
                "40.81",
                "36.94"
            ],
            [
                "+ensemble-of-3",
                "45.52",
                "[BOLD] 41.84",
                "[BOLD] 37.49"
            ],
            [
                "\u2212visual features",
                "[BOLD] 45.59",
                "41.75",
                "37.43"
            ],
            [
                "\u2212MS-COCO",
                "45.11",
                "40.52",
                "36.47"
            ],
            [
                "\u2212multi-lingual",
                "44.95",
                "40.09",
                "35.28"
            ],
            [
                "subs6M [ITALIC]  [ITALIC] LM detectron",
                "45.50",
                "41.01",
                "36.81"
            ],
            [
                "subs3M [ITALIC]  [ITALIC] LM gn2048",
                "45.38",
                "40.07",
                "36.82"
            ],
            [
                "subs3M [ITALIC]  [ITALIC] LM text-only",
                "44.87",
                "41.27",
                "36.59"
            ],
            [
                "+multi-modal finetune",
                "44.56",
                "41.61",
                "36.93"
            ]
        ],
        "question": "Is it true that When the experiment was repeated so that the finetuning phase included the text-only data, the performance returned to approximately the same level as without tuning (+multi-modal finetune row in Table 6)?",
        "answer_label": "yes"
    },
    {
        "id": "34c20bf4-d238-447f-bd59-ad3fde68d867",
        "table_caption": "Densely Connected Graph Convolutional Networks for Graph-to-Sequence Learning Table 2: Main results on AMR17. GCNSEQ (Damonte and Cohen, 2019) achieves 24.5 BLEU points. #P shows the model size in terms of parameters; \u201cS\u201d and \u201cE\u201d denote single and ensemble models, respectively.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] T",
            "#P",
            "B",
            "C"
        ],
        "table_content_values": [
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "S",
                "28,4M",
                "21.7",
                "49.1"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "S",
                "28.3M",
                "23.3",
                "50.4"
            ],
            [
                "Seq2SeqB (Beck et al.,  2018 )",
                "E",
                "142M",
                "26.6",
                "52.5"
            ],
            [
                "GGNN2Seq (Beck et al.,  2018 )",
                "E",
                "141M",
                "27.5",
                "53.5"
            ],
            [
                "DCGCN (ours)",
                "S",
                "[BOLD] 19.1M",
                "27.9",
                "57.3"
            ],
            [
                "DCGCN (ours)",
                "E",
                "92.5M",
                "[BOLD] 30.4",
                "[BOLD] 59.6"
            ]
        ],
        "question": "Is it true that Under the same setting, our model also consistently outperforms graph encoders based on recurrent neural networks or gating mechanisms?",
        "answer_label": "yes"
    },
    {
        "id": "33fb4b62-d5db-43e1-b19f-9006c5c5c618",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "36.85",
                "5.3782",
                "35.14",
                "55.01",
                "1.6016",
                "00.34",
                "09.81",
                "00.15",
                "10.31"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen",
                "39.23",
                "6.0217",
                "36.97",
                "55.52",
                "1.7623",
                "00.40",
                "03.59",
                "00.07",
                "04.05"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen+",
                "40.25",
                "6.1448",
                "37.50",
                "56.19",
                "1.8181",
                "00.21",
                "01.99",
                "00.05",
                "02.24"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.88",
                "3.9310",
                "32.11",
                "39.90",
                "0.5036",
                "07.73",
                "17.76",
                "09.52",
                "35.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.19",
                "6.0543",
                "37.38",
                "55.88",
                "1.8104",
                "00.17",
                "01.31",
                "00.25",
                "01.72"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen",
                "40.73",
                "6.1711",
                "37.76",
                "56.09",
                "1.8518",
                "00.07",
                "00.72",
                "00.08",
                "00.87"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen+",
                "40.51",
                "6.1226",
                "37.61",
                "55.98",
                "1.8286",
                "00.02",
                "00.63",
                "00.06",
                "00.70"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.66",
                "3.9511",
                "32.93",
                "39.29",
                "0.3855",
                "07.89",
                "15.60",
                "08.44",
                "31.94"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.48",
                "6.0269",
                "37.26",
                "56.19",
                "1.7999",
                "00.43",
                "02.84",
                "00.26",
                "03.52"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen",
                "41.57",
                "6.2830",
                "37.99",
                "56.36",
                "1.8849",
                "00.37",
                "01.40",
                "00.09",
                "01.86"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen+",
                "41.56",
                "6.2700",
                "37.94",
                "56.38",
                "1.8827",
                "00.21",
                "01.04",
                "00.07",
                "01.31"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "35.99",
                "5.0734",
                "34.74",
                "54.79",
                "1.5259",
                "00.02",
                "11.58",
                "00.02",
                "11.62"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen",
                "40.07",
                "6.1243",
                "37.45",
                "55.81",
                "1.8026",
                "00.05",
                "03.23",
                "00.01",
                "03.29"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen+",
                "40.80",
                "6.2197",
                "37.86",
                "56.13",
                "1.8422",
                "00.01",
                "01.87",
                "00.01",
                "01.88"
            ]
        ],
        "question": "Is it true that [CONTINUE] However, the results in bottom halves [CONTINUE] of Tables 2 and 3 do not support our hypothesis: we observe the main effect on SER from cleaning the missed slots, reducing both insertions and deletions?",
        "answer_label": "yes"
    },
    {
        "id": "33601bd8-365d-423b-9207-1d62d6031441",
        "table_caption": "Improved Semantics for the End-to-End Generation Challenge Corpus Table 3: Results evaluated on the cleaned test set (cf.\u00a0Table\u00a02 for column details; note that the numbers are not comparable to Table\u00a02 as the test set is different).",
        "table_column_names": [
            "Train",
            "Test",
            "[BOLD] System",
            "[BOLD] BLEU",
            "[BOLD] NIST",
            "[BOLD] METEOR",
            "[BOLD] ROUGE-L",
            "[BOLD] CIDEr",
            "[BOLD] Add",
            "[BOLD] Miss",
            "[BOLD] Wrong",
            "[BOLD] SER"
        ],
        "table_content_values": [
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "36.85",
                "5.3782",
                "35.14",
                "55.01",
                "1.6016",
                "00.34",
                "09.81",
                "00.15",
                "10.31"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen",
                "39.23",
                "6.0217",
                "36.97",
                "55.52",
                "1.7623",
                "00.40",
                "03.59",
                "00.07",
                "04.05"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "TGen+",
                "40.25",
                "6.1448",
                "37.50",
                "56.19",
                "1.8181",
                "00.21",
                "01.99",
                "00.05",
                "02.24"
            ],
            [
                "Original",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.88",
                "3.9310",
                "32.11",
                "39.90",
                "0.5036",
                "07.73",
                "17.76",
                "09.52",
                "35.03"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.19",
                "6.0543",
                "37.38",
                "55.88",
                "1.8104",
                "00.17",
                "01.31",
                "00.25",
                "01.72"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen",
                "40.73",
                "6.1711",
                "37.76",
                "56.09",
                "1.8518",
                "00.07",
                "00.72",
                "00.08",
                "00.87"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "TGen+",
                "40.51",
                "6.1226",
                "37.61",
                "55.98",
                "1.8286",
                "00.02",
                "00.63",
                "00.06",
                "00.70"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned",
                "[BOLD] Cleaned",
                "SC-LSTM",
                "23.66",
                "3.9511",
                "32.93",
                "39.29",
                "0.3855",
                "07.89",
                "15.60",
                "08.44",
                "31.94"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "40.48",
                "6.0269",
                "37.26",
                "56.19",
                "1.7999",
                "00.43",
                "02.84",
                "00.26",
                "03.52"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen",
                "41.57",
                "6.2830",
                "37.99",
                "56.36",
                "1.8849",
                "00.37",
                "01.40",
                "00.09",
                "01.86"
            ],
            [
                "Cleaned missing",
                "[BOLD] Cleaned",
                "TGen+",
                "41.56",
                "6.2700",
                "37.94",
                "56.38",
                "1.8827",
                "00.21",
                "01.04",
                "00.07",
                "01.31"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen\u2212",
                "35.99",
                "5.0734",
                "34.74",
                "54.79",
                "1.5259",
                "00.02",
                "11.58",
                "00.02",
                "11.62"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen",
                "40.07",
                "6.1243",
                "37.45",
                "55.81",
                "1.8026",
                "00.05",
                "03.23",
                "00.01",
                "03.29"
            ],
            [
                "1-1[0.5pt/2pt]3-12[0.5pt/2pt] Cleaned added",
                "[BOLD] Cleaned",
                "TGen+",
                "40.80",
                "6.2197",
                "37.86",
                "56.13",
                "1.8422",
                "00.01",
                "01.87",
                "00.01",
                "01.88"
            ]
        ],
        "question": "Is it true that Again, one possible explanation is that cleaning the missing slots provided more complex training examples?",
        "answer_label": "yes"
    },
    {
        "id": "51db7e0d-e291-45d8-bb0a-0210fa7cda1d",
        "table_caption": "Enhancing AMR-to-Text Generation with Dual Graph Representations Table 5: METEOR scores and differences to the S2S, in the LDC2017T10 test set, with respect to the graph diameter, sentence length and max node out-degree.",
        "table_column_names": [
            "<bold>Model</bold>",
            "<bold>Graph Diameter</bold> 0-7 \u0394",
            "<bold>Graph Diameter</bold> 7-13 \u0394",
            "<bold>Graph Diameter</bold> 14-20 \u0394"
        ],
        "table_content_values": [
            [
                "S2S",
                "33.2",
                "29.7",
                "28.8"
            ],
            [
                "G2S-GIN",
                "35.2 +6.0%",
                "31.8 +7.4%",
                "31.5 +9.2%"
            ],
            [
                "G2S-GAT",
                "35.1 +5.9%",
                "32.0 +7.8%",
                "31.5 +9.51%"
            ],
            [
                "G2S-GGNN",
                "36.2 +9.0%",
                "33.0 +11.4%",
                "30.7 +6.7%"
            ],
            [
                "[EMPTY]",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>",
                "<bold>Sentence Length</bold>"
            ],
            [
                "[EMPTY]",
                "0-20 \u0394",
                "20-50 \u0394",
                "50-240 \u0394"
            ],
            [
                "S2S",
                "34.9",
                "29.9",
                "25.1"
            ],
            [
                "G2S-GIN",
                "36.7 +5.2%",
                "32.2 +7.8%",
                "26.5 +5.8%"
            ],
            [
                "G2S-GAT",
                "36.9 +5.7%",
                "32.3 +7.9%",
                "26.6 +6.1%"
            ],
            [
                "G2S-GGNN",
                "37.9 +8.5%",
                "33.3 +11.2%",
                "26.9 +6.8%"
            ],
            [
                "[EMPTY]",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>",
                "<bold>Max Node Out-degree</bold>"
            ],
            [
                "[EMPTY]",
                "0-3 \u0394",
                "4-8 \u0394",
                "9-18 \u0394"
            ],
            [
                "S2S",
                "31.7",
                "30.0",
                "23.9"
            ],
            [
                "G2S-GIN",
                "33.9 +6.9%",
                "32.1 +6.9%",
                "25.4 +6.2%"
            ],
            [
                "G2S-GAT",
                "34.3 +8.0%",
                "32.0 +6.7%",
                "22.5 -6.0%"
            ],
            [
                "G2S-GGNN",
                "35.0 +10.3%",
                "33.1 +10.4%",
                "22.2 -7.3%"
            ]
        ],
        "question": "Is it true that [CONTINUE] Since the models have fewer examples of bigger graphs to learn from, this also leads to worse performance when handling graphs with higher diameters?",
        "answer_label": "yes"
    },
    {
        "id": "eb4529a1-e468-4159-9afe-25270977f2dc",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 2: Scores on supervised downstream tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Method",
            "SUBJ",
            "CR",
            "MR",
            "MPQA",
            "MRPC",
            "TREC",
            "SICK-E",
            "SST2",
            "SST5",
            "STS-B",
            "SICK-R"
        ],
        "table_content_values": [
            [
                "CBOW/784",
                "90.0",
                "[BOLD] 79.2",
                "[BOLD] 74.0",
                "87.1",
                "71.6",
                "85.6",
                "78.9",
                "78.5",
                "42.1",
                "61.0",
                "[BOLD] 78.1"
            ],
            [
                "CMOW/784",
                "87.5",
                "73.4",
                "70.6",
                "[BOLD] 87.3",
                "69.6",
                "[BOLD] 88.0",
                "77.2",
                "74.7",
                "37.9",
                "56.5",
                "76.2"
            ],
            [
                "Hybrid",
                "[BOLD] 90.2",
                "78.7",
                "73.7",
                "[BOLD] 87.3",
                "[BOLD] 72.7",
                "87.6",
                "[BOLD] 79.4",
                "[BOLD] 79.6",
                "[BOLD] 43.3",
                "[BOLD] 63.4",
                "77.8"
            ],
            [
                "cmp. CBOW",
                "+0.2%",
                "-0.6%",
                "-0.4%",
                "+0.2%",
                "+1.5%",
                "+2.3%",
                "+0.6%",
                "+1.4%",
                "+2.9%",
                "+3.9%",
                "-0.4%"
            ],
            [
                "cmp. CMOW",
                "+3.1%",
                "+7.2%",
                "+4.4%",
                "+0%",
                "+4.5%",
                "-0.5%",
                "+2.9%",
                "+6.7%",
                "+14.3",
                "+12.2%",
                "+2.1%"
            ]
        ],
        "question": "Is it true that On the TREC task, on the other hand, CMOW outperforms CBOW by 2.5 points?",
        "answer_label": "yes"
    },
    {
        "id": "017f3d64-a37c-4bcf-b972-a505fe7d5004",
        "table_caption": "RC-QED: Evaluating Natural Language Derivationsin Multi-Hop Reading Comprehension Table 4: Performance of RC-QEDE of our baseline models (see Section\u00a02.1 for further details of each evaluation metrics). \u201cNS\u201d indicates the use of annotated NLDs as supervision (i.e. using Ld during training).",
        "table_column_names": [
            "Model",
            "Answerability Macro P/R/F",
            "# Answerable",
            "Answer Prec.",
            "Derivation Prec. RG-L (P/R/F)",
            "Derivation Prec. BL-4"
        ],
        "table_content_values": [
            [
                "Shortest Path",
                "54.8/55.5/53.2",
                "976",
                "3.6",
                "56.7/38.5/41.5",
                "31.3"
            ],
            [
                "PRKGC",
                "52.6/51.5/50.7",
                "1,021",
                "45.2",
                "40.7/60.7/44.7",
                "30.9"
            ],
            [
                "PRKGC+NS",
                "53.6/54.1/52.1",
                "980",
                "45.4",
                "42.2/61.6/46.1",
                "33.4"
            ]
        ],
        "question": "Is it true that Although the PRKGC+NS model receives supervision about human-generated NLDs, paths with the maximum score do not match human-generated NLDs to any significant extent?",
        "answer_label": "no"
    },
    {
        "id": "74589c10-ac86-4451-a306-ebab9d94558d",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that On the other side, H-CMOW shows, among others, improvements at BShift?",
        "answer_label": "yes"
    },
    {
        "id": "3fb56301-2d20-409e-adba-fc469f55b83a",
        "table_caption": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog Table 3: Performance of different dialog agents on the multi-domain dialog corpus by interacting with the agenda-based user simulator. All the results except \u201cdialog turns\u201d are shown in percentage terms. Real human-human performance computed from the test set (i.e. the last row) serves as the upper bounds.",
        "table_column_names": [
            "Method",
            "Agenda Turns",
            "Agenda Inform",
            "Agenda Match",
            "Agenda Success"
        ],
        "table_content_values": [
            [
                "GP-MBCM",
                "2.99",
                "19.04",
                "44.29",
                "28.9"
            ],
            [
                "ACER",
                "10.49",
                "77.98",
                "62.83",
                "50.8"
            ],
            [
                "PPO",
                "9.83",
                "83.34",
                "69.09",
                "59.1"
            ],
            [
                "ALDM",
                "12.47",
                "81.20",
                "62.60",
                "61.2"
            ],
            [
                "GDPL-sess",
                "[BOLD] 7.49",
                "88.39",
                "77.56",
                "76.4"
            ],
            [
                "GDPL-discr",
                "7.86",
                "93.21",
                "80.43",
                "80.5"
            ],
            [
                "GDPL",
                "7.64",
                "[BOLD] 94.97",
                "[BOLD] 83.90",
                "[BOLD] 86.5"
            ],
            [
                "[ITALIC] Human",
                "[ITALIC] 7.37",
                "[ITALIC] 66.89",
                "[ITALIC] 95.29",
                "[ITALIC] 75.0"
            ]
        ],
        "question": "Is it true that Surprisingly, GDPL outperforms human in completing the task, and its average dialog turns are even lower than those of humans, though GDPL is superior in terms of match rate?",
        "answer_label": "no"
    },
    {
        "id": "fef58ae6-dbba-475a-a44e-29eff547da7d",
        "table_caption": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.",
        "table_column_names": [
            "Recall",
            "0.1",
            "0.2",
            "0.3",
            "AUC"
        ],
        "table_content_values": [
            [
                "Iteration=1",
                "0.531",
                "0.455",
                "0.353",
                "0.201"
            ],
            [
                "Iteration=2",
                "0.592",
                "0.498",
                "0.385",
                "0.375"
            ],
            [
                "Iteration=3",
                "0.650",
                "0.519",
                "0.422",
                "0.405"
            ],
            [
                "Iteration=4",
                "0.601",
                "0.505",
                "0.422",
                "0.385"
            ],
            [
                "Iteration=5",
                "0.575",
                "0.495",
                "0.394",
                "0.376"
            ]
        ],
        "question": "Is it true that We find that the performance does not reach the best when iteration is set to 3?",
        "answer_label": "no"
    },
    {
        "id": "edb918b8-c14e-4bdd-b9c3-0e99a12248e1",
        "table_caption": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance Table 5: Comparison on hard and soft alignments.",
        "table_column_names": [
            "Metrics",
            "cs-en",
            "de-en",
            "fi-en",
            "lv-en"
        ],
        "table_content_values": [
            [
                "RUSE",
                "0.624",
                "0.644",
                "0.750",
                "0.697"
            ],
            [
                "Hmd-F1 + BERT",
                "0.655",
                "0.681",
                "0.821",
                "0.712"
            ],
            [
                "Hmd-Recall + BERT",
                "0.651",
                "0.658",
                "0.788",
                "0.681"
            ],
            [
                "Hmd-Prec + BERT",
                "0.624",
                "0.669",
                "0.817",
                "0.707"
            ],
            [
                "Wmd-unigram + BERT",
                "0.651",
                "0.686",
                "<bold>0.823</bold>",
                "0.710"
            ],
            [
                "Wmd-bigram + BERT",
                "<bold>0.665</bold>",
                "<bold>0.688</bold>",
                "0.821",
                "<bold>0.712</bold>"
            ]
        ],
        "question": "Is it true that [CONTINUE] We also observe that WMD-BIGRAMS slightly outperforms WMD-UNIGRAMS on 3 out of 4 language pairs?",
        "answer_label": "yes"
    },
    {
        "id": "2e23ce40-52ea-404d-bf13-83d9dedb36a6",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 5: Scores for different training objectives on the supervised downstream tasks.",
        "table_column_names": [
            "Method",
            "SUBJ",
            "CR",
            "MR",
            "MPQA",
            "MRPC",
            "TREC",
            "SICK-E",
            "SST2",
            "SST5",
            "STS-B",
            "SICK-R"
        ],
        "table_content_values": [
            [
                "CMOW-C",
                "85.9",
                "72.1",
                "69.4",
                "87.0",
                "[BOLD] 71.9",
                "85.4",
                "74.2",
                "73.8",
                "37.6",
                "54.6",
                "71.3"
            ],
            [
                "CMOW-R",
                "[BOLD] 87.5",
                "[BOLD] 73.4",
                "[BOLD] 70.6",
                "[BOLD] 87.3",
                "69.6",
                "[BOLD] 88.0",
                "[BOLD] 77.2",
                "[BOLD] 74.7",
                "[BOLD] 37.9",
                "[BOLD] 56.5",
                "[BOLD] 76.2"
            ],
            [
                "CBOW-C",
                "[BOLD] 90.0",
                "[BOLD] 79.3",
                "[BOLD] 74.6",
                "[BOLD] 87.5",
                "[BOLD] 72.9",
                "85.0",
                "[BOLD] 80.0",
                "78.4",
                "41.0",
                "60.5",
                "[BOLD] 79.2"
            ],
            [
                "CBOW-R",
                "[BOLD] 90.0",
                "79.2",
                "74.0",
                "87.1",
                "71.6",
                "[BOLD] 85.6",
                "78.9",
                "[BOLD] 78.5",
                "[BOLD] 42.1",
                "[BOLD] 61.0",
                "78.1"
            ]
        ],
        "question": "Is it true that Consequently, CMOW-R also outperforms CMOW-C on 10 out of 11 supervised [CONTINUE] downstream tasks [CONTINUE] On average over all downstream tasks, the relative improvement is 20.8%?",
        "answer_label": "yes"
    },
    {
        "id": "6757f6db-0ade-4eaf-9769-d1a599048b35",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that A complementary behavior can be observed for H-CBOW, whose scores on Word Content are increased?",
        "answer_label": "yes"
    },
    {
        "id": "40dbe297-6a42-4c99-80d4-e1fe1f162ad7",
        "table_caption": "Automatically Identifying Complaints in Social Media Table 6: Complaint prediction results using logistic regression (with different types of linguistic features), neural network approaches and the most frequent class baseline. Best results are in bold.",
        "table_column_names": [
            "[BOLD] Model",
            "[BOLD] Acc",
            "[BOLD] F1",
            "[BOLD] AUC"
        ],
        "table_content_values": [
            [
                "Most Frequent Class",
                "64.2",
                "39.1",
                "0.500"
            ],
            [
                "Logistic Regression",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "Sentiment \u2013 MPQA",
                "64.2",
                "39.1",
                "0.499"
            ],
            [
                "Sentiment \u2013 NRC",
                "63.9",
                "42.2",
                "0.599"
            ],
            [
                "Sentiment \u2013 V&B",
                "68.9",
                "60.0",
                "0.696"
            ],
            [
                "Sentiment \u2013 VADER",
                "66.0",
                "54.2",
                "0.654"
            ],
            [
                "Sentiment \u2013 Stanford",
                "68.0",
                "55.6",
                "0.696"
            ],
            [
                "Complaint Specific (all)",
                "65.7",
                "55.2",
                "0.634"
            ],
            [
                "Request",
                "64.2",
                "39.1",
                "0.583"
            ],
            [
                "Intensifiers",
                "64.5",
                "47.3",
                "0.639"
            ],
            [
                "Downgraders",
                "65.4",
                "49.8",
                "0.615"
            ],
            [
                "Temporal References",
                "64.2",
                "43.7",
                "0.535"
            ],
            [
                "Pronoun Types",
                "64.1",
                "39.1",
                "0.545"
            ],
            [
                "POS Bigrams",
                "72.2",
                "66.8",
                "0.756"
            ],
            [
                "LIWC",
                "71.6",
                "65.8",
                "0.784"
            ],
            [
                "Word2Vec Clusters",
                "67.7",
                "58.3",
                "0.738"
            ],
            [
                "Bag-of-Words",
                "79.8",
                "77.5",
                "0.866"
            ],
            [
                "All Features",
                "[BOLD] 80.5",
                "[BOLD] 78.0",
                "[BOLD] 0.873"
            ],
            [
                "Neural Networks",
                "[EMPTY]",
                "[EMPTY]",
                "[EMPTY]"
            ],
            [
                "MLP",
                "78.3",
                "76.2",
                "0.845"
            ],
            [
                "LSTM",
                "80.2",
                "77.0",
                "0.864"
            ]
        ],
        "question": "Is it true that However, models trained using linguistic features on the training data do not obtain significantly higher predictive accuracy?",
        "answer_label": "no"
    },
    {
        "id": "0aa1974d-d983-4dcb-9007-35874e8431fc",
        "table_caption": "Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction Table 5: Precisions on the Wikidata dataset with different number of dynamic routing iterations.",
        "table_column_names": [
            "Recall",
            "0.1",
            "0.2",
            "0.3",
            "AUC"
        ],
        "table_content_values": [
            [
                "Iteration=1",
                "0.531",
                "0.455",
                "0.353",
                "0.201"
            ],
            [
                "Iteration=2",
                "0.592",
                "0.498",
                "0.385",
                "0.375"
            ],
            [
                "Iteration=3",
                "0.650",
                "0.519",
                "0.422",
                "0.405"
            ],
            [
                "Iteration=4",
                "0.601",
                "0.505",
                "0.422",
                "0.385"
            ],
            [
                "Iteration=5",
                "0.575",
                "0.495",
                "0.394",
                "0.376"
            ]
        ],
        "question": "Is it true that We find that the performance reach the best when iteration is set to 3?",
        "answer_label": "yes"
    },
    {
        "id": "7219f414-9986-4c5d-aea4-67f4f4dc1ed0",
        "table_caption": "From Text to Lexicon: Bridging the Gap betweenWord Embeddings and Lexical Resources Table 5: WCS performance, shared vocabulary, k=1. Best results across VSMs in bold.",
        "table_column_names": [
            "[EMPTY]",
            "WN-N P",
            "WN-N R",
            "WN-N F",
            "WN-V P",
            "WN-V R",
            "WN-V F",
            "VN P",
            "VN R",
            "VN F"
        ],
        "table_content_values": [
            [
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2",
                "Context: w2"
            ],
            [
                "type",
                ".700",
                ".654",
                ".676",
                ".535",
                ".474",
                ".503",
                ".327",
                ".309",
                ".318"
            ],
            [
                "x+POS",
                ".699",
                ".651",
                ".674",
                ".544",
                ".472",
                ".505",
                ".339",
                ".312",
                ".325"
            ],
            [
                "lemma",
                ".706",
                ".660",
                ".682",
                ".576",
                ".520",
                ".547",
                ".384",
                ".360",
                ".371"
            ],
            [
                "x+POS",
                "<bold>.710</bold>",
                "<bold>.662</bold>",
                "<bold>.685</bold>",
                "<bold>.589</bold>",
                "<bold>.529</bold>",
                "<bold>.557</bold>",
                "<bold>.410</bold>",
                "<bold>.389</bold>",
                "<bold>.399</bold>"
            ],
            [
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep",
                "Context: dep"
            ],
            [
                "type",
                ".712",
                ".661",
                ".686",
                ".545",
                ".457",
                ".497",
                ".324",
                ".296",
                ".310"
            ],
            [
                "x+POS",
                ".715",
                ".659",
                ".686",
                ".560",
                ".464",
                ".508",
                ".349",
                ".320",
                ".334"
            ],
            [
                "lemma",
                "<bold>.725</bold>",
                "<bold>.668</bold>",
                "<bold>.696</bold>",
                ".591",
                ".512",
                ".548",
                ".408",
                ".371",
                ".388"
            ],
            [
                "x+POS",
                ".722",
                ".666",
                ".693",
                "<bold>.609</bold>",
                "<bold>.527</bold>",
                "<bold>.565</bold>",
                "<bold>.412</bold>",
                "<bold>.381</bold>",
                "<bold>.396</bold>"
            ]
        ],
        "question": "Is it true that Lemma-based targets do not significantly outperform type-based targets in terms of F-measure in all cases?",
        "answer_label": "no"
    },
    {
        "id": "c77a5686-dae1-4f25-896b-87bf07e2494a",
        "table_caption": "CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model Table 1: Scores on the probing tasks attained by our models. Rows starting with \u201cCmp.\u201d show the relative change with respect to Hybrid.",
        "table_column_names": [
            "Dim",
            "Method",
            "Depth",
            "BShift",
            "SubjNum",
            "Tense",
            "CoordInv",
            "Length",
            "ObjNum",
            "TopConst",
            "SOMO",
            "WC"
        ],
        "table_content_values": [
            [
                "400",
                "CBOW/400",
                "32.5",
                "50.2",
                "78.9",
                "78.7",
                "53.6",
                "73.6",
                "79.0",
                "69.6",
                "48.9",
                "86.7"
            ],
            [
                "400",
                "CMOW/400",
                "[BOLD] 34.4",
                "68.8",
                "80.1",
                "[BOLD] 79.9",
                "[BOLD] 59.8",
                "81.9",
                "[BOLD] 79.2",
                "[BOLD] 70.7",
                "[BOLD] 50.3",
                "70.7"
            ],
            [
                "400",
                "H-CBOW",
                "31.2",
                "50.2",
                "77.2",
                "78.8",
                "52.6",
                "77.5",
                "76.1",
                "66.1",
                "49.2",
                "[BOLD] 87.2"
            ],
            [
                "400",
                "H-CMOW",
                "32.3",
                "[BOLD] 70.8",
                "[BOLD] 81.3",
                "76.0",
                "59.6",
                "[BOLD] 82.3",
                "77.4",
                "70.0",
                "50.2",
                "38.2"
            ],
            [
                "784",
                "CBOW/784",
                "33.0",
                "49.6",
                "79.3",
                "78.4",
                "53.6",
                "74.5",
                "78.6",
                "72.0",
                "49.6",
                "[BOLD] 89.5"
            ],
            [
                "784",
                "CMOW/784",
                "[BOLD] 35.1",
                "[BOLD] 70.8",
                "[BOLD] 82.0",
                "80.2",
                "[BOLD] 61.8",
                "82.8",
                "[BOLD] 79.7",
                "74.2",
                "[BOLD] 50.7",
                "72.9"
            ],
            [
                "800",
                "Hybrid",
                "35.0",
                "[BOLD] 70.8",
                "81.7",
                "[BOLD] 81.0",
                "59.4",
                "[BOLD] 84.4",
                "79.0",
                "[BOLD] 74.3",
                "49.3",
                "87.6"
            ],
            [
                "-",
                "cmp. CBOW",
                "+6.1%",
                "+42.7%",
                "+3%",
                "+3.3%",
                "+10.8%",
                "+13.3%",
                "+0.5%",
                "+3.2%",
                "-0.6%",
                "-2.1%"
            ],
            [
                "-",
                "cmp. CMOW",
                "-0.3%",
                "+-0%",
                "-0.4%",
                "+1%",
                "-3.9%",
                "+1.9%",
                "-0.9%",
                "+0.1%",
                "-2.8%",
                "+20.9%"
            ]
        ],
        "question": "Is it true that The hybrid model yields scores close to or even above the better model of the two on all tasks?",
        "answer_label": "yes"
    },
    {
        "id": "7a288829-0028-438d-a925-79cb2943fdc4",
        "table_caption": "Distant Learning for Entity Linking with Automatic Noise Detection Table 2: Results on the test set under two settings. 95% confidence intervals of F1 scores are shown.",
        "table_column_names": [
            "System",
            "All P",
            "All R",
            "All F1",
            "In  [ITALIC] E+ P",
            "In  [ITALIC] E+ R",
            "In  [ITALIC] E+ F1"
        ],
        "table_content_values": [
            [
                "Name matching",
                "15.03",
                "15.03",
                "15.03",
                "29.13",
                "29.13",
                "29.13"
            ],
            [
                "MIL (model 1)",
                "35.87",
                "35.87",
                "35.87 \u00b10.72",
                "69.38",
                "69.38",
                "69.38 \u00b11.29"
            ],
            [
                "MIL-ND (model 2)",
                "37.42",
                "[BOLD] 37.42",
                "37.42 \u00b10.35",
                "72.50",
                "[BOLD] 72.50",
                "[BOLD] 72.50 \u00b10.68"
            ],
            [
                "[ITALIC] \u03c4MIL-ND (model 2)",
                "[BOLD] 38.91",
                "36.73",
                "[BOLD] 37.78 \u00b10.26",
                "[BOLD] 73.19",
                "71.15",
                "72.16 \u00b10.48"
            ],
            [
                "Supervised learning",
                "42.90",
                "42.90",
                "42.90 \u00b10.59",
                "83.12",
                "83.12",
                "83.12 \u00b11.15"
            ]
        ],
        "question": "Is it true that [CONTINUE] MIL-ND significantly outperforms MIL: the 95% confidence intervals for them do not overlap?",
        "answer_label": "yes"
    }
]